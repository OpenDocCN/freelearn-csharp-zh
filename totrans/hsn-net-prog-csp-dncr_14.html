<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">The Transport Layer - TCP and UDP</h1>
                </header>
            
            <article>
                
<p>In previous chapters, we've looked at the interactions of different application layer protocols and how to program those interactions in .NET Core. In this chapter, we'll go one step closer to the hardware and start looking at transport layer protocols with <span><strong>Transmission Control Protocol</strong> (</span><strong>TCP</strong>) and <span><strong>User Datagram Protocol</strong> (</span><strong>UDP</strong>). We'll look at the connection-based and connectionless communication patterns that each implements, and we'll look at the strengths and weaknesses inherent to each approach. In addition, we'll examine how to write and interact with a software client that implements each protocol and use that to extend the functionality of our networked applications with custom behavior. Finally, we'll look at some of the advanced features of transport layer protocols, such as multicasting, for interacting with several hosts simultaneously to improve the performance of our network software.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Which responsibilities are delegated to the transport layer and how this layer meaningfully differs from the application layer and HTTP/SMTP/FTP</li>
<li>The distinction between connection-based and connectionless protocols and the challenges they seek to solve</li>
<li>How to initiate a TCP connection and send and receive TCP requests</li>
<li>How to establish and leverage UDP communication in C#</li>
<li>How to leverage multi-casting to improve performance in our TCP client</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>We'll be using sample applications available in the GitHub repo for the book here: <a href="https://github.com/PacktPublishing/Hands-On-Network-Programming-with-CSharp-and-.NET-Core/tree/master/Chapter%2011">https://github.com/PacktPublishing/Hands-On-Network-Programming-with-CSharp-and-.NET-Core/tree/master/Chapter 11</a>.</p>
<p><span>Check out the following video to see the code in action: <a href="http://bit.ly/2HY61eo">http://bit.ly/2HY61eo</a></span></p>
<p>We'll also be continuing to leverage the tools we used in <a href="a0c3481a-daca-484d-95f8-f08867c8c7b8.xhtml">Chapter 8</a>, <em>Sockets and Ports</em>. Specifically, if you haven't already done so, I recommend installing Postman, from here: <span><a href="https://www.getpostman.com/apps">https://www.getpostman.com/apps</a>  </span><span>Or you can install the Insomnia REST client, which can be found here: </span><a href="https://insomnia.rest/"><span>https://insomnia.rest/</span></a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The transport layer</h1>
                </header>
            
            <article>
                
<p>As we start to examine the intricacies of the transport layer, it's important to remember one of the most fundamental distinctions between the protocols of the transport layer and the protocols of the application layer; specifically, the distinction between what kinds of interactions each layer is concerned with. <span>The protocols of the application layer are concerned with the communication between business objects. They should only deal with the</span> high-level representations of your application's domain entities, and how those entities move through your system<span>.</span></p>
<p><span>Meanwhile, with transport layer protocols, the concern is around</span> <strong>atomic network packets</strong><span>, which are used to transmit context-agnostic data packets as well as to establish and negotiate connections.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The objectives of the transport layer</h1>
                </header>
            
            <article>
                
<p>In all of the application layer protocols we've examined thus far, we've been able to make some generous assumptions about the network requests we were transmitting. We just assumed that provided our <span><strong>Uniform Resource Identifier</strong> (</span><strong>URI</strong>) was correct and the remote host was active, we could establish a <strong>connection</strong> to our target system. Additionally, we could assume that the connection we established was a <strong>reliable</strong> one and that any requests we transmitted would be delivered, in their entirety, in such a way as to be readable by the remote host's listening application. We could comfortably assume that if an error occurred in transit, we would get sufficient information to identify the nature of the error and attempt to <strong>correct</strong> it.</p>
<p>Let's review if any of these assumptions can also be applied to the transport layer.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Establishing a connection</h1>
                </header>
            
            <article>
                
<p>In the transport layer, we can't make assumptions about an existing connection, because that's the layer at which the connections are established in the first place. Transport layer protocols are what expose specific ports on the local machine and negotiate the delivery of a packet to the designated port of a remote machine. If that connection requires a session to be maintained for the duration of the interaction, it's the transport layer protocol that's responsible for maintaining the state of that session (we'll see more about this when we explore connection-based communication).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ensuring reliability</h1>
                </header>
            
            <article>
                
<p>When it comes to reliability, the stable and consistent delivery of network packets, and the acceptance of response packets, this the job of the transport layer. If a session is broken due to a break in the chain of communication between two hosts, transport layer protocols are responsible for attempting to re-establish a connection and resume the network session based on its previous state. Transport protocols that guarantee successful delivery of packets must accept the responsibility of communicating with the transport layer of the remote host to validate that the application layer data was received successfully.</p>
<p>This is <span>especially important for application layer software that treats an open connection like a serial data stream. The notion of incoming data being processed in order requires that it can be read in order. That means the transport layer must have some mechanism for ensuring reliable, same-order delivery of network packets.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Error correction</h1>
                </header>
            
            <article>
                
<p>This reliability is also key to another responsibility of the transport layer: error correction. This includes being able to fix discrepancies in data received due to complications or interruptions at the network layer of the interaction. And, make no mistake, there are a lot of opportunities for potential interference, manipulation, or loss of the content of a network packet. The transport layer is responsible for mitigating these eventualities and re-requesting a fresh packet in the event of corruption. This data correction is usually accomplished with a simple <kbd>checksum</kbd> value, which can give a reliable indicator of any change being made to the data in transit.</p>
<p>Error handling should also be present to ensure the reliable ordering of packets. Because physical network infrastructure can, and often does, route multiple requests from one single host to another over multiple available network connections, and through multiple different switches, it's not uncommon for a packet that was sent later in the stream to arrive before packets that were sent earlier. The transport layer must have some way of identifying when that has happened, and be able to re-request the missing packet or re-arrange the received packets into their proper ordering.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Managing traffic</h1>
                </header>
            
            <article>
                
<p>It might not be obvious initially, but when we talk about thousands of ports being available to listen on a given machine, those thousands of ports exist only virtually. Obviously, there aren't 65, 536 wires plugged into the motherboard of your PC. Those ports are just a way for your (usually only one) network adapter to route traffic to the appropriate process currently running on your operating system. All incoming and outgoing network traffic has to pass through that single network adapter.</p>
<p>While it's the network layer software that manages direct traffic control, it typically does so by only provisioning access to the physical connection in short segments of uptime for the transport layer. It's the job of the transport layer software to manage a queue of incoming, unprocessed data, as well as one for outbound requests, and provision their delivery to the network layer when the resources are made available to do so. This use of resources with limited, intermittent availability can be a major boost to performance when done well, and a major bottleneck when implemented poorly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Segmentation of data</h1>
                </header>
            
            <article>
                
<p>As I mentioned when I discussed this topic in <a href="84e54d31-1726-477b-b753-4408a3ee6286.xhtml">Chapter 3</a>, <em>Communication Protocols</em>, the large, contiguous objects that are used to encapsulate data at the application layer are unsuitable for transport over a network. If you tried to block your network adapter for the duration of the transport of a 20 MB file, or a 13 GB file for that matter, the impact on the performance of any other network-dependent software on your machine would be absolutely unacceptable. Attempting to do so would block operations for any outgoing or incoming requests for far too long.</p>
<p>While application layer protocols can send massive payloads with all of their requests and just assume they'll be delivered correctly, the same cannot be said of transport layer packets. There is no other intermediary between the transport layer and the network adapter, so it's the responsibility of the transport layer to decompose large request payloads into smaller, discrete network packets that are suitable for transport over the network layer. This means that transport layer protocols have the added responsibility of applying sufficient context for the decomposed packets of an application layer payload to be reconstructed by the recipient machine, regardless of delivery order, typically accomplished with packet headers.</p>
<p>This isn't typically something you'll be implementing yourself with a language as high-level as C#, but understanding that it is going on behind the scenes will make concepts such as packet-sniffing and network-tracing much easier to grasp down the line.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The classes of transport layer protocols</h1>
                </header>
            
            <article>
                
<p>While we just discussed a number of responsibilities that transport layer protocols might assume, not every protocol at the transport layer implements every one of these features. Since it's important to understand what optional features will be available in a given implementation, standards organizations have defined a classification system for connection mode protocols based on the features they implement<span>. According to this classification scheme, there are five different classes of connection mode (or transport layer) protocols, with each implementing different combinations of the list of services that a transport protocol</span> might <span>implement.</span></p>
<div class="packt_infobox"><span>Defining the classification scheme for different implementation classes of transport protocols was actually a joint effort between standards organizations. The <strong>International Organization for Standardization</strong> (</span><strong>ISO</strong>)<span>, along with the <strong>International Telecommunication Union</strong> (<strong>ITU</strong>), issued recommendation</span> <span>X.224</span> <span>for this exact purpose.</span></div>
<p><span>The list of classifications is zero-indexed, from class <kbd>0</kbd> to class <kbd>4</kbd>, and they are described as follows.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Class 0 – Simple class</h1>
                </header>
            
            <article>
                
<p>This is described as providing the simplest type of transport connection, with sufficient data segmentation. It is explicitly described in the standard as being suitable only for network connections with acceptable residual error rate and an acceptable rate of signaled errors. Basically, given the simplicity of the protocol, it is only suitable for use on highly reliable local networks with nearly guaranteed error-free connections between hosts.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Class 1 – Basic recovery class</h1>
                </header>
            
            <article>
                
<p>Protocols that fall under class <kbd>1</kbd> are specified to provide basic transport connection with minimal overhead. However, what distinguishes class <kbd>1</kbd> from class <kbd>0</kbd> is that class <kbd>1</kbd> protocols are expected to recover from signaled errors, or errors that are immediately detectable, such as a network disconnection or a network reset. Protocols of this class are sufficient for use on networks with an acceptable residual error rate, but an unacceptable rate of signaled errors.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Class 2 – Multiplexing class</h1>
                </header>
            
            <article>
                
<p>The defining characteristic of class <kbd>2</kbd> protocols is their ability to multiplex several transport connections on to a single network connection. It's designed to work on the same exceptionally reliable networks as class <kbd>0</kbd> protocols. Because of the potential for multiple network connections to be leveraged over a single transport layer protocol, protocols within this classification may end up leveraging explicit flow control for the optimized use of the network layer resources. However, that explicit flow control is not a guaranteed property of class <kbd>2</kbd> protocols. In fact, it may be avoided in cases where multiplexing isn't necessary, as not managing flow control explicitly can reduce the overhead applied to packets in transit.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Class 3 – Error recovery and the multiplexing class</h1>
                </header>
            
            <article>
                
<p>The multiplexing class is, essentially, a combination of classes <kbd>1</kbd> and <kbd>2</kbd>. Protocols in class <kbd>3</kbd> introduce the performance benefits (or packet overhead) of the class <kbd>2</kbd> multiplexing functionality into a protocol with sufficient error recovery for a network with less-reliable signaled error rates, where class <kbd>1</kbd> would otherwise be preferred.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Class 4 – Detecting errors and recovery class</h1>
                </header>
            
            <article>
                
<p>Class <kbd>4</kbd> protocols are by far the most robust of any protocol. They are explicitly stated to be suitable for networks with an unacceptable residual error rate and an unacceptable signal error rate, which is to say, basically, any large-scale distributed network with a high probability of interference of interruption of service. Given the suitability of a class <kbd>4</kbd> protocol for use on such an unreliable network, it should come as no surprise that class <kbd>4</kbd> protocols are expected to both detect, and recover from, errors on the network. The errors for which a class <kbd>4</kbd> protocol should provide recovery include, but are not limited to, the following:</p>
<ul>
<li>Data packet loss</li>
<li>Delivery of a data packet out of sequence in a data stream</li>
<li>Data packet duplication</li>
<li>Data packet corruption</li>
</ul>
<p>Protocols in class <kbd>4</kbd> are also expected to provide the highest degree of resiliency against network failure, as well as increased throughput by way of improved multiplexing and packet segmentation. Needless to say, this is also the class of transport layer protocols with the highest amount of per-packet overhead introduced with each transaction over the network.</p>
<div class="packt_infobox"><span>Interestingly, the classification of a protocol only determines the minimum set of services you should expect the protocol to implement. That doesn't preclude that protocol from implementing a broader set of services than specified by its classification. Such is the case with TCP, which actually provides a handful of additional services that might be provisioned by software higher up in the network stack under more rigid implementations.</span></div>
<p>Class <kbd>4</kbd> captures both TCP/IP, which is broadly considered the most robust (or at least the most complex/complicated) transport layer protocol in wide use today, as well as UDP, which is its connectionless peer in terms of broad support and adoption. These are the classes of transport layer protocols you'll be interacting with directly when working in C#. To that end, let's look at perhaps the biggest distinction between TCP and UDP: their connection-based and connectionless communication patterns. In doing so, we'll have a much better idea of when and how to leverage each protocol.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Connection-based and connectionless communication</h1>
                </header>
            
            <article>
                
<p>There are two primary transport layer protocols we'll be working with in C#. The first is the TCP. Commonly called TCP/IP due to its prevalent use on the internet-based network software and tight coupling with the <strong>Internet Protocol</strong> (<strong>IP</strong>), TCP is the transport layer protocol underlying all of the application layer protocols we've looked at so far. The second protocol we'll be looking at is the  UDP. It stands as an alternative approach to TCP with respect to transport layer implementations, aiming to provide better performance in more tightly constrained use cases.</p>
<p>The primary distinction between these two protocols, however, is that TCP operates in what's known as a <strong>connection-based communication mode</strong>, whereas UDP operates in what's called a <strong>connectionless communication mode</strong>. So, what exactly are these communication modes?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Connection-based communication</h1>
                </header>
            
            <article>
                
<p>It may seem obvious what connection-based communication is at first glance. By its name, you might conclude it's just any communication that leverages a connection between two hosts. But what exactly do we mean when we say <em>connection</em>? It can't simply be some physical route between two hosts. After all, under that definition, how could two hosts communicate without connecting in some way? How would data travel between two machines if not over a connection?</p>
<p>The shortcomings of such a definition become even more obvious when you consider that connectionless communication is a valid mode of communication. With that point in mind, it's apparent that a connection, in this context, must refer to more than a simple channel between two hosts for data to travel across. So, then, what exactly is a connection? How do connection-based modes of communication leverage it?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Connections to establish sessions</h1>
                </header>
            
            <article>
                
<p>For the purposes of clarity and understanding, I think we would benefit from thinking of a connection as a session<em>.</em> In connection-oriented protocols, a session must first be established between the two hosts prior to any meaningful work being done. That session must be negotiated with a handshake between the two hosts, and it should coordinate the nature of the pending data transfer. The session allows the two hosts to determine what, if any, orchestration must happen between the two machines over the lifetime of the request to fulfill the request reliably.</p>
<p>Once the session is established, the benefits of a connection-based communication mechanism can be realized. This includes a guarantee of the ordered delivery of data, as well as the reliable re-transmission of lost data packets. This can happen because the session context gives both machines an interaction mechanism that will allow them to communicate when a message has been delivered and received. This shared, active context is important for our understanding of connection-based protocols, so let's look at how that session context is provisioned by the underlying network layer.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Circuit-switched versus packet-switched connections</h1>
                </header>
            
            <article>
                
<p>There are two ways that a session is provided for two hosts wanting to establish a connection. The first is to establish the session by way of a direct, hardware circuit link between hosts. This is what's known as a <strong>circuit-switched connection</strong>. The data needs no routing information applied to the header because it travels over a closed circuit between the two devices. This physical circuit connection is how public telephone networks were set up to establish connections. If you've ever seen old photographs of telephone operators using quarter-inch cables to connect two different ports in a gigantic circuit board, you've seen this exact routing mechanism in action (albeit in a very primitive implementation).</p>
<p>Establishing exclusive, direct, physical connections between two hosts has a lot of benefits. It guarantees that all packets will arrive in constant time since there's no downtime for intermediary routers and switches to parse the addressing information of the packet, or to wait for an opening in the data channel. It also guarantees the ordering of packets, since each one will travel across the same channel exactly ahead of the next packet transmitted.</p>
<p>The downside to this kind of connection-based communication, of course, is that it is incredibly costly to implement. There must be a mechanism at every possible intersection on the network to establish a dedicated circuit between any two other connections without interfering with other possible connections that may pass through that same intersection. Separately, it is incredibly costly to manage the mechanical switching necessary to engage and disengage a specific circuit as connections are established and closed. Thus, these kinds of physical networks haven't been in wide use for computational networks in decades.</p>
<p>The alternative approach is what's known as a <strong>packet-switched connection</strong>. These connections are established through the use of hardware switches and software deployed on routing devices that virtualize the behavior or a circuit-switched connection. With <span>connection</span> <span>mode, routers and switches set up an in-memory circuit that manages a queue of all inbound requests for a target location. Those devices parse the addressing information of each incoming packet and pass it into the queue for the appropriate circuit accordingly, and then forward along messages from those queues, in order, as soon as the physical resources become available. In doing so, the expectations for the behavior of a physical circuit-switched connection are maintained. So, for any software that is written to leverage a connection-based communication scheme, there's no functional difference between a circuit-switched connection or a packet-switched connection.</span></p>
<p><span>With this virtualization, the costs of implementing circuit-switched connection functionality at a physical level are mitigated. Of course, by mitigating the physical costs of a circuit-switch setup, we pay for it in performance costs. With packet-switched connections, there's added overhead with each connection because each packet must be parsed by each switch in the network path between the two hosts. Moreover, unless there is no other traffic on a given network switch, there is undoubtedly going to be downtime for a packet-switched connection every time the packets associated with that connection are put into a queue to wait for physical resources to be made available. However, as most of these operations are implemented at a firmware level, the total time cost for any given connection is actually reasonably small.</span> This model of a packet-switched network describes almost all modern <strong>Wide-area networks</strong> (<strong>WAN</strong>), including the internet.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TCP as a connection-oriented protocol</h1>
                </header>
            
            <article>
                
<p>Establishing a connection is one of the most important functions a TCP implementation provides for application-layer software that is leveraging it. When the TCP layer breaks up a request into packets and applies its headers, it does so on the assumption that the packets will be sent over a packet-switched network.</p>
<p>That means it must be certain that switches and routers along the network path have provisioned a virtual circuit for each payload between the two hosts. This certainty is provided by a multi-step handshake between the two hosts. The specific details of the handshake are a bit more complicated, but it can be boiled down to three fundamental transactions for each step in the process:</p>
<ol>
<li><strong>SYN</strong>: This stands for <strong>synchronization</strong> and is a request sent from the client to the server indicating a desire to establish a connection. The synchronization happens because the client generates a random integer, <em>n,</em> and transmits it along in the SYN request as a sequence number, which the server uses to establish that the appropriate message was received.</li>
<li><strong>SYN-ACK</strong>: This stands for <strong>synchronization and acknowledgment</strong> and is the response a server sends to an initial SYN request. To acknowledge that the request was received in the same state it was sent, the server increments and then returns the random synchronization integer it received from the client, <em>n+1</em>, as the acknowledgment message. It also sends a random integer of its own, <em>m,</em> as the sequence number.</li>
<li><strong>ACK</strong>: At this point, the client acknowledges that its own synchronization request was sent and received correctly, and confirms the same for the server by sending a payload setting the sequence number to the acknowledgment value it received from the server, <em>n+1</em>, and then incrementing and returning the sequence number from the server as its own acknowledgment value, <em>m+1</em>.</li>
</ol>
<p>Once these three signals have been sent and received accordingly, the connection has been established and data transfer can proceed accordingly.</p>
<p>Having this agreement between two hosts prior to data transmission is what allows TCP to achieve the resiliency that sets it apart from UDP. Since the host knows to expect an ordered sequence of packets, it can re-arrange packets that were received out of order once they arrive to ensure they are delivered in the appropriate order to the higher-level protocols that are expecting them. Moreover, if it doesn't receive all of the packets it's expecting, it can identify the missing packets based on the missing sequence numbers, and request re-transmission of exactly what was lost. Finally, this initialization of a connection gives the server an opportunity to communicate information about its processing capability and maximum throughput. By telling the client how much data can be processed at any given time, the client can throttle its own output to minimize data loss and network congestion.</p>
<p>The obvious downside, of course, is that all of these steps to establishing and leveraging a connection-parsing sequence of numbers and re-ordering data streams accordingly, and re-transmitting data, incurs a major time cost for the interactions. When such reliability is necessary (and in most enterprise network software, it is), you have no other choice but to leverage TCP or similarly resilient protocols. However, when the nature of your software, or the network infrastructure supporting it, can support less reliability, you have many high-performing alternatives for transport-layer protocols.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Connectionless communication</h1>
                </header>
            
            <article>
                
<p>As we previously established, a connection in transport-layer communication can be thought of instead as a session for communication. Thus, <strong>connectionless communication</strong> is a mode of communication in which data is transmitted without first establishing a mutual session between hosts. Instead, packets are sent out with their appropriate addressing information, and the responsibility of ensuring delivery falls entirely to the lower layers of the network stack. This obviously introduces the risk of a failed delivery being undetected: since, without any acknowledgment expected from the server, the client wouldn't know the packet delivery failed and required re-transmission, whereas without synchronizing a session first, the server wouldn't know to expect an inbound message. So, why is this mechanism used, and when is this sort of risk acceptable?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stateless protocols</h1>
                </header>
            
            <article>
                
<p>Without any session to manage, connectionless protocols are typically described as being stateless. Without a state to manage, each transaction happens without any broader context telling the recipient how an individual packet fits into the wider stream of incoming packets. As such, there is almost no ability to determine and ensure the proper sequencing of packets for re-construction by the recipient. Without that ability, connectionless protocols are typically leveraged in cases where packets can be wholly self-contained, or where the information lost in a dropped packet can be reconstructed by the recipient application based on the next packet received.</p>
<p>In the latter case, we can account for the statelessness of the protocol with state management in our applications. For example, imagine your server hosting an application that keeps track of a deterministic state. Now let's say that state is updated by a remote client, and those updates are sent in real time, with minimum latency, over a connectionless protocol, such as UDP. Because the state of the application is deterministic, if a single packet is lost, the server may still be able to determine which update was made based on the next packet received if its own update could only be reached from a specific state set in the lost packet.</p>
<p>Using this architecture, there would be a time cost incurred by the application, as every time a packet was lost, some processing would need to happen to deduce the value of the lost packet and update its internal state accordingly. However, in cases where the network is reliable enough that packet loss is an infrequent occurrence, the reduced latency of a connectionless communication mode can more than make up for the occasional processing cost of a dropped packet over the lifetime of the application. So, on a reliable enough connection, the trade-off could prove extremely worthwhile. While less common in business applications, these protocols are frequently leveraged in high-throughput, low-latency interactive applications such as networked multiplayer video games.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Broadcasting over connectionless communication</h1>
                </header>
            
            <article>
                
<p>One of the benefits of this lack of shared state being managed between two hosts is the ability of connectionless communication to multicast. In this way, a single host can transmit the same packet out to multiple recipients simultaneously, as the outbound port isn't bound by a single active connection with a single other host. This multicasting, or broadcasting, is especially useful for services such as a live video stream or feed, where a single source server is transmitting to an arbitrary number of potential consumers simultaneously. With the already-low overhead of connectionless packet transmission, this can allow high throughput of data to a broad spectrum of consumers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Establishing connections over connectionless communication</h1>
                </header>
            
            <article>
                
<p>If you're anything like me, you probably noticed a bit of a chicken-and-egg problem with connection-based communication modes as I initially described them. Specifically, how can you establish a session between two hosts that rely on connection-based communication without first having a session between those two hosts?</p>
<p class="mce-root">Of course, the obvious answer is that connections are established by way of an initial, connectionless communication request. The initial SYN message of a TCP connection request is sent over the connectionless communication IP. In this way, you could say that connection-based communication is built on the back of connectionless communication. In fact, in the case of TCP, the connection-based interactions are so dependent on the connectionless interactions of IP that the two are typically lumped together and identified as the TCP/IP suite.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">UDP as a connectionless communication protocol</h1>
                </header>
            
            <article>
                
<p>Just as TCP is the connection-based communication protocol of choice on the internet, UDP often serves as the connectionless communication protocol of choice. UDP exhibits all of the expected traits of a connectionless protocol, including the lack of any handshake or session negotiation prior to data transfer, and minimal error-checking and error correction techniques. So, what are the contexts in which UDP is useful?</p>
<p>The need for such speed and the acceptability of intermittent packet loss is perfectly suited to low-level network operations to send out notifications or basic queries of other devices on the network. It's for that reason that UDP is the protocol of choice for <span><strong>Domain Name System</strong> (</span><strong>DNS</strong>) lookups and the <strong>Dynamic Host Configuration Protocol</strong> (<strong>DHCP</strong>). In both of these contexts, the requesting host needs an immediate response to a single, simple query. In the case of DNS lookup, the request is for each IP address registered for a given domain name. The UDP packet can simply address the DNS server directly, and can contain only the domain name of the resource being looked up. Once that information is received, the DNS server can respond with addressing information on its own time, trusting that whichever application requested the IP addresses will likely be listening for the response. Once the DNS lookup request is initially sent out by the client, <span>if there's been no response</span> <span>after a given timeout period,</span> <span>an identical packet will be transmitted from the client. This way, in the off-chance of a lost packet, there's a mechanism for error recovery (the timeout period); meanwhile, in the far-more-likely scenario that the packet is successfully transmitted, the query result will be returned substantially faster than if a connection were established first.</span></p>
<p>This same behavior is what enables a DHCP request to be satisfied in near-real time. When a new network device requests an IP address from the DHCP server, it has no specific information about the other devices on its own network. Therefore, it must broadcast out a DHCP request and hope that an adjacent node is available to act as the DHCP server and provision an IP address for the device. These needs, for low-latency and the need to broadcast packets, mean that DHCP requests are the ideal use case for a connectionless protocol such as UDP.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Detecting errors in connectionless protocols</h1>
                </header>
            
            <article>
                
<p>I have discussed at length that connectionless transport layer protocols are far more susceptible to errors, as there is no mechanism inherent to the protocol for detecting errors. I've already discussed how we can detect and even correct for errors in a connectionless transport layer protocol from the application layer that's leveraging it. However, at least within UDP, there is at least one simple error-detection mechanism transmitted with each packet, and that is a <strong>checksum</strong>.</p>
<p>If you've never heard the term before, a checksum is similar to a hash function where each input will provide a drastically different output. In UDP packets, the checksum input is essentially the entirety of the headers and body of the packet. Those bytes are sent through a standard algorithm for generating the checksum. Then, once the packet is received, the recipient puts the content of the packet through the same checksum algorithm as the client, and validates that it received the same response as was delivered. If there is even a minor discrepancy, the recipient can be certain that some data was modified in transit and an error has occurred.</p>
<p>Responding to, or correcting, this error is outside the scope of the error-handling mechanisms of UDP. Typically, if the value of the packet was critical for continued operation of the recipient system, that system may request re-transmission of the packet. However, in most cases, a mismatched checksum simply indicates to the recipient that the packet is invalid and can be discarded from the processing queue.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TCP in C#</h1>
                </header>
            
            <article>
                
<p>So, now that we've explored in-depth the objectives, functions, and limitations of various transport layer protocols, let's take a look at how we can interact with those protocols in C#. We'll start by taking a close look at the classes and features exposed by .NET Core for implementing TCP requests directly from our application code. We'll see how stepping down in the network stack gives us a degree of flexibility and control over our network operations that wasn't previously available in the application layer protocols we've explored in previous chapters. To do this, we'll be creating two applications, as we did in <a href="e93c024e-3366-46f3-b565-adc20317e6ec.xhtml">Chapter 9</a>, <em>HTTP in .NET</em>. One of the applications will be our TCP client, and one will be the listening TCP server. We'll see the results from each request and response, confirming the expected behavior of our software, by writing to the standard output for each of our two applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Initializing a TCP server</h1>
                </header>
            
            <article>
                
<p>Let's first create our <kbd>TCP</kbd> client, the same as we have with every application before, by creating a directory for it, and then using the CLI to create a console app:</p>
<pre><strong>dotnet new console -n SampleTcpClient</strong></pre>
<p>Then, within the same directory, we'll create our <kbd>TCP</kbd> server application with the same command:</p>
<pre><strong>dotnet new console -n SampleTcpServer</strong></pre>
<p>Now we're ready to start setting up our interactions. When we were last interacting directly with sockets exposing a port back in <a href="a0c3481a-daca-484d-95f8-f08867c8c7b8.xhtml">Chapter 8</a><em>, Sockets and Ports</em>, we were using Postman to generate HTTP requests against a given endpoint. Now, however, since we'll be writing our own TCP messages directly in code, we won't be constrained to processing the standardized HTTP headers generated by Postman. We can define our own mechanism for interactions between hosts. For ease of processing, we'll just let our client and server work with simple string messages.</p>
<p>To start these interactions, we'll set up a listening server. We need to do this to know what port our client will be connecting to. So, navigating to the <kbd>Main()</kbd> method of your <kbd>SampleTcpServer</kbd> application, we'll start by defining our listening ports, and then starting up an instance of the <kbd>TcpListener</kbd> class, like so:</p>
<pre>public static void Main(string[] args) {<br/>    int port = 54321;<br/>    IPAddress address = IPAddress.Any;<br/>    TcpListener server = new TcpListener(address, port);<br/>    ...<br/>}</pre>
<p>The <kbd>TcpListener</kbd> class is a custom wrapper around a bare <kbd>Socket</kbd> instance. With the constructor, we designate the port and IP we want to listen on for incoming requests. If we had used a bare socket, we'd have to process and either respond to or discard every single incoming network request that ran against our designated port. With the <kbd>TcpListener</kbd> instance, though, we won't have to respond to any requests that aren't sent via TCP. We'll take a look at this once we set up our client class, but this is immensely useful when you're listening for such low-level network requests on an open port.</p>
<p>The constructor we used accepts an instance of the <kbd>IPAddress</kbd> class, and any <kbd>int</kbd> that designates a valid port (so nothing negative, and nothing above 65,535). So, for this project, we'll be using port <kbd>54321</kbd> to listen for incoming TCP requests. For our <kbd>IPAddress</kbd> instance, we're using the <kbd>Any</kbd> static <kbd>IPAddress</kbd> instance that is exposed by the class. By doing this, we'll see and be able to respond to any TCP request whose target host IP address or domain name would resolve to our host machine. If we didn't do this, and instead specified an individual IP address, we wouldn't respond to any requests whose IP address didn't match that exactly, even if the address resolved to the same machine. So, we would do this:</p>
<pre>IPAddress address = IPAddress.Parse("127.0.0.1");</pre>
<p>After doing so, we could send a TCP request to <kbd>tcp://0.0.0.0:54321</kbd>, and you wouldn't see any request register on our <kbd>TcpListener</kbd> instance. You might expect that our request would be detected, since the <kbd>0.0.0.0</kbd> <span>IP addresses</span> and <kbd>127.0.0.1</kbd> both resolve to the same local machine, but because, in this example, we designated our <kbd>TcpListener</kbd> to only listen for requests to the <kbd>127.0.0.1</kbd> <span>IP address</span>, that's exactly what it does. Meanwhile, our request to <kbd>0.0.0.0</kbd> goes unresolved. So, unless you're writing distinct listeners for distinct IP addresses held by your host machine, (or by a series of host machines your application might be deployed across), I would recommend using <kbd>IPAddress.Any</kbd> wherever possible.</p>
<p>Now we have to set up our server to run and listen for requests against that port. First, we'll start the server, and then we'll set up a context where we listen indefinitely for incoming requests. This is typically done with an intentionally infinite loop. Now, if you've ever accidentally found yourself stuck inside an infinite loop, you know it's something you should only ever start when you mean to. However, since we want our application to listen indefinitely, the simplest and most reliable way to do so is to prevent our <kbd>Main()</kbd> method from resolving by encapsulating our primary business logic in a simple infinite loop:</p>
<pre>server.Start();<br/><br/>var loggedNoRequest = false;<br/>var loggedPending = false;<br/><br/>while (true) {<br/>  if (!server.Pending()) {<br/>    if (!loggedNoRequest) {<br/>      Console.WriteLine("No pending requests as of yet");<br/>      Console.WriteLine("Server listening...");<br/>      loggedNoRequest = true;<br/>    }<br/>  } else {<br/>    if (!loggedPending) {<br/>      Console.WriteLine("Pending TCP request...");<br/>      loggedPending = true;<br/>    }<br/>  }<br/>}</pre>
<p>If you compile and build what we've written so far, you'll see the two console statements print to the screen, and then your application will look as if it's hanging for quite some time, and that's exactly what you would hope to see. What's happening behind the scenes is that you've initiated the <kbd>TcpListener</kbd> instance by calling <kbd>Start()</kbd> on it.</p>
<p>This will cause the instance to accept incoming requests on its designated port until either you explicitly call the <kbd>Stop()</kbd> method on the class, or it receives a total number of connections greater than the <kbd>MaxConnections</kbd> property of the <kbd>SocketOptionName</kbd> enum (which is set to over two billion, so it's unlikely that limit will be reached in our little local TCP server).</p>
<p>Once our server is listening, we start our listening loop and check to see whether our socket has received any pending requests. If it hasn't (and we haven't logged it since the last request), we indicate as much with a simple console log, and then move along, continuing with the <kbd>while</kbd> loop until we have something to process. For now, we shouldn't see anything in the pending state, so let's set up our client project to change that.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Initializing a TCP client</h1>
                </header>
            
            <article>
                
<p>Now we'll need to initialize our TCP client application in much the same way as we did with our server. Instead of using the <kbd>TcpListener</kbd> class, though, we'll be using the <kbd>TcpClient</kbd> class to create connections with our server that we can write to and read from within our project. The difference between the two in our case is that, when we created a <kbd>TcpListener</kbd>, we needed to initialize it with the address and port on which it would be listening. There is no default constructor, because without a port on which to listen, the class can't perform its most basic functions.</p>
<p>With an instance of the <kbd>TcpClient</kbd>, however, we don't need to initialize it with an address or port specification. The client instance could feasibly be used to connect to multiple, distinct remote processes (ports on a single remote host) or hosts (different IP addresses altogether). As such, we only need to specify our connection target when we attempt to make a connection. For now, let's just establish the connection to confirm that our server responds to listening requests appropriately:</p>
<pre>public static async Task Main(string[] args) {<br/>  int port = 54321;<br/>  IPAddress address = IPAddress.Parse("127.0.0.1");<br/>  using (TcpClient client = new TcpClient()) {<br/>    client.Connect(address, port);<br/>    if (client.Connected) {<br/>      Console.WriteLine("We've connected from the client");<br/>    }<br/>  }<br/>  Thread.Sleep(10000);<br/>}</pre>
<p>Here, we specified <span>IP address</span> <kbd>127.0.0.1</kbd>, but, as I said before, we could have specified any alias IP address that would resolve to our local machine. Once we've created our client, we can use it to connect to the port we designated as listening on our server application. Then, just to confirm that the connection was established and that our client knows about it, we write a simple log statement, sleep the thread for <kbd>10</kbd> seconds to observe the message in our console, and then terminate the program.</p>
<p>In order to see this succeed, start your server application first, so that it's started and listening on the designated port. Then, once you see the messages show up in your console window indicating that the server is waiting for a pending request, start your client application. You should see the <span class="packt_screen">We've connected...</span> message in your client window, and the <span class="packt_screen">Pending TCP request...</span> message in your server window. Once you see both messages, you know your connections are being established, and you can terminate both applications.</p>
<p>And, here, let's consider why we use the <kbd>loggedPending</kbd> flag. It should be pretty obvious why we used the <kbd>loggedNoRequest</kbd> flag to prevent us from printing out the log messages every time we stepped through our loop until we received an incoming request. However, the reason we have to do the same thing when we have a pending request is the server will hold the <kbd>Pending</kbd> state until its inbound message queue has been read from and flushed. So, since our server doesn't yet read from and empty the incoming request stream if we didn't have that check, and we connected to our server, our console would quickly overflow with <span class="packt_screen">Pending TCP request...</span> messages.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Connection information without data transfer</h1>
                </header>
            
            <article>
                
<p>Before we get to the work of building and parsing TCP requests in our projects, I just want to take a moment to note the benefit of the connection-based approach, and how .NET Core leverages it to give engineers fine-tuned control over their network transactions. Note that once we send the connection request from our client, we get an immediate notification from the server that a connection was established. No message was actually sent, and no response was received by the server. In fact, the connection remains open even if the server is synchronously locked and prevented from actively transmitting anything. This is the handshake interaction of TCP at work, and gives us access to a lot of information about the state of the connection prior to actually sending a message.</p>
<p>What's especially nice for application developers, though, is that the connection is established and managed by the <kbd>TcpClient</kbd> class itself. With only a single call to the <kbd>Connect(IPAddress, int)</kbd> method, the TcpClient library notified our server that we wished to establish a connection, await the acknowledgment, and finally acknowledge the server's response to open the connection. This is one of the greatest strengths of .NET Core; the ease of use of a high-level application programming language, coupled with access to, and control over, low-level network interactions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transmitting data on an active connection</h1>
                </header>
            
            <article>
                
<p>Now that we've established a connection, our server can decide what to do with that connection, and the requests transmitted across it. However, before we change gears back to our server, let's generate a message from our client for the server to process in the first place. We'll be using something of a mutation test to confirm that all of our data is being processed and returned by the server accordingly. So, at each step of the way, we'll be modifying our initial message and logging the results. Each step of the way, our message should look different than the last system that wrote it.</p>
<div class="packt_infobox">If you've never heard the term <strong>mutation test</strong>, it's a simple way of tracking that changes to your system are detected by the tests that validate your system. The idea is that you make a change or a mutation somewhere in your code, and confirm that somewhere downstream, usually in your unit tests, that change has an impact, typically by failing a previously passing unit test.</div>
<p>We'll start by writing a message with a header and a payload. This will just be a simple greeting for our server, and a message we expect our server to return to us, unchanged, as part of its response. We'll separate the two messages with a simple <kbd>|</kbd> delimiter. Then we'll convert it to a byte array that's suitable for transmission over our connection, and send the request. So, let's set that up before moving on to the server:</p>
<pre>var message = "Hello server | Return this payload to sender!";<br/>var bytes = Encoding.UTF8.GetBytes(message);<br/>using (var requestStream = client.GetStream()) {<br/>  requestStream.Write(bytes, 0, bytes.Length);<br/>}</pre>
<p>The <kbd>requestStream</kbd> variable we created is an instance of the <kbd>NetworkStream</kbd> class created to write and read data over an open socket. With this, we'll be able to send our initial message, and then, eventually, read the response from the server. But, first, let's take a look at how to use our <kbd>TcpListener</kbd> instance to accept and parse an incoming request.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Accepting an incoming TCP request on the server</h1>
                </header>
            
            <article>
                
<p>Now that our client is actually sending a readable message, let's listen for the request on our pending connection. To do that, we'll actually get another instance of the <kbd>TcpClient</kbd> class directly from our listener. This is simply the class is used to interact with the open connection, so once we accept it, we'll be reading from and writing to that open connection in much the same way that our sample client program has been. First, though, we'll have to accept the pending connection, using the thread-blocking <kbd>AcceptTcpClient()</kbd> call. Since we're now responding to our pending request, we can get rid of our log message and replace it with our new code:</p>
<pre>loggedNoRequest = false;<br/>byte[] bytes = new byte[256];<br/><br/>using (var client = await server.AcceptTcpClientAsync()) {<br/>  using (var tcpStream = client.GetStream()) {<br/>    await tcpStream.ReadAsync(bytes, 0, bytes.Length);<br/>    var requestMessage = Encoding.UTF8.GetString(bytes);<br/>    Console.WriteLine(requestMessage);<br/>  }<br/>}</pre>
<p>Starting our server, we should see in our server log that it's listening for pending connection requests. Then, once we run our client, we should see our request message from the client logged to the server's console, followed by another indicator that the server has started listening for incoming requests again. If we run the client again, we'll see the same sequence of events until we eventually shut down the server.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The request/response model on the server</h1>
                </header>
            
            <article>
                
<p>To finish the request/response interaction, we'll generate a new message, using the payload of the original request, and return it to our client. As we complete these two applications, we'll have the client drive the interactions with the server from here on out. So, our server will be up and running, returning responses that echo the payload of the requests, until it receives a signal message indicating it should shut itself down. Meanwhile, our client will send intermittent requests with new payloads, until eventually sending the termination signal to our server. To serve that purpose, we'll add the following lines to our server application:</p>
<pre>bool done = false;<br/>string DELIMITER = "|";<br/>string TERMINATE = "TERMINATE";</pre>
<p>We'll use this as a signal that we should stop listening for requests and terminate the server. Next, we'll add the following conditional code to our server's listening loop:</p>
<pre>  ...<br/>  requestStream.Read(bytes, 0, bytes.Length);<br/>  var requestMessage = Encoding.UTF8.GetString(bytes).Replace("\0", string.Empty);<br/><br/>  if (requestMessage.Equals(TERMINATE)) {<br/>    done = true;<br/>  } else {<br/>    Console.WriteLine(requestMessage);<br/>  }<br/>}</pre>
<p>Our response transmission code will go inside the <kbd>else</kbd> statement in that conditional block, and so our loop will simply continue logging the request message, and then appending the payload to the response, until the terminating signal is received, at which point the loop is broken and we'll shut our server down. So, lastly, we'll modify our <kbd>while</kbd> loop to check for the value of our <kbd>done</kbd> condition instead of running in an infinite loop:</p>
<pre>while (!done) {<br/>    ...</pre>
<p>Next, let's go ahead and parse the message for its payload, using our delimiter to separate the two components of our message, and then apply the result to our server's response:</p>
<pre>} else {<br/>  Console.WriteLine(requestMessage);<br/>  var payload = requestMessage.Split(DELIMITER).Last();<br/>  var responseMessage = $"Greetings from the server! | {payload}";<br/>  var responseBytes = Encoding.UTF8.GetBytes(responseMessage);<br/>  await tcpStream.WriteAsync(responseBytes, 0, responseBytes.Length);<br/>}</pre>
<p class="mce-root">Finally, on the line after the closing brace for our listening loop, let's shut down our server, and if you're running the application in Debug-mode from Visual Studio, allow our program to end after a brief delay to check the log results:</p>
<pre>  }<br/>  server.Stop();<br/>  Thread.Sleep(10000);<br/>}</pre>
<p>And, with that, our <kbd>SampleTcpServer</kbd> application is complete. It will stay active and listen for requests until it's explicitly instructed to terminate itself. And the whole time, it will log each request it receives and return its own custom response. You can use the source code in the GitHub repository for this chapter to check your implementation against my own, but, as always, I'd encourage you to modify it on your own and start investigating what other methods are available. And, as you do so, always be thinking about how you could use this code in your own custom networking software.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Finalizing the TCP client</h1>
                </header>
            
            <article>
                
<p>Our server is designed and written to remain active and listening for any potential incoming requests. A client, on the other hand, should only ever be set up for a single purpose, execute on that purpose, and then close its connection, freeing up the resources of the server for any other consumers that may need to access it. For this reason, we won't be writing any persistent listening loops. Instead, we will simply process each of a handful of request/response round trips before terminating the server and then shutting down our own application. However, to create a slightly more realistic simulation of multiple clients accessing our TCP server, we'll be dropping and recreating our <kbd>TcpClient</kbd> instance for each subsequent request, and injecting a random delay in between each request.</p>
<p>The first order of business, though, is accepting the response from our server. So, inside our <kbd>SampleTcpClient</kbd> application, we'll be adding a few lines to create a new byte array for use as a message buffer for the response and then reading our <kbd>requestStream</kbd> into our buffer for processing and logging. So, let's add that code and then we'll see how we can extend it to finish our simulation:</p>
<pre>using (var requestStream = client.GetStream()) {<br/>  await requestStream.WriteAsync(bytes, 0, bytes.Length);<br/>  var responseBytes = new byte[256];<br/>  await requestStream.ReadAsync(responseBytes, 0, responseBytes.Length);<br/>  var responseMessage = Encoding.UTF8.GetString(responseBytes);<br/>  Console.WriteLine(responseMessage);<br/>}</pre>
<p>I would think none of this is surprising at this point. We're essentially executing the exact same thing as the server, but in reverse order. Where as on the server, we were reading from the stream, and then writing <em>to</em> the stream, in the client code, we're first writing <em>to</em> the stream, and then reading from the stream. Mechanically though, this is the same sort of interaction we've seen since we first looked at how to interact with raw C# Stream objects back in <a href="9d6266fb-4428-4044-b63b-44f1317f64e7.xhtml">Chapter 4</a><em>, Packets and Streams.</em> Hopefully, by now, you're starting to see the value in the incremental, brick-by-brick approach we've taken to building a foundation for network programming up to this point (assuming you haven't already).</p>
<p>At any rate, let's modify our client to transmit a handful of pre-defined messages before finally sending the termination signal. To do that, let's build out a short array of the messages we'll be sending to the server so that we can easily increment through them in our code, sending distinct messages with each outbound request:</p>
<pre>var messages = new string[] {<br/>  "Hello server | Return this payload to sender!",<br/>  "To the server | Send this payload back to me!",<br/>  "Server Header | Another returned message.",<br/>  "Header Value | Payload to be returned",<br/>  "TERMINATE"<br/>};</pre>
<p>Next, let's wrap the request/response transactions in a <kbd>while</kbd> loop (not an active listening loop as we saw with our server, but a simple incremental loop). We'll use an iterator variable, starting at zero, to move through the our messages, checking its value against the length of our messages array to determine when to break out of our loop and let our application terminate:</p>
<pre>var i = 0;<br/>while (i &lt; messages.Length) {<br/>  using (TcpClient client = new TcpClient()) {<br/>    ...</pre>
<p>Because our <kbd>TcpClient</kbd> instance is created by the <kbd>using</kbd> statement within our <kbd>while</kbd> loop, the variable goes out of scope with each iteration. We thus create a new connection every time we step back through the beginning of the loop. Next, we have to change the code that builds our request message byte-array to iterate through the <kbd>messages</kbd> string array:</p>
<pre>var bytes = Encoding.UTF8.GetBytes(messages[i++]);</pre>
<p>Finally, at the end of our <kbd>while</kbd> loop, we'll sleep our thread for a random amount of time between <kbd>2</kbd> and <kbd>10</kbd> seconds, logging the <kbd>sleepDuration</kbd> each time:</p>
<pre>  ...<br/>  var sleepDuration = new Random().Next(2000, 10000);<br/>  Console.WriteLine($"Generating a new request in {sleepDuration/1000} seconds");<br/>  Thread.Sleep(sleepDuration);<br/>}</pre>
<p>Finally, if you're running in Debug-mode, you'll want to <span>throw in one last</span> <kbd>Thread.Sleep()</kbd> <span>for good measure,</span> <span>after the <kbd>while</kbd> loop,</span> <span>to ensure we have enough time to examine the results of our requests before our application shuts down.</span></p>
<p><span>After completing the client and running both applications, my terminals logged exactly the messages that I hoped they would:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-924 image-border" src="assets/50954a2a-67e4-4336-85d4-5f4c2da42e86.png" style="width:96.00em;height:50.08em;"/></p>
<p class="mce-root">And, with this, we've written our own custom TCP server and clients. While this example was fairly trivial in its function, I hope you can see the high degree of flexibility these .NET classes open up for you with respect to custom TCP implementations. With these two sample applications, you have all the tools at your disposal necessary to write your own custom application layer protocol with a custom TCP server optimized to support it. Or you could write applications whose network interactions side-step the application layer protocol overhead altogether! The problems you encounter in your personal or professional projects will dictate how you choose to use this toolset, but now, hopefully, you'll be ready to leverage it when you need to.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">UDP in C#</h1>
                </header>
            
            <article>
                
<p>Now that we've looked at how to implement TCP in C#, let's take a look at its connectionless counterpart in the suite of transport layer protocols, UDP. By its very nature, the sample client and server we'll be writing will be a fair bit simpler than the TCP in terms of setup code, but we'll be using the same pattern we used in the previous section for defining the behavior of our sample application. So, we'll be transmitting requests and accepting and logging responses between a client and a server.</p>
<p>The difference here, however, is that both the client and the server will be implemented in the exact same way. This is because there is no <kbd>UdpListener</kbd> class, because UDP doesn't actively listen for connections. Instead, a UDP server simply accepts in bound packets whenever it is set up to look for a new one. For this reason, we'll only be looking at the client application's implementation, and I'll leave the server source code for you to pull down from GitHub and use to test and validate the behavior of the client.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Initializing a UDP client</h1>
                </header>
            
            <article>
                
<p>We'll start by creating a new console app that will serve as our <kbd>UDP</kbd> client:</p>
<pre class="mce-root"><strong>dotnet new console -n SampleUdpClient</strong></pre>
<p>Then inside our project, the first thing we'll want to do is define a well-known IP endpoint that our client will be interacting with. We'll be working against localhost once again, with an arbitrary port exposed, just as we did in the previous section about TCP. Once we have that defined, though, we're just about ready to start generating requests. The beauty of a connection-less protocol is that we don't have to first establish any sort of interaction with our remote host. Provided we know what the address of the host is, we can simply send out our datagrams.</p>
<pre>public static async Task Main(string[] args) {<br/>  using (var client = new UdpClient(34567)) {<br/>    var remoteEndpoint = new IPEndPoint(IPAddress.Parse("127.0.0.1"), 45678);<br/>  <br/>    var message = "Testing UDP";<br/>    byte[] messageBytes = Encoding.UTF8.GetBytes(message);<br/>    await client.SendAsync(messageBytes, messageBytes.Length, remoteEndpoint);<br/>    ...<br/>  }<br/>}</pre>
<p>And, just like that, if you run the server application and then run the client, you'll see your message logged to the server's console! So, what exactly is going on here, and what are we doing when we initialize our <kbd>UdpClient</kbd>?</p>
<p>The first thing we do is initialize our <kbd>UdpClient</kbd> with a port number. If we intend to use this client to receive incoming UDP datagrams (which we eventually will), it will be accepting them on the port it was initialized with. So, our client will be listening on port <kbd>34567</kbd>. Next, <span>we take the time to define the explicit <kbd>IPEndPoint</kbd> that we would be sending our datagrams to.</span></p>
<p><span>This isn't technically necessary, as you can define your request target with their hostname and port as part of the <kbd>SendAsync()</kbd> method using an overloaded method signature. However, since we'll be extending this method to also accept responses, it's easier for our purposes to explicitly define the <kbd>IPEndPoint</kbd> instance once at the start of the method. Finally,</span> <span>we build our datagram as an array of bytes representing the characters of our message string, just as we did in the previous section, and send the message along with the help of our newly initialized <kbd>UdpClient</kbd>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The send/receive paradigm</h1>
                </header>
            
            <article>
                
<p>One thing you might have noticed about using a <kbd>UdpClient</kbd>, as opposed to the more robust <kbd>TcpClient</kbd> class, is that UDP doesn't leverage streams at all. With no underlying connection for a <kbd>Stream</kbd> to represent, and the potential for UDP data to be lost, or delivered out of order, there is no direct correlation between the behavior of a UDP request and the abstraction provided by a <kbd>Stream</kbd> instance. Because of this, the <kbd>UdpClient</kbd> class provided by .NET Core implements a simple call/response mechanism through its <kbd>Send</kbd> and <kbd>Receive</kbd> methods. Neither of these two methods requires a prior communication or interaction with the remote host to execute. Instead, they behave as more of a fire-and-forget trigger for some events to happen on the network.</p>
<p>Interestingly, though, when you want to leverage the <kbd>SendAsync()</kbd> method, which doesn't block your application's thread, you <em>can</em> choose to first establish a connection with your remote host. Keep in mind, though, that this isn't quite the same as establishing a connection in TCP. Instead, this simply configures your <kbd>UdpClient</kbd> so that it attempts to send all outgoing packets to the specific remote host to which it is connected.</p>
<p>The connection in this context is only a logical one and it only exists within the application it's established in. So, while an established TCP connection was detectable from both our client and server applications simultaneously, the same is not true in our UDP application. While running our UDP client and server simultaneously, the server application has no way of detecting the connection established by the client.</p>
<p>Once we've connected our <kbd>UdpClient</kbd> to a given <kbd>IPEndPoint</kbd>, every <kbd>SendAsync()</kbd> call is assumed to be configured for the connected endpoint. If you want to send a message to an arbitrary endpoint while your <kbd>UdpClient</kbd> instance is connected to a different endpoint, you'll have to disconnect your client first, or explicitly pass the new endpoint as a parameter for your <kbd>SendAsync()</kbd> call. In the context of our sample application, this won't come up as an issue, but it could come up fairly quickly in real-world contexts, so it's important you keep that in mind as you define your send/receive patterns for a given application.</p>
<p>With that understanding in mind, let's prepare to receive the response from our UDP server application. First, though, we'll modify our application to connect to our remote endpoint at the outset. Next, to demonstrate how to establish a connection with a <kbd>UdpClient</kbd> instance, we'll remove the endpoint parameter from our <kbd>SendAsync()</kbd> call. Finally, we'll listen for a message with <kbd>ReceiveAsync()</kbd>. At that point, we'll be handling the packet's buffer object just as we have with every byte-array buffer before:</p>
<pre>public static async Task Main(string[] args) {<br/>  using (var client = new UdpClient(34567)) {<br/>    var remoteEndpoint = new IPEndPoint(IPAddress.Parse("127.0.0.1"), 45678);<br/>    <br/>    client.Connect(remoteEndpoint);<br/><br/>    var message = "Testing UDP";<br/>    byte[] messageBytes = Encoding.UTF8.GetBytes(message);<br/>    await client.SendAsync(messageBytes, messageBytes.Length);<br/><br/>    var response = await client.ReceiveAsync();<br/>    var responseMessage = Encoding.UTF8.GetString(response.Buffer);<br/> Console.WriteLine(responseMessage);<br/><br/> Thread.Sleep(10000);<br/> }<br/>}</pre>
<p>And, with that, we've got our UDP client wired up to send a packet and await a response from our server.</p>
<p>You may have deduced this from our discussions about connectionless communication throughout this chapter, but whenever you're sending a message using UDP (or any other connectionless protocol), it is an inherently non-blocking operation. This is due to the lack of any sort of acknowledgment from the server. So, from our application's perspective, once a UDP packet has reached our network card for transmission, its delivery is out of our hands.</p>
<p>Meanwhile, the <kbd>Receive()</kbd> operation in UDP is inherently blocking<em>.</em> Since there's no established connection or stream buffer to hold an incoming message until our server or client is ready to process the packet, any software we right that must accept and receive UDP packets will have to be very explicit about when and how long it is acceptable to block our execution while we wait for a packet that may never arrive. The asynchronous versions of the transmission methods provide some flexibility, but, ultimately, it's a limitation of the protocol that we can't escape. Given that, it's in your best interest to be mindful of that limitation and design your UDP software around it from the start.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multicasting packets</h1>
                </header>
            
            <article>
                
<p>Perhaps one of the single greatest advantages of using connectionless communication, such as UDP, is the ability to send out packets to a large number of recipients in a single transaction. This is commonly called <strong>multicasting</strong>, or <strong>broadcasting</strong>, and it enables everything from network device discovery and host registration to most live television or video streams broadcast over the internet. It's a somewhat niche feature that, if I had to guess, most of the people reading this will never have a good reason to leverage, but it is certainly worth understanding. With that said, let's look at how to enable this feature in our .NET Core apps.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multicasting in .NET</h1>
                </header>
            
            <article>
                
<p>With most of the packet transmissions we've looked at so far, we've been addressing a specific port on a specific machine, addressed via host name or IP address. However, this obviously won't suit our needs if our goal is to send the same packet to as many IP addresses as can listen for it. And it certainly won't work if we're trying to discover devices on our network and aren't even sure of their IP addresses in the first place. Instead, most network devices will listen for requests to their specific IP addresses as well as a special range of IP addresses designed specifically to catch broadcast packets from other devices on their network (typically, that multicast IP address is going to be <kbd>255.255.255.255</kbd>, but not necessarily).</p>
<p>If you want to multicast a number of packets out of a single port from your host, you can do so simply by configuring your <kbd>UdpClient</kbd> instance to allow multiple clients to access an open port with the <span><kbd>ExclusiveAddressUse</kbd> boolean property. By setting that property</span> to <kbd>false</kbd>, you enable multiple <kbd>UdpClient</kbd> to leverage the same port at the same time, giving your application the ability to transmit messages to as many remote hosts as you have clients configured to interact with them.</p>
<p>Separately, if you want to listen for multicast packets, you can set up a <kbd>UdpClient</kbd> to be a part of a <kbd>MulticastGroup</kbd> by applying the appropriate <kbd>MulticastGroupOptions</kbd> settings to your client or socket. Doing so sets your client to listen along with any other registered listeners to packets being multicast by a single transmitting host.</p>
<p>As I said at the start of this section, multicasting and listening for multicast packets is an incredibly niche operation, and it's unlikely you'll find yourself needing to account for it in your daily work. As such, I won't be spending any more time on the subject. However, if you're curious, I would strongly encourage you to plumb the documentation for it online. For now, though, I just wanted to make sure you had at least some exposure to the concept and understood that there were features available to you in the <kbd>UdpClient</kbd> class that you could leverage to achieve or listen for multicast data transmission. For now, though, I think it's time we transition to a much more ubiquitously used transport layer protocol. And so, let's get our hands dirty with the internet protocol. It's time we explored IP.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This chapter served as a major paradigm shift for our understanding of network programming. We looked at how the responsibilities of the transport layer are wholly distinct from those of the application layer and we took an extremely close look at just what those transport layer responsibilities are. We learned that the <span><strong>Internet Engineering Task Force</strong> (</span><strong>IETF</strong>) has classified the various approaches to transport layer responsibilities based around the services and features a protocol might support, and how we can use those classifications to determine the best circumstances in which to employ a given transport layer protocol.</p>
<p>Next, we learned how connection-based protocols, such as TCP, use preliminary handshakes between clients and servers to establish an active connection, or session, between two hosts prior to the transmission of any data between the two. We saw how these sessions enable connection-based communication protocols to provide reliable interactions between the hosts, with substantial error-detection and error-correction support. Then we considered how connectionless protocols provide a number of advantages in their own right, including low-overhead and low-latency interactions between hosts over sufficiently reliable networks. Then we took a look at some of the strategies that can be employed by connectionless protocols, or the application layer protocols on top of them, to mitigate the unreliability of connectionless communication.</p>
<p>Finally, with this perspective in our minds, we were able to dive head-first into implementing both connection-based and connectionless clients and servers in C# and .NET using some incredibly simple libraries provided by the framework. We used a client and server designed to simulate interactions over TCP and UDP, and, in doing so, saw how the designers of .NET Core have conceptualized some of the characteristics of each protocol and implemented those characteristics in code. And now that we have such an in-depth understanding of both of these transport layer protocols, we're ready to fully examine the intricacies and nuances of the most ubiquitous transport layer protocol of all, the <strong>Internet Protocol</strong>. And that's exactly what we'll be doing in the next chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What are the four classifications of transport layer protocols?</li>
<li>What are the primary functions and responsibilities of transport layer protocols?</li>
<li>What is meant by a connection in connection-based communication modes?</li>
<li>What does TCP stand for? Why is it typically referred to as TCP/IP?</li>
<li>Describe the handshake process used to establish a connection over TCP.</li>
<li>What does UDP stand for? What are some of the advantages of UDP?</li>
<li>What are the biggest drawbacks of connectionless communication?</li>
<li>What is multicasting? What is broadcasting, and how is it enabled in UDP?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p><span>For additional information about TCP, UDP, and the transport layer generally, I recommend reading</span> <span><em>Understanding TCP/IP</em> by Alena Kabelová and Libor Dostálek, available from Packt Publishing at the following link:</span></p>
<p><a href="https://www.packtpub.com/networking-and-servers/understanding-tcpip">https://www.packtpub.com/networking-and-servers/understanding-tcpip</a>.</p>


            </article>

            
        </section>
    </body></html>