<html><head></head><body>
        

                            
                    <h1 class="header-title">Scaling Microservices</h1>
                
            
            
                
<p>Imagine you are part of a development and support team that is responsible for developing the company's flagship product—TaxCloud. TaxCloud helps taxpayers file their own taxes and charges them a small fee upon the successful filing of taxes. Consider you had developed this application using microservices. Now, say the product gets popular and gains traction, and suddenly, on the last day of tax filing, you get a rush of consumers wanting to use your product and file their taxes. However, the payment service of your system is slow, which has almost brought the system down, and all the new customers are moving to your competitor's product. This is a lost opportunity for your business.</p>
<p>Even though this is a fictitious scenario, it can very well happen to any business. In e-commerce, we have always experienced these kinds of things in real life, especially on special occasions such as Christmas and Black Friday. All in all, they point toward one major significant characteristic—the scalability of the system. Scalability is one of the most important non-functional requirements of any mission-critical system. Serving a couple of users with hundreds of transactions is not the same as serving millions of users with several million transactions. In this chapter, we will discuss scalability in general. We'll also discuss how to scale microservices individually, what to consider when we design them, and how to avoid cascading failure using different patterns. By the end of this chapter, you will have learned about:</p>
<ul class="ul-list">
<li>Horizontal scaling</li>
<li>Vertical scaling</li>
<li>The Scale Cube model of scalability</li>
<li>Scaling infrastructure using Azure scale sets and Docker Swarm</li>
<li>Scaling a service design through data model caching and response caching</li>
<li>The circuit breaker pattern</li>
<li>Service discovery</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Scalability overview</h1>
                
            
            
                
<p>Design decisions impact the scalability of a single microservice. As with other application capabilities, decisions that are made during the design and early coding phases largely influence the scalability of services.</p>
<p>Microservice scalability requires a balanced approach between services and their supporting infrastructures. Services and their infrastructures also need to to be scaled in harmony.</p>
<p>Scalability is one of the most important non-functional characteristics of a system as it can handle more payload. It is often felt that scalability is usually a concern for large-scale distributed systems. Performance and scalability are two different characteristics of a system. Performance deals with the throughput of the system, whereas scalability deals with serving the desired throughput for a larger number of users or a larger number of transactions.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Scaling infrastructure</h1>
                
            
            
                
<p>Microservices are modern applications and usually take advantage of the cloud. Therefore, when it comes to scalability, the cloud provides certain advantages. However, it is also about automation and managing costs. So even in the cloud, we need to understand how to provision infrastructure, such as virtual machines or containers, to successfully serve our microservices-based application even in the case of sudden traffic spikes.</p>
<p>Now we will visit each component of our infrastructure and see how we can scale it. The initial scaling up and scaling out methods are applied more to hardware scaling. With the Auto Scaling feature, you will understand Azure virtual manager scale sets. Finally, you will learn about scaling with containers in Docker Swarm mode. </p>


            

            
        
    

        

                            
                    <h1 class="header-title">Vertical scaling (scaling up)</h1>
                
            
            
                
<p><strong>Scaling up</strong> is a term used for achieving scalability by adding more resources to the same machine. It includes the addition of more memory or processors with higher speed or simply the migration of applications to a more powerful macOS.</p>
<p>With upgrades in hardware, there is a limit as to how you can scale the machine. It is more likely that you are just shifting the bottleneck rather than solving the real problem of improving scalability. If you add more processors to the machine, you might shift the bottleneck to memory. Processing power does not increase the performance of your system linearly. At a certain point, the performance of a system stabilizes even if you add more processing capacity. Another aspect of scaling up is that since only one machine is serving all the requests, it becomes a single point of failure as well.</p>
<p>In summary, scaling vertically is easy since it involves no code changes; however, it is quite an expensive technique. Stack Overflow is one of those rare examples of a .NET-based system that is scaled vertically.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Horizontal scaling (scaling out)</h1>
                
            
            
                
<p>If you do not want to scale vertically, you can always scale your system horizontally. Often, it is also referred to as <strong>scaling out</strong>. Google has really made this approach quite popular. The Google search engine is running out of inexpensive hardware boxes. So, despite being a distributed system, scaling out helped Google in its early days expand its search process in a short amount of time while being inexpensive. Most of the time, common tasks are assigned to worker machines and their output is collected by several machines doing the same task. This kind of arrangement also survives through failures. To scale out, load balancing techniques are useful. In this arrangement, a load balancer is usually added in front of all the clusters of the nodes. So, from a consumer perspective, it does not matter which machine/box you are hitting. This makes it easy to add capacity by adding more servers. Adding servers to clusters improves scalability linearly.</p>
<p>Scaling out is a successful strategy when the application code does not depend on the server it is running on. If the request needs to be executed on a specific server, that is, if the application code has server affinity, it will be difficult to scale out. However, in the case of stateless code, it is easier to get that code executed on any server. Hence, scalability is improved when a stateless code is run on horizontally scaled machines or clusters.</p>
<p>Due to the nature of horizontal scaling, it is a commonly used approach across the industry. You can see many examples of large scalable systems managed this way, for example, Google, Amazon, and Microsoft. We recommend that you scale microservices in a horizontal fashion as well.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Microservice scalability</h1>
                
            
            
                
<p>In this section, we will review the scaling strategies available for microservices. We will look at the Scale Cube model of scalability, how to scale the infrastructure layer for microservices, and embed scalability in microservice design.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Scale Cube model of scalability</h1>
                
            
            
                
<p>One way to look at scalability is by understanding Scale Cube. In the book <em>The Art of Scalability: Scalable Web Architecture, Processes, and Organizations for the Modern Enterprise</em>, Martin L. Abbott and Michael T. Fisher define Scale Cube as viewing and understanding system scalability. Scale Cube applies to microservice architectures as well:</p>
<div><img class="image-border" height="409" src="img/2d5760c4-f65e-462d-8133-ddf0ba025fe9.png" width="445"/></div>
<p>In this three-dimensional model of scalability, the origin (0,0,0) represents the least scalable system. It assumes that the system is a monolith deployed on a single server instance. As shown, a system can be scaled by putting the right amount of effort into three dimensions. To move a system towards the right scalable direction, we need the right trade-offs. These trade-offs will help you gain the highest scalability for your system. This will help your system cater to increasing customer demand. This is signified by the <strong>Scale Cube</strong> model. Let's look into every axis of this model and discuss what they signify in terms of microservice scalability.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Scaling of x axis</h1>
                
            
            
                
<p>Scaling over the <em>x </em>axis means running multiple instances of an application behind a load balancer. This is a very common approach used in monolithic applications. One of the drawbacks of this approach is that any instance of an application can utilize all the data available for the application. It also fails to address application complexity.</p>
<p>Microservices should not share a global state or a kind of data store that can be accessed by all the services. This will create a bottleneck and a single point of failure. Hence, approaching microservice scaling merely over the <em>x </em>axis of Scale Cube would not be the right approach.</p>
<p>Now let's look at <em>z </em>axis scaling. We have skipped <em>y </em>axis scaling for a reason.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Scaling of z axis </h1>
                
            
            
                
<p>The <em>z </em>axis scaling is based on a split, which is based on the customer or requestor of a transaction. While <em>z </em>axis splits may or may not address the monolithic nature of instructions, processes, or code, they very often do address the monolithic nature of the data necessary to perform these instructions, processes, or code. Naturally, in <em>z </em>axis scaling, there is one dedicated component responsible for applying the bias factor. The bias factor might be a country, request origin, customer segment, or any form of subscription plan associated with the requestor or request. Note that <em>z </em>axis scaling has many benefits, such as improved isolation and caching for requests; however, it also suffers from the following drawbacks:</p>
<ul class="ul-list">
<li>It has increased application complexity.</li>
<li>It needs a partitioning scheme, which can be tricky especially if we ever need to repartition data.</li>
<li>It doesn't solve the problems of increasing development and application complexity. To solve these problems, we need to apply <em>y </em>axis scaling.</li>
</ul>
<p>Due to the preceding nature of <em>z </em>axis scaling, it is not suitable for use in the case of microservices.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Scaling of y axis </h1>
                
            
            
                
<p>The <em>y </em>axis scaling is based on a functional decomposition of an application into different components. The <em>y </em>axis of Scale Cube represents the separation of responsibility by the role or type of data, or work performed by a certain component in a transaction. To split the responsibility, we need to split the components of the system as per their actions or roles performed. These roles might be based on large portions of a transaction or a very small one. Based on the size of the roles, we can scale these components. This splitting scheme is referred to as <em>service or resource-oriented splits</em>.</p>
<p>This very much resembles what we see in microservices. We split the entire application based on its roles or actions, and we scale individual microservice as per its role in the system. This resemblance is not accidental; it is the product of the design. So we can fairly say that <em>y </em>axis scaling is quite suitable for microservices.</p>
<p>Understanding <em>y </em>axis scaling is very significant for scaling a microservice architecture-based system. So, effectively, we are saying that microservices can be scaled by splitting them as per their roles and actions. Consider an order management system that is designed to, say, meet certain initial customer demand; for this, splitting the application into services such as customer service, order service, and payment service will work fine. However, if demand increases, you would need to review the existing system closely. You might discover the sub-components of an already existing service, which can very well be separated again since they are performing a very specific role in that service and the application as a whole. This revisit to design with respect to increased demand/load may trigger the re-splitting of the order service into a quote service, order processing service, order fulfillment service, and so on. Now, a quote service might need more computing power, so we might push more instances (identical copies behind it) when compared to other services.</p>
<p>This is a near real-world example of how we should scale microservices on the AFK Scale Cube's three-dimensional model. You can observe this kind of three-dimensional scalability and <em>y </em>axis scaling of services in some well-known microservice architectures that belong to the industry, such as Amazon, Netflix, and Spotify.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Characteristics of a scalable microservice</h1>
                
            
            
                
<p>In the Scale Cube section, we largely focused on scaling the characteristics of an entire system or application. In this section, we will focus on scaling the characteristics of an individual microservice. A microservice is said to be scalable and performant when it exhibits the following major characteristics:</p>
<ul class="ul-list">
<li class="">Known growth curve: For example, in the case of an order management system, we need to know how many orders are supported by the current services and how they are proportionate to the order fulfillment service metric (measured in <em>requests per seconds</em>). The currently measured metrics are called <strong>baseline figures</strong>.</li>
<li>Well-studied usage metrics: The traffic pattern generally reveals customer demand, and based on customer demand, many parameters mentioned in the previous sections regarding microservices can be calculated. Hence, microservices are instrumented, and monitoring tools are the necessary companions of microservices.</li>
<li class="">Effective use of infrastructure resources: Based on qualitative and quantitative parameters, the anticipation of resource utilization can be done. This will help the team predict the cost of infrastructure and plan for it.</li>
<li class="">Ability to measure, monitor, and increase the capacity using an automated infrastructure: Based on the operational and growth pattern of the resource consumption of microservices, it is very easy to plan for future capacity. Nowadays, with cloud elasticity, it is even more important to be able to plan and automate capacity. Essentially, cloud-based architecture is cost-driven architecture.</li>
<li>Known bottlenecks: Resource requirements include the specific resources (compute, memory, storage, and I/O) that each microservice needs. Identifying these are essential for a smoother operational and scalable service. If we identify resource bottlenecks, they can be worked on and eliminated.</li>
<li>Has dependency scaling in the same ratio: This is self-explanatory. However, you cannot just focus on a microservice, leaving its dependencies as bottlenecks. A microservice is as scalable as its least scaling dependency.</li>
<li>Fault tolerant and highly available: Failure is inevitable in distributed systems. If you encounter a microservice instance failure, it should be automatically rerouted to a healthy instance of the microservice. Just putting load balancers in front of microservice clusters won't be sufficient in this case. Service discovery tools are quite helpful for satisfying this characteristic of scalable microservices.</li>
<li class="">Has a scalable data persistence mechanism: Individual data store choices and design should be scalable and fault-tolerant for scalable microservices. Caching and separating out read and write storage will help in this case.</li>
</ul>
<p>Now, while we are discussing microservices and scalability, the natural arrangement of scaling comes into the picture, which is nothing but the following:</p>
<ul class="ol-list">
<li>Scaling the infrastructure: Microservices operate well over dynamic and software-defined infrastructure. So, scaling the infrastructure is an essential component of scaling microservices.</li>
<li>Scaling around service design: Microservice design comprises of an HTTP-based API as well as a data store in which the local state for the services is stored.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Scaling the infrastructure</h1>
                
            
            
                
<p>In this section, we will visit all the layers of the microservice infrastructure and see them in relation to each other, that is, how each individual infrastructure layer can be scaled. In our microservice implementation, there are two major components. One is virtual machines and the other is the container hosted on the virtual or physical machine. The following diagram shows a logical view of the microservice infrastructure:</p>
<div><img class="image-border" height="357" src="img/55de4676-81c0-41bd-82d4-1c02be78f4f0.png" width="488"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Scaling virtual machines using scale sets</h1>
                
            
            
                
<p>Scaling virtual machines is quite simple and easy in Azure Cloud. This is where microservices shine through. With scale sets, you can raise the instances of the same virtual machine images in a short amount of time, and automatically too, based on the ruleset. Scale sets are integrated with Azure Autoscale.</p>
<p>Azure virtual machines can be created in such a way so that as a group, they always serve the requests even if the volume of the requests increases. In specific situations, they can also be deleted automatically if those virtual machines are not needed to perform the workload. This is taken care of by the virtual machine scale set. </p>
<p>Scale sets also integrate well with load balancers in Azure. Since they are represented as compute resources, they can be used with Azure's Resource Manager. Scale sets can be configured so that virtual machines can be created or deleted on demand. This helps manage virtual machines with the mindset of <kbd>pets vs. cattle</kbd>, which we saw earlier in the chapter in terms of deployment.</p>
<p>For applications that need to scale compute resources in and out, scale operations are implicitly balanced across the fault and update domains.</p>
<p>With scale sets, you don't need to correlate loops of independent resources, such as NICs, storage accounts, and virtual machines. Even while scaling out, how are we going to take care of the availability of these virtual managers? All such concerns and challenges have already been addressed with virtual machine scale sets.</p>
<p>A scale set allows you to automatically grow and shrink an application based on demand. Let's say there's a threshold of 40% utilization. So, maybe once we reach 40% utilization, we'll begin to experience performance degradation. And at 40% utilization, new web servers get added. A scale set allows you to set a rule, as mentioned in the previous sections. An input to a scale set is a virtual machine. The rules on a scale set say that at 40% average CPU, for five minutes, Azure will add another virtual machine to the scale set. After doing this, calibrate the rule again. If the performance is still above 40%, add a third virtual machine until it reaches the acceptable threshold. Once the performance drops below 40%, it will start deleting these virtual machines based on traffic inactivity and so on to reduce the cost of operation.</p>
<p>So by implementing a scale set, you can construct a rule for the performance and make your application bigger to handle the greater load by simply automatically adding and removing virtual machines. You, as the administrator, will be left with nothing to do once these rules are established.</p>
<p>Azure Autoscale measures performance and determines when to scale up and down. It is also integrated with the load balancer and NAT. Now, the reason they're integrated with the load balancer and with NAT is because as we add these additional virtual machines, we're going to have a load balancer and a NAT device in front. As requests keep coming in, in addition to deploying the virtual machine, we've got to add a rule that allows traffic to be redirected to the new instances. The great thing about scale sets is that they not only add virtual machines but also work with all the other components of the infrastructure, including things such as network load balancers.</p>
<p>In the Azure Portal, a scale set can be viewed as a single entry, even though it has multiple virtual machines included in it. To look at the configuration and specification details of virtual machines in a scale set, you will have to use the Azure Resource Explorer tool. It's a web-based tool available at <a href="https://resources.azure.com">https://resources.azure.com</a>. Here you can view all the objects in your subscription. You can view scale sets in the Microsoft.Compute section.</p>
<p>Building a scale set is very easy using the Azure templates repository. Once you create your own <strong>Azure Resource Manager</strong> (<strong>ARM</strong>) template, you can also create custom templates based on scale sets. Due to scope and space constraints, we have omitted a detailed discussion and instructions on how to build a scale set. You can follow these instructions by utilizing the ARM templates given at <a href="https://github.com/gbowerman/azure-myriad">https://github.com/gbowerman/azure-myriad</a>.</p>
<p>An availability set is an older technology, and this feature has limited support. Microsoft recommends that you migrate to virtual machine scale sets for faster and more reliable autoscale support.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Auto Scaling</h1>
                
            
            
                
<p>With the help of monitoring solutions, we can measure the performance parameters of an infrastructure. This is usually in the form of performance SLAs. Auto Scaling gives us the ability to increase or decrease the resources available to the system, based on performance thresholds.</p>
<p>The Auto Scaling feature adds additional resources to cater to increased load. It works in reverse, as well. If the load is reduced, then Auto Scaling reduces the number of resources available to perform the task. Auto Scaling does it all without pre-provisioning the resources, and does this in an automated way.</p>
<p>Auto Scaling can scale in both ways—vertically (adding more resources to the existing resource type) or horizontally (adding resources by creating another instance of that type of resource).</p>
<p>The Auto Scaling feature makes a decision regarding adding or removing resources based on two strategies. One is based on the available metrics of the resource or on meeting some system threshold value. The other type of strategy is based on time, for example, between 9 a.m. and 5 p.m. IST, instead of three web servers; the system needs 30 web servers.</p>
<p>Azure monitoring instruments every resource; all the metric-related data is collected and monitored. Based on the data collected, Auto Scaling makes decisions.</p>
<p>Azure Monitor autoscale applies only to virtual machine scale sets, cloud services, and app services (for example, web apps).</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Container scaling using Docker Swarm</h1>
                
            
            
                
<p>Earlier, in the chapter on deployment, we looked at how to package a microservice into a Docker container. We also discussed in detail why containerization is useful in the microservice world. In this section, we will advance our skills with Docker and also see how easily we can scale our microservices with Docker Swarm.</p>
<p>Inherently, microservices are distributed systems and need to be distributed and isolated resources. Docker Swarm provides container orchestration clustering capabilities so that multiple Docker engines can work as single virtual engines. This is similar to load balancer capabilities; besides, it also creates new instances of containers or deletes containers, if the need arises.</p>
<p>You can use any of the available service discovery mechanisms, such as DNS, consul, or zookeeper tools, with Docker Swarm.</p>
<p>A swarm is a cluster of Docker engines or nodes where you can deploy your microservices as <em>services</em>. Now, do not confuse these services with microservices. Services are a different concept in Docker implementation. A <strong>service</strong> is the definition of the tasks to execute on the worker nodes. You may want to understand the node we are referring to in the last sentence. The node, in Docker Swarm context, is used for the Docker engine participating in a cluster. A complete swarm demo is possible, and ASP.NET Core images are available in the ASP.NET-Docker project on GitHub (<a href="https://github.com/aspnet/aspnet-docker">https://github.com/aspnet/aspnet-docker</a>).</p>
<p>The Azure Container Service has recently been made available. It is a good solution for scaling and orchestrating Linux or Windows containers using DC/OS, Docker Swarm, or Google Kubernetes.</p>
<p>Now that we have understood how to scale a microservice infrastructure, let's revisit the scalability aspects of microservice design in the following sections.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Scaling service design</h1>
                
            
            
                
<p class="mce-root">In these sections, we will look at the components/concerns that need to be taken care of while designing or implementing a microservice. With infrastructure scaling taking care of service design, we can truly unleash the power of the microservice's architecture and get a lot of business value in terms of making a microservice a true success story. So, what are the components in service design? Let's have a look.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Data persistence model design</h1>
                
            
            
                
<p>In traditional applications, we have always relied on relational databases to persist user data. Relational databases are not new to us. They emerged in the 70s as a way of storing persistent information in a structured way that would allow you to make queries and perform data maintenance.</p>
<p>In today's world of microservices, modern applications need to be scaled at the hyperscale stage. We are not recommending here that you abandon the use of relational databases in any sense. They still have their valid use cases. However, when we mix read and write operations in a single database, complications arise where we need to have increased scalability. Relational databases enforce relationships and ensure the consistency of data. Relational databases work on the well-known ACID model. So, in relational databases, we use the same data model for both read and write operations.</p>
<p>However, the needs of read and write operations are quite different. In most cases, read operations usually have to be quicker than write operations. Read operations can also be done using different filter criteria, returning a single row or a result set. In most write operations, there is a single row or column involved, and usually, write operations take a bit longer when compared to read operations. So, we can either optimize and serve reads or optimize and serve writes in the same data model.</p>
<p>How about we split the fundamental data model into two halves: one for all the read operations and the other for all the write operations? Now things become far simpler, and it is easy to optimize both the data models with different strategies. The impact of this on our microservices is that they, in turn, become highly scalable for both kinds of operations.</p>
<p>This particular architecture is known as <strong>Common Query Responsibility Segregation</strong> (<strong>CQRS</strong>). As a natural consequence, CQRS also gets extended in terms of our programming model. Now, the database-object relationship between our programming models has become much simpler and more scalable.</p>
<p>With this comes the next fundamental element in scaling a microservice implementation: the caching of data.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Caching mechanism</h1>
                
            
            
                
<p>Caching is the simplest way to increase the application's throughput. The principle is very easy. Once the data is read from data storage, it is kept as close as possible to the processing server. In future requests, the data is served directly from the data storage or cache. The essence of caching is to minimize the amount of work that a server has to do. HTTP has a built-in cache mechanism embedded in the protocol itself. This is the reason it scales so well.</p>
<p>With respect to microservices, we can cache at three levels, namely client side, proxy, and server side. Let's look at each of them.</p>
<p>First, we have client-side caching. With client-side caching, clients store cached results. So the client is responsible for doing the cache invalidation. Usually, the server provides guidance, using mechanisms, such as cache control and expiry headers, about how long it can keep the data and when it can request fresh data. With browsers supporting HTML5 standards, there are more mechanisms available, such as local storage, an application cache, or a web SQL database, in which the client can store more data.</p>
<p>Next, we move onto the proxy side. Many reverse proxy solutions, such as Squid, HAProxy, and NGINX, can act as cache servers as well.</p>
<p>Now let's discuss server-side caching in detail. In server-side caching, we have the following two types:</p>
<ul class="ol-list">
<li>Response caching: This is an important kind of caching mechanism for a web application UI, and honestly, it is simple and easy to implement as well. In response to caching, cache-related headers get added to the responses served from microservices. This can drastically improve the performance of your microservice. In ASP.NET Core, you can implement response caching using the <kbd>Microsoft.AspNetCore.ResponseCaching</kbd> package.</li>
<li>Distributed caching for persisted data: A distributed cache enhances microservice throughput due to the fact that the cache will not require an I/O trip to any external resource. This has the following advantages:<br/>
<ul class="ul-list">
<li>Microservice clients get the exact same results.</li>
<li>The distributed cache is backed up by a persistence store and runs as a different remote process. So even if the app server restarts or has any problems, it in no way affects the cache.</li>
<li>The source's data store has fewer requests made to it.</li>
</ul>
</li>
</ul>
<p>You can use distributed providers, such as CacheCow, Redis (for our book <em>Azure Redis Cache</em>), or Memcache, in a clustered mode for scaling your microservice implementation.</p>
<p>In the following section, we will provide an overview of CacheCow and Azure Redis Cache.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">CacheCow</h1>
                
            
            
                
<p>CacheCow comes into the picture when you want to implement HTTP caching on both the client and server. This is a lightweight library, and currently, ASP.NET Web API support is available. CacheCow is open source and comes with an MIT license that is available on GitHub (<a href="https://github.com/aliostad/CacheCow">https://github.com/aliostad/CacheCow</a>)<a href="https://github.com/aliostad/CacheCow">.</a></p>
<p>To get started with CachCow, you need to get ready for both the server and client. The important steps are:</p>
<ul>
<li>Install the <kbd>Install-Package CacheCow.Server</kbd> NuGet package within your ASP.NET Web API project; this will be your server.</li>
<li>Install the <kbd>Install-Package CacheCow.Client</kbd>  NuGet package within your client project; the client application will be WPF, Windows Form, Console, or any other web application.</li>
<li>Create a cache store. You need to create a cache store at the server side that requires a database for storing cache metadata (<a href="https://github.com/aliostad/CacheCow/wiki/Getting-started#cache-store">https://github.com/aliostad/CacheCow/wiki/Getting-started#cache-store</a>).</li>
</ul>
<p>If you want to use memcache, refer to <a href="https://github.com/aliostad/CacheCow/wiki/Getting-started">https://github.com/aliostad/CacheCow/wiki/Getting-started</a> for more information.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Azure Redis Cache</h1>
                
            
            
                
<p>Azure Redis Cache is built on top of an open source called <strong>Redis</strong> (<a href="https://github.com/antirez/redis">https://github.com/antirez/redis</a>), which is an in-memory database and persists on a disk. As per Microsoft (<a href="https://azure.microsoft.com/en-in/services/cache/">https://azure.microsoft.com/en-in/services/cache/</a>):</p>
<p>"Azure Redis Cache is based on the popular open source Redis cache. It gives you access to a secure, dedicated Redis cache, managed by Microsoft and accessible from any application within Azure."</p>
<p>Getting started with Azure Redis Cache is very simple with the help of these steps:</p>
<ol>
<li>Create a web API project—refer to our code example in <a href="047f5d0b-a008-48e2-9c7f-c57c16e671f9.xhtml">Chapter 2</a>, <em>Implementing Microservices</em>.</li>
<li>Implement Redis—for a referral point use <a href="https://github.com/StackExchange/StackExchange.Redis">https://github.com/StackExchange/StackExchange.Redis</a> and install the <kbd>Install-Package StackExchange.Redis</kbd> NuGet package.</li>
<li>Update your config file for <kbd>CacheConnection</kbd> (<a href="https://docs.microsoft.com/en-us/azure/redis-cache/cache-web-app-howto#configure-the-application-to-use-redis-cache">https://docs.microsoft.com/en-us/azure/redis-cache/cache-web-app-howto#configure-the-application-to-use-redis-cache</a>).</li>
<li>Then publish on Azure (<a href="https://docs.microsoft.com/en-us/azure/redis-cache/cache-web-app-howto#publish-the-application-to-azure">https://docs.microsoft.com/en-us/azure/redis-cache/cache-web-app-howto#publish-the-application-to-azure</a>).</li>
</ol>
<p>You can also use this template to create Azure Redis Cache:<br/>
 <a href="https://github.com/Azure/azure-quickstart-templates/tree/master/201-web-app-redis-cache-sql-database">https://github.com/Azure/azure-quickstart-templates/tree/master/201-web-app-redis-cache-sql-database<br/>
<br/></a> For complete details on Azure Redis Cache refer to this URL:<br/>
 <a href="https://docs.microsoft.com/en-us/azure/redis-cache/">https://docs.microsoft.com/en-us/azure/redis-cache/</a></p>


            

            
        
    

        

                            
                    <h1 class="header-title">Redundancy and fault tolerance</h1>
                
            
            
                
<p>We understand that a system's ability to deal with failure and recover from failure is not the same as that offered by scalability. However, we cannot deny that they are closely related abilities in terms of the system. Unless we address the concerns of availability and fault tolerance, it will be challenging to build highly scalable systems. In a general sense, we achieve availability by making redundant copies available to different parts/components of the system. So, in the upcoming section, we will touch upon two such concepts.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Circuit breakers</h1>
                
            
            
                
<p>A circuit breaker is a safety feature in an electronic device that, in the event of a short circuit, breaks the electricity flow and protects the device, or prevents any further damage to the surroundings. This exact idea can be applied to software design. When a dependent service is not available or not in a healthy state, a circuit breaker prevents calls from going to that dependent service and redirects the flow to an alternate path for a configured period of time.</p>
<p>In his famous book, <em>Release It! Design and Deploy Production-Ready Software</em>, Michael T. Nygard gives details about the circuit breaker. A typical circuit breaker pattern is shown in the following diagram:</p>
<div><img class="image-border" height="429" src="img/83e7a3c1-1650-48ec-b0a7-93ba4e863052.png" width="414"/></div>
<p>As shown in the diagram, the circuit breaker acts as a state machine with three states.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Closed state</h1>
                
            
            
                
<p>This is the initial state of the circuit, which depicts a normal flow of control. In this state, there is a failure counter. If <kbd>OperationFailedException</kbd> occurs in this flow, the failure counter is increased by <kbd>1</kbd>. If the failure counter keeps increasing, meaning the circuit encounters more exception, and reaches the failure threshold set, the circuit breaker transitions to an <em>Open</em> state. But if the calls succeed without any exception or failure, the failure count is reset.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Open state</h1>
                
            
            
                
<p>In the <em>Open</em> state, a circuit has already tripped and a timeout counter has started. If a timeout is reached and a circuit still keeps on failing, the flow of code enters into the Half-Open state.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Half-Open state</h1>
                
            
            
                
<p>In the Half-Open state, the state machine/circuit breaker component resets the timeout counter and again tries to open the circuit, reinitiating the state change to the Open state. However, before doing so, it tries to perform regular operations, say a call to the dependency; if it succeeds, then instead of the Open state, the circuit breaker component changes the state to Closed. This is so that the normal flow of the operation can happen, and the circuit is closed again.</p>
<p>For .NET-based microservices, if you want to implement the circuit breaker and a couple of fault-tolerant patterns, there is a good library named <em>Polly</em> available in the form of a NuGet package. It comes with extensive documentation and code samples, and moreover, has a fluent interface. You can add <em>Polly</em> from <a href="http://www.thepollyproject.org/">http://www.thepollyproject.org/</a> or by just issuing the <kbd>install--Package Polly</kbd> command from the package manager console in Visual Studio.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Service discovery</h1>
                
            
            
                
<p>For a small implementation, how can you determine the address of a microservice? For any .NET developer, the answer is that we simply put the IP address and port of service in the configuration file and we are good. However, when you deal with hundreds or thousands of them dynamically, configured at runtime, you have a service location problem.</p>
<p>Now if you peek a bit deeper, we are trying to solve two parts of the problem:</p>
<ul class="ol-list">
<li>Service registration: This is the process of registration within the central registry of some kind, where all the service-level metadata, host lists, ports, and secrets are stored.</li>
<li class="">Service discovery: Establishing communication at runtime with a dependency through a centralized registry component is service discovery.</li>
</ul>
<p>Any service registration and discovery solution needs to have the following characteristics to make it considerable as a solution for the microservice services discovery problem:</p>
<ul class="ul-list">
<li>The centralized registry itself should be highly available</li>
<li class="">Once a specific microservice is up, it should receive the requests automatically</li>
<li class="">Intelligent and dynamic load balancing capabilities should exist in the solution</li>
<li class="">The solution should be able to monitor the capability of the service health status and the load it is subjected to</li>
<li class="">The service discovery mechanism should be capable of diverting the traffic to other nodes or services from unhealthy nodes, without any downtime or impact on its consumers</li>
<li class="">If there is a change in the service location or metadata, the service discovery solution should be able to apply the changes without impacting the existing traffic or service instances</li>
</ul>
<p>Some of the service discovery mechanisms are available within the open source community. They are as follows:</p>
<ul class="ul-list">
<li>Zookeeper: Zookeeper (<a href="http://zookeeper.apache.org/">http://zookeeper.apache.org/</a>) is a centralized service for maintaining configuration information and naming, providing distributed synchronization, and providing group services. It's written in Java, is strongly consistent (CP), and uses the Zab (<a href="http://www.stanford.edu/class/cs347/reading/zab.pdf">http://www.stanford.edu/class/cs347/reading/zab.pdf</a>) protocol to coordinate changes across the ensemble (cluster).</li>
<li class="">Consul: Consul makes it simple for services to register themselves and discover other services via a DNS or HTTP interface. It registers external services, such as SaaS providers, as well. It also acts as a centralized configuration store in the form of key values. It also has failure detection properties. It is based on the peer-to-peer gossip protocol.</li>
<li class="">Etcd: Etcd is a highly available key-value store for shared configuration and service discovery. It was inspired by Zookeeper and Doozer. It's written in Go, uses Raft (<a href="https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf">https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf</a>) for consensus, and has an HTTP-plus JSON-based API.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>Scalability is one of the critical advantages of pursuing the microservice architectural style. We looked at the characteristics of microservice scalability. We discussed the Scale Cube model of scalability and how microservices can scale on the <em>y </em>axis via functional decomposition of the system. Then we approached the scaling problem with the scaling infrastructure. In the infrastructure segment, we looked at the strong capability of Azure Cloud to scale, utilizing the Azure scale sets and container orchestration solutions, such as Docker Swarm, DC/OS, and Kubernetes.</p>
<p>In the later stages of the chapter, we focused on scaling with a service design and discussed how our data model should be designed. We also discussed certain considerations, such as having a split CQRS style model, while designing the data model for high scalability. We also briefly touched on caching, especially distributed caching, and how it improves the throughput of the system. In the last section, to make our microservices highly scalable, we discussed the circuit breaker pattern and service discovery mechanism, which are essential for the scalability of microservice architecture.</p>
<p>In the next chapter, we will look at the reactive nature of microservices and the characteristics of reactive microservices.</p>


            

            
        
    </body></html>