<html><head></head><body>
		<div><h1 id="_idParaDest-201" class="chapter-number"><a id="_idTextAnchor205"/>12</h1>
			<h1 id="_idParaDest-202"><a id="_idTextAnchor206"/>Performance Optimization and Tuning</h1>
			<p>Performance separates applications that are used and loved from those that are uninstalled and forever forgotten.</p>
			<p>It is not enough to have an application that responds to users’ needs. To be used frequently (and likely daily), an application needs to bootstrap and perform tasks quickly.</p>
			<p>This speed and responsiveness directly impact user satisfaction, as people have increasingly high expectations for digital experiences. Studies have shown that even small delays in load times or task completion can significantly reduce user engagement and overall satisfaction.</p>
			<p>In this chapter, we will discuss different areas where performance can be improved and what techniques we can use to achieve this. More specifically, we will cover the following:</p>
			<ul>
				<li>The different areas to be considered to improve application performance</li>
				<li>How to instrument an application to identify performance problems</li>
				<li>How to improve your application performance</li>
			</ul>
			<h1 id="_idParaDest-203"><a id="_idTextAnchor207"/>Technical requirements</h1>
			<p>The code for this chapter can be found in the GitHub repository accompanying this book, <a href="https://github.com/PacktPublishing/Building-CLI-Applications-with-C-Sharp-and-.NET/tree/main/Chapter12">https://github.com/PacktPublishing/Building-CLI-Applications-with-C-Sharp-and-.NET/tree/main/Chapter12</a>.</p>
			<h1 id="_idParaDest-204"><a id="_idTextAnchor208"/>Performance optimization areas</h1>
			<p>The <a id="_idIndexMarker452"/>performance of an application is not only a matter of code. It is a series of fine tuning, at different levels, that helps achieve performance efficiency.</p>
			<p>Performance optimization therefore occurs in areas such as the following:</p>
			<ul>
				<li><strong class="bold">Application design and architecture</strong>: The longer the path you must walk, the longer<a id="_idIndexMarker453"/> it takes you to get to your destination. As I always tell my customers, you may run twice as fast as me, but if your path is twice as long as mine, we will arrive at our destination at the same time. The idea here is that using performant frameworks and libraries is of little use if your architecture is not efficient. Too often, I see architectures that are over-decoupled, with too many hops and context switching, leading to applications that are not performant and slow. The key is to build an architecture that balances performance with the optimal level of decoupling. From a design perspective, I often see designs that can be improved (leading to more efficient and performant applications) by finding a shorter path to achieve a goal. Of course, you won’t do that for every piece of code, but you will want to focus your attention on hot paths, namely paths that are often used by your users. There may be no point in optimizing the performance of a functionality that is used by one user once a year. You must find a balance between the cost of the optimization effort (it takes time, so it has a cost) and the benefit you are expecting from it.</li>
				<li><strong class="bold">Infrastructure</strong>: If <a id="_idIndexMarker454"/>we host the application on an infrastructure, we must ensure that this infrastructure is efficient and has been optimized to maximize the throughput of the application while minimizing its latency. However, in the context of CLI applications, the application runs on the user’s computer, so we might be tempted to say that there is nothing to do here, but we would be wrong! There are tuning tasks we can perform that will positively impact performance. For example, we can reduce resource utilization so that running the application on the user’s computer will consume the least amount of resources and therefore will be executed efficiently, even if the computer is running other applications side by <a id="_idIndexMarker455"/>side or has little horsepower.</li>
				<li><strong class="bold">Frameworks and libraries</strong>: Of <a id="_idIndexMarker456"/>course, using efficient and performant frameworks and libraries helps to improve the application’s performance. For example, every new release of .NET promises better performance. Hence, upgrading the .NET version can be an easy way to improve the performance of our application. The same goes for the libraries that we use: some are known to have better performance than others.</li>
				<li><strong class="bold">Coding practices</strong>: The<a id="_idIndexMarker457"/> last piece of the puzzle is the coding practices. We have already mentioned hot spots and hot paths, but coding practices also include using the most appropriate data structures.</li>
			</ul>
			<p>Before we start optimizing our application’s performance, we need to instrument it and identify its hot spots and hot paths.</p>
			<h1 id="_idParaDest-205"><a id="_idTextAnchor209"/>Instrumenting .NET applications</h1>
			<p>Multiple tools <a id="_idIndexMarker458"/>exist to help us instrument.NET applications. The main difference between these tools is their scope of action.</p>
			<p>Nevertheless, a key benefit of instrumentation is the ability to detect memory leaks and identify slow code paths.</p>
			<p>Instrumentation can be achieved both during the development phase and continuously, while the application is running in production.</p>
			<table id="table001-2" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Development-time profiling</strong></p>
						</td>
						<td class="No-Table-Style">
							<p>Visual Studio Diagnostic Tools, <code>BenchmarkDotNet</code>, <code>dotTrace</code>, <code>dotMemory</code>, and <code>PerfView</code> are great for profiling CPU, memory leaks and allocation, and application performance.</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Production-time monitoring</strong></p>
						</td>
						<td class="No-Table-Style">
							<p>Azure Application Insights, AppDynamics, and New Relic help monitor and diagnose performance issues in real time in production environments.</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 12.1 – Some popular instrumentation tools</p>
			<p>You may <a id="_idIndexMarker459"/>have noticed the terms <em class="italic">“profiling”</em> and <em class="italic">“monitoring.”</em> There are some key differences between them:</p>
			<ul>
				<li>Profiling provides a detailed, granular view of an application’s performance, often focusing on specific code sections or methods. This includes CPU usage per functionality or method, memory allocation, execution time, and method call frequency and duration.</li>
				<li>Monitoring is usually done in production and provides an overview of the application’s health, looking at broader performance trends and operational data over time rather than focusing on individual code paths. This includes CPU and memory usage across the entire application, error rates (exceptions, failures), response times and throughput (e.g., how long requests take, how many requests per second), and the application’s resource usage (disk I/O, network usage, etc.).</li>
			</ul>
			<p>Because a CLI application runs on the user’s computer, it may be harder to monitor it. It requires permission from the user to collect the necessary data, usually at frequent intervals. We may then expect the user to refuse to share telemetry data, and therefore monitoring may not be possible.</p>
			<p>While it is important to know the tools that help us instrument our applications, it is equally important to understand where to use them, in other words, how to identify these areas that may be good candidates for performance optimization. In that regard, it is important to be able to identify hot spots and hot paths.</p>
			<h2 id="_idParaDest-206"><a id="_idTextAnchor210"/>Hot spots versus hot paths</h2>
			<p>This is not the <a id="_idIndexMarker460"/>first time in this chapter that I have mentioned hot spots and hot paths. However, I haven’t taken the time to explain them. Let’s fix this right away!</p>
			<p>A <strong class="bold">hot spot</strong> is an <a id="_idIndexMarker461"/>area of intense activity in the code, typically referring to frequently executed methods that consume a significant amount of execution time. Therefore, a hot spot represents<a id="_idIndexMarker462"/> a potential optimization target for improving the overall performance of the application.</p>
			<p>A <strong class="bold">hot path</strong> refers<a id="_idIndexMarker463"/> to an execution path through the code that is frequently taken and therefore contributes <em class="italic">significantly</em> to the application’s runtime. Hot paths can help locate inefficiently used resources, such as memory usage and allocation.</p>
			<p>The question that may arise here is <em class="italic">“what process can we follow to identify the application’s hot spots and </em><em class="italic">hot paths?”</em></p>
			<h2 id="_idParaDest-207"><a id="_idTextAnchor211"/>Identifying the application’s hot spots and hot paths</h2>
			<p>Fortunately, identifying an <a id="_idIndexMarker464"/>application’s hot spots and hot paths does not have to be done by shots in the dark. Instead, we can follow a structured process that consists of three steps: profiling, analysis, and optimization. If monitoring is implemented, it will serve as an input for that process, since this process should be performed periodically to ensure optimal performance of the application.</p>
			<p>The process is described in the following table:</p>
			<table id="table002" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Step</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">What </strong><strong class="bold">to do</strong></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<ol>
								<li>Profiling and data collection</li>
							</ol>
						</td>
						<td class="No-Table-Style">
							<ul>
								<li>Use performance profilers to gather data about your application’s execution. A library such as <code>BenchmarkDotNet</code> can collect detailed information about CPU usage, memory consumption, and execution times.</li>
								<li>Collect metrics on method execution times, resource usage, and frequency of calls to identify performance bottlenecks.</li>
							</ul>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<ol>
								<li value="2">Analysis and identification</li>
							</ol>
						</td>
						<td class="No-Table-Style">
							<ul>
								<li>Analyze the profiler output and find:<ul><li>Methods with high execution times</li><li>Frequently called methods</li><li>Areas of high CPU or memory usage</li><li>Long-running database queries or I/O operations</li></ul></li>
								<li>Look for patterns in the data that indicate potential hot spots or hot paths:<ul><li>Methods that consume a disproportionate amount of resources</li><li>Execution paths that are frequently taken and contribute significantly to the overall runtime</li></ul></li>
							</ul>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<ol>
								<li value="3">Optimization</li>
							</ol>
						</td>
						<td class="No-Table-Style">
							<ul>
								<li>Once hot spots and hot paths are identified, implement optimizations targeting these areas.</li>
								<li>Use benchmarking tools such as <code>BenchmarkDotNet</code> to measure and compare the performance of the code before and after optimization to assess the gain in performance. You may also measure and compare different implementations to identify the most optimal one.</li>
							</ul>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 12.2 – Identifying hot spots and hot paths</p>
			<p>We mentioned that <code>BenchmarkDotNet</code> can help us profile our application. Then, it is time to<a id="_idIndexMarker465"/> learn how to use it.</p>
			<h2 id="_idParaDest-208"><a id="_idTextAnchor212"/>Profiling Bookmarkr with BenchmarkDotNet</h2>
			<p>Although <code>BenchmarkDotNet</code> is considered to be a benchmarking library (that is, it is used<a id="_idIndexMarker466"/> to compare different implementation alternatives against a baseline to identify which one is the most performant), when used strategically, it can also<a id="_idIndexMarker467"/> identify hot spots and hot paths in our code.</p>
			<p>Let’s see how we can leverage this library to profile our CLI application.</p>
			<p>The first thing we <a id="_idIndexMarker468"/>need to do is to reference the <code>BenchmarkDotNet</code> library. This can be achieved by executing the following command:</p>
			<pre class="console">
dotnet add package BenchmarkDotNet</pre>			<p>The next step is to configure benchmark collection and reporting. For that matter, let’s add the following block of code at the very beginning of the <code>Main</code> method:</p>
			<pre class="source-code">
if(args.Length &gt; 0 &amp;&amp; args[0].ToLower() == "benchmark")
{
    BenchmarkRunner.Run&lt;Benchmarks&gt;();
    return 0;
}</pre>			<p>This allows us to run the benchmarks if we execute the application and pass <code>benchmark</code> as a parameter.</p>
			<p>What this block of code does is ask <code>BenchmarkDotNet</code> (via the <code>BenchmarkRunner</code> class) to run all the benchmarks that will be found in the <code>Benchmarks</code> class.</p>
			<p>Let’s create that <code>Benchmarks</code> class!</p>
			<p>Following the folder structure convention that we defined in previous chapters, we will create a <code>Benchmarks</code> folder within which we will create a <code>Benchmarks.cs</code> file.</p>
			<p>We can either have a single class where all benchmarks are located, or we can create one benchmark class for every command or service to be benchmarked. We will take the first approach in this chapter as we will only benchmark the <code>export</code> command.</p>
			<p>Let’s add our <a id="_idIndexMarker469"/>first benchmark method. Its code looks like this:</p>
			<pre class="source-code">
public async Task ExportBookmarks()
{
    var exportCmd = new ExportCommand(_service!, "export", "Exports 
    all bookmarks to a file");
    var exportArgs = new string[] { "--file", "bookmarksbench.json" };
    await exportCmd.InvokeAsync(exportArgs);
}</pre>			<p>This <a id="_idIndexMarker470"/>method creates an instance of the <code>ExportCommand</code> class and executes it by calling its <code>InvokeAsync</code> method, passing in the required parameters for the command.</p>
			<p>Right now, this method is not yet considered as a benchmark by the <code>BenchmarkRunner</code> class. The reason is that for a method to be considered as a benchmark, it needs to be decorated with the <code>[Benchmark]</code> attribute. Let’s fix this!</p>
			<pre class="source-code">
[Benchmark]
public async Task ExportBookmarks()
{
    var exportCmd = new ExportCommand(_service!, "export", "Exports 
    all bookmarks to a file");
    var exportArgs = new string[] { "--file", "bookmarksbench.json" };
    await exportCmd.InvokeAsync(exportArgs);
}</pre>			<p>Awesome! But we are not ready to run it yet…</p>
			<p>See what’s missing?</p>
			<p>You got it! The <code>ExportCommand</code> class takes an instance of type <code>IBookmarkService</code> as a parameter, but we haven’t so far provided such an instance of an object.</p>
			<p>Since we<a id="_idIndexMarker471"/> already have such an instance defined in the <code>Program</code> class, you may expect that we can pass it to the <code>Benchmarks</code> class through its constructor, and this would be a<a id="_idIndexMarker472"/> perfectly reasonable assumption. However, the <code>BenchmarkRunner</code> class does not allow us to do so (at least with the current version of <code>BenchmarkDotNet</code>).</p>
			<p>What we will do instead is to instantiate this object in the <code>Benchmarks</code> class directly. The code will then look like this:</p>
			<pre class="source-code">
#region Properties
private IBookmarkService? _service;
#endregion
#region GlobalSetup
[GlobalSetup]
public void BenchmarksGlobalSetup()
{
    _service = new BookmarkService();
}
#endregion</pre>			<p>Notice that the instantiation of the service is not performed in the class constructor but rather in a method decorated with the <code>[GlobalSetup]</code> attribute. This special attribute instructs <code>BenchmarkDotNet</code> to call this method once before executing each benchmark method. This is to have a clean instance of the service for each benchmark method, hence preventing side effects from previous benchmarks.</p>
			<p class="callout-heading">GlobalSetup versus class constructor</p>
			<p class="callout">The execution time of the <code>[GlobalSetup]</code> method is not taken into account in calculating the benchmarked method execution time, as opposed to the execution time of the constructor. While this might seem negligible, it will not be if the method is meant to be executed a significant number of times.</p>
			<p>We are <a id="_idIndexMarker473"/>now ready to execute our benchmark.</p>
			<p>To do this, we <a id="_idIndexMarker474"/>first need to build the application, but this time we need to build it in <code>Release</code> mode. Otherwise, <code>BenchmarkDotNet</code> will generate an error. The reason is that running a program in <code>Debug</code> mode is not optimal and has a significant performance cost compared to running the program in <code>Release</code> mode, which is the mode the application should be run on in production. Therefore, when benchmarking our application, we should do it in its optimal performance mode.</p>
			<p class="callout-heading">Debug vs Release modes</p>
			<p class="callout">Building the code in <code>Debug</code> mode produces unoptimized code with full symbolic <code>debug</code> information, enabling easier debugging and breakpoint setting. In contrast, <code>Release</code> mode generates optimized code for better performance and smaller file sizes. <code>Release</code> builds typically omit <code>debug</code> symbols, inline methods, and apply various optimizations that can make debugging more challenging but result in faster execution. While <code>Debug</code> builds are ideal for development and troubleshooting, <code>Release</code> builds are used when deploying to production.</p>
			<p>Building the application in <code>Release</code> mode can be achieved by typing the following:</p>
			<pre class="console">
dotnet build -c Release</pre>			<p>We then run the benchmarks by typing the following:</p>
			<pre class="console">
dotnet C:\code\Chap12\bookmarkr\bin\Release\net8.0\bookmarkr.dll benchmark</pre>			<p><code>C:\code\Chap12\bookmarkr\bin\Release\net8.0</code> is the location of the generated DLL of the Bookmarkr application.</p>
			<p>The result is as follows:</p>
			<div><div><img src="img/B22400_12_01.jpg" alt="Figure 12.1 – Benchmarking the export command"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.1 – Benchmarking the export command</p>
			<p>The<a id="_idIndexMarker475"/> benchmark method has run 98 times and, on average, it takes 6.356 milliseconds to run the <code>export</code> command, which is not bad at all, is it?</p>
			<p>You can see <a id="_idIndexMarker476"/>the table in the middle of the screen. This table compiles the metrics per benchmark method. Let’s explain what each of its columns represents:</p>
			<ul>
				<li><code>Mean</code>: This represents the average duration of the benchmarked method over all its executions (98 in our example).</li>
				<li><code>Error</code>: Simply stated, this value represents the precision of the mean value’s measurement. The smaller the error, the more precise the measurement of the mean value. As an example, since our mean value is 6.356 ms and the error is 0.7840 ms, all measurements fall within the range of 6.356 ms ± 0.7840 ms, which means between 5.572 ms and 7.140 ms.</li>
				<li><code>StdDev</code>: This value represents the standard deviation of all measurements. It quantifies the amount of variation or dispersion in the execution times. In other words, a lower value of <code>StdDev</code> indicates that the execution times are clustered closely around the mean.</li>
			</ul>
			<p class="callout-heading">Benchmarking is not only for commands!</p>
			<p class="callout">Although we are benchmarking a command here, it is important to note that benchmarking does not only apply to commands but rather to all code artifacts that may have an impact on the application’s performance, which also includes services. Therefore, by benchmarking commands <em class="italic">and</em> the services they use, we can determine the percentage of the execution time and memory consumption that is attributable to the service and the command.</p>
			<p>Great! There is, however, one<a id="_idIndexMarker477"/> measurement that we haven’t seen here, which is the measurement of memory consumption. Let’s fix that!</p>
			<p>To collect <a id="_idIndexMarker478"/>data about memory consumption, we simply need to add the <code>[MemoryDiagnoser]</code> tag on top of the <code>Benchmarks</code> class, as follows:</p>
			<pre class="source-code">
[MemoryDiagnoser]
public class Benchmarks
{
    // …
}</pre>			<p>Now, if we run the code in exactly the same way as before, we get the following results:</p>
			<div><div><img src="img/B22400_12_02.jpg" alt="Figure 12.2 – Benchmarking memory consumption"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.2 – Benchmarking memory consumption</p>
			<p>Notice that <a id="_idIndexMarker479"/>now we have a new column called <code>Allocated</code>, which represents the amount of allocated memory for every execution of the benchmarked method, in kilobytes. This <a id="_idIndexMarker480"/>column is interesting for two reasons:</p>
			<ul>
				<li>It allows us to see if the benchmarked method is using way too much memory than it should (or way more than it is expected to use). This can indicate memory leaks in our code that require deeper investigation.</li>
				<li>When we optimize our code, we can see if the new implementation has an impact on memory consumption. For example, we could come up with an implementation that speeds up the execution time at the expense of significant memory consumption.</li>
			</ul>
			<p class="callout-heading">Execution time versus memory consumption optimization</p>
			<p class="callout">You may be wondering whether we should concentrate on optimizing memory consumption or execution time. The decision about where to focus our attention and energy depends on what we value the most, memory consumption or execution time. It is interesting to note that, in some situations, we may even be able to optimize both at the same time! To do that, we have to come up with a creative implementation that addresses both concerns by leveraging advanced features of the frameworks and libraries that we use, combined with advanced and creative algorithms.</p>
			<p>While <code>BenchmarkDotNet</code> helps us <a id="_idIndexMarker481"/>identify optimization opportunities during the development phase, it is important to implement monitoring so that we can continuously check the application’s performance while it is being used in production.</p>
			<h2 id="_idParaDest-209"><a id="_idTextAnchor213"/>Monitoring BookmarkrSyncr with Azure Application Insights</h2>
			<p>We <a id="_idIndexMarker482"/>mentioned earlier that a CLI application runs locally on the user’s computer and that the <a id="_idIndexMarker483"/>user may refuse to allow <a id="_idIndexMarker484"/>us to collect telemetry data that is absolutely essential for monitoring. That is why we won’t implement monitoring in Bookmarkr but rather in <strong class="bold">BookmarkrSyncr</strong>, the external web service invoked by Bookmarkr. Since this is a web service hosted and managed by us, we can implement monitoring and ensure that telemetry data will be collected, therefore ensuring that monitoring can take place.</p>
			<p>Since this web service is deployed <a id="_idIndexMarker485"/>to the <strong class="bold">Microsoft Azure</strong> cloud platform, we will rely on Azure Application Insights, the <strong class="bold">application performance monitoring</strong> (<strong class="bold">APM</strong>) solution <a id="_idIndexMarker486"/>provided natively by the Microsoft Azure cloud platform.</p>
			<p>When we deployed BookmarkrSyncr to Microsoft Azure, we created an infrastructure for hosting it. More specifically, we created <a id="_idIndexMarker487"/>an <strong class="bold">Azure App Service</strong> instance. As part of the process of creating this service, we are offered the opportunity to create an instance of the <strong class="bold">Azure Application Insights</strong> service. This service is a monitoring solution that is provided and managed for us by Microsoft.</p>
			<p>Azure Application Insights<a id="_idIndexMarker488"/> is a fantastic service that allows us to monitor performance, availability, failed requests, exceptions, page views, traces, browser timings, usage (including <strong class="bold">user flows</strong>, which allow us to identify hot paths in the application), and even access live metrics so we can monitor in real time. Another great feature of <strong class="bold">Azure Application Insights</strong> is the ability to configure alerts to be triggered if a certain metric reaches a certain threshold, for example, if the server response time (which measures the duration between receiving the HTTP request and sending the response to the client) is above the maximum allowed value as defined by our organization’s standards. When an alert is raised, we can then trigger an automated processing or a notification (such as an email to a specific group of people).</p>
			<p>To see<em class="italic"> </em><a id="_idIndexMarker489"/>what monitoring with <strong class="bold">Azure Application Insights</strong> may look like, check out (this article on Microsoft Learn, which can be found at <a href="https://learn.microsoft.com/en-us/azure/azure-monitor/app/overview-dashboard">https://learn.microsoft.com/en-us/azure/azure-monitor/app/overview-dashboard</a>.</p>
			<p>Okay. Now <a id="_idIndexMarker490"/>that we know how to identify the areas of our application that require performance tuning (using profiling and monitoring), let’s discuss the most common techniques that<a id="_idIndexMarker491"/> we can use to enhance the performance of our application.</p>
			<h1 id="_idParaDest-210"><a id="_idTextAnchor214"/>Common performance optimization techniques</h1>
			<p>It is <a id="_idIndexMarker492"/>worth mentioning that the techniques we will be discussing here do not only apply to CLI applications but can rather be applied to any kind of application. Let’s break these techniques down according to the categories we presented earlier. For every category, I will give you a list of techniques commonly used for it.</p>
			<p><strong class="bold">Application design </strong><strong class="bold">and architecture</strong>:</p>
			<ul>
				<li>Establish the shortest path to achieve a goal, removing all unnecessary intermediaries. </li>
				<li>This can be achieved by using efficient algorithms. </li>
				<li>Find the optimal balance between decoupling and low latency. </li>
				<li>Use lazy loading for resources that aren't immediately needed. </li>
				<li>Implement efficient error handling and logging mechanisms. </li>
				<li>Design for scalability from the start.</li>
			</ul>
			<p><strong class="bold">Infrastructure</strong>:</p>
			<ul>
				<li>When packaging and distributing your application, compile it in <code>Release</code> mode. While <code>Debug </code>mode is great during the development phase, it may add a significant performance overhead. </li>
				<li>Also, when packaging and distributing your application, compile it as platform-specific if the target platform is known ahead of time or if the packaging and distribution mechanism is not cross-platform. For example, distributing our application as a <strong class="bold">Winget</strong> package means that it will exclusively be used on the Windows platform. The same goes with an apt-get package (where the application will run exclusively on <strong class="bold">Linux</strong>) and with <strong class="bold">Homebrew</strong> (where the application will run exclusively on <strong class="bold">macOS</strong>). It is therefore easy to know what platform-specific compilation should be used and will make .NET apply all the possible optimizations, which is something it wouldn’t do if the target platform is not known ahead of time (an example of that is file handling, which is different on Windows, Linux, and macOS). This will result in a version of the application that runs in the most efficient manner on that target platform. </li>
				<li>You might also choose to use <strong class="bold">AOT</strong> (<strong class="bold">Ahead-Of-Time</strong>) compilation to precompile your code to native code (instead of relying on <strong class="bold">JIT</strong>) for faster startup times or to reduce the dependency on runtime compilation. This could be particularly useful if you're targeting environments like mobile (iOS/Android) or WebAssembly, where JIT might not be feasible. Note that platform targeting and AOT can be combined for even better performance optimization.</li>
			</ul>
			<p><strong class="bold">Frameworks </strong><strong class="bold">and libraries:</strong></p>
			<ul>
				<li>Avoid using libraries that rely on reflection, unless absolutely necessary. </li>
				<li>Choose lightweight frameworks and libraries that align with your specific needs. Beware of libraries that pull off tenth of other libraries when you reference them. </li>
				<li>Keep dependencies up to date to benefit from performance improvements.</li>
				<li>Consider using micro-frameworks for smaller, focused tasks.</li>
			</ul>
			<p><strong class="bold">Coding practices:</strong></p>
			<ul>
				<li>Rely on asynchronous operations whenever possible. This will avoid blocking the main thread and increases the feeling of responsiveness of the application.</li>
				<li>Choose the most optimized data types or data structures for the pursued purpose. This will ensure we have the minimal footprint on the computer’s resources.</li>
				<li>Whenever possible, try to achieve a task with as little memory allocation as possible. For example, at the time of this writing, .NET 9 was released and introduces split operations with no memory allocation by calling <code>AsSpan().Split(…)</code>.</li>
				<li>Implement a caching mechanism to avoid unnecessary calls to external dependencies (such as web services or databases).</li>
				<li> Optimize database queries and implement proper indexing. </li>
				<li>Speaking about databases, if you are using an <code>AsNoTracking()</code> to significantly improve query performance and reduce memory usage, especially when dealing with large datasets or read-only operations. This method tells the ORM not to track changes to the retrieved entities, bypassing the change tracking mechanism and resulting in faster queries with lower memory overhead. </li>
				<li>Use connection pooling, which consists in reusing established database connections instead of creating a new one for every request. This is because it can be expensive to establish a connection to a database, therefore connection pooling reduces connection latency and enables high database throughput (transactions per second) on the server.</li>
				<li>Implement proper memory management and dispose of unused resources.</li>
			</ul>
			<p>We have seen<a id="_idIndexMarker493"/> a bunch of techniques that are commonly used for optimizing the performance of any kind of application that is built with any technology stacks, including CLI applications built with .NET.</p>
			<p>Let’s now apply some of these techniques to enhance Bookmarkr’s performance.</p>
			<h1 id="_idParaDest-211"><a id="_idTextAnchor215"/>Optimizing Bookmarkr’s performance</h1>
			<p>We<a id="_idIndexMarker494"/> cannot optimize what is already perfect, can we?</p>
			<p>Just kidding. Of course we can! There is always room for improvement.</p>
			<p>Let’s see some of the quick wins that we can apply to enhance the performance of our beloved CLI application.</p>
			<p>Looking at the handler method of the <code>ExportCommand</code> class (namely, <code>OnExportCommand</code>), we can see that it already leverages async operations. This is a great start and is actually one of the techniques we described earlier.</p>
			<p>However, the handler method can be optimized. To illustrate this, let’s create a copy of the <code>ExportCommand</code> class and name it <code>ExportCommandOptimized</code>. Let’s copy the code from the <code>ExportCommand</code> as is, and we will optimize it in a moment.</p>
			<p>The reason we are creating a copy of the original class rather than directly optimizing it is so that we can add a benchmark method for the optimized version and compare it with the original one.</p>
			<p>In the handler method of the <code>ExportCommandOptimized</code> class, let’s change these two lines of code:</p>
			<pre class="source-code">
string json = JsonSerializer.Serialize(bookmarks, new JsonSerializerOptions { WriteIndented = true });
await File.WriteAllTextAsync(outputfile.FullName, json, token);</pre>			<p>Replace them with the following two lines:</p>
			<pre class="source-code">
using var fileStream = new FileStream(outputfile.FullName, FileMode.Create, FileAccess.Write, FileShare.None, 4096, true);
await JsonSerializer.SerializeAsync(fileStream, bookmarks, new JsonSerializerOptions { WriteIndented = true }, token);</pre>			<p>Let’s see<a id="_idIndexMarker495"/> what we have done:</p>
			<ul>
				<li>Using <code>JsonSerializer.SerializeAsync</code> is more efficient for large datasets as it streams the JSON directly to the file without keeping the entire serialized string in memory</li>
				<li>Using <code>FileStream</code> with async operations allows better control over file I/O operations and can improve performance, especially for large files</li>
			</ul>
			<p>Okay. Let’s compare this new implementation with the original one.</p>
			<p>To do this, let’s add the following benchmark method to the <code>Benchmarks</code> class:</p>
			<pre class="source-code">
[Benchmark]
public async Task ExportBookmarksOptimized()
{
    var exportCmd = new ExportCommandOptimized(_service!, "export", 
    "Exports all bookmarks to a file");
    var exportArgs = new string[] { "--file", "bookmarksbench.json" };
    await exportCmd.InvokeAsync(exportArgs);
}</pre>			<p>This <code>benchmark</code> method is identical to the previous one. Well, almost identical… The only difference is that we are instantiating (and invoking) the <code>ExportCommandOptimized</code> class rather than the <code>ExportCommand</code> class.</p>
			<p>Since we want to compare the new, optimized, implementation against the original one, we will modify the <code>[</code><code>Benchmark]</code> attribute of the original method to look like this.</p>
			<p>This <a id="_idIndexMarker496"/>instructs <code>BenchmarkDotNet</code> to use this method as a baseline for the comparison:</p>
			<pre class="source-code">
[Benchmark(Baseline = true)]</pre>			<p>Let’s rebuild the application (in <code>Release</code> mode, of course) and execute the benchmarks.</p>
			<p>The results are the following:</p>
			<div><div><img src="img/B22400_12_03.jpg" alt="Figure 12.3 – Benchmarking the new implementation against the original one"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.3 – Benchmarking the new implementation against the original one</p>
			<p>Notice the appearance of two new columns:</p>
			<ul>
				<li><code>Ratio</code>: This indicates the average measure of the performance relative to the baseline benchmark method</li>
				<li><code>RatioSD</code>: This indicates the average standard deviation relative to the standard deviation of the baseline benchmark method</li>
			</ul>
			<p>The value of <code>0.91</code> in the <code>Ratio</code> column indicates that the optimized implementation (<code>ExportCommandOptimized</code>) is on average 9% faster than the baseline implementation (<code>ExportCommand</code>). We mentioned earlier that the implementation we made in <code>ExportCommandOptimized</code> is especially more performant when dealing with large files. Therefore, we can expect it to be even faster than the baseline implementation<a id="_idIndexMarker497"/> as the output file becomes larger.</p>
			<p>Awesome! We now know how to improve the performance of our beloved CLI application and we have made our users happy.</p>
			<h1 id="_idParaDest-212"><a id="_idTextAnchor216"/>Summary</h1>
			<p>In this chapter, we explored the various areas of performance optimization, we learned techniques to identify performance hot spots and hot paths, and we saw how to improve their performance, with the ultimate goal of offering our users a great and efficient application that they will love to use.</p>
			<p>Hopefully, you have understood that there is not one single area or action that leads to better performance, but rather a series of fine-tuning here and there that do the trick.</p>
			<p>Awesome! So, we have an application that efficiently provides great functionality.</p>
			<p>There is, however, one key area that we have not yet covered when it comes to building CLI applications (and, for that matter, any kind of application). That key area is <strong class="bold">security</strong>, and this is the topic of the next chapter.</p>
			<h1 id="_idParaDest-213"><a id="_idTextAnchor217"/>Your turn!</h1>
			<p>Following along with the provided code is a great way to learn through practice.</p>
			<p>A better way is by challenging yourself to achieve tasks. Hence, I challenge you to improve the Bookmarkr application by adding the following features.</p>
			<h2 id="_idParaDest-214"><a id="_idTextAnchor218"/>Task #1 – Write more benchmarks</h2>
			<p>In this chapter, we have illustrated writing benchmark methods by only writing a benchmark for the <code>export</code> command. However, as we mentioned earlier, benchmarks do not only apply to commands, but they can also apply to services.</p>
			<p>That’s why you are tasked with writing additional benchmark methods for each command and for the services used by the Bookmarkr application.</p>
			<h2 id="_idParaDest-215"><a id="_idTextAnchor219"/>Task #2 – Fine-tune Bookmarkr for optimal performance</h2>
			<p>Throughout this chapter, we haven’t implemented every performance optimization opportunity, and we have probably missed some (was that intentional? *wink wink*). Therefore, you are tasked with identifying other potential performance optimizations in Bookmarkr and implementing them.</p>
		</div>
	</body></html>