<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<meta charset="utf-8"/>
<meta content="packt" name="generator"/>
<title>2 Automated Testing</title>


<!-- kobo-style -->
<style id="koboSpanStyle" type="text/css" xmlns="http://www.w3.org/1999/xhtml">.koboSpan { -webkit-text-combine: inherit; }</style>
</head>
<body>

<h1 data-number="3">2 Automated Testing</h1>

<h2 data-number="3.1">Before you begin: Join our book community on Discord</h2>
<p>Give your feedback straight to the author himself and chat to other early readers on our Discord server (find the "architecting-aspnet-core-apps-3e" channel under EARLY ACCESS SUBSCRIPTION).</p>
<p><a href="https://packt.link/EarlyAccess">https://packt.link/EarlyAccess</a></p>
<p><img alt="Qr code Description automatically generated" src="img/file2.png" style="width:10em"/></p>
<p>This chapter focuses on automated testing and how helpful it can be for crafting better software. It also covers a few different types of tests and the foundation of <strong>test-driven development</strong> (<strong>TDD</strong>). We also outline how testable ASP.NET Core is and how much easier it is to test ASP.NET Core applications than old ASP.NET MVC applications. This chapter overviews automated testing, its principles, xUnit, ways to sample test values, and more. While other books cover this topic more in-depth, this chapter covers the foundational aspects of automated testing. We are using parts of this throughout the book, and this chapter ensures you have a strong enough base to understand the samples.In this chapter, we cover the following topics:</p>
<ul>
<li>An overview of automated testing</li>
<li>Testing .NET applications</li>
<li>Important testing principles</li>
</ul>


<h2 data-number="3.2">Introduction to automated testing</h2>
<p>Testing is an integral part of the development process, and automated testing becomes crucial in the long run. You can always run your ASP.NET Core website, open a browser, and click everywhere to test your features. That’s a legitimate approach, but it is harder to test individual rules or more complex algorithms that way. Another downside is the lack of automation; when you first start with a small app containing a few pages, endpoints, or features, it may be fast to perform those tests manually. However, as your app grows, it becomes more tedious, takes longer, and increases the likelihood of making a mistake. Of course, you will always need real users to test your applications, but you want those tests to focus on the UX, the content, or some experimental features you are building instead of bug reports that automated tests could have caught early on.There are multiple types of tests and techniques in the testing space. Here is a list of three broad categories that represent how we can divide automated testing from a code correctness standpoint:</p>
<ul>
<li>Unit tests</li>
<li>Integration tests</li>
<li>End-to-end (E2E) tests</li>
</ul>
<p>Usually, you want a mix of those tests, so you have fast unit tests testing your algorithms, slower tests that ensure the integrations between components are correct, and slow E2E tests that ensure the correctness of the system as a whole.The test pyramid is a good way of explaining a few concepts around automated testing. You want different granularity of tests and a different number of tests depending on their complexity and speed of execution. The following test pyramid shows the three types of tests stated above. However, we could add other types of tests in there as well. Moreover, that’s just an abstract guideline to give you an idea. The most important aspect is the <strong>return on investment</strong> (<strong>ROI</strong>) and execution speed. If you can write one integration test that covers a large surface and is fast enough, this might be worth doing instead of multiple unit tests.</p>
<figure>
<img alt="Figure 2.1: The test pyramid" src="img/file3.png"/><figcaption aria-hidden="true">Figure 2.1: The test pyramid</figcaption>
</figure>
<blockquote>
<p>I cannot stress this enough; the execution speed of your tests is essential to receive fast feedback and know immediately that you have broken something with your code changes. Layering different types of tests allows you to execute only the fastest subset often, the not-so-fast occasionally, and the very slow tests infrequently. If your test suite is fast-enough, you don’t even have to worry about it. However, if you have a lot of manual or E2E UI tests that take hours to run, that’s another story (that can cost a lot of money).</p>
</blockquote>
<p>Finally, on top of running your tests using a test runner, like in Visual Studio, VS Code, or the CLI, a great way to ensure code quality and leverage your automated tests is to run them in a CI pipeline, validating code changes for issues.Tech-wise, back when .NET Core was in pre-release, I discovered that the .NET team was using xUnit to test their code and that it was the only testing framework available. xUnit has become my favorite testing framework since, and we use it throughout the book. Moreover, the ASP.NET Core team made our life easier by designing ASP.NET Core for testability; testing is easier than before.Why are we talking about tests in an architectural book? Because testability is a sign of a good design. It also allows us to use tests instead of words to prove some concepts. In many code samples, the test cases are the consumers, making the program lighter without building an entire user interface and focusing on the patterns we are exploring instead of getting our focus scattered over some boilerplate UI code.</p>
<blockquote>
<p>To ensure we do not deviate from the matter at hand, we use automated testing moderately in the book, but I strongly recommend that you continue to study it, as it will help improve your code and design</p>
</blockquote>
<p>Now that we have covered all that, let’s explore those three types of tests, starting with unit testing.</p>

<h3 data-number="3.2.1">Unit testing</h3>
<p>Unit tests focus on individual units, like testing the outcome of a method. Unit tests should be fast and not rely on any infrastructure, such as a database. Those are the kinds of tests you want the most because they run fast, and each one tests a precise code path. They should also help you design your application better because you use your code in the tests, so you become its first consumer, leading to you finding some design flaws and making your code better. If you don’t like using your code in your tests, that is a good indicator that nobody else will. Unit tests should focus on testing algorithms (the ins and outs) and domain logic, not the code itself; how you wrote the code should have no impact on the intent of the test. For example, you are testing that a <code>Purchase</code> method executes the logic required to purchase one or more items, not that you created the variable <code>X</code>, <code>Y</code>, or <code>Z</code> inside that method.</p>
<blockquote>
<p>Don’t discourage yourself if you find it challenging; writing a good test suite is not as easy as it sounds.</p>
</blockquote>


<h3 data-number="3.2.2">Integration testing</h3>
<p>Integration tests focus on the interaction between components, such as what happens when a component queries the database or what happens when two components interact with each other.Integration tests often require some infrastructure to interact with, which makes them slower to run. By following the classic testing model, you want integration tests, but you want fewer of them than unit tests. An integration test can be very close to an E2E test but without using a production-like environment.</p>
<blockquote>
<p>We will break the test pyramid rule later, so always be critical of rules and principles; sometimes, breaking or bending them can be better. For example, having one good integration test can be better than <em>N</em> unit tests; don’t discard that fact when writing your tests. See also Grey-box testing.</p>
</blockquote>


<h3 data-number="3.2.3">End-to-end testing</h3>
<p>End-to-end tests focus on application-wide behaviors, such as what happens when a user clicks on a specific button, navigates to a particular page, posts a form, or sends a <code>PUT</code> request to some web API endpoint. E2E tests are usually run on infrastructure to test your application and deployment.</p>


<h3 data-number="3.2.4">Other types of tests</h3>
<p>There are other types of automated tests. For example, we could do load testing, performance testing, regression testing, contract testing, penetration testing, functional testing, smoke testing, and more. You can automate tests for anything you want to validate, but some tests are more challenging to automate or more fragile than others, such as UI tests.</p>
<blockquote>
<p>If you can automate a test in a reasonable timeframe, think ROI: do it! In the long run, it should pay off.</p>
</blockquote>
<p>One more thing; don’t blindly rely on metrics such as code coverage. Those metrics make for cute badges in your GitHub project’s <code>readme.md</code> file but can lead you off track, resulting in you writing useless tests. Don’t get me wrong, code coverage is a great metric when used correctly, but remember that one good test can be better than a lousy test suite covering 100% of your codebase. If you are using code coverage, ensure you and your team are not gaming the system.Writing good tests is not easy and comes with practice.</p>
<blockquote>
<p>One piece of advice: keep your test suite healthy by adding missing test cases and removing obsolete or useless tests. Think about use case coverage, not how many lines of code are covered by your tests.</p>
</blockquote>
<p>Before moving forward to testing styles, let’s inspect a hypothetical system and explore a more efficient way to test it.</p>


<h3 data-number="3.2.5">Picking the right test style</h3>
<p>Next is a dependency map of a hypothetical system. We use that diagram to pick the most meaningful type of test possible for each piece of the program. In real life, that diagram will most likely be in your head, but I drew it out in this case. Let’s inspect that diagram before I explain its content:</p>
<figure>
<img alt="Figure 2.2: Dependency map of a hypothetical system" src="img/file4.png"/><figcaption aria-hidden="true">Figure 2.2: Dependency map of a hypothetical system</figcaption>
</figure>
<p>In the diagram, the <strong>Actor</strong> can be anything from a user to another system. <strong>Presentation</strong> is the piece of the system that the <strong>Actor</strong> interacts with and forwards the request to the system itself (this could be a user interface). <strong>D1</strong> is a component that has to decide what to do next based on the user input. <strong>C1</strong> to <strong>C6</strong> are other components of the system (could be classes, for example). <strong>DB</strong> is a database.D1 must choose between three code paths: interact with the components C1, C4, or C6. This type of logic is usually a good subject for unit tests, ensuring the algorithm yields the correct result based on the input parameter. Why pick a unit test? We can quickly test multiple scenarios, edge cases, out-of-bound data cases, and more. We usually mock the dependencies away in this type of test and assert that the subject under test made the expected call on the desired component.Then, if we look at the other code paths, we could write one or more integration tests for component C1, testing the whole chain in one go (C1, C5, and C3) instead of writing multiple mock-heavy unit tests for each component. If there is any logic that we need to test in components C1, C5, or C3, we can always add a few unit tests; that’s what they are for.Finally, C4 and C6 are both using C2. Depending on the code (that we don’t have here), we could write integration tests for C4 and C6, testing C2 simultaneously. Another way would be to unit test C4 and C6, and then write integration tests between C2 and the DB. If C2 has no logic, the latter could be the best and the fastest, while the former will most likely yield results that give you more confidence in your test suite in a continuous delivery model.When it is an option, I recommend evaluating the possibility of writing fewer meaningful integration tests that assert the correctness of a use case over a suite of mock-heavy unit tests. Remember always to keep the execution speed in mind.That may seem to go “against” the test pyramid, but does it? If you spend less time (thus lower costs) testing more use cases (adding more value), that sounds like a win to me. Moreover, we must not forget that mocking dependencies tends to make you waste time fighting the framework or other libraries instead of testing something meaningful and can add up to a high maintenance cost over time.Now that we have explored the fundamentals of automated testing, it is time to explore testing approaches and TDD, which is a way to apply those testing concepts.</p>



<h2 data-number="3.3">Testing approaches</h2>
<p>There are various approaches to testing, such as <strong>behavior-driven development</strong> (<strong>BDD</strong>), <strong>acceptance test-driven development</strong> (<strong>ATDD</strong>), and <strong>test-driven development</strong> (<strong>TDD)</strong>. The DevOps culture brings a mindset that embraces automated testing in line with its <strong>continuous integration</strong> (<strong>CI</strong>) and <strong>continuous deployment</strong> (<strong>CD</strong>) ideals. We can enable CD with a robust and healthy suite of tests that gives a high degree of confidence in our code, high enough to deploy the program when all tests pass without fear of introducing a bug.</p>

<h3 data-number="3.3.1">TDD</h3>
<p>TDD is a software development method that states that you should write one or more tests before writing the actual code. In a nutshell, you invert your development flow by following the <strong>Red-Green-Refactor</strong> technique, which goes like this:</p>
<ol>
<li>You write a failing test (red).</li>
<li>You write just enough code to make your test pass (green).</li>
<li>You refactor that code to improve the design by ensuring all the tests pass.</li>
</ol>
<blockquote>
<p>We explore the meaning of <strong>refactoring</strong> next.</p>
</blockquote>


<h3 data-number="3.3.2">ATDD</h3>
<p>ATDD is similar to TDD but focuses on acceptance (or functional) tests instead of software units and involves multiple parties like customers, developers, and testers.</p>


<h3 data-number="3.3.3">BDD</h3>
<p>BDD is another complementary technique originating from TDD and ATDD. BDD focuses on formulating test cases around application behaviors using spoken language and involves multiple parties like customers, developers, and testers. Moreover, practitioners of BDD often leverage the <em>given–when–then</em> grammar to formalize their test cases. Because of that, BDD output is in a human-readable format allowing stakeholders to consult such artifacts.The given–when–then template defines the way to describe the behavior of a user story or acceptance test, like this:</p>
<ul>
<li><em>Given</em> one or more preconditions (context)</li>
<li><em>When</em> something happens (behavior)</li>
<li><em>Then</em> one or more observable changes are expected (measurable side effects)</li>
</ul>
<p>ATDD and BDD are great areas to dig deeper into and can help design better apps; defining precise user-centric specifications can help build only what is needed, prioritize better, and improve communication between parties. For the sake of simplicity, we stick to unit testing, integration testing, and a tad of TDD in the book. Nonetheless, let’s go back to the main track and define refactoring.</p>


<h3 data-number="3.3.4">Refactoring</h3>
<p>Refactoring is about (continually) improving the code without changing its behavior.An automated test suite should help you achieve that goal and should help you discover when you break something. No matter whether you do TDD or not, I do recommend refactoring as often as possible; this helps clean your codebase, and it should also help you get rid of some technical debt at the same time.Okay, but what is <strong>technical debt</strong>?</p>


<h3 data-number="3.3.5">Technical debt</h3>
<p><strong>Technical debt</strong> represents the corners you cut short while developing a feature or a system. That happens no matter how hard you try because life is life, and there are delays, deadlines, budgets, and people, including developers (yes, that’s you and me).The most crucial point is understanding that you cannot avoid technical debt altogether, so it’s better to embrace that fact and learn to live with it instead of fighting it. From that point forward, you can only try to limit the amount of technical debt you, or someone else, generate and ensure to always refactor some of it over time each sprint (or the unit of time that fits your projects/team/process).One way to limit the piling up of technical debt is to refactor the code often. So, factor the refactoring time into your time estimates. Another way is to improve collaboration between all the parties involved. Everyone must work toward the same goal if you want your projects to succeed.You will sometimes cut the usage of best practices short due to external forces like people or time constraints. The key is coming back at it as soon as possible to repay that technical debt, and automated tests are there to help you refactor that code and eliminate that debt elegantly. Depending on the size of your workplace, there will be more or less people between you and that decision.</p>
<blockquote>
<p>Some of these things might be out of your control, so you may have to live with more technical debt than you had hoped. However, even when things are out of your control, nothing stops you from becoming a pioneer and working toward improving the enterprise’s culture. Don’t be afraid to become an agent of change and lead the charge.</p>
</blockquote>
<p>Nevertheless, don’t let the technical debt pile up too high, or you may not be able to pay it back, and at some point, that’s where a project begins to break and fail. Don’t be mistaken; a project in production can be a failure. Delivering a product does not guarantee success, and I’m talking about the quality of the code here, not the amount of generated revenue (I’ll leave that to other people to evaluate).Next, we look at different ways to write tests, requiring more or less knowledge of the inner working of the code.</p>



<h2 data-number="3.4">Testing techniques</h2>
<p>Here we look at different ways to approach our tests. Should we know the code? Should we test user inputs and compare them against the system results? How to identify a proper value sample? Let’s start with white-box testing.</p>

<h3 data-number="3.4.1">White-box testing</h3>
<p>White-box testing is a software testing technique that uses knowledge of the internal structure of the software to design tests. We can use white-box testing to find defects in the software’s logic, data structures, and algorithms. </p>
<blockquote>
<p>This type of testing is also known as clear-box testing, open-box testing, transparent-box testing, glass-box testing, and code-based testing.</p>
</blockquote>
<p>Another benefit of white-box testing is that it can help optimize the code. By reviewing the code to write tests, developers can identify and improve inefficient code structures, improving overall software performance. The developer can also improve the application design by finding architectural issues while testing the code.</p>
<blockquote>
<p>White-box testing encompasses most unit and integration tests.</p>
</blockquote>
<p>Next, we look at black-box testing, the opposite of white-box testing.</p>


<h3 data-number="3.4.2">Black-box testing</h3>
<p>Black-box testing is a software testing method where a tester examines an application’s functionality without knowing the internal structure or implementation details. This form of testing focuses solely on the inputs and outputs of the system under test, treating the software as a “black box” that we can’t see into.The main goal of black-box testing is to evaluate the system’s behavior against expected results based on requirements or user stories. Developers writing the tests do not need to know the codebase or the technology stack used to build the software.We can use black-box testing to assess the correctness of several types of requirements, like:</p>
<ol>
<li><strong>Functional testing</strong>: This type of testing is related to the software’s functional requirements, emphasizing what the system does, a.k.a. behavior verification.</li>
<li><strong>Non-functional testing</strong>: This type of testing is related to non-functional requirements such as performance, usability, reliability, and security, a.k.a. performance evaluation.</li>
<li><strong>Regression testing</strong>: This type of testing ensures the new code does not break existing functionalities, a.k.a. change impact.</li>
</ol>
<p>Next, let’s explore a hybrid between white-box and black-box testing.</p>


<h3 data-number="3.4.3">Grey-box testing</h3>
<p>Grey-box testing is a blend between white-box and black-box testing. Testers need only partial knowledge of the application’s internal workings and use a combination of the software’s internal structure and external behavior to craft their tests.We implement grey-box testing use cases in <em>Chapter 16</em>, <em>Request-Endpoint-Response (REPR)</em>. Meanwhile, let’s compare the three techniques.</p>


<h3 data-number="3.4.4">White-box vs. Black-box vs. Grey-box testing</h3>
<p>To start with a concise comparison, here’s a table that compares the three broad techniques:</p>
<table>
<tbody>
<tr class="odd">
<td><strong>Feature</strong></td>
<td><strong>Whitebox Testing</strong></td>
<td><strong>Blackbox Testing</strong></td>
<td><strong>Gray-box Testing</strong></td>
</tr>
<tr class="even">
<td>Definition</td>
<td>Testing based on the internal design of the software</td>
<td>Testing based on the behavior and functionality of the software</td>
<td>Testing that combines the internal design and behavior of the software</td>
</tr>
<tr class="odd">
<td>Knowledge of code required</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr class="even">
<td>Types of defects found</td>
<td>Logic, data structure, architecture, and performance issues</td>
<td>Functionality, usability, performance, and security issues</td>
<td>Most types of issues</td>
</tr>
<tr class="odd">
<td>Coverage per test</td>
<td>Small; targeted on a unit</td>
<td>Large; targeted on a use case</td>
<td>Up to large; can vary in scope</td>
</tr>
<tr class="even">
<td>Testers</td>
<td>Usually performed by developers.</td>
<td>Testers can write the tests without specific technical knowledge of the application’s internal structure.</td>
<td>Developers can write the tests, while testers also can with some knowledge of the code.</td>
</tr>
<tr class="odd">
<td>When to use each style?</td>
<td>Write unit tests to validate complex algorithms or code that yields multiple results based on many inputs. These tests are usually high-speed so you can have many of them.</td>
<td>Write if you have specific scenarios you want to test, like UI tests, or if testers and developers are two distinct roles in your organization. These usually run the slowest and require you to deploy the application to test it. You want as few as possible to improve the feedback time.</td>
<td>Write to avoid writing black-box or white-box tests. Layer the tests to cover as much as possible with as few tests as possible. Depending on the application’s architecture, this type of test can yield optimal results for many scenarios.</td>
</tr>
</tbody>
</table>
<p>Let’s conclude next and explore a few advantages and disadvantages of each technique.</p>


<h3 data-number="3.4.5">Conclusion</h3>
<p>White-box testing includes unit and integration tests. Those tests run fast, and developers use them to improve the code and test complex algorithms. However, writing a large quantity of those tests takes time. Writing brittle tests that are tightly coupled with the code itself is easier due to the proximity to the code, increasing the maintenance cost of such test suites. It also makes it prone to overengineering your application in the name of testability.Black-box testing encompasses different types of tests that tend towards end-to-end testing. Since the tests target the external surface of the system, they are less likely to break when the system changes. Moreover, they are excellent at testing behaviors, and since each test tests an end-to-end use case, we need fewer of them, leading to a decrease in writing time and maintenance costs. Testing the whole system has drawbacks, including the slowness of executing each test, so combining black-box testing with other types of tests is very important to find the right balance between the number of tests, test case coverage, and speed of execution of the tests.Grey-box testing is a fantastic mix between the two others; you can treat any part of the software as a black box, leverage your inner-working knowledge to mock or stub parts of the test case (like to assert if the system persisted a record in the database), and test end-to-end scenarios more efficiently. It brings the best of both worlds, significantly reducing the number of tests while increasing the test surface considerably for each test case. However, doing grey-box testing on smaller units or heavily mocking the system may yield the same drawbacks as white-box testing. Integration tests or almost-E2E tests are good candidates for grey-box testing. We implement grey-box testing use cases in <em>Chapter 16</em>, <em>Request-Endpoint-Response (REPR)</em>. Meanwhile, let’s explore a few techniques to help optimize our test case creation by applying different techniques, like testing a small subset of values to assert the correctness of our programs by writing an optimal number of tests.</p>



<h2 data-number="3.5">Test case creation</h2>
<p>Multiple ways exist to break down and create test cases to help find software defects with a minimal test count. Here are some techniques to help minimize the number of tests while maximizing the test coverage:</p>
<ul>
<li>Equivalence Partitioning</li>
<li>Boundary Value Analysis</li>
<li>Decision Table Testing</li>
<li>State Transition Testing</li>
<li>Use Case Testing</li>
</ul>
<p>I present the techniques theoretically. They apply to all sorts of tests and should help you write better test suites. Let’s have a quick look at each.</p>

<h3 data-number="3.5.1">Equivalence Partitioning</h3>
<p>This technique divides the input data of the software into different equivalence data classes and then tests these classes rather than individual inputs. An equivalence data class means that all values in that partition set should lead to the same outcome or yield the same result. Doing this allows for limiting the number of tests considerably.For example, consider an application that accepts an integer value between 1 and 100 (inclusive). Using equivalence partitioning, we can divide the input data into two equivalence classes:</p>
<ul>
<li>Valid</li>
<li>Invalid</li>
</ul>
<p>To be more precise, we could further divide it into three equivalence classes:</p>
<ul>
<li>Class 1: Less than 1 (Invalid)</li>
<li>Class 2: Between 1 and 100 (Valid)</li>
<li>Class 3: Greater than 100 (Invalid)</li>
</ul>
<p>Then we can write three tests, picking one representative from each class (e.g., 0, 50, and 101) to create our test cases. Doing so ensures a broad coverage with minimal test cases, making our testing process more efficient.</p>


<h3 data-number="3.5.2">Boundary Value Analysis</h3>
<p>This technique focuses on the values at the boundary of the input domain rather than the center. This technique is based on the principle that errors are most likely to occur at the boundaries of the input domain.The <strong>input domain</strong> represents the set of all possible inputs for a system. The <strong>boundaries</strong> are the edges of the input domain, representing minimum and maximum values.For example, if we expect a function to accept an integer between 1 and 100 (inclusive), the boundary values would be 1 and 100. With Boundary Value Analysis, we would create test cases for these values, values just outside the boundaries (like 0 and 101), and values just inside the boundaries (like 2 and 99).Boundary Value Analysis is a very efficient testing technique that provides good coverage with a relatively small number of test cases. However, it’s unsuitable for finding errors within the boundaries or for complex logic errors. Boundary Value Analysis should be used on top of other testing methods, such as equivalence partitioning and decision table testing, to ensure the software is as defect-free as possible.</p>


<h3 data-number="3.5.3">Decision Table Testing</h3>
<p>This technique uses a decision table to design test cases. A decision table is a table that shows all possible combinations of input values and their corresponding outputs.It’s handy for complex business rules that can be expressed in a table format, enabling testers to identify missing and extraneous test cases.For example, our system only allows access to a user with a valid username and password. Moreover, the system denies access to users when it is under maintenance. The decision table would have three conditions (username, password, and maintenance) and one action (allow access). The table would list all possible combinations of these conditions and the expected action for each combination. Here is an example:</p>
<table>
<tbody>
<tr class="odd">
<td><strong>Valid Username</strong></td>
<td><strong>Valid Password</strong></td>
<td><strong>System under Maintenance</strong></td>
<td><strong>Allow Access</strong></td>
</tr>
<tr class="even">
<td>True</td>
<td>True</td>
<td>False</td>
<td>Yes</td>
</tr>
<tr class="odd">
<td>True</td>
<td>True</td>
<td>True</td>
<td>No</td>
</tr>
<tr class="even">
<td>True</td>
<td>False</td>
<td>False</td>
<td>No</td>
</tr>
<tr class="odd">
<td>True</td>
<td>False</td>
<td>True</td>
<td>No</td>
</tr>
<tr class="even">
<td>False</td>
<td>True</td>
<td>False</td>
<td>No</td>
</tr>
<tr class="odd">
<td>False</td>
<td>True</td>
<td>True</td>
<td>No</td>
</tr>
<tr class="even">
<td>False</td>
<td>False</td>
<td>False</td>
<td>No</td>
</tr>
<tr class="odd">
<td>False</td>
<td>False</td>
<td>True</td>
<td>No</td>
</tr>
</tbody>
</table>
<p>The main advantage of Decision Table Testing is that it ensures we test all possible input combinations. However, it can become complex and challenging to manage when systems have many input conditions, as the number of rules (and therefore test cases) increases exponentially with the number of conditions.</p>


<h3 data-number="3.5.4">State Transition Testing</h3>
<p>We usually use State Transition Testing to test software with a state machine since it tests the different system states and their transitions. It’s handy for systems where the system behavior can change based on its current state. For example, a program with states like “logged in” or “logged out”.To perform State Transition Testing, we need to identify the states of the system and then the possible transitions between the states. For each transition, we need to create a test case. The test case should test the software with the specified input values and verify that the software transitions to the correct state. For example, a user with the state “logged in” must transition to the state “logged out” after signing out.The main advantage of State Transition Testing is that it tests sequences of events, not just individual events, which could reveal defects not found by testing each event in isolation. However, State Transition Testing can become complex and time-consuming for systems with many states and transitions.</p>


<h3 data-number="3.5.5">Use Case Testing</h3>
<p>This technique validates that the system behaves as expected when used in a particular way by a user. Use cases could have formal descriptions, be user stories, or take any other form that fits your needs.A use case involves one or more actors executing steps or taking actions that should yield a particular result. A use case can include inputs and expected outputs. For example, when a user (actor) that is “signed in” (precondition) clicks the “sign out” button (action), then navigates to the profile page (action), the system denies access to the page and redirects the users to the sign in page, displaying an error message (expected behaviors).Use case testing is a systematic and structured approach to testing that helps identify defects in the software’s functionality. It is very user-centric, ensuring the software meets the users’ needs. However, creating test cases for complex use cases can be difficult. In the case of a user interface, the time to execute end-to-end tests of use cases can take a long time, especially as the number of tests grows.</p>
<blockquote>
<p>It is an excellent approach to think of your test cases in terms of functionality to test, whether using a formal use case or just a line written on a napkin. The key is to test behaviors, not code.</p>
</blockquote>
<p>Now that we have explored these techniques, it is time to introduce the xUnit library, ways to write tests, and how tests are written in the book. Let’s start by creating a test project.</p>



<h2 data-number="3.6">How to create an xUnit test project</h2>
<p>To create a new xUnit test project, you can run the <code>dotnet new xunit</code> command, and the CLI does the job for you by creating a project containing a <code>UnitTest1</code> class. That command does the same as creating a new xUnit project from Visual Studio.For unit testing projects, name the project the same as the project you want to test and append <code>.Tests</code> to it. For example, <code>MyProject</code> would have a <code>MyProject.Tests</code> project associated with it. We explore more details in the <em>Organizing your tests</em> section below.The template already defines all the required NuGet packages, so you can start testing immediately after adding a reference to your project under test.</p>
<blockquote>
<p>You can also add project references using the CLI with the <code>dotnet add reference</code> command. Assuming we are in the <code>./test/MyProject.Tests</code> directory and the project file we want to reference is in the <code>./src/MyProject</code> directory; we can execute the following command to add a reference:</p>
</blockquote>
<pre><code>dotnet add reference ../../src/MyProject.csproj.</code></pre>
<p>Next, we explore some xUnit features that will allow us to write test cases.</p>


<h2 data-number="3.7">Key xUnit features</h2>
<p>In xUnit, the <code>[Fact]</code> attribute is the way to create unique test cases, while the <code>[Theory]</code> attribute is the way to make data-driven test cases. Let’s start with facts, the simplest way to write a test case.</p>

<h3 data-number="3.7.1">Facts</h3>
<p>Any method with no parameter can become a test method by decorating it with a <code>[Fact]</code> attribute, like this:</p>
<div><pre><code>public class FactTest
{
    [Fact]
    public void Should_be_equal()
    {
        var expectedValue = 2;
        var actualValue = 2;
        Assert.Equal(expectedValue, actualValue);
    }
}</code></pre>
</div>
<p>You can also decorate asynchronous methods with the fact attribute when the code under test needs it:</p>
<div><pre><code>public class AsyncFactTest
{
    [Fact]
    public async Task Should_be_equal()
    {
        var expectedValue = 2;
        var actualValue = 2;
        await Task.Yield();
        Assert.Equal(expectedValue, actualValue);
    }
}</code></pre>
</div>
<p>In the preceding code, the highlighted line conceptually represents an asynchronous operation and does nothing more than allow using the <code>async</code>/<code>await</code> keywords.When we run the tests from Visual Studio’s Test Explorer, the test run result looks like this:</p>
<figure>
<img alt="Figure 2.3: Test results in Visual Studio" src="img/file5.png"/><figcaption aria-hidden="true">Figure 2.3: Test results in Visual Studio</figcaption>
</figure>
<p>You may have noticed from the screenshot that the test classes are nested in the <code>xUnitFeaturesTest</code> class, part of the <code>MyApp</code> namespace, and under the <code>MyApp.Tests</code> project. We explore those details later in the chapter.Running the <code>dotnet test</code> CLI command should yield a result similar to the following:</p>
<div><pre><code>Passed!  - Failed:     0, Passed:    23, Skipped:     0, Total:    23, Duration: 22 ms - MyApp.Tests.dll (net8.0)</code></pre>
</div>
<p>As we can read from the preceding output, all tests are passing, none have failed, and none were skipped. It is as simple as that to create test cases using xUnit.</p>
<blockquote>
<p>Learning the CLI can be very helpful in creating and debugging CI/CD pipelines, and you can use them, like the <code>dotnet test</code> command, in any script (like bash and PowerShell).</p>
</blockquote>
<p>Have you noticed the <code>Assert</code> keyword in the test code? If you are not familiar with it, we will explore assertions next.</p>


<h3 data-number="3.7.2">Assertions</h3>
<p>An assertion is a statement that checks whether a particular condition is <code>true</code> or <code>false</code>. If the condition is <code>true</code>, the test passes. If the condition is <code>false</code>, the test fails, indicating a problem with the subject under test.Let’s visit a few ways to assert correctness. We use barebone xUnit functionality in this section, but you can bring in the assertion library of your choice if you have one.</p>
<blockquote>
<p>In xUnit, the assertion throws an exception when it fails, but you may never even realize that. You do not have to handle those; that’s the mechanism to propagate the failure result to the test runner.</p>
</blockquote>
<p>We won’t explore all possibilities, but let’s start with the following shared pieces:</p>
<div><pre><code>public class AssertionTest
{
    [Fact]
    public void Exploring_xUnit_assertions()
    {
        object obj1 = new MyClass { Name = "Object 1" };
        object obj2 = new MyClass { Name = "Object 1" };
        object obj3 = obj1;
        object? obj4 = default(MyClass);
        //
        // Omitted assertions
        // 
        static void OperationThatThrows(string name)
        {
            throw new SomeCustomException { Name = name };
        }
    }
    private record class MyClass
    {
        public string? Name { get; set; }
    }
    private class SomeCustomException : Exception
    {
        public string? Name { get; set; }
    }
}</code></pre>
</div>
<p>The two preceding record classes, the <code>OperationThatThrows</code> method, and the variables are utilities used in the test to help us play with xUnit assertions. The variables are of type <code>object</code> for exploration purposes, but you can use any type in your test cases. I omitted the assertion code that we are about to see to keep the code leaner.The following two assertions are very explicit:</p>
<div><pre><code>Assert.Equal(expected: 2, actual: 2);
Assert.NotEqual(expected: 2, actual: 1);</code></pre>
</div>
<p>The first compares whether the actual value equals the expected value, while the second compares if the two values are different. <code>Assert.Equal</code> is probably the most commonly used assertion method.</p>
<blockquote>
<p>As a rule of thumb, it is better to assert equality (<code>Equal</code>) than assert that the values are different (<code>NotEqual</code>). Except in a few rare cases, asserting equality will yield more consistent results and close the door to missing defects.</p>
</blockquote>
<p>The next two assertions are very similar to the equality ones but assert that the objects are the same instance or not (the same instance means the same reference):</p>
<div><pre><code>Assert.Same(obj1, obj3);
Assert.NotSame(obj2, obj3);</code></pre>
</div>
<p>The next one validates that the two objects are equal. Since we are using record classes, it makes it super easy for us; <code>obj1</code> and <code>obj2</code> are not the same (two instances) but are equal (see <em>Appendix A</em> for more information on record classes):</p>
<div><pre><code>Assert.Equal(obj1, obj2);</code></pre>
</div>
<p>The next two are very similar and assert that the value is <code>null</code> or not:</p>
<div><pre><code>Assert.Null(obj4);
Assert.NotNull(obj3);</code></pre>
</div>
<p>The next line asserts that <code>obj1</code> is of the <code>MyClass</code> type and then returns the argument (<code>obj1</code>) converted to the asserted type (<code>MyClass</code>). If the type is incorrect, the <code>IsType</code> method will throw an exception:</p>
<div><pre><code>var instanceOfMyClass = Assert.IsType&lt;MyClass&gt;(obj1);</code></pre>
</div>
<p>Then we reuse the <code>Assert.Equal</code> method to validate that the value of the <code>Name</code> property is what we expect:</p>
<div><pre><code>Assert.Equal(expected: "Object 1", actual: instanceOfMyClass.Name);</code></pre>
</div>
<p>The following code block asserts that the <code>testCode</code> argument throws an exception of the <code>SomeCustomException</code> type:</p>
<div><pre><code>var exception = Assert.Throws&lt;SomeCustomException&gt;(
    testCode: () =&gt; OperationThatThrows("Toto")
);</code></pre>
</div>
<p>The <code>testCode</code> argument executes the <code>OperationThatThrows</code> inline function we saw initially. The <code>Throws</code> method allows us to test some exception properties by returning the exception in the specified type. The same behavior as the <code>IsType</code> method happens here; if the exception is of the wrong type or no exception is thrown, the <code>Throws</code> method will fail the test.</p>
<blockquote>
<p>It is a good idea to ensure that not only the proper exception type is thrown, but the exception carries the correct values as well.</p>
</blockquote>
<p>The following line asserts that the value of the <code>Name</code> property is what we expect it to be, ensuring our program would propagate the proper exception:</p>
<div><pre><code>Assert.Equal(expected: "Toto", actual: exception.Name);</code></pre>
</div>
<p>We covered a few assertion methods, but many others are part of xUnit, like the <code>Collection</code>, <code>Contains</code>, <code>False</code>, and <code>True</code> methods. We use many assertions throughout the book, so if these are still unclear, you will learn more about them.Next, let’s look at data-driven test cases using theories.</p>


<h3 data-number="3.7.3">Theories</h3>
<p>For more complex test cases, we can use theories. A theory contains two parts:</p>
<ul>
<li>A <code>[Theory]</code> attribute that marks the method as a theory.</li>
<li>At least one data attribute that allows passing data to the test method: <code>[InlineData]</code>, <code>[MemberData]</code>, or <code>[ClassData]</code>.</li>
</ul>
<p>When writing a theory, your primary constraint is ensuring that the number of values matches the parameters defined in the test method. For example, a theory with one parameter must be fed one value. We look at some examples next.</p>
<blockquote>
<p>You are not limited to only one type of data attribute; you can use as many as you need to suit your needs and feed a theory with the appropriate data.</p>
</blockquote>
<p>The <code>[InlineData]</code> attribute is the most suitable for constant values or smaller sets of values. Inline data is the most straightforward way of the three because of the proximity of the test values and the test method.Here is an example of a theory using inline data:</p>
<div><pre><code>public class InlineDataTest
{
    [Theory]
    [InlineData(1, 1)]
    [InlineData(2, 2)]
    [InlineData(5, 5)]
    public void Should_be_equal(int value1, int value2)
    {
        Assert.Equal(value1, value2);
    }
}</code></pre>
</div>
<p>That test method yields three test cases in the Test Explorer, where each can pass or fail individually. Of course, since 1 equals 1, 2 equals 2, and 5 equals 5, all three test cases are passing, as shown here:</p>
<figure>
<img alt="Figure 2.4: Inline data theory test results" src="img/file6.png"/><figcaption aria-hidden="true">Figure 2.4: Inline data theory test results</figcaption>
</figure>
<p>We can also use the <code>[MemberData]</code> and <code>[ClassData]</code> attributes to simplify the test method’s declaration when we have a large set of data to tests. We can also do that when it is impossible to instantiate the data in the attribute. We can also reuse the data in multiple test methods or encapsulate the data away from the test class.Here is a medley of examples of the <code>[MemberData]</code> attribute usage:</p>
<div><pre><code>public class MemberDataTest
{
    public static IEnumerable&lt;object[]&gt; Data =&gt; new[]
    {
        new object[] { 1, 2, false },
        new object[] { 2, 2, true },
        new object[] { 3, 3, true },
    };
    public static TheoryData&lt;int, int, bool&gt; TypedData =&gt;new TheoryData&lt;int, int, bool&gt;
    {
        { 3, 2, false },
        { 2, 3, false },
        { 5, 5, true },
    };
    [Theory]
    [MemberData(nameof(Data))]
    [MemberData(nameof(TypedData))]
    [MemberData(nameof(ExternalData.GetData), 10, MemberType = typeof(ExternalData))]
    [MemberData(nameof(ExternalData.TypedData), MemberType = typeof(ExternalData))]
    public void Should_be_equal(int value1, int value2, bool shouldBeEqual)
    {
        if (shouldBeEqual)
        {
            Assert.Equal(value1, value2);
        }
        else
        {
            Assert.NotEqual(value1, value2);
       }
    }
    public class ExternalData
    {
        public static IEnumerable&lt;object[]&gt; GetData(int start) =&gt; new[]
        {
            new object[] { start, start, true },
            new object[] { start, start + 1, false },
            new object[] { start + 1, start + 1, true },
        };
        public static TheoryData&lt;int, int, bool&gt; TypedData =&gt; new TheoryData&lt;int, int, bool&gt;
        {
            { 20, 30, false },
            { 40, 50, false },
            { 50, 50, true },
        };
    }
}</code></pre>
</div>
<p>The preceding test case yields 12 results. If we break it down, the code starts by loading three sets of data from the <code>Data</code> property by decorating the test method with the <code>[MemberData(nameof(Data))]</code> attribute. This is how to load data from a member of the class the test method is declared in.Then, the second property is very similar to the <code>Data</code> property but replaces <code>IEnumerable&lt;object[]&gt;</code> with a <code>TheoryData&lt;…&gt;</code> class, making it more readable and type-safe. Like with the first attribute, we feed those three sets of data to the test method by decorating it with the <code>[MemberData(nameof(TypedData))]</code> attribute. Once again, it is part of the test class.</p>
<blockquote>
<p>I strongly recommend using <code>TheoryData&lt;…&gt;</code> by default.</p>
</blockquote>
<p>The third data feeds three more sets of data to the test method. However, that data originates from the <code>GetData</code> method of the <code>ExternalData</code> class, sending <code>10</code> as an argument during the execution (the <code>start</code> parameter). To do that, we must specify the <code>MemberType</code> instance where the method is located so xUnit knows where to look. In this case, we pass the argument <code>10</code> as the second parameter of the <code>MemberData</code> constructor. However, in other cases, you can pass zero or more arguments there.Finally, we are doing the same for the <code>ExternalData.TypedData</code> property, which is represented by the <code>[MemberData(nameof(ExternalData.TypedData), MemberType = typeof(ExternalData))]</code> attribute. Once again, the only difference is that the property is defined using <code>TheoryData</code> instead of <code>IEnumerable&lt;object[]&gt;</code>, which makes its intent clearer.When running the tests, the data provided by the <code>[MemberData]</code> attributes are combined, yielding the following result in the Test Explorer:</p>
<figure>
<img alt="Figure 2.5: Member data theory test results" src="img/file7.png"/><figcaption aria-hidden="true">Figure 2.5: Member data theory test results</figcaption>
</figure>
<p>These are only a few examples of what we can do with the <code>[MemberData]</code> attribute.</p>
<blockquote>
<p>I understand that’s a lot of condensed information, but the goal is to cover just enough to get you started. I don’t expect you to become an expert in xUnit by reading this chapter.</p>
</blockquote>
<p>Last but not least, the <code>[ClassData]</code> attribute gets its data from a class implementing <code>IEnumerable&lt;object[]&gt;</code> or inheriting from <code>TheoryData&lt;…&gt;</code>. The concept is the same as the other two. Here is an example:</p>
<div><pre><code>public class ClassDataTest
{
    [Theory]
    [ClassData(typeof(TheoryDataClass))]
    [ClassData(typeof(TheoryTypedDataClass))]
    public void Should_be_equal(int value1, int value2, bool shouldBeEqual)
    {
        if (shouldBeEqual)
        {
            Assert.Equal(value1, value2);
        }
        else
        {
            Assert.NotEqual(value1, value2);
        }
    }
    public class TheoryDataClass : IEnumerable&lt;object[]&gt;
    {
        public IEnumerator&lt;object[]&gt; GetEnumerator()
        {
            yield return new object[] { 1, 2, false };
            yield return new object[] { 2, 2, true };
            yield return new object[] { 3, 3, true };
        }
        IEnumerator IEnumerable.GetEnumerator() =&gt; GetEnumerator();
    }
    public class TheoryTypedDataClass : TheoryData&lt;int, int, bool&gt;
    {
        public TheoryTypedDataClass()
        {
            Add(102, 104, false);
        }
    }
}</code></pre>
</div>
<p>These are very similar to <code>[MemberData]</code>, but we point to a type instead of pointing to a member.In <code>TheoryDataClass</code>, implementing the <code>IEnumerable&lt;object[]&gt;</code> interface makes it easy to <code>yield return</code> the results. On the other hand, in the <code>TheoryTypedDataClass</code> class, by inheriting <code>TheoryData</code>, we can leverage a list-like <code>Add</code> method. Once again, I find inheriting from <code>TheoryData</code> more explicit, but either way works with xUnit. You have many options, so choose the best one for your use case.Here is the result in the Test Explorer, which is very similar to the other attributes:</p>
<figure>
<img alt="Figure 2.6: Test Explorer" src="img/file8.png"/><figcaption aria-hidden="true">Figure 2.6: Test Explorer</figcaption>
</figure>
<p>That’s it for the theories—next, a few last words before organizing our tests.</p>


<h3 data-number="3.7.4">Closing words</h3>
<p>Now that facts, theories, and assertions are out of the way, xUnit offers other mechanics to allow developers to inject dependencies into their test classes. These are named fixtures. Fixtures allow dependencies to be reused by all test methods of a test class by implementing the <code>IClassFixture&lt;T&gt;</code> interface. Fixtures are very helpful for costly dependencies, like creating an in-memory database. With fixtures, you can create the dependency once and use it multiple times. The <code>ValuesControllerTest</code> class in the <code>MyApp.IntegrationTests</code> project shows that in action.It is important to note that xUnit creates an instance of the test class for every test run, so your dependencies are recreated every time if you are not using the fixtures.You can also share the dependency provided by the fixture between multiple test classes by using <code>ICollectionFixture&lt;T&gt;</code>, <code>[Collection]</code>, and <code>[CollectionDefinition]</code> instead. We won’t get into the details here, but at least you know it’s possible and know what types to look for when you need something similar.Finally, if you have worked with other testing frameworks, you might have encountered <strong>setup</strong> and <strong>teardown</strong> methods. In xUnit, there are no particular attributes or mechanisms for handling setup and teardown code. Instead, xUnit uses existing OOP concepts:</p>
<ul>
<li>To set up your tests, use the class constructor.</li>
<li>To tear down (clean up) your tests, implement <code>IDisposable</code> or <code>IAsyncDisposable</code> and dispose of your resources there.</li>
</ul>
<p>That’s it, xUnit is very simple and powerful, which is why I adopted it as my main testing framework several years ago and chose it for this book.Next, we learn to write readable test methods.</p>



<h2 data-number="3.8">Arrange, Act, Assert</h2>
<p>Arrange, Act, Assert (AAA or 3A) is a well-known method for writing readable tests. This technique allows you to clearly define your setup (arrange), the operation under test (act), and your assertions (assert). One efficient way to use this technique is to start by writing the 3A as comments in your test case and then write the test code in between. Here is an example:</p>
<div><pre><code>[Fact]
public void Should_be_equals()
{
    // Arrange
    var a = 1;
    var b = 2;
    var expectedResult = 3;
    // Act
    var result = a + b;
    // Assert
    Assert.Equal(expectedResult, result);
}</code></pre>
</div>
<p>Of course, that test case cannot fail, but the three blocks are easily identifiable with the 3A comments.In general, <strong>you want the Act block of your unit tests to be a single line</strong>, making the test focus clear. If you need more than one line, the chances are that something is wrong in the test or the design.</p>
<blockquote>
<p>When the tests are very small (only a few lines), removing the comments might help readability. Furthermore, when you have nothing to set up in your test case, delete the Arrange comment to improve its readability further.</p>
</blockquote>
<p>Next, we learn how to organize tests into projects, directories, and files.</p>


<h2 data-number="3.9">Organizing your tests</h2>
<p>There are many ways of organizing test projects inside a solution, and I tend to create a unit test project for each project in the solution and one or more integration test projects.A unit test is directly related to a single unit of code, whether it’s a method or a class. It is straightforward to associate a unit test project with its respective code project (assembly), leading to a one-on-one relationship. One unit test project per assembly makes them portable, easier to navigate, and even more so when the solution grows.</p>
<blockquote>
<p>If you have a preferred way to organize yours that differs from what we are doing in the book, by all means, use that approach instead.</p>
</blockquote>
<p>Integration tests, on the other hand, can span multiple projects, so having a single rule that fits all scenarios is challenging. One integration test project per solution is often enough. Sometimes we can need more than one, depending on the context.</p>
<blockquote>
<p>I recommend starting with one integration test project and adding more as needed during development instead of overthinking it before getting started. Trust your judgment; you can always change the structure as your project evolves.</p>
</blockquote>
<p>Folder-wise, at the solution level, creating the application and its related libraries in an <code>src</code> directory helps isolate the actual solution code from the test projects created under a <code>test</code> directory, like this:</p>
<figure>
<img alt="Figure 2.7: The Automated Testing Solution Explorer, displaying how the projects are organized" src="img/file9.png"/><figcaption aria-hidden="true">Figure 2.7: The Automated Testing Solution Explorer, displaying how the projects are organized</figcaption>
</figure>
<p>That’s a well-known and effective way of organizing a solution in the .NET world.</p>
<blockquote>
<p>Sometimes, it is not possible or unwanted to do that. One such use case would be multiple microservices written under a single solution. In that case, you might want the tests to live closer to your microservices and not split them between <code>src</code> and <code>test</code> folders. So you could organize your solution by microservice instead, like one directory per microservice that contains all the projects, including tests.</p>
</blockquote>
<p>Let’s now dig deeper into organizing unit tests.</p>

<h3 data-number="3.9.1">Unit tests</h3>
<p>How you organize your test projects may make a big difference between searching for your tests or making it easy to find them. Let’s look at the different aspects, from the <code>namespace</code> to the test code itself.</p>

<h4 data-number="3.9.1.1">Namespace</h4>
<p>I find it convenient to create unit tests in the same namespace as the subject under test when creating unit tests. That helps get tests and code aligned without adding any additional using statements. To make it easier when creating files, you can change the default namespace used by Visual Studio when creating a new class in your test project by adding <code>&lt;RootNamespace&gt;[Project under test namespace]&lt;/RootNamespace&gt;</code> to a <code>PropertyGroup</code> of the test project file (<code>*.csproj</code>), like this:</p>
<div><pre><code>&lt;PropertyGroup&gt;
  ...
  &lt;RootNamespace&gt;MyApp&lt;/RootNamespace&gt;
&lt;/PropertyGroup&gt;</code></pre>
</div>


<h4 data-number="3.9.1.2">Test class name</h4>
<p>By convention, I name test classes <code>[class under test]Test.cs</code> and create them in the same directory as in the original project. Finding tests is easy when following that simple rule since the test code is in the same location of the file tree as the code under test but in two distinct projects.</p>
<figure>
<img alt="Figure 2.8: The Automated Testing Solution Explorer, displaying how tests are organized" src="img/file10.png"/><figcaption aria-hidden="true">Figure 2.8: The Automated Testing Solution Explorer, displaying how tests are organized</figcaption>
</figure>


<h4 data-number="3.9.1.3">Test code inside the test class</h4>
<p>For the test code itself, I follow a multi-level structure similar to the following:</p>
<ul>
<li>One test class is named the same as the class under test.</li>
<li>One nested test class per method to test from the class under test.</li>
<li>One test method per test case of the method under test.</li>
</ul>
<p>This technique helps organize tests by test case while keeping a clear hierarchy, leading to the following hierarchy:</p>
<ul>
<li>Class under test</li>
<li>Method under test</li>
<li>Test case using that method</li>
</ul>
<p>In code, that translates to the following:</p>
<div><pre><code>namespace MyApp.IntegrationTests.Controllers;
public class ValuesControllerTest
{
    public class Get : ValuesControllerTest
    {
        [Fact]
        public void Should_return_the_expected_strings()
        {
            // Arrange
            var sut = new ValuesController();
            // Act
            var result = sut.Get();
            // Assert
            Assert.Collection(result.Value,
                x =&gt; Assert.Equal("value1", x), 
                x =&gt; Assert.Equal("value2", x) 
            );
        }
    }
}</code></pre>
</div>
<p>This convention allows you to set up tests step by step. For example, by inheriting the outer class (the <code>ValuesControllerTest</code> class here) from the inner class (the <code>Get</code> nested class), you can create top-level private mocks or classes shared by all nested classes and test methods. Then, for each method to test, you can modify the setup or create other private test elements in the nested classes. Finally, you can do more configuration per test case inside the test method (the <code>Should_return_the_expected_strings</code> method here).</p>
<blockquote>
<p>Don’t go too hard on reusability inside your test classes, as it can make tests harder to read from an external eye, such as a reviewer or another developer that needs to play there. Unit tests should remain focused, small, and easy to read: a unit of code testing another unit of code. Too much reusability may lead to a brittle test suite.</p>
</blockquote>
<p>Now that we have explored organizing unit tests, let’s look at integration tests.</p>



<h3 data-number="3.9.2">Integration tests</h3>
<p>Integration tests are harder to organize because they depend on multiple units, can cross project boundaries, and interact with various dependencies.We can create one integration test project for most simple solutions or many for more complex scenarios.When creating one, you can name the project <code>IntegrationTests</code> or start with the entry point of your tests, like a REST API project, and name the project <code>[Name of the API project].IntegrationTests</code>. At this point, how to name the integration test project depends on your solution structure and intent.When you need multiple integration projects, you can follow a convention similar to unit tests and associate your integration projects one-to-one: <code>[Project under test].IntegrationTests</code>.Inside those projects, it depends on how you want to attack the problem and the structure of the solution itself. Start by identifying the features under test. Name the test classes in a way that mimics your requirements, organize those into sub-folders (maybe a category or group of requirements), and code test cases as methods. You can also leverage nested classes, as we did with unit tests.</p>
<blockquote>
<p>We write tests throughout the book, so you will have plenty of examples to make sense of all this if it’s not clear now.</p>
</blockquote>
<p>Next, we implement an integration test by leveraging ASP.NET Core features.</p>



<h2 data-number="3.10">Writing ASP.NET Core integration tests</h2>
<p>When Microsoft built ASP.NET Core from the ground up, they fixed and improved so many things that I cannot enumerate them all here, including testability.Nowadays, there are two ways to structure a .NET program:</p>
<ul>
<li>The classic ASP.NET Core <code>Program</code> and the <code>Startup</code> classes. This model might be found in existing projects (created before .NET 6).</li>
<li>The minimal hosting model introduced in .NET 6. This may look familiar to you if you know Node.js, as this model encourages you to write the start-up code in the Program.cs file by leveraging top-level statements. You will most likely find this model in new projects (created after the release of .NET 6).</li>
</ul>
<p>No matter how you write your program, that’s the place to define how the application’s composition and how it boots. Moreover, we can leverage the same testing tools more or less seamlessly.In the case of a web application, the scope of our integration tests is often to call the endpoint of a controller over HTTP and assert the response. Luckily, in .NET Core 2.1, the .NET team added the <code>WebApplicationFactory&lt;TEntry&gt;</code> class to make the integration testing of web applications easier. With that class, we can boot up an ASP.NET Core application in memory and query it using the supplied <code>HttpClient</code> in a few lines of code. The test classes also provide extension points to configure the server, such as replacing implementations with mocks, stubs, or other test-specific elements.Let’s start by booting up a classic web application test.</p>

<h3 data-number="3.10.1">Classic web application</h3>
<p>In a classic ASP.NET Core application, the <code>TEntry</code> generic parameter of the <code>WebApplicationFactory&lt;TEntry&gt;</code> class is usually the <code>Startup</code> or <code>Program</code> class of your project under test.</p>
<blockquote>
<p>The test cases are in the <code>Automated Testing</code> solution under the <code>MyApp.IntegrationTests</code> project.</p>
</blockquote>
<p>Let’s start by looking at the test code structure before breaking it down:</p>
<div><pre><code>namespace MyApp.IntegrationTests.Controllers;
public class ValuesControllerTest : IClassFixture&lt;WebApplicationFactory&lt;Startup&gt;&gt;
{
    private readonly HttpClient _httpClient;
    public ValuesControllerTest(
        WebApplicationFactory&lt;Startup&gt; webApplicationFactory)
    {
        _httpClient = webApplicationFactory.CreateClient();
    }
    public class Get : ValuesControllerTest
    {
        public Get(WebApplicationFactory&lt;Startup&gt; webApplicationFactory)
            : base(webApplicationFactory) { }
        [Fact]
        public async Task Should_respond_a_status_200_OK()
        {
            // Omitted Test Case 1
        }
        [Fact]
        public async Task Should_respond_the_expected_strings()
        {
            // Omitted Test Case 2
        }
    }
}</code></pre>
</div>
<p>The first piece of the preceding code that is relevant to us is how we get an instance of the <code>WebApplicationFactory&lt;Startup&gt;</code> class. We inject a <code>WebApplicationFactory&lt;Startup&gt;</code> object into the constructor by implementing the <code>IClassFixture&lt;T&gt;</code> interface (a xUnit feature). We can also use the factory to configure the test server, but we don’t need to here, so we can only keep a reference on the <code>HttpClient</code>, preconfigured to connect to the in-memory test server.Then, we may have noticed we have the nested <code>Get</code> class that inherits the <code>ValuesControllerTest</code> class. The <code>Get</code> class contains the test cases. By inheriting the <code>ValuesControllerTest</code> class, we can leverage the <code>_httpClient</code> field from the test cases we are about to see.In the first test case, we use <code>HttpClient</code> to query the <code>http://localhost/api/values</code> URI, accessible through the in-memory server. Then, we assert that the status code of the HTTP response was a success (<code>200 OK</code>):</p>
<div><pre><code>[Fact]
public async Task Should_respond_a_status_200_OK()
{
    // Act
    var result = await _httpClient
        .GetAsync("/api/values");
    // Assert
    Assert.Equal(HttpStatusCode.OK, result.StatusCode);
}</code></pre>
</div>
<p>The second test case also sends an HTTP request to the in-memory server but deserializes the body’s content as a string[] to ensure the values are the same as expected instead of validating the status code:</p>
<div><pre><code>[Fact]
public async Task Should_respond_the_expected_strings()
{
    // Act
    var result = await _httpClient
        .GetFromJsonAsync&lt;string[]&gt;("/api/values");
    // Assert
    Assert.Collection(result,
        x =&gt; Assert.Equal("value1", x),
        x =&gt; Assert.Equal("value2", x)
    );
}</code></pre>
</div>
<blockquote>
<p>As you may have noticed from the test cases, the <code>WebApplicationFactory</code> preconfigured the <code>BaseAddress</code> property for us, so we don’t need to prefix our requests with <code>http://localhost</code>.</p>
</blockquote>
<p>When running those tests, an in-memory web server starts. Then, HTTP requests are sent to that server, testing the complete application. The tests are simple in this case, but you can create more complex test cases in more complex programs.Next, we explore how to do the same for minimal APIs.</p>


<h3 data-number="3.10.2">Minimal hosting</h3>
<p>Unfortunately, we must use a workaround to make the <code>Program</code> class discoverable when using minimal hosting. Let’s explore a few workarounds that leverage minimal APIs, allowing you to pick the one you prefer.</p>

<h4 data-number="3.10.2.1">First workaround</h4>
<p>The <strong>first workaround</strong> is to use any other class in the assembly as the <code>TEntryPoint</code> of <code>WebApplicationFactory&lt;TEntryPoint&gt;</code> instead of the <code>Program</code> or <code>Startup</code> class. This makes what <code>WebApplicationFactory</code> does a little less explicit, but that’s all. Since I tend to prefer readable code, I do not recommend this.</p>


<h4 data-number="3.10.2.2">Second workaround</h4>
<p>The <strong>second workaround</strong> is to add a line at the bottom of the <code>Program.cs</code> file (or anywhere else in the project) to change the autogenerated <code>Program</code> class visibility from <code>internal</code> to <code>public</code>. Here is the complete <code>Program.cs</code> file with that added line (highlighted):</p>
<div><pre><code>var builder = WebApplication.CreateBuilder(args);
var app = builder.Build();
app.MapGet("/", () =&gt; "Hello World!");
app.Run();
public partial class Program { }</code></pre>
</div>
<p>Then, the test cases are very similar to the ones of the classic web application explored previously. The only difference is the program itself, both programs don’t do the same thing.</p>
<div><pre><code>namespace MyMinimalApiApp;
public class ProgramTest : IClassFixture&lt;WebApplicationFactory&lt;Program&gt;&gt;
{
    private readonly HttpClient _httpClient;
    public ProgramTest(
        WebApplicationFactory&lt;Program&gt; webApplicationFactory)
    {
        _httpClient = webApplicationFactory.CreateClient();
    }
    public class Get : ProgramTest
    {
        public Get(WebApplicationFactory&lt;Program&gt; webApplicationFactory) 
            : base(webApplicationFactory) { }
        [Fact]
        public async Task Should_respond_a_status_200_OK()
        {
            // Act
            var result = await _httpClient.GetAsync("/");
            // Assert
            Assert.Equal(HttpStatusCode.OK, result.StatusCode);
        }
        [Fact]
        public async Task Should_respond_hello_world()
        {
            // Act
            var result = await _httpClient.GetAsync("/");
            // Assert
            var contentText = await result.Content.ReadAsStringAsync();
            Assert.Equal("Hello World!", contentText);
        }
    }
}</code></pre>
</div>
<p>The only change is the expected result as the endpoint returns the <code>text/plain</code> string <code>Hello World!</code> instead of a collection of strings serialized as JSON. The test cases would be identical if the two endpoints produced the same result.</p>


<h4 data-number="3.10.2.3">Third workaround</h4>
<p>The <strong>third workaround</strong> is to instantiate <code>WebApplicationFactory</code> manually instead of leveraging a fixture. We can use the <code>Program</code> class, which requires changing its visibility by adding the following line to the <code>Program.cs</code> file:</p>
<div><pre><code>public partial class Program { }</code></pre>
</div>
<p>However, instead of injecting the instance using the <code>IClassFixture</code> interface, we instantiate the factory manually. To ensure we dispose the <code>WebApplicationFactory</code> instance, we also implement the <code>IAsyncDisposable</code> interface.Here’s the complete example, which is very similar to the previous workaround:</p>
<div><pre><code>namespace MyMinimalApiApp;
public class ProgramTestWithoutFixture : IAsyncDisposable
{
    private readonly WebApplicationFactory&lt;Program&gt; _webApplicationFactory;
    private readonly HttpClient _httpClient;
    public ProgramTestWithoutFixture()
    {
        _webApplicationFactory = new WebApplicationFactory&lt;Program&gt;();
        _httpClient = _webApplicationFactory.CreateClient();
    }
    public ValueTask DisposeAsync()
    {
        return ((IAsyncDisposable)_webApplicationFactory)
            .DisposeAsync();
    }
    // Omitted nested Get class
}</code></pre>
</div>
<p>I omitted the test cases in the preceding code block because they are the same as the previous workarounds. The full source code is available on GitHub: <a href="https://adpg.link/vzkr">https://adpg.link/vzkr</a>.</p>
<blockquote>
<p>Using class fixtures is more performant since the factory and the server get created only once per test run instead of recreated for every test method.</p>
</blockquote>


<h4 data-number="3.10.2.4">Creating a test application</h4>
<p>Finally, we can create a dedicated class that instantiates <code>WebApplicationFactory</code> manually. It leverages the other workarounds but makes the test cases more readable. By encapsulating the setup of the test application in a class, you will improve the reusability and maintenance cost in most cases.First, we need to change the <code>Program</code> class visibility by adding the following line to the <code>Project.cs</code> file:</p>
<div><pre><code>public partial class Program { }</code></pre>
</div>
<p>Now that we can access the Program class without the need to allow internal visibility to our test project, we can create our test application like this:</p>
<div><pre><code>namespace MyMinimalApiApp;
public class MyTestApplication : WebApplicationFactory&lt;Program&gt; {}</code></pre>
</div>
<p>Finally, we can reuse the same code to test our program but instantiate <code>MyTestApplication</code> instead of <code>WebApplicationFactory&lt;Program&gt;</code>, highlighted in the following code:</p>
<div><pre><code>namespace MyMinimalApiApp;
public class MyTestApplicationTest
{
    public class Get : ProgramTestWithoutFixture
    {
        [Fact]
        public async Task Should_respond_a_status_200_OK()
        {
            // Arrange
            await using var app = new MyTestApplication();
            var httpClient = app.CreateClient();
            // Act
            var result = await httpClient.GetAsync("/");
            // Assert
            Assert.Equal(HttpStatusCode.OK, result.StatusCode);
        }
    }
}</code></pre>
</div>
<p>You can also leverage fixtures, but for the sake of simplicity, I decided to show you how to instantiate our new test application manually.And that’s it. We have covered multiple ways to work around integration testing minimal APIs simplistically and elegantly. Next, we explore a few testing principles before moving to architectural principles in the next chapter.</p>




<h2 data-number="3.11">Important testing principles</h2>
<p>One essential thing to remember when writing tests is to test use cases, not the code itself; we are testing features’ correctness, not code correctness. Of course, if the expected outcome of a feature is correct, that also means the codebase is correct. However, it is not always true the other way around; correct code may yield an incorrect outcome. Also, remember that code costs money to write, while features deliver value.To help with that, test requirements should revolve around <strong>inputs and outputs</strong>. When specific values go into your subject under test, you expect particular values to come out. Whether you are testing a simple <code>Add</code> method where the ins are two or more numbers, and the out is the sum of those numbers, or a more complex feature where the ins come from a form, and the out is the record getting persisted in a database, most of the time, we are testing that inputs produced an output or an outcome.Another concept is to divide those units as a query or a command. No matter how you organize your code, from a simple single-file application to a microservices architecture-base Netflix clone, all simple or compounded operations are queries or commands. Thinking about a system this way should help you test the ins and outs. We discuss queries and commands in several chapters, so keep reading to learn more.Now that we have laid this out, what if a unit must perform multiple operations, such as reading from a database, and then send multiple commands? You can create and test multiple smaller units (individual operations) and another unit that orchestrates those building blocks, allowing you to test each piece in isolation. We explore how to achieve this throughout the book.In a nutshell, when writing automated tests:</p>
<ul>
<li>In case of a query, we assert the output of the unit undergoing testing based on its input parameters.</li>
<li>In case of a command, we assert the outcome of the unit undergoing testing based on its input parameters.</li>
</ul>
<p>We explore numerous techniques throughout the book to help you achieve that level of separation, starting with architectural principles in the next chapter.</p>


<h2 data-number="3.12">Summary</h2>
<p>This chapter covered automated testing, such as unit and integration tests. We also briefly covered end-to-end tests, but covering that in only a few pages is impossible. Nonetheless, how to write integration tests can also be used for end-to-end testing, especially in the REST API space.We explored different testing approaches from a bird’s eye view, tackled technical debt, and explored multiple testing techniques like black-box, white-box, and grey-box testing. We also peaked at a few formal ways to choose the values to test, like equivalence partitioning and boundary value analysis.We then looked at xUnit, the testing framework used throughout the book, and a way of organizing tests. We explored ways to pick the correct type of test and some guidelines about choosing the right quantity for each kind of test. Then we saw how easy it is to test our ASP.NET Core web applications by running it in memory. Finally, we explored high-level concepts that should guide you in writing testable, flexible, and reliable programs.Now that we have talked about testing, we are ready to explore a few architectural principles to help us increase programs’ testability. Those are a crucial part of modern software engineering and go hand in hand with automated testing.</p>


<h2 data-number="3.13">Questions</h2>
<p>Let’s take a look at a few practice questions:</p>
<ol>
<li>Is it true that in TDD, you write tests before the code to be tested?</li>
<li>What is the role of unit tests?</li>
<li>How big can a unit test be?</li>
<li>What type of test is usually used when the subject under test has to access a database?</li>
<li>Is doing TDD required?</li>
<li>Do you need to know the inner working of the application to do black-box testing?</li>
</ol>


<h2 data-number="3.14">Further reading</h2>
<p>Here are some links to build upon what we have learned in the chapter:</p>
<ul>
<li>xUnit: <a href="https://xunit.net/">https://xunit.net/</a></li>
<li>If you use Visual Studio, I have a few handy snippets to help improve productivity. They are available on GitHub: <a href="https://adpg.link/5TbY">https://adpg.link/5TbY</a></li>
</ul>


</body>
</html>
