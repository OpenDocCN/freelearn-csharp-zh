- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Integrating with OpenAI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore how to integrate advanced AI capabilities into
    your applications using OpenAI’s powerful language models. You will learn to set
    up the Azure OpenAI service and deploy models, laying the foundation for integrating
    AI into your applications. By implementing AI-enhanced features in web applications,
    such as smart-pasting and smart text areas, you will enhance the user experience
    with intelligent data processing and content generation. Additionally, you’ll
    build and integrate a ChatGPT-like chatbot for interactive AI-driven conversations.
    Finally, you will enable seamless data analysis by connecting your Azure OpenAI
    service to an existing Azure Search service data index.
  prefs: []
  type: TYPE_NORMAL
- en: Before we dive into recipes, it’s crucial to highlight the ethical implications
    of using AI models, particularly concerning user data. In some cases, by using
    and deploying AI models, you consent to training those models on your application
    content. You must be vigilant about the privacy and security of your users’ data
    and implement clear warnings and acceptance forms within your applications, allowing
    users to consent to or opt out of data sharing. By prioritizing transparency and
    user autonomy, you safeguard user trust and adhere to responsible AI practices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the recipes we will cover in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up an Azure OpenAI service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing smart pasting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a smart text area
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding a ChatBot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connecting an Azure OpenAI service to an existing data index
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will rely heavily on the Azure services and a few NuGet
    packages that may still be in preview when you install them. You will find all
    the details and warnings described in detail in each of the impacted recipes,
    so you have nothing to worry about. Before you dive in, make sure you have the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: An active Azure account with access to the Azure Portal (if you don’t have one
    yet, you can start with a time-limited, free account at [https://azure.microsoft.com/en-us/free](https://azure.microsoft.com/en-us/free)
    )
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A pre-created resource group in Azure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A pre-installed Npm package manager, with globally available **npm** command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Blazor Web App project with per component/page render modes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find all the code examples (and data samples) from the following recipes
    in a dedicated GitHub repository at [https://github.com/PacktPublishing/Blazor-Web-Development-Cookbook/tree/main](https://github.com/PacktPublishing/Blazor-Web-Development-Cookbook/tree/main)
    . Just be aware that we will implement some recipes only on the server side –
    you’ll understand why as you read through the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up an Azure OpenAI service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Azure OpenAI Service is a cloud-based offering from Microsoft Azure that provides
    access to OpenAI’s powerful language models. A **large language model** ( **LLM**
    ) is an optimized cost-function, trained on human texts, that can generate human-like
    text, allowing you to take chatbots, content generation, or language translation
    to the next level. By leveraging the Azure OpenAI service, you can integrate advanced
    AI capabilities into your Blazor application without managing the underlying infrastructure.
    You’re also getting access to existing GPT models.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s set up an Azure OpenAI service in the Azure cloud using the Azure portal,
    deploy a dedicated GPT-4 model, and locate access details required for integration
    on the application side.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we won’t write any code just yet; instead, we will focus on
    setting up the Azure OpenAI service. To get started, here are some prerequisites:'
  prefs: []
  type: TYPE_NORMAL
- en: You will need an Azure account and access to the Azure Portal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should create a resource group beforehand; we will use one named **blazor-cookbook**
    dedicated to this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the time of writing, the process of setting up the Azure OpenAI service consists
    of two phases.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first phase, do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You must complete a request form to gain access to the Azure OpenAI service.
    To find the request form, follow the first two steps outlined in the *How to do
    it* section that follows.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: After submitting the form, you will need to wait for approval from the **Azure
    Cognitive Services team** . You will receive a confirmation email once your request
    has been approved, marking the end of the first phase.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: We will walk through the second phase in the *How to do* *it* section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to add the Azure OpenAI service to your Azure resources:'
  prefs: []
  type: TYPE_NORMAL
- en: Open your resource group in Azure Portal and navigate to the Azure Marketplace
    by clicking the **Create** button in the top navigation bar.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.1: Navigating to Azure Marketplace from the resource group overview](img/B22020_10_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: Navigating to Azure Marketplace from the resource group overview'
  prefs: []
  type: TYPE_NORMAL
- en: In the Azure Marketplace, use the search bar in the top panel to find the **Azure
    OpenAI** service and start the creation process by clicking the **Create** button
    on the resulting tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.2: Navigating to the Azure OpenAI service creation panel](img/B22020_10_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: Navigating to the Azure OpenAI service creation panel'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the **Create Azure OpenAI** panel, provide the necessary details to create
    an Azure OpenAI instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the subscription for the service in the **Subscription** field.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the resource group where you want to create the service in the **Resource**
    **Group** field.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the hosting region in the **Region** field.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide a unique name for the service in the **Name** field.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the pricing plan in the **Pricing** **tier** field.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.3: First step of the Azure OpenAI service creation process – defining
    instance details](img/B22020_10_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: First step of the Azure OpenAI service creation process – defining
    instance details'
  prefs: []
  type: TYPE_NORMAL
- en: After reviewing the terms and conditions, click **Next** to proceed.
  prefs: []
  type: TYPE_NORMAL
- en: In the **Network** step, select the network availability for the service that
    best suits your needs and confirm by clicking **Next** .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.4: Second step of the Azure OpenAI service creation process – configuring
    network](img/B22020_10_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4: Second step of the Azure OpenAI service creation process – configuring
    network'
  prefs: []
  type: TYPE_NORMAL
- en: Leave the **Tags** step unchanged unless you have tag policies to follow in
    your organization and proceed by clicking **Next** .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the **Review + submit** step, review the service summary and confirm the
    creation request by clicking the **Create** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.5: Last step of the Azure OpenAI creation process – reviewing instance
    details](img/B22020_10_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.5: Last step of the Azure OpenAI creation process – reviewing instance
    details'
  prefs: []
  type: TYPE_NORMAL
- en: Once the deployment completes, open your resource group overview and select
    the **Azure** **OpenAI** service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.6: Selecting the Azure OpenAI instance from the resource group
    overview](img/B22020_10_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.6: Selecting the Azure OpenAI instance from the resource group overview'
  prefs: []
  type: TYPE_NORMAL
- en: Find the **Model deployments** feature in the **Resource Management** section
    in the left menu. Open the Azure OpenAI Studio by clicking the **Manage** **deployments**
    button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.7: Navigating to Azure OpenAI Studio to manage model deployments](img/B22020_10_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7: Navigating to Azure OpenAI Studio to manage model deployments'
  prefs: []
  type: TYPE_NORMAL
- en: In the Azure OpenAI Studio, find the **Deployments** feature in the **Management**
    section on the left menu and start the deployment process by clicking the **Create
    new** **deployment** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.8: Initiating model deployment through the Azure OpenAI Studio](img/B22020_10_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.8: Initiating model deployment through the Azure OpenAI Studio'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fill out the **Deploy model** form with the details of the model you intend
    to use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name the deployment in the **Deployment** **name** field.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose the model you want to deploy from the **Select a** **model** dropdown.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select a specific model version or the **Auto-update to default** option in
    the **Model** **version** dropdown.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose the type of deployment in the **Deployment** **type** dropdown.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.9: Filling the model deployment details and deploying the model](img/B22020_10_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.9: Filling the model deployment details and deploying the model'
  prefs: []
  type: TYPE_NORMAL
- en: After filling out all required fields, confirm the deployment by clicking **Create**
    .
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In *step 1* , we open the Azure Portal and find the resource group where we
    want to deploy the Azure OpenAI service. From the top bar of the overview panel,
    we select the **Create** option. Azure will redirect us to the Azure Marketplace,
    where we can choose the services to install.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 2* , we utilize the search bar at the top of the Azure Marketplace
    to look for Azure OpenAI. The result tab has a **Create** button, which we use
    to start the creation process.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 3* , we arrive at the first step of the Azure OpenAI creation – **Basics**
    . In this step, we fill out all the basic details of the instance we’re about
    to create. We choose the Azure subscription to define the owner of the service.
    Then, we select the appropriate resource group from the list assigned to the subscription.
    Next, we define the instance details, such as the hosting region and pricing tier.
    Be careful, as different areas have different AI models available. Also, depending
    on the pricing tier you select, you may incur service usage costs. To avoid that,
    opt for a free pricing tier (it has restricted scalability and request limits
    but will be enough for the recipes in this chapter). You can review the availability
    and pricing details by clicking the **View full pricing details** link. Then,
    we provide the instance name and a pricing tier. Once we have filled in all required
    fields, we move to the next step by clicking **Next** .
  prefs: []
  type: TYPE_NORMAL
- en: In *step 4* , we arrive at the **Network** step of the Azure OpenAI creation.
    In the **Network** tab, we define the discoverability of the service. We can disable
    network access entirely, configure private endpoints, set up network security
    within Azure, or make the instance publicly accessible. To keep it simple, we
    allow the instance access from any network, including the internet, and confirm
    by clicking **Next** to proceed.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 5* , we arrive at the **Tags** step of the Azure OpenAI creation. The
    **Tags** tab allows defining custom tags describing services. Unless you have
    tag-based policies defined in your organization, tags won’t have any functional
    impact. Hence, we leave the **Tags** panel unchanged and proceed to the last step
    by clicking **Next** .
  prefs: []
  type: TYPE_NORMAL
- en: In *step 6* , we arrive at the **Review + submit** panel, where we get the last
    chance to review the details of the instance we’re about to create. When everything
    checks out, we confirm the creation by clicking **Create** .
  prefs: []
  type: TYPE_NORMAL
- en: It will take some time for the service deployment to complete. When completed,
    we proceed to *step 7* . We navigate to the overview panel of the resource group
    and select the Azure OpenAI instance.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 8* , we find the **Resource Management** submenu and navigate to the
    **Model deployments** feature. In that panel, we click the **Manage Deployments**
    button and get redirected to the Azure OpenAI Studio for further steps.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 9* , in the Azure OpenAI Studio, we find the **Management** submenu
    and navigate to the **Deployments** panel. In the **Deployments** navigation bar,
    we click the **Create new deployment** button to initialize a model deployment
    for the Azure OpenAI service.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 10* , we arrive at the **Deploy model** submission form, where we must
    configure the deployment details. First, we define the deployment name – we will
    later use that name to specify which model to use for executing requests from
    the Blazor app. Next, we choose the model to deploy. Depending on the region where
    the Azure OpenAI instance is hosted, we can choose from a different set of AI
    models provided by Azure. To keep it simple, we opt to deploy GPT-4o. After choosing
    the model, we specify the version to use. From the dropdown, we can select a specific
    GPT model version or choose **Auto-update to default** to use the latest stable
    model. In the deployment form, we can fine-tune a rate limit for requests and
    a content filter, which we leave at default values. We can also enable **Dynamic
    quota** , allowing Azure to automatically scale up the tokens per minute limit
    when there’s higher traffic. When we have filled in all deployment details, we
    can start the process by clicking **Create** .
  prefs: []
  type: TYPE_NORMAL
- en: 'When the deployment completes, you’ll see the model in the **Deployments**
    panel of Azure OpenAI Studio:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.10: Azure OpenAI model deployments overview, showing the deployed
    model](img/B22020_10_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.10: Azure OpenAI model deployments overview, showing the deployed
    model'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To communicate with the Azure OpenAI instance and the deployed AI model, you
    will need the model deployment name (which we set in *step 10* ) and the Azure
    OpenAI API access details.
  prefs: []
  type: TYPE_NORMAL
- en: To find those details, navigate to the resource group and the created Azure
    OpenAI instance. In the menu on the left, select the **Keys and Endpoint** item
    in the **Resource** **Management** section.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.11: Navigating to the panel with the Azure OpenAI instance API
    access details](img/B22020_10_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.11: Navigating to the panel with the Azure OpenAI instance API access
    details'
  prefs: []
  type: TYPE_NORMAL
- en: You’ll arrive at the API details panel, which includes the **Endpoint** pointing
    to the Azure OpenAI instance, the location where it’s hosted, and two API keys.
    Having two API keys ensures continuous service availability when you need to regenerate
    one of them.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.12: API access details panel, with API keys and URI](img/B22020_10_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.12: API access details panel, with API keys and URI'
  prefs: []
  type: TYPE_NORMAL
- en: You will need the endpoint, API key, and deployed model name for all the upcoming
    recipes, so store them securely in your secrets storage or keep them handy in
    a notepad for quick access as we proceed through the implementations.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ve only touched on the Azure OpenAI service, covering the scope required
    to integrate OpenAI into a Blazor application. If you’d like to learn more, access
    the Microsoft Learn resource here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://learn.microsoft.com/en-us/azure/ai-services/openai/overview](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview
    )'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing smart pasting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One common challenge in web development is dealing with unstandardized data,
    such as when you receive an email or other data that needs to be accurately input
    into a claim form. This task can quickly become tedious and frustrating, as manually
    copying and pasting data into the correct fields is time-consuming and prone to
    errors.
  prefs: []
  type: TYPE_NORMAL
- en: The **SmartComponents** repository is an open-source repository with components
    enabling you to add AI-driven features to your .NET applications quickly and without
    an in-depth knowledge of prompt engineering. Among other features, **SmartComponents**
    can enhance the pasting of the unstructured data to fit the expected form. Even
    though **SmartComponents** is not in the **Microsoft** namespace, it is under
    the official .NET Platform GitHub account and is fully endorsed, developed, and
    maintained by the Microsoft Team.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s implement a smart-pasting feature allowing users to paste copied text
    directly into designated fields without any preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we dive into making the pasting smarter, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a **Chapter10** / **Recipe02** directory – this will be your working
    directory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copy the **Models** file from the **Chapter10** / **Data** directory in the
    GitHub repository to the working directory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copy the **SmartComponents** folder from the GitHub repository to your solution
    folder, and add all projects inside to your solution. The **SmartComponents**
    folder contains a clone of the **SmartComponents** repository, updated to support
    the latest Azure OpenAI updates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have the Azure OpenAI details ready (you can see how to get them in the *There’s
    more…* section of the *Setting up Azure OpenAI* *service* recipe)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these instructions to enhance pasting in your application with AI:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the **csproj** file of the server-side project and include two
    required **SmartComponents** projects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Still on the server side, open the **Program.cs** file and register **SmartComponents**
    with the OpenAI backend:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Locate the **appSettings** file of the server-side project and extend the application
    settings with an area for **SmartComponents** configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Navigate to the **csproj** file of the client-side project and include the
    **SmartComponents** project required by the WebAssembly renderer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a new routable **FillClaim** component, referencing the **SmartComponents**
    assembly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the **@code** block of the **FillClaim** component, declare a **Claim**
    form parameter of the **ClaimViewModel** type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the **FillClaim** markup, construct an **EditForm** frame, binding it to
    the **Claim** parameter. If **EditForm** is not recognized as a component, include
    a **@using Microsoft.AspNetCore.Components.Forms** reference at the top of the
    **FillClaim** component:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Inside the **FillClaim** form, add fields for entering event and customer details:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add a submit button within the form to confirm the input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, below the submit button, embed a **SmartPasteButton** component with
    a default icon:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In *step 1* , we start by configuring the server side of the application. We
    navigate to the project configuration file and add references to two projects
    required to make **SmartComponents** work on the server. The **SmartComponents.AspNetCore**
    project contains server components powered by AI, while the **SmartComponents.Inference.OpenAI**
    project contains an implementation of services to communicate with OpenAI backend.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 2* , we navigate to the **Program.cs** file in the server-side project
    and register **SmartComponents** in the dependency-injection container. We also
    register an **OpenAIInferenceBackend** implementation as the default prompts configuration
    for **SmartComponents** to use. Custom inference implementations come in handy
    when you leverage the AI to generate texts. We will explore that later, in the
    *Implementing a smart text* *area* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 3* , we complete the setup of **SmartComponents** by navigating to
    the **appSettings.json** file on the server side. As **appSettings.json** is a
    configuration source of the application, we extend the JSON with a **SmartComponents**
    section and key nodes, representing the API key and endpoint and the model deployment
    name that the **SmartComponents** components must use.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 4* , we jump to the client side of the application. In-line with default
    Blazor component packages, **SmartComponents** also has component counterparts
    for rendering in WebAssembly mode. We navigate to the configuration file of the
    client-side project and add a **SmartComponents.AspNetCore.Components** project
    reference there.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 5* , we create a routable **FillClaim** component and reference the
    **SmartComponents** assembly. Next, we build a form where the support team can
    fill in claim details with the help of AI.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 6* , we initialize an **@code** block and declare a **Claim** parameter
    that will also act as the backing model of the claim form. If you’re new to form
    creation in Blazor, we covered that in detail in [*Chapter 6*](B22020_06.xhtml#_idTextAnchor203)
    .
  prefs: []
  type: TYPE_NORMAL
- en: In *step 7* , we construct a form frame using **EditForm** and bind it to the
    **Claim** model.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 8* , we build a simple form body, allowing the user to fill in an event
    name, date, customer name, and email – enough to identify and process the claim.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 9* , we complete the form by adding a submit button.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, in *step 10* , we enhance the form with AI by embedding the **SmartPasteButton**
    component within the form’s body. We also declare the **SmartPasteButton** component
    to render with a default icon. With that simple setup, you can now transform unstructured
    data into a ready-to-send form with the help of a ( smart) button.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.13: A result of smart pasting an e-mail with a claim into a form](img/B22020_10_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.13: A result of smart pasting an e-mail with a claim into a form'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**SmartComponents** can also work with the OpenAI API key. If you already have
    an OpenAI account, navigate to the following URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://platform.openai.com/api-keys](https://platform.openai.com/api-keys)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, you’ll be able to create an API key that allows you to access the ChatGPT
    API:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.14: Creating an API key to access the OpenAI API](img/B22020_10_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.14: Creating an API key to access the OpenAI API'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have the API key, open the **appSettings.json** file of the server-side
    application and update the **SmartComponents** section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding configuration, the **ApiKey** node still represents your API
    key, while the **DeploymentName** node now defines the GPT model you want to use.
    Notice that the **Endpoint** node is no longer needed. When you don’t provide
    an **Endpoint** value explicitly, **SmartComponents** will fall back to the default
    OpenAI API URI.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a smart text area
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You’ve probably seen the generative power of AI in action – you provide a context,
    and a wall of sensible text appears. No more writer’s block, right? Generative
    AI is a game-changer for all text-driven features in your application. You can
    take store item descriptions or event descriptions from a list of bullet points
    into well-written copy in seconds. With **SmartComponents** , we can easily connect
    to an AI model and leverage the generative power, making content creation faster
    and more intuitive.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s implement a text area where the support team can fill in a message attached
    to a response to a client’s claim.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we explore the AI-powered text area implementation, we must do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a **Chapter10** / **Recipe03** directory – this will be our working directory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copy the **FillClaim** component from the *Implementing smart pasting* recipe
    or from the **Chapter10** / **Recipe02** directory in the GitHub repository
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copy the **Models** from the **Chapter10** / **Data** directory in the GitHub
    repository
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re starting here, review instructions from *step 1* to *step 4* of the
    *Implementing smart pasting* recipe for an initial **SmartComponents** configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these instructions to add a smart text area to your application:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the **@code** block of the **FillClaim** component and add a **replier**
    variable that defines the person filling out the claim form:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Jump to the **EditForm** body in the **FillClaim** markup and extend the form
    by embedding a **SmartTextArea** component above the submission button. Attach
    the **replier** variable to the **UserRole** parameter of the **SmartTextArea**
    component and bind the text area value to the **Message** property of the **Claim**
    instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In *step 1* , we jump straight to the **FillClaim** component. First, we move
    to the **@code** block and declare a **replier** variable, where we put a brief
    but detailed description of the persona that we want the AI to represent. Considering
    that AI models learn from content written by humans, you should strive to make
    the **replier** description as natural as you would sound when speaking to a friend.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 2* , we locate the **EditForm** markup in the **FillClaim** markup.
    Above the submission button, we embed a **SmartTextArea** component. The **SmartTextArea**
    component supports the bind-value binding pattern (you can learn more about binding
    in [*Chapter 3*](B22020_03.xhtml#_idTextAnchor095) ) and allows defining standard
    **textarea** attributes, such as **rows** or **cols** , representing the default
    size of the text box. It also allows setting a **UserRole** parameter – that’s
    where we attach our persona definition stored in **replier** .
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s all it takes to add a generative field to your application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.15: AI helping to write a claim response as user types the message](img/B22020_10_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.15: AI helping to write a claim response as user types the message'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we used the default inference configuration provided by the **SmartComponents**
    package. However, you can customize the prompt and AI behavior by implementing
    a custom **SmartTextAreaInference** logic. Since AI communication and processing
    only happen on the server, you must keep prompt customization in the server-side
    project.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a **ClaimReplyInference** class, inheriting from **SmartTextAreaInference**
    , and customize suggestions coming in the **FillClaim** form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In **ClaimReplyInference** , we override the **BuildPrompt()** method. We leverage
    the base implementation to build the prompt but customize it afterward. First,
    we append an additional **ChatMessage** instance to the **Messages** collection
    the **prompt** already has. We define that new **ChatMessage** role as **System**
    . The **System** message sets the overall behavior of the AI model, indicating
    that we expect a professional tone of suggestions. Lastly, we customize the value
    of the prompt’s **Temperature** property. The **Temperature** setting controls
    the randomness of the AI responses, with lower values making the output more focused
    and deterministic and higher values making it more creative and varied.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having **ClaimReplyInference** in place, we must add it to the dependency injection
    container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In the **Program** entry class, we register the **SmartTextAreaInference** class
    as a singleton. The **SmartTextArea** component will automatically discover the
    new implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, users will get more official-sounding suggestions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.16: AI generating suggestions in a professional tone, to help a
    user replay to a claim](img/B22020_10_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.16: AI generating suggestions in a professional tone, to help a user
    replay to a claim'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can customize all available **SmartComponents** components and fine-tune
    the AI behavior to your application needs. If you want to learn more, check out
    the official **SmartComponents** docs on GitHub at [https://github.com/dotnet-smartcomponents/smartcomponents/tree/main](https://github.com/dotnet-smartcomponents/smartcomponents/tree/main)
    .
  prefs: []
  type: TYPE_NORMAL
- en: Adding a ChatBot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ChatGPT, developed by OpenAI, is an advanced conversational AI model that has
    gained significant attention since its release. It’s designed to understand and
    generate human-like text based on the input it receives, making interactions with
    it feel natural and intuitive. The versatility of the GPT models enables their
    application in numerous contexts, from customer support and personal assistants
    to educational tools and entertainment.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s construct a primitive chat UI and connect it to the Azure OpenAI service
    to embed a ChatGPT-like chat functionality in the Blazor application.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to **SmartComponents** , which we explored in previous chapters, the
    chat will require Azure OpenAI API access. To avoid leaking API access details,
    we move to the server side of the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we dive into building AI-powered chat, we must do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a **Chapter10** / **Recipe04** directory – this will be your working
    directory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copy the **InputModel** from the **Chapter10** / **Data** directory in the GitHub
    repository
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prepare the Azure OpenAI Service connection details (you can see how to get
    them in the *There’s more…* section of the *Setting up an Azure OpenAI* *service*
    recipe)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to add an AI-powered chat to an application:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the configuration file of the server-side project and include the
    latest version of the **Azure.AI.OpenAI** package (at the time of writing, it’s
    still in preview):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Open the **appsettings.json** file with the server project configuration and
    add a **ChatBot** section with the required nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Move to the **Program.cs** entry file of the server-side project, and right
    after the **builder** instance is initialized, intercept the chat configuration
    into variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Below the configuration variables, register an **AzureOpenAIClient** service
    as a singleton by passing the **endpoint** and **apiKey** variables into the service
    constructor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After registering **AzureOpenAIClient** , add a **ChatClient** service to the
    dependency injection container as scoped. Leverage the **AzureOpenAIClient** API
    and **deploymentName** to construct the **ChatClient** instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a routable **ChatBot** component, rendering in the **InteractiveServer**
    mode and referencing the **OpenAI.Chat** assembly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the **@code** block in the **ChatBot** component and inject the
    **ChatClient** service as **Chat** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Below the service injection, initialize a **Model** instance to bind to the
    input form and **Messages** collection to persist chat messages to display on
    the UI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Below the **Messages** collection, initialize a **_messages** collection to
    hold messages in a form transferable to the Azure OpenAI Service. Start the **_messages**
    collection with a system prompt, defining the chatbot’s persona:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next to the backing variables, implement a **SendMessage()** method. Start
    by checking the validity of the **Model** state. If the input is valid, convert
    it to the **UserChatMessage** object and add the message to the backing collections:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Still within the **SendMessage()** method, request chat completion by passing
    the **_messages** collection to the **CompleteChatAsync()** method of the **Chat**
    service and resolve the response payload:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Complete the **SendMessage()** method by persisting the received response in
    the **Messages** collection and in the **_messages** collection as an **AssistantChatMessage**
    object. Lastly, reset **Value** of the **Model** object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Move to the **ChatBot** markup and construct a simple **EditForm** form with
    a single input field bound to the **Model** variable, triggering **SendMessage()**
    when submitted. If **EditForm** is not recognized as a component, include a **@using
    Microsoft.AspNetCore.Components.Forms** reference at the top of the **FillClaim**
    component:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Below the input form, iterate over the elements in the **Messages** collection
    and render them in separate paragraphs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In *step 1* , we start by adding the **Azure.AI.OpenAI** package to the server
    side of the application. If you’ve been using the **NuGet Package Manager** ,
    you’ll have to include prerelease versions of the packages, as **Azure.AI.OpenAI**
    is still in preview at the time of writing.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 2* , we add a chatbot configuration section to the **appsettings.json**
    file. We will need an **ApiKey** node, an API **Endpoint** node, and a **DeploymentName**
    node, to specify the name of the model we want to use.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 3* , we navigate to the **Program.cs** file of the server-side project,
    where we register the necessary services in the dependency injection container.
    First, we intercept the chatbot configuration values into **endpoint** , **apiKey**
    , and **deploymentName** by accessing the configuration reader from the **builder**
    instance.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 4* , we register the **AzureOpenAIClient** service as a singleton,
    passing the **endpoint** value as the Azure OpenAI URI and initializing **AzureKeyCredentials**
    with an **apiKey** value. We can have a shared instance of the **AzureOpenAIClient**
    service as it’s thread- and scope-safe by design but consider the memory impact
    in your implementations.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 5* , we add one more service to the dependency injection container
    – we register a **ChatClient** service as scoped. We construct the **ChatClient**
    object by resolving the **AzureOpenAIClient** instance from the services collection
    and invoking its **GetChatClient()** method with the **deploymentName** value.
    Having services in place, we construct the UI part.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 6* , we create a routable **ChatBot** component that references the
    **OpenAI.Chat** assembly, as we will need access to the **ChatClient** class definition.
    We also need the **ChatBot** component to render in **InteractiveServer** mode,
    since our users will interact with the chat.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 7* , we initialize the **@code** block and inject the **ChatClient**
    service from the dependency injection.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 8* , we initialize a **Model** instance to bind the input form where
    users fill in their messages, as well as a **Messages** collection, where we persist
    the text representation of the chat and user messages to render them in the markup.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 9* , we initialize one more collection – **_messages** . In **_messages**
    , we persist messages in the form of **ChatMessage** objects. With that, we can
    easily provide the full context of the conversation when requesting a new response
    from the Azure OpenAI service; without the history of the messages, we would limit
    the chat context to the last message the user sends. We also start off **_messages**
    with a predefined **SystemChatMessage** object. The **SystemChatMessage** object
    allows us to inject a prompt, where we define how the chatbot should behave, but
    the prompt itself is not a part of the conversation.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 9* , we implement a **SendMessage()** method where all the chatting
    logic goes. At the beginning, we check whether the submitted **Model** value is
    valid and fast-return when there’s nothing to process. Then, we wrap the user
    input into a **UserChatMessage** object. We must use the **UserChatMessage** objects
    when sending user inputs so the AI can interpret them accordingly. Next, we add
    the **UserChatMessage** instance to the **_messages** context collection and format
    the user input into a chat-like version to add it to the renderable **Messages**
    collection.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 10* , we leverage the **Chat** instance and its **CompleteChatAsync()**
    method to request a new chat response from the Azure OpenAI. Notice that we send
    the entire **_messages** collection as part of the request so that GPT in the
    cloud has the full context of the conversation. Then, we unpack the message from
    the received response **Content** property.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 11* , we push the unpacked payload to the backing variables. This time,
    we wrap the received message in an **AssistantChatMessage** object before adding
    it to the **_messages** collection. The **AssistantChatMessage** type represents
    responses from the AI itself. Next, we construct a chat-like message to add to
    the **Messages** collection to render it for the user to see. Finally, we clear
    the **Model** value to accept another message from the user.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 12* , we implement a primitive markup so the user can interact with
    the chat. We add a call to action at the top and construct an **EditForm** form.
    We bind the form to the **Model** instance and attach the **SendMessage()** method
    to its submission callback. Within the **EditForm** markup, we add a single **InputText**
    field where the user provides their chat requests and a button allowing them to
    submit the form and trigger the chat generation.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 13* , below the **EditForm** component, we construct a simple loop
    where we iterate over the chat-like messages in the **Messages** collection and
    render them in separate paragraphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that simple implementation, you already get a ready-to-talk chat prototype:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.17: Primitive chat UI with a powerful AI-powered backend in action](img/B22020_10_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.17: Primitive chat UI with a powerful AI-powered backend in action'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Depending on the session or message length that you expect to handle with the
    chat, you should consider cleaning the context of the conversation periodically.
    This will help maintain the efficiency and effectiveness of the chat functionality.
    Managing the length of the chat context impacts both the cost and responsiveness
    of the chat. Longer contexts can lead to higher costs due to increased API usage
    and potentially slower response times as more data is processed.
  prefs: []
  type: TYPE_NORMAL
- en: One effective strategy is to implement a **circular buffer** of a fixed size.
    In a circular buffer, new elements are added to the end of the buffer while the
    oldest elements are overwritten when the buffer reaches its capacity. This approach
    ensures that the chat context remains within a manageable size, keeping the conversation
    relevant and efficient.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you’d want to explore the **Azure.AI.OpenAI** possibilities further, visit
    the package docs at [https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/openai/Azure.AI.OpenAI/README.md](https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/openai/Azure.AI.OpenAI/README.md)
    .
  prefs: []
  type: TYPE_NORMAL
- en: Connecting an Azure OpenAI service to an existing data index
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Azure, you can have multiple existing data sources, ranging from Azure Cosmos
    DB to various Azure Cognitive services with tokenized and indexed data. While
    the Azure OpenAI service works with commonly available GPT models, it also allows
    you to connect a chosen model to your specific data source. With this integration,
    you can analyze and extract data more intuitively by interacting with your application
    through natural language.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s connect the Azure OpenAI service to an existing Azure Search service data
    index. By doing so, we will leverage the power of AI to analyze our internal data
    seamlessly.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we explore connecting Azure Search data to Azure OpenAI, we must do
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: On the server-side on your application, create a **Chapter10** / **Recipe05**
    directory – this will be your working directory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copy the **ChatBot** component from the *Adding a ChatBot* recipe or from the
    **Chapter10** / **Recipe04** directory in the GitHub repository
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re starting here, register all the Azure services as shown in the **Configure**
    file, in the **Chapter10** / **Recipe04** directory in the GitHub repository
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these instructions to connect Azure OpenAI to the Azure Search data
    and enable analyzing the data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the **appsettings.json** file on the server side and add a new **Search**
    section with **ApiKey** , **Endpoint** , and **Index** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Move to the **Program.cs** file of the server-side project. Below the builder
    and Azure OpenAI services initializations, intercept the search data access details
    into **searchEndpoint** , **searchApiKey** , and **searchIndex** variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Below the intercepted search configuration, register **ChatCompletionOptions**
    as a singleton. As part of the **ChatCompletionOptions** initialization, build
    an **AzureSearchChatDataSource** instance and attach it to the constructed completion
    options:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At the time of writing, the **Azure.AI.OpenAI** package is in preview and your
    IDE may interpret using the **AddDataSource()** method of the **ChatCompletionOptions**
    class as a compilation error. To suppress the error, add the required **#pragma**
    directive at the top of the **Program.cs** file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Navigate to the **@code** block of the **ChatBot** component and inject the
    **ChatCompletionOptions** instance next to the **Chat** client:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Still within the **@code** block, inside the **SendMessage()** method, locate
    where we invoke the **CompleteChatAsync()** method of the **Chat** service and
    pass **ChatOptions** as a second parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In *step 1* , we navigate to the **appsettings.json** configuration file of
    the server-side project. We extend the configuration file with a **Search** section
    where we require the **ApiKey** , **Endpoint** , and **Index** values.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 2* , we stay on the server side but move to the **Program.cs** project
    entry file. We intercept the search configuration into **searchEndpoint** , **searchApiKey**
    , and **searchIndex** variables, so we can use them to connect data to the Azure
    OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 3* , we register a singleton **ChatCompletionOptions** object in the
    application’s dependency injection container. **ChatCompletionOptions** is used
    to configure the behavior of chat completions, allowing us to customize and extend
    the functionality of our chat service. As part of the **ChatCompletionOptions**
    initialization logic, we construct an instance of **AzureSearchChatDataSource**
    , which represents the search data connection details and requires providing an
    endpoint, API key, and index name. We’ve intercepted this from the **appsettings.json**
    file. We use the **AddDataSource()** method of the **ChatCompletionOptions** instance
    to attach the search data access.
  prefs: []
  type: TYPE_NORMAL
- en: As **Azure.AI.OpenAI** is still in preview at the time of writing, your IDE
    may flag the use of the **AddDataSource()** method as a compilation error – that’s
    nothing to worry about. The Azure team will adjust this before releasing the stable
    package. For now, we can suppress the warning by adding a **#pragma** directive
    at the top of the **Program.cs** file with the **AOAI001** validation code we
    need to suppress, as we do in *step 4* . Next, we move to the **ChatBot** component
    and attach the enhanced completion options to our chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 5* , we go straight to the **@code** block of the **ChatBot** component
    and inject the **ChatCompletionOptions** instance from the dependency injection
    container as **ChatOptions** .
  prefs: []
  type: TYPE_NORMAL
- en: In *step 6* , we locate the **SendMessage()** method and find where we invoke
    the **CompleteChatAsync()** method of the **Chat** service to get a response from
    the Azure OpenAI. We’re already passing a **_messages** collection to the **CompleteChatAsync()**
    method, but it also accepts a second parameter of the **ChatCompletionOptions**
    type – that’s where we pass the injected **ChatOptions** instance with access
    to the Azure Search data.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You don’t have to have the data source and Azure OpenAI in the same resource
    group. In fact, you don’t even have to own the data source. Azure OpenAI will
    work correctly and generate contextualized results as long as you provide a valid
    set of configuration details. This flexibility allows you to leverage existing
    data sources and integrate them with Azure OpenAI seamlessly, enhancing the functionality
    of your applications without needing to consolidate or migrate resources.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the recipe implementation, we’ve used a **#pragma** preprocessor directive.
    Preprocessor directives have different purposes and allow adjusting your code
    behavior on a lower level. If you’re curious to learn more, check out this Microsoft
    Learn resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://learn.microsoft.com/en-us/dotnet/csharp/language-reference/preprocessor-directives](https://learn.microsoft.com/en-us/dotnet/csharp/language-reference/preprocessor-directives)'
  prefs: []
  type: TYPE_NORMAL
