<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Transport Layer Security</h1>
                </header>
            
            <article>
                
<p>Now that we've seen how network interactions are executed down to the lowest level, we need to understand how those interactions can be secured for users. One of the most fundamental aspects of the public Internet is the ability to secure certain interactions between two hosts. In this chapter, we'll explore how that's done. We'll start by looking at the underlying security mechanisms that supported the original <strong>Secure Sockets Layer</strong> (<strong>SSL</strong>), which was the standard for secured network interactions for decades. Then, we'll take a close look at its successor, <strong>Transport Layer Security</strong> (<strong>TLS</strong>), and consider some of the reasons for the transition. Finally, we'll see how both of these mechanisms are intended to provide secure interactions between network hosts by implementing our own simulation of the protocol. In doing so, we'll also see how we can leverage TLS and network security, right out of the box, with .NET Core.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>The level of data integrity and session privacy users should expect when an application leverages TLS</li>
<li>Why SSL is being deprecated, and what TLS does to support secure connections</li>
<li>Understanding how to leverage the out-of-the-box capabilities of .NET Core to support TLS</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>In this chapter, we'll be writing sample software that configures and leverages both SSL and TLS from a Web API application. You'll need your .NET Core IDE or code editor of choice, and you can access the sample code here: <a href="https://github.com/PacktPublishing/Hands-On-Network-Programming-with-CSharp-and-.NET-Core/tree/master/Chapter%2013">https://github.com/PacktPublishing/Hands-On-Network-Programming-with-CSharp-and-.NET-Core<span>/tree/master/Chapter 13</span></a><span>.</span></p>
<p><span>Check out the following video to see the code in action: <a href="http://bit.ly/2HY63Ty">http://bit.ly/2HY63Ty</a></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Private connections and SSL</h1>
                </header>
            
            <article>
                
<p>The moment the Internet began supporting something as simple as real-time chat, or even email, the need for a secure connection between two network hosts became apparent. Imagine sending a confidential message to a friend of yours without being able to make any reasonable assumption that your message would remain private. Certainly, you would restrict your online interactions to only the most mundane of tasks and messages. And that's only considering our intuitive desire for privacy in personal matters. That says nothing for the need to protect private, personally identifying information that could be used by a malicious actor to commit fraud.</p>
<p>Without some measure of security in our online interactions, no one would dream of doing anything as critical as banking, accessing medical information, or paying our taxes. Indeed, tasks that seem so basic and fundamental to a modern user of the Internet would be unthinkable without some measure of protection from malicious third parties. It's precisely these scenarios that secure connections are designed to facilitate. But have you ever wondered how they work? Have you ever considered what that lock icon in your Chrome address bar means?</p>
<p>That's what we'll be exploring in this section. We'll see what first prompted the need for a secure interaction mechanism between network hosts. Then, we'll look at how that secure interaction is secured. We'll find out how your browser knows how to warn you of potentially insecure connections, and how we can provide that level of security for our own users. Finally, we'll learn what SSL is and how to leverage it in our own software to provide peace of mind for our own consumers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Establishing secure connections</h1>
                </header>
            
            <article>
                
<p>If you've ever followed a link to a website and had your browser first warn you that you were about to enter an insecure connection, you may have wondered why and how that warning was generated. The answer is that your browser detected an attempt to establish a connection using the SSL standard. SSL is a universally agreed standard for establishing an encrypted link between a remote server and its client.</p>
<p>You might remember from <a href="e93c024e-3366-46f3-b565-adc20317e6ec.xhtml">Chapter 9</a>,<em> HTTP in .NET,</em> that the <kbd>https://</kbd> schema designation is the schema for Hypertext Transfer Protocol Secure.<span> That schema designation is a signal to your browser that the content sent back and forth between your machine, and the remote host should be encrypted first.</span><span> </span><span>When you navigate to a URL with a schema, your browser will first try to negotiate a secure connection with the server. Its ability to do so, or inability to do so in some cases, is what determines whether or not you are presented with a warning prompt prior to rendering the content that's received from the server.</span></p>
<p>When a user, or circumstance, dictates that a secure connection should be used (for instance, by specifying HTTPS as the schema in a URI) it's the responsibility of the software establishing that connection to ensure that it's secured. This means that if you were to write a web browser from scratch (in a lower-level language, like C++), your software would be responsible for authenticating the server on the receiving end for any https requests your users want to make. So, how is this connection established? The primary mechanisms for a well-established secure connection are cryptographic translation and third-party authority. Let's take a look at third-party authority first, as it's a bit more straightforward than the cryptography machinations at work with SSL.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Trusted certificate authorities</h1>
                </header>
            
            <article>
                
<p>When a server claims to support a secure connection (historically, via SSL, and today, via TLS), the client must have some way of ensuring that the server is who it claims to be. Without this identity verification step, it would be very simple for malicious actors to invalidate the premise of a secure connection. You would simply need to set up a working dummy version of a targeted website as a trap. Then, by providing fraudulent links to their malicious site, disguised as a link to the legitimate site, they could trick vulnerable users into providing access credentials, user information, and more to a malicious dummy interface. The whole purpose of HTTPS is to provide users with an assurance that their information is being delivered to the entity they intend to deliver it to, in such a way that no one can see what they're sending while it's en route.</p>
<p>Establishing that a server on the other end of a secure connection is, in fact, the entity it is claiming to be is done with authentication certificates, issued by a <strong>t<span>rusted certificate authority</span></strong> <span>(<strong>CA</strong>)</span><span>. A CA is an organization or entity that will generate, sign, and issue an authentication certificate to any server that wants to support interactions over HTTPS or TLS. Specifically, the certificates issued by a trusted authority are a cryptographically secure </span>X.509 public key certificate<span>.</span></p>
<p><span>This public-key encryption is something we'll see more of shortly, but it's basically a one-way security mechanism that allows the owner of a private key to validate a freely distributed public key. The public key is generated with a combination of the identity of the recipient and the private key, which must remain secret for the certificate to remain valid. Then, whenever a client wants to validate the identity of the <strong>server</strong>, they take the certificate, along with the identity of the server that presented it, directly back to the trusted authority. </span><span>Using their private key, </span><span>the trusted authority checks the </span><span>identity and the certificate's public key to ensure that it hasn't been tampered with, or fraudulently generated.</span></p>
<p>The whole process can be broken down into two key steps. First, the server requests and is issued an X.509 certificate from a <strong>trusted certificate authority</strong>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-928 image-border" src="assets/b3ad1a40-ddd8-404f-a6f3-5adbcc9906a5.png" style="width:32.33em;height:12.58em;"/></p>
<p>Then, whenever a client wants to establish a secure connection with the server, it must first establish the identity of the server by checking its X.509 certificate with the issuing CA:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/aa084d55-41d7-42c7-832e-b874c3a928aa.png" style="width:28.83em;height:17.25em;"/></p>
<p>And through this series of round-trips, the server's identity can be assured, and the client can, to at least some degree, trust that the secured connection is with the intended entity.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The basis of trust in certificate authorities</h1>
                </header>
            
            <article>
                
<p>As you've probably already guessed, though, this system of certificate authorities has a certain measure of trust baked in at various levels. First, the user must trust that their web browser has actually requested a certificate from the remote server, and requested authentication from the signing authority. Next, the user and the browser must trust that the CA is only authenticating valid certificates for valid certificate holders.</p>
<p class="mce-root"/>
<p>This may seem obvious, but it's entirely possible that a CA is not as trustworthy as you would hope. In 2013, we learned that there were serious violations of basic Internet security protocols by government intelligence agencies, including working with trusted certificate authorities to generate signed and authenticated certificates for invalid holders for the purposes of surveillance and counterintelligence operations. Regardless of your personal opinions about the ethical implications of such actions, it cannot be argued that by doing so, the agencies responsible severely undermined the trust of engineers and the broader public in the validity of a trusted third-party security measure.</p>
<p>So long as the CA can be (reasonably) trusted, though, the certificates that are issued and signed by that authority can also, generally, be considered trustworthy. And with those certificates and the validation of the trusted authority, the identity of the server can be established. Once that step is complete, it's time to secure the packets in transit.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Simple and mutual authentication</h1>
                </header>
            
            <article>
                
<p>So far, we've only been concerned with validating the identity of the server when establishing a secure connection. This is what's called a <strong>simple authentication</strong> mechanism<strong>.</strong> The client is given a certificate from the server, and validates it with a certificate authority. Provided the certificate is valid, the client is permitted to proceed with a secure connection. There's no effort made by the server to authenticate the client.</p>
<p>However, this pattern could just as easily be applied to the client as well. This extension of certificate validation procedures is what's called <strong>mutual authentication.</strong> In a mutual authentication scheme, both parties are authenticated with their own certificates. This can allow the server to authenticate a user without having to request access credentials or authentication information directly from the client every time a connection is made.</p>
<p>With mutual authentication, the server is still required to deliver an X.509 certificate that's issued by a trusted authority to any client that wants to establish a secure connection. The client is also still responsible for validating that certificate with the authority. What's different is how the client's certificate is acquired and validated. While a server must allow a third-party authority to sign its certificate, the client need not bother. Instead, in most mutual authentication scenarios, the server itself signs and issues an X.509 certificate for the client.</p>
<p class="mce-root"/>
<p>The server will need its own private cryptographic key to generate the public key that was issued to the client, but as long as it has that key, it can validate any certificates it has issued. In this way, the server can reliably restrict access to only those clients it has issued a certificate too. The identity verification step is a matter of process, and the owner of the server is responsible for determining what constitutes sufficient identity verification prior to issuing a certificate. However, once that process is established, it should have every reason to trust certificates presented by a client, which it can validate.</p>
<p class="mce-root">Now, it might not be immediately apparent why a server would require a trusted authority to sign its identity certificate, while a client does not. This is because of the specific nature of client-server interactions on the internet. In almost all cases, a client is responsible for initiating a connection with the server. The server, by design, has no prior knowledge of when or from where a given request will be received from a given client.<span> </span></p>
<p class="mce-root"><span>For each request, the server must assert its own identity in such a way that the client can rely on the assertion. It must be able to do this for any client, regardless of whether a prior relationship has been established. </span><span>There must be some way of validating the server's identity for any given client and any given request. So, the job of initially validating, and then subsequently verifying, the identity of the server is centralized to an entity all potential clients can trust, and use as a single shared resource: the trusted authority. </span></p>
<p class="mce-root"><span>With a client certificate, however, the server can reasonably trust its own public key validation, and apply its private key to validate the certificate with the client's asserted identity. The encryption mechanism for generating an X.509 certificate is the same, regardless of whether it's being executed by a server or a trusted authority validating the certificate. The only difference is where trust is being placed and why.</span></p>
<p class="mce-root"><span>A client uses third-party authority because, otherwise, how could a client trust a certificate signed by a server if the client doesn't know the server's assertions can be trusted in the first place? The server doesn't need to use a third-party authority, because there is no trust being placed in the client. The server is performing its own validation with its own private key. An invalid client certificate simply wouldn't pass validation. Thus, a server-issued certificate is sufficient to identify the client. So, once a client has installed the certificate on their host machine, they can use it to access the server, fully authenticated, and establish and leverage secure connections without any additional steps.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Encrypting transmitted data</h1>
                </header>
            
            <article>
                
<p>Once the identity of the server has been established in such a way that the client can trust any interaction between the two hosts, the next step is to ensure no one else can observe those interactions. To do this, packets must be encrypted in transit. However, to leverage an encryption mechanism that both the client and the server can use, the specifics must be established beforehand.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Asymmetric and symmetric cryptography</h1>
                </header>
            
            <article>
                
<p> </p>
<p>When determining the identity of a server with a signed X.509 certificate, clients are using what's called <strong>asymmetric cryptography</strong>. All that means is that there is an imbalance between the parties with respect to the<span> </span><span>distribution of secret information that's necessary for the cryptography</span><span>. Two parties are using the same cryptographic scheme, but only one party has access to the secret key. This system is necessary for something like a security certificate because some of that information must be made publicly available to anyone who asks for it. Remember, by the time the certificate is changing hands from a server to a client, a secure connection hasn't been established yet. Any malicious parties that want to read that information from the packets in transit can do so freely. Asymmetric cryptography schemes account for that inevitability and are designed to remain secure, even when the public keys are freely distributed.</span></p>
<p>Once a secure connection is finally established, however, hosts will leverage what's known as <strong>symmetric cryptography</strong>. This is where the secret information that's necessary to encrypt and decrypt a message is shared equally (or symmetrically) between all parties involved. Both parties in the exchange will have to agree upon a secure encryption algorithm for which both hosts have an implementation available to leverage. Next, they'll need to agree upon the cryptographic keys they will each use with that algorithm to decrypt messages that are encrypted by the other party. <span>It's this symmetrical cryptography method that is used by two hosts to communicate over a secure transport protocol.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Negotiating cryptographic keys</h1>
                </header>
            
            <article>
                
<p>Now, you might have noticed a bit of a chicken and egg problem with using symmetrical cryptography to avoid eavesdropping on packets that are transmitted between hosts. Specifically, how can you send shared private cryptographic keys that will be used to establish a secure connection without first<em> </em>establishing a secure connection? For that, we have to leverage the same asymmetric cryptography we looked at when we were considering how to validate client certificates in a mutual authentication setup.</p>
<p>The first step in establishing a secure connection after validating the identity of the server through a CA is to establish the algorithm both parties will use for the encryption of packets. There are a number of what are considered secure cryptographic algorithms (though there is a number which, for reasons outside the scope of this book, were previously considered secure but are not anymore), and each of them can be found in the <kbd>System.Security.Cryptography</kbd> namespace of C#. The reason for establishing the algorithm is that it's entirely possible that the two hosts don't have implementations of the same set of algorithms, so it's important that they identify a mutually implemented algorithm before they proceed.</p>
<p>Once the algorithm is selected, the hosts must exchange a set of unique private keys that they'll use for the lifetime of the session to encrypt and decrypt packets. To make this exchange, the server first sends a public<em> </em>key for which only it has the private key. Then, the client uses this public key to encrypt a random number, and returns the value to the server. At this point, it's perfectly acceptable if the packet is intercepted. The relevant information (the random number generated by the client) is encrypted, and the public key used for that encryption is useless without the private key, which hasn't been transmitted and thus cannot have been intercepted.</p>
<p>When the server receives the random number generated by the client, it can decrypt it using its private key, and then use the number as the initialization value for a cryptographic key that will be appropriate for the agreed upon an algorithm that will be used throughout the session. The client will have done the same, and with that both parties will have established a shared secret without ever having to transmit the details of that secret in an insecure way.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The SSL protocol</h1>
                </header>
            
            <article>
                
<p class="mce-root">For years, the standard for implementing secure network connections was what's known as the <strong>SSL</strong>. Piggybacking on earlier efforts to develop a secure transport mechanism for network interactions, SSL was developed by a company called NetScape as a way of establishing a global standard for secure networking. The history of SSL as a standard actually serves as an illuminating warning about the nature of network security.</p>
<p>Security on a network will always be a cat-and-mouse game between hackers and security researchers. Every effort that's made to establish a secure algorithm for reliable encryption of data will almost always fall prey to an unforeseen exploit that renders the algorithm useless as a security measure. This has been true of everything, from network security protocols to digital rights management applications, to basic OS-level libraries and utilities.</p>
<div class="packt_infobox">While it's well outside the scope of this book, there's a very interesting bit of research being done in the computer science community that could have a major impact on the state of security software. It's around a theorem in algorithm analysis that, if it is eventually <em>dis</em>proved, could simultaneously invalidate every known security algorithm in use. If you're curious about it, I recommend researching the P=NP Problem. Be prepared for high-level math if you start reading it, but also be advised that there is still, at the time of this publishing, a million dollar reward for the first person to prove or disprove the theorem.</div>
<p>This constant leapfrogging of secure algorithms and exploits of those algorithms is especially true of early versions of SSL. In fact, SSL 1.0 was never even released to the public due to glaring security flaws discovered in the protocol during testing phases. Instead, SSL 2.0 was released to the public in early 1995. It was barely a year later, however, that the next version, a complete overhaul of the protocol, was released as SSL 3.0 in 1996. This was due to another series of major flaws that were quickly discovered by the hacker community that rendered SSL 2.0 insufficiently secure for many confidential transactions. Compared to its previous iterations, SSL 3.0 enjoyed a relatively long shelf life before a successor was finally designed for it again in 1999.</p>
<p>The somewhat jarring leaps from one version to the next in the SSL lifespan were precipitated by flaws in the hashing and encryption algorithms underlying each standard. With SSL 2.0, a combination of vulnerable processes in handling secure keys, along with flaws in the algorithms that were used to generate those keys, added up to a glaringly insecure protocol. It provided no protection for the initial handshake, leaving the interaction open to exactly the kind of man-in-the-middle attacks we described in the previous section. It used hash algorithms that had known collisions (when two different inputs can generate the same hashed output), rendering its keys functionally insecure. Finally, quirks in its implementation of the CA verification process rendered most consumer-facing websites unable to support the protocol in the first place. All of this combined illustrates why such a major redesign of the protocol was necessary so quickly for version 3.0.</p>
<p>The story of SSL 3.0 is a fair bit more successful than its predecessor. While some parts of its cryptographic key generation algorithm relied entirely on insecure hash functions, it also incorporated the new (at the time, at least) SHA-1 standard. This new algorithm had no known hash collisions and thus strengthened the secure claim of the new protocol. It also introduced the pattern for CA support that is still seen today, enabling wider adoption and support of the protocol by public-facing websites.</p>
<p>The 3.0 iteration was not without its flaws, though. Because it still relied, at least in part, on a hash algorithm with known collisions, it was not considered sufficiently secured for highly critical or classified applications by the US government's <strong>Federal Information Processing Standard</strong> (<strong>FIPS</strong>). Furthermore, while there were far fewer procedural vulnerabilities in the design of the protocol (as opposed to cryptographic vulnerabilities), it was found to be vulnerable to a rather sophisticated procedural attack in October 2014. This vulnerability cemented the need for the official deprecation of the standard in 2015. This opened the door for its successor, TLS.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TLS as the new standard</h1>
                </header>
            
            <article>
                
<p>Currently, the global standard for secure network interactions, TLS, was originally developed as an improvement over the then standard SSL protocol back in 1999. While it was designed to be an upgrade to the existing SSL 3.0 protocol, there were sufficient differences in the design of each protocol to make interoperability between the two schemes infeasible. Instead, the authors released it as the first version of a newer, more secure protocol.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A minor evolution from SSL</h1>
                </header>
            
            <article>
                
<p>When it was introduced, TLS certainly represented an improvement over SSL 3.0. However, the major distinctions were in the header design for the packets exchanged during the handshake phase of connection establishment. The underlying algorithms and principles remained mostly unchanged. In fact, were it not for the header incompatibility, TLS might just as well have been named SSL 4.0 at the time of its release.</p>
<div class="packt_infobox">Remember from <a href="0311281d-9a21-4dd0-a3cd-4e047ced436a.xhtml">Chapter 12</a>,<em> The Internet Protocol</em>, that it was this same header-incompatibility issue that made IPv4 and IPv6 interoperability impossible. The ability to parse a standardized header is the most fundamental first step of any shared interaction between hosts. A mismatch or incompatibility between the headers of two versions of a protocol will make that first step impossible, rendering the packets unreadable. This will often have the impact of preventing mutual compatibility between versions of a protocol with different header definitions. </div>
<p>There were certainly security improvements with TLS, however. The first of which was that in TLS, no single part of its cryptographic algorithms relied entirely on a hashing algorithm with known collisions. Any key that was generated was always done so with at least some inputs from a cryptographically secure hash algorithm.</p>
<p>It also introduced added protection against lost or modified data during the handshake phase of a connection attempt. It did this by sending a secure hash of every message that was transmitted from both parties during the final step of connection negotiation. This way, the client and the server can both check the result against their own hashes and validate that each host perceived the same interactions, eliminating the possibility of a man-in-the-middle attack modifying any part of their exchange. However, this still didn't guarantee that a man-in-the-middle attack wasn't successful in attempting to read packets—only that none were successfully modified.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Forward secrecy</h1>
                </header>
            
            <article>
                
<p>One of the most important features that was introduced by TLS was the notion of forward secrecy<strong>.</strong> This is a concept in secured communication where, by using a unique session key over the course of each interaction, two hosts can guarantee that even if a secret is exposed through some attack, previous interactions will remain secure, even if they were recorded and stored by the attacker. This is because the unique session key is used as an additional input to the encryption mechanism for securing messages. Thus, private keys alone are insufficient to decrypt messages previously sent.</p>
<p>For perfect secrecy to work, the session key must be randomly generated via a non-deterministic function. For those not aware, in the context of computer science, a function is considered non-deterministic if and only if it could return two distinct results given the exact same inputs. Typically, this non-determinism is achieved by use of a random number generator, and some other temporary external state. So, if I had, for example, the <kbd>GetNextPrime(startingIndex)</kbd> method, then we would expect it to be deterministic. For any number, <em>n, </em>there is only one next prime after it. Every attempt to call the method with <kbd>GetNextPrime(n)</kbd> would result in the same output. Meanwhile, if I had a method called <kbd>RollDie(sides)</kbd> then I would reasonably expect that to be non-deterministic. I could pass in the same <kbd>sides</kbd>, parameter five times and get five completely distinct results. That's a non-deterministic algorithm in a nutshell.</p>
<p>This concept of non-determinism is important when generating a session key because it ensures that subsequent attempts to generate the same session key would fail. Thus, when an attacker gains access to the private keys of a server, they would still be missing a critical component that's necessary to decode messages from previous sessions. That missing piece of the puzzle would be much easier to find if session keys were persisted beyond the lifetime of the session, though. For that reason, it's critical to ensure for forward secrecy that a session key is destroyed as soon as a session is terminated.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reliability of secured messages</h1>
                </header>
            
            <article>
                
<p>The integrity of the data in a packet is critical to its successful delivery and decryption by the recipient host. If even one bit is out of place, the value of the decrypted message would become wholly unreadable. This property of secure encryption that even minor changes in a message will result in drastic changes to its encrypted counterpart, and vice versa, which means it is far more difficult to recover from errors occurring in transit than with unsecured messages. For that reason, TLS leverages what are called <strong>message authentication codes (MACs)</strong>.</p>
<p>These MACs are used to validate both that the data that's been transmitted has not been modified in any way, and that it was sent by the host that the recipient is expecting. In TLS, these codes are generated using sequential packet ID numbers as an added security measure. This sequential tag provides an additional level of verification on a message, thus increasing the complexity of the work a malicious actor would have to perform to successfully modify both a payload and its corresponding MAC for successful fraudulent packet delivery.</p>
<p>This additional security measure, along with the session ID that's used for forward secrecy, and the multilayered secure hash algorithms that's used to generate shared keys for encryption and decryption provide a robust foundation for TLS, and have allowed its reliable use across a small number of versions for nearly two decades.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring TLS in .NET Core</h1>
                </header>
            
            <article>
                
<p><span>While we've explored the nature of the interactions between hosts that want to establish secure communications in depth, we've kept the discussion to a fairly high level so far. There's a reason for this, though. In .NET Core, you'll never be coding the specific steps of the TLS protocol directly.</span><span> </span><span>As a high-level language executed on a portable runtime, .NET Core is not the ideal environment for attempting to implement those operations on your own. And, as you have probably already figured out, the ASP.NET Core libraries that are used to facilitate low-level socket interactions between hosts already implement TLS for us. We just have to know how to configure it and enforce its use. So, while the step-by-step interactions of TLS are important</span><span> </span><span>for any network engineer</span><span> to understand, the low-level details are well beyond the scope of this book.</span></p>
<p>For the purposes of this demonstration, though, we'll write a simple Web API project that simulates<em> </em>the interactions of a TLS handshake. Hopefully, this will cement some of the more abstract ideas in your mind by giving them a concrete in-code representation. We'll also configure our application to leverage HTTPS so that you can see the steps you'll be taking in your own projects to provide this feature.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Enabling HTTPS in .NET Core</h1>
                </header>
            
            <article>
                
<p>The first thing we'll do is set up our project as a new Web API project using the .NET CLI command:</p>
<pre><strong>dotnet new webapi -n DemoTLS</strong></pre>
<p>Now, we want to allow clients to interact with our services using TLS. Thankfully, to enable this, we don't actually have to do anything! Astute readers will remember that in my previous demo application in <a href="e93c024e-3366-46f3-b565-adc20317e6ec.xhtml"/><a href="e93c024e-3366-46f3-b565-adc20317e6ec.xhtml">Chapter 9, </a><em>HTTP in .NET</em>, in the <em>Using launchSettings.json</em> section, I had you remove the following line of code from the <kbd>Startup.cs</kbd> file of our application:</p>
<pre>app.UseHttpsRedirection();</pre>
<p>Well, it turns out that by not removing that, our application's web server will respond to any standard HTTP requests with a <kbd>307 - Temporary Redirect</kbd> status code, directing the client to the appropriate port for HTTPS interactions. We took that out of our previous demo to simplify our discussion of the specifics of HTTP in <a href="e93c024e-3366-46f3-b565-adc20317e6ec.xhtml"/><a href="e93c024e-3366-46f3-b565-adc20317e6ec.xhtml"/><a href="e93c024e-3366-46f3-b565-adc20317e6ec.xhtml"/><a href="e93c024e-3366-46f3-b565-adc20317e6ec.xhtml">Chapter 9</a>, <em>HTTP in .NET</em>, but now let's leave it in and see it in action.</p>
<p class="mce-root"/>
<p>Simply run your application, and when your default browser opens up, you should notice that it's routed to the <kbd>https://localhost:5001/api/values</kbd> launch URL that is configured for every new Web API project. There's nothing particularly interesting about that, but now open up your browser developer tools, and navigate to the tab that displays the request traffic. I'm using Chrome, myself, and can access the developer tools by navigating through the settings or simply hitting <em>F12</em>. Once in the <span class="packt_screen">Network</span> tab, there's an option to preserve the network logs for a browser session, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-930 image-border" src="assets/60fb5bfc-5f36-4bbe-8978-66da42249410.png" style="width:67.58em;height:13.17em;"/></p>
<p>After choosing that setting, attempt to navigate directly to the unsecured URL for your API at <kbd>http://localhost:5000/api/values</kbd> and then look at the response you get in your <span class="packt_screen">Network</span> tab. You should see your browser automatically reload the page to the secure URL again, and the following response in your <span class="packt_screen">Network</span> tab:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-931 image-border" src="assets/6f7b00c0-5e87-4ec2-a3e0-82c614aa888f.png" style="width:38.00em;height:22.33em;"/></p>
<p>This is the behavior that your web server provides when you enable HTTPS with the <kbd>UseHttpsRedirection</kbd> middleware configuration. The initial request for the unsecured URL was not satisfied, and instead the browser was given a directive to use the secured URL. That's what the second line in our log is. It's telling us that the navigation to the secured URL (the one that returned a <span class="packt_screen">200</span> response) was initiated by the server when we tried to navigate to the unsecured URL (the <span class="packt_screen">Initiator</span> field of our log). That's a lot of power and reliability to get from a web server without any work on our part!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Enforcing HTTPS with HSTS</h1>
                </header>
            
            <article>
                
<p>Now, if we want to notify our clients that they should be using HTTPS at all times, instead of simply trusting them to follow our redirect messages, we can do so by leveraging <strong>HTTP Strict Transport Security</strong> (<strong>HSTS</strong>). This is an interaction mechanism whereby a web server can notify any client interacting with it over an HTTPS connection that all<em> </em>subsequent interactions should happen over HTTPS. It delivers this notification by way of an HSTS header with a  <kbd>Strict-Transport-Security</kbd> key and some arbitrary value, such as an expiration timestamp, after which the client can resume attempts to connect using unsecured URLs.</p>
<p>If the client complies with HSTS, it will respond to the header by updating any cached links that hold references to the unsecured URL to now hold references to the secured URL. It will also prevent any user interaction with the unsecured URL, even when a connection to the secure URL cannot be made.</p>
<p>Now, you might be curious how we can enable this feature in our web server, and begin returning that header in our responses. Well, hopefully you won't be surprised that it's just as simple as enabling HTTPS redirection in the first place. If you look inside the conditional <kbd>if/else</kbd> statement that checks if your application is currently running in a development environment, you'll see the following:</p>
<pre>if (env.IsDevelopment()) {<br/>  app.UseDeveloperExceptionPage();<br/>} else {<br/>  // The default HSTS value is 30 days. You may want to change this for production scenarios, see https://aka.ms/aspnetcore-hsts.<br/>  app.UseHsts();<br/>}</pre>
<p>As you can see, we're already leveraging HSTS. We just can't see as much because we're running our application locally using development settings. This is very much by design. It's strongly recommended that you don't use HSTS in development because the header value and its corresponding expiration are considered highly cacheable by browsers. This could make debugging and troubleshooting especially difficult during development. <span>On your local environment, the setting isn't even available out of the box because the local loopback address is excluded by the middleware by default.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">HTTPS port configuration</h1>
                </header>
            
            <article>
                
<p>It's important to note that the only reason our application successfully redirects to our HTTPS URL is because we are listening on a distinct port for incoming HTTPS requests by default. The <kbd>launchSettings.json</kbd> file for a new Web API project is always configured to listen on both an HTTP and an HTTPS port. That same port is leveraged by the <kbd>UseHttpsRedirection</kbd> middleware when it's invoked on our application. Without configuring an HTTPS port for our application to listen on, the redirection middleware will simply fail to resolve, and requests that are made of the unsecured HTTP URLs will be processed and responded to accordingly.</p>
<p>There are a number of ways to configure the port that your middleware should redirect users to, but in each case you still have to make sure you're also configuring your web server to listen over that port. This includes registering <kbd>HttpsRedirectionOptions</kbd> in your application's service resolver, with the following code segment inside your <kbd>ConfigureServices(IServiceCollection services)</kbd> method, as shown here:</p>
<pre>services.AddHttpsRedirection(options =&gt; {<br/>    options.RedirectStatusCode = StatusCodes.Status307TemporaryRedirect;<br/>    options.HttpsPort = 443;<br/>});</pre>
<p>Alternatively, you can set a secure scheme in the <kbd>UseUrls()</kbd> method of the <kbd>IWebHostBuilder</kbd> object when you're configuring your web server in your <kbd>Program.cs</kbd> file. This has the added bonus of simultaneously configuring your web server to listen on a secure port, while also configuring the <kbd>UseHttpsRedirection()</kbd> middleware to redirect users to it.</p>
<p>As you can hopefully see by now, though, the default support for HTTPS and secure interactions provided by .NET Core will make your life substantially easier. This is especially the case when you have to implement any sort of confidential interactions, as we'll be doing in the next chapter when we discuss authentication and authorization in web applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Trusting your development certificate</h1>
                </header>
            
            <article>
                
<p>As we discussed at length in the section of this chapter about CA, any attempt to interact with a resource via HTTPS through a typical browser will result in a warning if the browser cannot verify the identity of the server with a trusted certificate authority. Since our applications will typically be hosted and run locally while we're developing and debugging them, we won't have access to a certificate signed by a trusted CA. Instead, we'll be using what's called a self-signed certificate. This is exactly the kind of certificate I warned you about being untrustworthy, since you can't trust the signature of a server until you can trust that you know who the server is. However, in this case, where we're developing an application locally and testing responses through our browser or REST client, we know <em>exactly </em>who the server is. It's us!</p>
<p>Since this scenario is common, Microsoft and Windows provide a simple one-time mechanism for getting past the untrusted certificate issue when testing HTTPS locally. Every time you install the .NET Core SDK, it includes an HTTPS development certificate that is issued by your web server whenever it is being hosted by the <kbd>dotnet</kbd> runtime application. To configure your local web browsers and other clients to trust this self-signed certificate, you only need to register it with your OS using the following CLI command:</p>
<pre><strong><span>dotnet dev-certs https --trust</span></strong></pre>
<p>Running this command will add the self-signed certificate that was included in your .NET SDK into the trusted root certificates store of your operating system. This store is then used by any application that needs to validate that an external host is who they claim to be. By storing our development certificate in this store with that CLI command, we can eliminate the warnings and alerts from our browsers any time we want to test an application that is configured to use HTTPS. And just like that, you're all set to leverage TLS within your .NET Core application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The TLS handshake simulation</h1>
                </header>
            
            <article>
                
<p>To keep things simple and to clarify the contents of this chapter, the actual implementation of the demo API we wrote will highlight each step in the TLS handshake. I've renamed my one controller to <kbd>TlsController</kbd>, and have implemented each step as its own controller action. The purpose of this is to reflect the conceptual steps taken by your web server whenever a user connects with your application over TLS.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Identity verification</h1>
                </header>
            
            <article>
                
<p>As you may recall, the first step of the TLS protocol is to establish the identity of the server. In that step, the client simply initiates a secure connection by sending a request to a secure endpoint (an endpoint leveraging HTTPS) and the server responds with an X.509 certificate. To that end, we've created a simple <kbd>GET</kbd> method named <kbd>initiate-connection</kbd>, which returns a certificate, which here is just a string:</p>
<pre>[HttpGet("initiate-connection")]<br/>public ActionResult&lt;string&gt; GetCertificate() {<br/>    return "SSL_CERTIFICATE";<br/>}</pre>
<p class="mce-root">As you may recall, the responsibility for interacting with a trusted CA falls on the client. So, at this point, we merely wait for them to confirm that we are who we say we are. Once they notify us that the certificate has been verified, we can send over our public encryption key, which they can use to encrypt their subsequent requests in the handshake protocol. For that interaction, we have the following <kbd>certificate-verified</kbd> method:</p>
<pre>[HttpGet("certificate-verified")]<br/>public ActionResult&lt;string&gt; GetVerification() {<br/>    return "PUBLIC_KEY_FOR_ENCRYPTING_HANDSHAKE";<br/>}</pre>
<p>And just like that, we're ready to start negotiating our encryption scheme.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Negotiating the encryption scheme</h1>
                </header>
            
            <article>
                
<p>Now that we've given them a public encryption key, we're waiting on our client to encrypt their next message with it. Both hosts must establish which encryption algorithms they support so that a mutual strong algorithm can be agreed upon. For that, we give our clients an endpoint named hash-algorithms, which will return all of the secure algorithms we support, and allow them to choose one they also support for use:</p>
<pre>[HttpGet("hash-algorithms-requested")]<br/>public ActionResult&lt;IEnumerable&lt;string&gt;&gt; GetAlgorithms() {<br/>    return new string[] {<br/>        "SHA-256",<br/>        "AES",<br/>        "RSA"<br/>    };<br/>}</pre>
<p>Once they've determined which algorithm is most suitable for their needs and purposes, they'll notify us. So, we have another method configured to handle that response from our client. However, once this one is done, we can use the algorithm they've selected, along with our private keys, to generate a session key that will serve as the shared secret between us for the data-transfer segment of the communication session. So, the last method of our simulation API uses a static <kbd>SessionService</kbd> class to store the selected algorithm, and then uses it to return a shared key, generated from our private keys and a random session key:</p>
<pre>[HttpPost("hash-algorithm-selected")]<br/>public ActionResult&lt;string&gt; Post([FromBody] string sharedAlgorithm) {<br/>    SessionService.CurrentAlgorithm = sharedAlgorithm;<br/>    return SessionService.GenerateSharedKeyWithPrivateKeyAndRandomSessionKey();<br/>}</pre>
<p>With that method, our session is established, and data transfer can proceed safely. Hopefully, by breaking it down into it's most basic, high-level steps like that, it's a bit more clear what's going on every time you navigate to a website with the <kbd>https://</kbd> schema. More importantly, now you know how to configure and enforce HTTPS and TLS from within your .NET Core projects going forward.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>We covered a lot in this chapter, while managing to focus on a rather narrow subject. We started by taking a high-level view of the necessary <span>steps </span>to secure communications between two hosts over an open network. Then, we looked at how each of those steps is implemented (conceptually, at least). First, we looked at the process of verifying the identity of the host you want to interact with. We learned about trusted certificate authorities, and learned how they are leveraged by web clients to validate the identity of a server by examining a signed, cryptographic certificate.</p>
<p>In exploring this topic, we also considered how much trust must be placed in these CAs, and how that level of trust opens the wider public up to an incredibly high level of risk if it is ever violated. We also learned why a CA is necessary to validate a server identity, but is not necessary to validate the identity of a client in a mutual authentication scenario.</p>
<p>Next, we looked at how two hosts, whose identities have been sufficiently verified, can proceed to secure their communications over the course of a session. We saw how symmetric and asymmetric encryption is used to make sure that interactions are encrypted well before even a single byte of application data is transmitted.</p>
<p>Next, we looked at how these high-level steps for securing a communication session have been standardized and leveraged by secure protocols over the years. We saw how frequently security vulnerabilities can render a protocol functionally insecure, and how subsequent versions or standards can leverage ever-increasing tool sets to stay ahead of vulnerabilities and evolve over time.</p>
<p>Finally, we looked at how all of this is handled in the .NET Core framework. We saw how to configure our web services to support and rely on TLS, and how to avoid some of the additional overhead of using a CA while we're still in the development phase of a project. All of this has positioned us well to consider how to leverage this to allow for authentication and authorization in an application, which we'll be exploring in the next chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What does HTTPS stand for? How is it distinct from HTTP?</li>
<li>What is a trusted certificate authority? What is their role in verifying the identity of a server?</li>
<li>What is the difference between simple and mutual authentication?</li>
<li>What is the difference between symmetric and asymmetric cryptography?</li>
<li>What is forward secrecy? How is it provided?</li>
<li>What is a non-deterministic function? Why is it important for securing a communication session?</li>
<li>What are message authentication codes? How are they used to provide reliability in TLS?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p>For more information about establishing secure software practices and the principles underlying SSL, TLS, and all things concerning network security, I recommend <em>Cybersecurity – Attack and Defense Strategies,</em> <em>Yuri Diogenes,</em> <em>Dr. Erdal Ozkaya, Packt Publishing</em>. It's an illuminating guide to the daily considerations of the engineers who design secure protocols like TLS. It's available through Packt Publishing, here: <a href="https://www.packtpub.com/networking-and-servers/cybersecurity-attack-and-defense-strategies">https://www.packtpub.com/networking-and-servers/cybersecurity-attack-and-defense-strategies.</a></p>
<p><a href="https://www.packtpub.com/networking-and-servers/cybersecurity-attack-and-defense-strategies"/></p>
<p>To understand just how much risk you assume whenever you expose your software to an open network, I'd also recommend <em>Network Vulnerability Assessment,</em> <em>Sagar Rahalkar, Packt Publishing</em>. It approaches the problem more from the perspective of a DevOps engineer or systems engineer than that of a software engineer, but I think it's important to have that kind of big-picture understanding. And this book is a great resource for that. It's also available through Packt, here: <a href="https://www.packtpub.com/networking-and-servers/network-vulnerability-assessment">https://www.packtpub.com/networking-and-servers/network-vulnerability-assessment.</a></p>


            </article>

            
        </section>
    </body></html>