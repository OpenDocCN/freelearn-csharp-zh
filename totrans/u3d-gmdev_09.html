<html><head></head><body>
<div><div><h1 class="chapterNumber">9</h1>
<h1 class="chapterTitle" id="_idParaDest-196">Visual Effects</h1>
<p class="normal">The game does not currently have all of its features. We’ve gone from concept to blockout, then developed the game into a state where it is playable. This doesn’t mean we are finished quite yet! We need to look at how we can bring more emotional immersion to the player. Luckily for us, Unity provides fantastic assets and tools for us to take the current state of our game and visually turn it up another notch. This is done through various vehicles, such as shaders, particles, and other polishing tools that we will cover in <em class="chapterRef">Chapter 12</em>, <em class="italic">Final Touches</em>.</p>
<p class="normal">These topics are very complex. For now, we will go over the main focus of visual effects by looking at shaders and particles at a high level. Then, we will progress to an overview of their advanced forms. Because we are using the <strong class="keyWord">Universal Render Pipeline</strong> (<strong class="keyWord">URP</strong>) for this project, we will go over important tools such as Shader Graph, VFX Graph, and Shuriken. Shader Graph visually displays shader detailing and coding. VFX Graph was created to help you understand the various properties of GPU particles. Shuriken, a CPU-focused particle authoring tool, is available in all render pipelines. We will cover that as well. </p>
<p class="normal">In this chapter, the following topics will be covered:</p>
<ul>
<li class="bulletList">Visual effects overview</li>
<li class="bulletList">Shader Graph</li>
<li class="bulletList">Particle Systems:<ul>
<li class="bulletList">Shuriken</li>
<li class="bulletList">VFX Graph</li>
</ul>
</li>
</ul>
<h1 class="heading-1" id="_idParaDest-197">Visual effects overview</h1>
<p class="normal">Getting started with visual effects may feel daunting at the onset. We currently have a simple scene. The world <a id="_idIndexMarker576"/>won’t feel alive and immersive without time spent making deliberate answers to questions needing to be solved to progress the narrative and design of the game. </p>
<p class="normal">Throughout this book, you’ve worked through many general game design questions. You’ve been able to answer them yourself and may use them for any project you may want to work on. Then, you were given the answers we discovered ourselves while creating this project and how it would move forward. We had a pretty good idea of what the feel would be for the player: fantasy exploration in the simplest of terms. We now need to be able to go through our scene and find areas in which we need a touch more fantasy. Exploration is done through the mechanics and narrative design. </p>
<p class="normal">Fantasy is a broad term. We could have gone with any theme really. We decided to push through this vague starting point and found a theme of light science fiction focused on an ancient race. These beings hold onto the power of the celestial bodies of the natural space surrounding them. Working with nature, they at some point constructed a cave the player will explore. We need to come up with a way to embody this storytelling through visual gameplay, and the player accepting themself as the main character of this world is what we are aiming for. </p>
<p class="normal">To carry out this visual storytelling, we need to incorporate the multiple visual effects tools available in Unity. Shader Graph allows us to build shaders that can have interesting properties that play with lights and twinkling effects in various ways. Shuriken provides us with particle systems to add ambient dust, glowing particles around bioluminescent plants, and simple details of other fantasy elements. VFX Graph allows us to push simple particles to the limit and bring GPU systems into play. By pushing GPU systems into play, VFX Graph will give us the ability to use many particles. Though this isn’t practical, you could spawn tens of millions of particles. Finally, we will use the lighting in Unity to provide hints to the player for where to look and set the mood and tone of the current action, system, or place.</p>
<p class="normal">To start this chapter, we will be laying the foundation of terms and going over the individual visual effects tools in detail. After these explanations, we can then expand upon how we incorporate the tools that Unity has to offer into our workspace to create a visually immersive environment. Moving forward, this section may become a useful reference point to come back to re-familiarize yourself with the tools and their purpose.</p>
<h1 class="heading-1" id="_idParaDest-198">Shader Graph</h1>
<p class="normal">Shader Graph is a visual scripting tool designed to allow artist-driven shader creation. Shaders are <a id="_idIndexMarker577"/>scripts that inform the graphics pipeline how to render materials. A material is an instance of a shader with parameters set for a certain mesh inside of a GameObject. A simple example of this could be leather. If you think of leather, you could probably think about many different types of leather. What color is it? Is it shiny? Does it have a unique texture? All of these options are parameters in the shader that can be set in the material to render the object properly. </p>
<p class="normal">In Unity, a shader <a id="_idIndexMarker578"/>is written in <strong class="keyWord">High-Level Shader Language</strong> (<strong class="keyWord">HLSL</strong>). Knowing how to properly write this code requires a detailed understanding of the graphics pipeline in your computer and can be a bit daunting to get going. The graphics pipeline is a complex conceptual model. Simplified, the computer goes through layers and stages to understand what 3D visual graphics are in a scene, then takes that information and renders those visuals to a 2D screen. </p>
<p class="normal">In fact, just reading the paragraph above might’ve seemed a bit confusing. This is natural. There are many moving parts here and we will break this down within this portion of the chapter. In an effort to not dive in too deep with HLSL, we will be focusing purely on Shader Graph. If after spending time in Shader Graph you want to move into a more technical position, we recommend learning HLSL and handwriting shaders. Once you have learned HLSL and handwriting shaders you will have a solid foundation of general shader creation.</p>
<p class="normal">Let’s go through the setup of Shader Graph first, then how to create a shader. We should then take some time to talk about basic and commonly used nodes that are used to create shaders. Nodes are a visual representation of a block or chunk of code. These nodes can be linked together to create larger functions of visually layered effects.</p>
<h2 class="heading-2" id="_idParaDest-199">Setup</h2>
<p class="normal">We have <a id="_idIndexMarker579"/>been talking about using the <strong class="keyWord">URP</strong> project, which should automatically have Shader Graph installed on it. If it isn’t installed, you can easily install it by heading to the <strong class="screenText">Package Manager</strong> and installing it from the Unity Registry packages. </p>
<p class="normal"><em class="italic">Figure 9.1</em> below shows what’s needed to install Shader Graph properly. </p>
<p class="normal">If you have a green checkmark like in the figure below, then it is installed already!</p>
<figure class="mediaobject"><img alt="Graphical user interface, text, application  Description automatically generated" height="403" src="img/B17304_09_01.png" width="824"/></figure>
<p class="packt_figref">Figure 9.1: Checking if Shader Graph is installed</p>
<p class="normal">Now that <a id="_idIndexMarker580"/>we have either installed Shader Graph or verified it’s installed, let’s move on to creating your first shader.</p>
<h2 class="heading-2" id="_idParaDest-200">Creation</h2>
<p class="normal">Right-clicking <a id="_idIndexMarker581"/>in an open area of your project window gives you the marking menu for that space. We want to make a new shader, so we mouse over <strong class="screenText">Create</strong> &gt; <strong class="screenText">Shader</strong> &gt; <strong class="screenText">Universal Render Pipeline</strong> and then get four options. These four options are the basics from URP that automatically set up the shader settings for you. Refer to <em class="italic">Figure 9.2</em> for the path to follow to create your shader.</p>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" height="413" src="img/B17304_09_02.png" width="825"/></figure>
<p class="packt_figref">Figure 9.2: Path to create a shader in Unity Editor</p>
<p class="normal">You may <a id="_idIndexMarker582"/>be wondering, “Which one should I choose?” If so, that is a great question. Let’s go over all four of these types just in case you want to mess around with any of the other types after we move along with our choice. From top to bottom we have <strong class="keyWord">Lit</strong>, <strong class="keyWord">Sprite Lit</strong>, <strong class="keyWord">Sprite Unlit</strong>, and <strong class="keyWord">Unlit</strong> Shader Graphs to work with. Let’s go through each of these types in that order.</p>
<h3 class="heading-3" id="_idParaDest-201">Lit Shader Graph</h3>
<p class="normal">The Lit Shader <a id="_idIndexMarker583"/>Graph lets you render 3D objects with <a id="_idIndexMarker584"/>real-world lighting information. The shaders that use this will be <a id="_idIndexMarker585"/>using a <strong class="keyWord">Physically Based Rendering</strong> (<strong class="keyWord">PBR</strong>) lighting model. A PBR model allows 3D surfaces to have a photo-realistic quality of materials like stone, wood, glass, plastic, and metals across various lighting conditions. With the Lit Shader’s assistance, lighting and reflections across these objects can adhere to dynamic shifts, such as bright light to a dark cave environment, accurately.</p>
<h3 class="heading-3" id="_idParaDest-202">Sprite Lit Shader Graph</h3>
<p class="normal">URP comes <a id="_idIndexMarker586"/>with a 2D rendering and lighting system. This system <a id="_idIndexMarker587"/>would be used in this graph type as the items that this shader would be rendered on would be sprites. A sprite is a two-dimensional bitmap (an array of binary data that represents the color of each pixel) that is integrated into a larger scene. This will allow the sprites to take in the needed lighting information.</p>
<h3 class="heading-3" id="_idParaDest-203">Sprite Unlit Shader Graph</h3>
<p class="normal">This is <a id="_idIndexMarker588"/>similar to the Shader Lit Shader Graph above, but <a id="_idIndexMarker589"/>the difference with the Sprite Unlit Shader Graph is that it is considered always fully lit and will take in no lighting information. This graph also only uses the 2D rendering and lighting system in the URP.</p>
<h3 class="heading-3" id="_idParaDest-204">Unlit Shader Graph</h3>
<p class="normal">The unlit <a id="_idIndexMarker590"/>type of graph for URP uses the 3D PBR lighting model <a id="_idIndexMarker591"/>like the Lit Shader Graph type. The major difference is that it won’t take in lighting information. This is the most performant shader in URP.</p>
<h2 class="heading-2" id="_idParaDest-205">Shader Graph interface</h2>
<p class="normal">Choose the Lit Shader Graph type for our Shader Graph. When you right-click in the project <a id="_idIndexMarker592"/>window, a new shader file will be created. Double-clicking this file will open the <strong class="screenText">Shader Graph</strong> window. </p>
<p class="normal">There are some pieces we should go over so you can understand what is covered in the following subsections. We need to go over the <strong class="screenText">Master Stack</strong>, <strong class="screenText">Blackboard</strong>, <strong class="screenText">Graph Inspector</strong>, <strong class="screenText">Main Preview</strong>, and then <strong class="screenText">Nodes</strong>. In <em class="italic">Figure 9.3</em> below, four out of five of these sections are displayed. We will be going over them in detail in this chapter.</p>
<figure class="mediaobject"><img alt="Graphical user interface  Description automatically generated" height="446" src="img/B17304_09_03.png" width="826"/></figure>
<p class="packt_figref">Figure 9.3: Shader Graph window breakdown</p>
<h3 class="heading-3" id="_idParaDest-206">Master Stack</h3>
<p class="normal">The green-outlined <a id="_idIndexMarker593"/>item in <em class="italic">Figure 9.3</em> is the Master Stack. There <a id="_idIndexMarker594"/>are two sections to the Master Stack, <strong class="screenText">Vertex</strong> and <strong class="screenText">Fragment</strong>. The <strong class="screenText">Vertex</strong> block of the Master Stack contains instructions to the actual vertexes <a id="_idIndexMarker595"/>of the 3D object that the material with this <a id="_idIndexMarker596"/>assigned shader will be manipulated. Within this block, you can affect the vertex’s <strong class="screenText">Position</strong>, <strong class="screenText">Normal</strong>, or <strong class="screenText">Tangent</strong> attribute. These three attributes occur everywhere in a 2D and 3D environment. <strong class="screenText">Position</strong> represents a vertex’s location in object space. <strong class="screenText">Normal</strong> is used to calculate which direction light reflects off or is emitted out from the surface. <strong class="screenText">Tangent</strong> alters the appearance of vertexes on your surface to define the object’s horizontal (U) texture direction.</p>
<p class="normal">In our case, we do not need to change any of the vertex attributes, so we will move on to the fragment shader portion and leave the object space alone.</p>
<p class="normal">Fragment instructions can be thought of as possible pixels on your screen. We can affect the pixels depending on the changes made to the inputs to the stack. The attributes that are listed in the fragment shader depend on the shader type we choose when making the shader. </p>
<p class="normal">The blocks inside of the <strong class="screenText">Fragment</strong> stack <a id="_idIndexMarker597"/>are called Fragment Nodes. If you do not need a particular Fragment Node, you can remove it by right-clicking on the <strong class="screenText">Fragment Node</strong> individually and selecting <strong class="screenText">Delete</strong>. You can also add other Fragment Nodes to the stack by right-clicking the bottom of the <strong class="screenText">Fragment</strong> frame and selecting <strong class="screenText">Add Node</strong>. </p>
<p class="normal">In <em class="italic">Figure 9.4</em>, you can see a selection of all the node choices to add to the stack. If your shader isn’t <a id="_idIndexMarker598"/>set up to accept those new Fragment <a id="_idIndexMarker599"/>Nodes, they will be gray and not usable. For now, let’s go through the Lit Shader Fragment options.</p>
<figure class="mediaobject"><img alt="Graphical user interface  Description automatically generated" height="643" src="img/B17304_09_04.png" width="431"/></figure>
<p class="packt_figref">Figure 9.4: Fragment Node options</p>
<p class="normal">The Fragment Nodes in the Fragment stack represent the pixels that will potentially be displayed in clip space or on your screen. 3D objects also have a 2D representation of their faces in the form of UVs. UVs are a two-dimensional texture representation of a 3D object. UVs have a flat, 2D axis, 0 (U) to 1 (V) graph. This particular UV graph is a representation of each vertex on this UV plane stretched over a 3D object. The UV graph is also called a UV texture space.</p>
<p class="normal">Looking at <em class="italic">Figure 9.5</em> below, you can see that the geometry has been unwrapped to make it flat. This is like papercraft or origami. Knowing how this works sets up the concept of <a id="_idIndexMarker600"/>how shaders can manipulate not only the <a id="_idIndexMarker601"/>vertex of a mesh but also the color of the faces.</p>
<figure class="mediaobject"><img alt="A picture containing box, businesscard  Description automatically generated" height="278" src="img/B17304_09_05.png" width="823"/> </figure>
<p class="packt_figref">Figure 9.5: Basic mesh, UV layout, gradient on base color</p>
<p class="normal">We wanted to show you how we achieved the gradient within Shader Graph for <em class="italic">Figure 9.5</em>. Though this isn’t the simplest graph we could’ve started with, it does a good job of breaking down some key concepts early. </p>
<p class="normal">If you look at <em class="italic">Figure 9.6</em> below, you will see our graph to build a gradient that we placed in the <strong class="screenText">Base Color</strong>. The gradient is then applied to the 0-1 space of the 3D object we assigned this material with this shader. It is not a complex shader as it has hardcoded gradients coming from the UV node. </p>
<p class="normal">We will add more complexity to the parameters we make in the <strong class="screenText">Blackboard</strong> in the next section. </p>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated with medium confidence" height="482" src="img/B17304_09_06.png" width="825"/></figure>
<p class="packt_figref">Figure 9.6: Test gradient shader</p>
<p class="normal">We will <a id="_idIndexMarker602"/>be breaking down commonly used nodes in <a id="_idIndexMarker603"/>the next part of this chapter. For now, a quick explanation of what we are doing will help break this down. </p>
<h4 class="heading-4">Base color</h4>
<p class="normal">We are taking <a id="_idIndexMarker604"/>the UV’s 0-1 space, represented here with two gradients, <em class="italic">x </em>and <em class="italic">y</em> in the Red and Green channels. Red and Green channels are a part of color space; there are <strong class="keyWord">Red</strong> (<strong class="keyWord">R</strong>), <strong class="keyWord">Green</strong> (<strong class="keyWord">G</strong>), <strong class="keyWord">Blue</strong> (<strong class="keyWord">B</strong>), and <strong class="keyWord">Alpha</strong> (<strong class="keyWord">A</strong>) channels. RGB represents the color values. The Alpha channel indicates the opacity of each pixel, from 0 (fully transparent) through to 1 (opaque).</p>
<p class="normal">We’ve seen that the 0-1 space starts from the bottom left and ends linearly in the top right in Unity. This means that the Green channel will be a linear gradient from bottom to top. Splitting off that channel allows us to manipulate the 0-1 in a Lerp node, with red replacing black and teal replacing white. There is a lot going on throughout the next few sections, but stick with it! As we break down the nodes, it will be much easier to follow per node.</p>
<h4 class="heading-4">Normal</h4>
<p class="normal">Normals tell each fragment how they are supposed to react to light that hits the face. </p>
<p class="normal">This is extremely <a id="_idIndexMarker605"/>useful to add detail to a face without changing the silhouette, reducing the number of polygons needed for higher detail. Looking at <em class="italic">Figure 9.7</em> below, you can see that it looks as though there are bumps pulled out of the cube. This isn’t a color change; this is light reacting to the surface of the faces. If you look closely, there are no protrusions on the edges. This is because the shape of the cube didn’t change. This is just a cube that acts as it does because of the normal maps.</p>
<figure class="mediaobject"><img alt="Minecraft with RTX PBR Texturing Guide | GeForce News | NVIDIA" height="316" src="img/B17304_09_07.png" width="586"/></figure>
<p class="packt_figref">Figure 9.7: Normals on a cube</p>
<p class="normal">On the left side of <em class="italic">Figure 9.7</em>, the left cube is blue due to the normal map using its color scheme from the RGB channels in the tangent space. This means that a flat normal map would represent <em class="italic">0</em>, <em class="italic">0</em>, <em class="italic">1</em>. Red and Green channels are used to represent the shifts in how the light should act on either the <em class="italic">x</em> tangent or the <em class="italic">y</em> tangent. When we work with materials in <em class="chapterRef">Chapter 12</em>, <em class="italic">Final Touches</em>, we will go further into detail about the functions of a normal map.</p>
<h4 class="heading-4">Metallic</h4>
<p class="normal">Metallic is exactly what it sounds like. This is how metallic the material is! That isn’t a great definition though, so let’s try to break it down a little. </p>
<p class="normal">The metallic field <a id="_idIndexMarker606"/>is a scaler float from 0 to 1, with 0 being not metallic and 1 being bare metal. Metallic materials take in their surrounding environment’s color. In <em class="italic">Figure 9.8</em> we have four spheres with four different settings for their material properties. We’re only using the URP/Lit shader that comes with URP out of the box to test these out. For this section we will only look at the left two spheres. The far-left sphere is white and the metal setting is set to 0. This material is not taking in any of the environment’s color. It’s only taking in lighting information and its base color of white.</p>
<p class="normal">The second sphere, though it still has white for a base color, has the metallic setting set to 1. The smoothness is set to 0.5 to be neutral, which you will hear more about shortly. If you look closely the second sphere has the Unity default skybox in the color. Now we need to throw in smoothness to this material.</p>
<figure class="mediaobject"><img alt="A picture containing egg  Description automatically generated" height="260" src="img/B17304_09_08.png" width="825"/></figure>
<p class="packt_figref">Figure 9.8: From left to right: No metal, all metal, all metal not smooth, all metal all smooth</p>
<h4 class="heading-4">Smoothness</h4>
<p class="normal">Continuing with <em class="italic">Figure 9.8</em>, we will move on to the right two spheres. The third sphere from the left <a id="_idIndexMarker607"/>is very interesting. This one has a base color of white, the metal setting set to 1, and smoothness set to 0. This means that the entire sphere is fully diffused! Diffused in this instance means that all the colors of the environment are blended across the whole sphere, leading to an almost perfect true neutral gray. The fourth sphere is the same, but smoothness is set to 1. This means that the entire sphere is reflecting the direct environment.</p>
<h4 class="heading-4">Emissive</h4>
<p class="normal">For an object to emit or radiate light you look <a id="_idIndexMarker608"/>toward an <strong class="keyWord">emissive map</strong>. The purpose of an emissive <a id="_idIndexMarker609"/>map is to be able to push colors to have brighter values than 1. Pushing past the value of 1 allows that part of your object to glow brightly. This is useful for lava, sci-fi lights, or anything you want to emit brightness. Otherwise, the Fragment Node defaults to black and no emission is created.</p>
<figure class="mediaobject"><img alt="A picture containing rock, stone  Description automatically generated" height="367" src="img/B17304_09_09.png" width="634"/></figure>
<p class="packt_figref">Figure 9.9: Left is no emission, right has emission with 2.4 intensity, no bloom</p>
<p class="normal">This doesn’t look like an emissive glowing mushroom! This is due to emission needing a post-processing volume. To do this, create an empty GameObject and name it <code class="inlineCode">_PostProcess</code>. We’re naming it this to give it a distinctly different name. Using the underscore as a prefix, we’re letting our developers know that this object houses logic only. There aren’t GameObjects for use in the game itself. Then add a <strong class="screenText">Volume</strong> component, seen below in <em class="italic">Figure 9.10</em>.</p>
<figure class="mediaobject"><img alt="A picture containing text, electronics, screenshot  Description automatically generated" height="227" src="img/B17304_09_10.png" width="635"/></figure>
<p class="packt_figref">Figure 9.10: Volume added to post-process GameObject</p>
<p class="normal">We’re not <a id="_idIndexMarker610"/>done yet! We need to add a profile and an override to get our bloom set up. Pressing the <strong class="screenText">New</strong> button on the right-hand side of the <strong class="screenText">Volume</strong> component will create a profile for your settings to be added to. This allows you to store those settings for other scenes if you need to. When you add a profile, you will then be able to add an override. </p>
<p class="normal">We want to click on <strong class="screenText">Add Override</strong>, then <strong class="screenText">Post Process</strong>, and finally <strong class="screenText">Bloom</strong>. Then select the <code class="inlineCode">Intensity</code> Boolean checkbox to allow for intensity to be changed. Change it to <code class="inlineCode">1</code>. The settings are shown below in <em class="italic">Figure 9.11</em>.</p>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated with medium confidence" height="675" src="img/B17304_09_11.png" width="705"/></figure>
<p class="packt_figref">Figure 9.11: Bloom override for post-processing volume</p>
<p class="normal">Now we <a id="_idIndexMarker611"/>see it emitting light around the mushroom on the screen. This isn’t adding light into the scene; it is only adding a brightness value outside of the mesh to the render on the screen. </p>
<figure class="mediaobject"><img alt="A picture containing hole  Description automatically generated" height="353" src="img/B17304_09_12.png" width="608"/></figure>
<p class="packt_figref">Figure 9.12: Left: no emissive, Right: emissive and bloom set up</p>
<p class="normal">We have <a id="_idIndexMarker612"/>a shiny, glowing mushroom! Go forth and add emission to things! We will now look into <strong class="keyWord">Ambient Occlusion</strong>.</p>
<h4 class="heading-4">Ambient Occlusion</h4>
<p class="normal">The point <a id="_idIndexMarker613"/>of <strong class="keyWord">Ambient Occlusion</strong> (<strong class="keyWord">AO</strong>) is to add dark spots to <a id="_idIndexMarker614"/>sections to show creases. This adds a nice, clean effect of shadows even if there aren’t any specific lights making shadows. AO is designed for light at all angles. This attribute expects values from 0 to 1. If you do not have an AO map for your model, it’s best to leave it at 1. In <em class="chapterRef">Chapter 12</em>, <em class="italic">Finishing Touches</em>, we will be working with Myvari’s material, which will have an AO map to go over.</p>
<p class="normal">This was the master stack in summary. Each of the attributes on the stack can be used to provide unique shaders. Something that helps with even more customization is the <strong class="screenText">Blackboard</strong> part of Shader Graph. </p>
<h3 class="heading-3" id="_idParaDest-207">Blackboard</h3>
<p class="normal">The <strong class="screenText">Blackboard</strong> allows <a id="_idIndexMarker615"/>the user to create properties and keywords <a id="_idIndexMarker616"/>to use in shaders that can be dynamically changed in various ways. Properties can be used within the shader or be exposed to the material in the Inspector using the <strong class="screenText">Exposed</strong> checkbox. </p>
<p class="normal">You can locate the <strong class="screenText">Blackboard</strong> button on the top right of the <strong class="screenText">Shader Graph</strong> window. Once you click this button, <strong class="screenText">Blackboard</strong> will open its own separate UI.</p>
<figure class="mediaobject"><img alt="A picture containing text  Description automatically generated" height="582" src="img/B17304_09_13.png" width="248"/></figure>
<p class="packt_figref">Figure 9.13: Variable types available in the Blackboard</p>
<p class="normal">There are 16 available data types that can be created. These data types can be changed through scripting during runtime as well. Keywords are designed to be changed per material instance during runtime. There are only a few options for this as the compiler needs to account for all the variations. This is handy for things like mobile specifications. You may make an enum (user-defined set of constraints) or list for different systems to change the fidelity (perceptual quality) of the shader to adapt to platform limitations.</p>
<h3 class="heading-3" id="_idParaDest-208">Graph Inspector</h3>
<p class="normal">The <strong class="screenText">Graph Inspector</strong> gives you <a id="_idIndexMarker617"/>the options for the shader type itself. We chose <a id="_idIndexMarker618"/>to start with a URP/Lit shader as our base, which sets specific settings in the <strong class="screenText">Graph Inspector</strong> for us. <em class="italic">Figure 9.14</em> below shows the <strong class="screenText">Graph Inspector</strong>. These are the settings that are set by default with the URP/Lit.</p>
<figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" height="614" src="img/B17304_09_14.png" width="462"/></figure>
<p class="packt_figref">Figure 9.14: Graph Inspector</p>
<p class="normal">Each of <a id="_idIndexMarker619"/>these properties available to the shader we are building has great use <a id="_idIndexMarker620"/>in certain situations. When we go over using them in<em class="chapterRef">, Finishing Touches</em>, we will explain why we are making the changes to the graph settings. For now, understand that the material we chose was the <strong class="screenText">Lit </strong>option and it defaults to a metallic opaque workflow. This means that you do not see the color from items behind it as it isn’t transparent. </p>
<h3 class="heading-3" id="_idParaDest-209">Main Preview</h3>
<p class="normal">The <strong class="screenText">Main Preview</strong> is an <a id="_idIndexMarker621"/>overall glance at what the <a id="_idIndexMarker622"/>shader would look like in the game. The default is a sphere. </p>
<p class="normal">You can right-click in the window to access multiple options, such as to include a custom mesh, as noted in <em class="italic">Figure 9.15</em> below.</p>
<figure class="mediaobject"><img alt="Graphical user interface  Description automatically generated" height="273" src="img/B17304_09_15.png" width="436"/></figure>
<p class="packt_figref">Figure 9.15: Screenshot of the main preview in Shader Graph and its options</p>
<p class="normal">Until you plug in some nodes to the Master Stack, this preview will default as a gray sphere. Let’s talk about nodes next!</p>
<h3 class="heading-3" id="_idParaDest-210">Nodes</h3>
<pre>Figure 9.6 </em>to play with the gradient yourself. </pre>
<figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" height="386" src="img/B17304_09_16.png" width="415"/></figure>
<p class="packt_figref">Figure 9.16: Node creation menu</p>
<p class="normal">If you took <a id="_idIndexMarker625"/>some time to build this out, you may have also <a id="_idIndexMarker626"/>opened some of the node groupings and realized that there is a very large number of nodes to choose from. This may cause a little bit of anxiety. Fortunately, we will go over some common nodes that are used in many shaders in the next section.</p>
<h2 class="heading-2" id="_idParaDest-211">Commonly used nodes</h2>
<p class="normal">Below is a simple list of commonly used nodes to make shaders. We want to stress that this isn’t the full list of nodes. In fact, there are over 200 nodes now in Shader Graph 10+. Going over all of them could literally be a book or two. The interesting thing about these sets of nodes is that they can be built up to make some incredible shaders. Something to keep in mind when reading about these is that there may be information in the previous node that helps describe the current node you are reading. Read through them all even if you’re reasonably comfortable with, say, knowing how to add, for example.</p>
<h3 class="heading-3" id="_idParaDest-212">Add</h3>
<p class="normal">To be able to explain <strong class="screenText">Add</strong>, we need to make sure you remember that 0 means absence, or the color <a id="_idIndexMarker627"/>black in this case. This means that a value of 1 is white. We normalize <a id="_idIndexMarker628"/>our values on a 0-1 scale across many applications. You may remember that UV coordinates are also between 0 and 1. This is no mistake! So, if I had two scalars, or <code class="inlineCode">Vector1s</code>, and added them, the value would be greater. </p>
<p class="normal">Let’s show a quick example: 0.4 + 0.4 = 0.8.</p>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated with medium confidence" height="338" src="img/B17304_09_17.png" width="319"/></figure>
<p class="packt_figref">Figure 9.17: Add node</p>
<p class="normal">0.4 is a darker-than-medium-gray value. If we add two of them together, we almost get white! 0.8 is 80% to pure white in value, seen in <em class="italic">Figure 9.17</em>.</p>
<h3 class="heading-3" id="_idParaDest-213">Color</h3>
<p class="normal">This node is <a id="_idIndexMarker629"/>a Vector4 with nice visual sugar on top. Vector4 (<em class="italic">0</em>, <em class="italic">0</em>, <em class="italic">0</em>, <em class="italic">0</em>) represents Red, Green, Blue, and Alpha in shader values. <strong class="screenText">Color</strong> has an interface for you to select <a id="_idIndexMarker630"/>the color you want, and it will set up the RGB values for you with an Alpha slider for that value while outputting a Vector4. </p>
<figure class="mediaobject"><img alt="Graphical user interface, text, application, chat or text message  Description automatically generated" height="166" src="img/B17304_09_18.png" width="277"/></figure>
<p class="packt_figref">Figure 9.18: Color node</p>
<p class="normal">This would be difficult with the Vector4 node as there is no color visual to know what your values need to be.</p>
<h3 class="heading-3" id="_idParaDest-214">Lerp</h3>
<p class="normal"><strong class="screenText">Lerp</strong> stands for <strong class="keyWord">Linear Interpolation</strong>. The Lerp <a id="_idIndexMarker631"/>node can be used in many applications. One instance <a id="_idIndexMarker632"/>is how we set up a gradient <a id="_idIndexMarker633"/>to be used for the base color in <em class="italic">Figure 9.6</em>. There are three inputs: A, B, and T. You can think of it as A is 0, B is 1, and T is the driver. The driver (T) is a value of 0-1. However, the value will map to the value of A, B, and the values in between. If T is at 0, it will display an 100% value of what is in A. If T has a value of 1, it will display an 100% value of B. Now, if T is at 0.4, then that will be in between the values of A and B: 40% A and 60% B. </p>
<figure class="mediaobject"><img alt="Graphical user interface  Description automatically generated" height="397" src="img/B17304_09_19.png" width="280"/></figure>
<p class="packt_figref">Figure 9.19: Lerp node</p>
<p class="normal">This is difficult to visualize with just numbers alone. Luckily, in <em class="italic">Figure 9.19 </em>we used the UV to input T as a gradient. This allows us to see the 0-1 going from bottom to top. You are seeing a gradient from A to B from bottom to top.</p>
<h3 class="heading-3" id="_idParaDest-215">Multiply</h3>
<p class="normal">We’ve <a id="_idIndexMarker634"/>seen the <strong class="screenText">Add</strong> and <strong class="screenText">Lerp</strong> nodes; now we need to work through <a id="_idIndexMarker635"/>another operation, <strong class="screenText">Multiply</strong>. By the nature of basic arithmetic, <strong class="screenText">Multiply</strong> will make the value lower. </p>
<p class="normal">This is happening because we are in the range of 0-1. Let’s put together an example in <em class="italic">Figure 9.20. </em></p>
<figure class="mediaobject"><img alt="Graphical user interface  Description automatically generated" height="364" src="img/B17304_09_20.png" width="341"/></figure>
<p class="packt_figref">Figure 9.20: Multiply node</p>
<p class="normal">We have <a id="_idIndexMarker636"/>used the same example we used for the add node but we are <a id="_idIndexMarker637"/>using multiplication instead of using addition. Simple math states .4 * .4 = .16.</p>
<h3 class="heading-3" id="_idParaDest-216">Sample Texture 2D</h3>
<p class="normal">This node <a id="_idIndexMarker638"/>allows you to take textures you have authored in <a id="_idIndexMarker639"/>other <strong class="keyWord">Digital Content Creation </strong>(<strong class="keyWord">DCC</strong>) software <a id="_idIndexMarker640"/>such as Photoshop and use the color information to manipulate the attributes of the master stack. There are two inputs, the texture you want to sample and the UVs, as seen in <em class="italic">Figure 9.21</em>. </p>
<figure class="mediaobject"><img alt="A screenshot of a video game  Description automatically generated with medium confidence" height="347" src="img/B17304_09_21.png" width="438"/></figure>
<p class="packt_figref">Figure 9.21: Sample Texture 2D</p>
<p class="normal">The UV node allows you to manipulate the UV coordinates of your mesh. A nice feature of this node is that it outputs the Vector4 as well as each float individually from the node itself.</p>
<h3 class="heading-3" id="_idParaDest-217">Saturate</h3>
<p class="normal">There may <a id="_idIndexMarker641"/>be a time when your values go above 1. This may happen <a id="_idIndexMarker642"/>because you are working with multiple nodes that push your values outside the 0-1 range. </p>
<figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" height="180" src="img/B17304_09_22.png" width="295"/></figure>
<p class="packt_figref">Figure 9.22: Saturate node</p>
<p class="normal">If this happens, you can input the data into a saturate node and it will return all of your values within the 0-1 range. Place the float value in the <strong class="screenText">In</strong> portion, and the <strong class="screenText">Out</strong> value will be normalized to the 0-1 range.</p>
<h3 class="heading-3" id="_idParaDest-218">Split</h3>
<p class="normal">As we <a id="_idIndexMarker643"/>saw in the <strong class="screenText">Sample Texture 2D node</strong>, the Vector4 was split into <a id="_idIndexMarker644"/>individual outputs. This isn’t always the case. The <strong class="screenText">Color</strong> node only outputs a Vector4. If you only want to use the Red channel value from <strong class="screenText">Color</strong>, how could you get it? You guessed it, a split node. Input a Vector2, 3, or 4 and use whichever channel you desire as a float.</p>
<figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" height="222" src="img/B17304_09_23.png" width="284"/></figure>
<p class="packt_figref">Figure 9.23: Split node</p>
<p class="normal">This is very helpful to be able to break out an image that you put four grayscale images onto. We call that <strong class="keyWord">channel packing</strong> so you <a id="_idIndexMarker645"/>can have three images on one texture lookup.</p>
<h3 class="heading-3" id="_idParaDest-219">UV</h3>
<p class="normal">There may <a id="_idIndexMarker646"/>be a time when you need to manipulate the UVs of the object <a id="_idIndexMarker647"/>you want to render. A reason for this may be that you want to tile the UVs because the scale of the item was larger or smaller than what was intended. Another reason to use the UV node is that it automatically creates horizontal and vertical gradients. </p>
<figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" height="365" src="img/B17304_09_24.png" width="251"/></figure>
<p class="packt_figref">Figure 9.24: UV node</p>
<p class="normal">If split, the R channel is the horizontal gradient and the G channel is the vertical gradient.</p>
<h3 class="heading-3" id="_idParaDest-220">Vectors</h3>
<p class="normal">These guys <a id="_idIndexMarker648"/>are used everywhere. You’ll notice that a Vector1 is named a <strong class="keyWord">Float</strong>. Another name for Vector1 is Scalar. Something else you may have noticed is <a id="_idIndexMarker649"/>that the outputs are all different colors. Float is cyan, Vector2 is green, Vector3 is yellow, and Vector4 is pink. This is incredibly useful to know the colors as they are shown on the connection lines between the nodes. These nodes are used in infinite use cases. Do you need three points of data? Vector3!</p>
<figure class="mediaobject"><img alt="Graphical user interface  Description automatically generated" height="148" src="img/B17304_09_25.png" width="824"/></figure>
<p class="packt_figref">Figure 9.25: Vector nodes</p>
<p class="normal">With all <a id="_idIndexMarker650"/>of these basic nodes, you can make some powerful shaders to make beautiful <a id="_idIndexMarker651"/>materials with. In <em class="chapterRef">Chapter 12</em>, <em class="italic">Finishing Touches</em>, we will be covering shaders for multiple purposes and showing how we use these nodes to create some nice visual candy. Now let’s move away from Shader Graph and work with Particle Systems so we can add nice visual effects to our experiences.</p>
<h1 class="heading-1" id="_idParaDest-221">Particle Systems</h1>
<p class="normal">When you think of visual effects in a video game, the first thing that pops into your head most likely <a id="_idIndexMarker652"/>is the trails of legendary weapons in ARPGs or the amazing explosions from intense first-person-shooter campaigns. Whatever popped into your head, there are systems that allow for these effects to happen. Particle Systems allow for the spawning of meshes with certain rules to create these effects. Two of them in Unity are Shuriken and VFX Graph. </p>
<h2 class="heading-2" id="_idParaDest-222">Shuriken</h2>
<p class="normal">This system is <a id="_idIndexMarker653"/>full of features to help spawn 3D meshes (structural collection of vertices, edges, and faces that define a 3D object). You can create fire <a id="_idIndexMarker654"/>embers, trails, explosions, smoke, and everything else to help sell the experience that has been defined. As you can see in <em class="italic">Figure 9.26</em> there are a lot of options to go over. We will leave the explanation of this to the examples covered in <em class="chapterRef">Chapter 12</em>, <em class="italic">Finishing Touches</em> when we create Shuriken-based effects.</p>
<p class="normal">Some high-level knowledge of Shuriken to know is that it is a Particle System that uses the CPU to direct the particles. </p>
<p class="normal">This limits the amount of particles that can spawn directly on that hardware. </p>
<figure class="mediaobject"><img alt="A picture containing graphical user interface  Description automatically generated" height="710" src="img/B17304_09_26.png" width="825"/></figure>
<p class="packt_figref">Figure 9.26: Shuriken Particle System</p>
<p class="normal">Shuriken is <a id="_idIndexMarker655"/>fantastic for particle systems on the CPU, but if you want to have large <a id="_idIndexMarker656"/>amounts of particles moving around, VFX Graph is the way to go. This makes GPU-driven particles and can handle many thousands of particles at once.</p>
<h2 class="heading-2" id="_idParaDest-223">VFX Graph</h2>
<p class="normal">Firstly, you will <a id="_idIndexMarker657"/>most likely need to install VFX Graph. Open the <strong class="screenText">Package Manager</strong> as you <a id="_idIndexMarker658"/>have done before and find <strong class="screenText">Visual Effects Graph</strong> from within <strong class="screenText">Unity Registry</strong> and install it! After you’ve finished this, you will need to create a VFX Graph system. In your project window, right-click and then select <strong class="screenText">Create</strong> &gt; <strong class="screenText">Visual Effects</strong> &gt; <strong class="screenText">Visual Effects Graph</strong>.</p>
<figure class="mediaobject"><img alt="Graphical user interface  Description automatically generated" height="907" src="img/B17304_09_27.png" width="662"/></figure>
<p class="packt_figref">Figure 9.27: Installing and creating your first VFX Graph system</p>
<p class="normal">Opening VFX Graph will present you with a new window. This window is like the <strong class="screenText">Shader Graph</strong>. You’ll notice there is a <strong class="screenText">Blackboard</strong>, which we can use to create parameters that can be changed at runtime and is exposed in the inspector. </p>
<p class="normal">There is a UI that is unique to the VFX Graph and a few specific terms: <strong class="keyWord">Contexts</strong>, <strong class="keyWord">Blocks</strong>, <strong class="keyWord">Nodes</strong>, and <strong class="keyWord">Variables</strong>. <strong class="keyWord">Contexts</strong> are portions of the system broken down, such as <strong class="screenText">Spawning</strong>, <strong class="screenText">Initialization</strong>, <strong class="screenText">Update</strong>, and <strong class="screenText">Output</strong>. </p>
<p class="normal">Each of these <strong class="keyWord">contexts</strong> has <strong class="keyWord">blocks</strong> inside of them and can be added to the <strong class="keyWord">contexts</strong> by right-clicking. The <strong class="screenText">Spawning</strong> <strong class="keyWord">context</strong> controls how many instances of the system feeds into an <strong class="screenText">Initialize</strong> <strong class="keyWord">context</strong>. <strong class="screenText">Initialize</strong> <strong class="keyWord">context</strong> processes the <strong class="keyWord">Spawn Event</strong> and engages a new particle simulation. <strong class="screenText">Update</strong> <strong class="keyWord">context</strong> takes in the <strong class="screenText">Initialized</strong> particles and executes explicit behaviors upon certain conditions. <strong class="screenText">Output</strong> <strong class="keyWord">contexts</strong> account for the simulation data and render every living particle according to the <strong class="screenText">Output</strong> <strong class="keyWord">context</strong> configuration. However, <strong class="screenText">Output</strong> <strong class="keyWord">context</strong> does not modify simulated data.</p>
<p class="normal">The first context is spawning. This context will allow you to add blocks to affect the spawning logic <a id="_idIndexMarker659"/>of each system. There are some questions that should be considered here: How <a id="_idIndexMarker660"/>many particles should spawn from this system? How quickly should those particles spawn? When are they spawned?</p>
<figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" height="270" src="img/B17304_09_28.png" width="532"/></figure>
<p class="packt_figref">Figure 9.28: Spawn context</p>
<p class="normal">After the spawning is completed, you need to have parameters to initialize them with. These blocks answer these types of questions: Where do the particles spawn? Are they spawned moving or do they have velocity? How long does each particle live for?</p>
<figure class="mediaobject"><img alt="A screenshot of a video game  Description automatically generated with medium confidence" height="442" src="img/B17304_09_29.png" width="532"/></figure>
<p class="packt_figref">Figure 9.29: Initialize context</p>
<p class="normal">Now that <a id="_idIndexMarker661"/>you have them spawning, it may be a good idea to add <a id="_idIndexMarker662"/>some unique behavior to them as they update, or they will be floating sphere gradients. You can expect this to answer the ultimate question: How will the particles change over time?</p>
<figure class="mediaobject"><img alt="Timeline  Description automatically generated" height="136" src="img/B17304_09_30.png" width="467"/></figure>
<p class="packt_figref">Figure 9.30: Update context</p>
<p class="normal">Then finally, as you have an understanding of how many particles there are, where the particles are, and what the particles are doing, you can now decide what they will look like. </p>
<p class="normal">The questions are: Which way are the particles facing? What shader is being used?</p>
<figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" height="639" src="img/B17304_09_31.png" width="526"/></figure>
<p class="packt_figref">Figure 9.31: Output context</p>
<p class="normal">You may <a id="_idIndexMarker663"/>have noticed the left side of the blocks sometimes have circle inputs like <a id="_idIndexMarker664"/>the Shader Graph. If you thought that you could put some input to this, maybe from nodes, then you’re correct! There are nodes that you can work through to get the right data flowing to the blocks that are in each context.</p>
<h3 class="heading-3" id="_idParaDest-224">Nodes</h3>
<p class="normal">As this was <a id="_idIndexMarker665"/>in Shader Graph, the nodes are meant to take data values and manipulate them in a way to get the desired outcome. In VFX Graph’s case the values are meant to be read into one of the blocks as opposed to one of the attributes from the <strong class="keyWord">Master Stack</strong>. For the most part you will utilize the <strong class="keyWord">Operator</strong> nodes and variables that are created in the <strong class="screenText">Blackboard</strong> to complete the complex maths.</p>
<h1 class="heading-1" id="_idParaDest-225">Summary</h1>
<p class="normal">We learned that visual effects have heavy technical implications through two major sources in Unity: shaders and particles. Taking our time through shaders, we built an example of a material on a 3D object to get the concept down so when we have multiple different scenarios, we can follow how the shader was created. This was done through Shader Graph. After that, we dove into the concept of particles. Shuriken was shown to get a simple understanding of CPU particles and will be used in later chapters to explain the finishing touches. GPU particles are created through VFX Graph; we went over the interface and some vocabulary of VFX Graph so when we use it later, there is an understanding to work off. </p>
<p class="normal">Visual effects are a very large topic to master. Mastering these tools takes a long time. Take your time when working through this and fail fast. This will guide you through understanding visual effects.</p>
<p class="normal">The next chapter covers the implementation of sound in the game. Sounds are generally overlooked until nearly the end of games, but they are integral to ensuring the product has captivating emotional tie-ins with the environment and the character. We will go over implementations, some sound design, and other sound-focused knowledge points in the next chapter.</p>
</div>
</div></body></html>