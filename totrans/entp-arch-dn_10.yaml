- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Master Data Management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we showed you a method to design information entities
    in such a way that they do not have any technical coupling, in an effort for the
    information system containing them to be free to evolve when the business changes.
    If the data model is a pure reflection of the business represented, it makes it
    much easier to follow business changes (and change is the only constant) because
    there won’t be some technical constraint in our way forcing us to compromise on
    the quality of the design, and thus on the performance of the system as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will start talking about the implementation of the data
    model into something concrete (if this can be said about software, which is mostly
    virtual). It is only in *Chapters 16 to 19* that we will code what we will call
    for the rest of the book the “*data referential(s)*”. For now, we will start some
    actual software architecture to welcome the data model, persist the associated
    entities, and so on. There are many responsibilities in the data referential,
    and the discipline of handling these essential resources in an information system
    is called **Master Data Management** (**MDM**). At first sight, these responsibilities
    might look like those you would trust a database with, or even find in a resource-based
    API. But this chapter should convince you that there are many more things to the
    model that justify this use of a neologism like “data referential”.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to defining the functions of the data referential, MDM is about
    choosing the right architecture, defining the streams of data in the overall information
    system, and even putting in place governance of the data, which involves finding
    who is responsible for what action on the data in order to keep the system in
    shape. Having clean and available master data may be the single most important
    factor in the quality of the system. Reporting cannot be done without clean data
    and most business processes depend on the availability of the data referential.
    Also, some regulatory reasons, such as accounting or compliance questions, demand
    high-quality data.
  prefs: []
  type: TYPE_NORMAL
- en: After showing the different types of data referential that you may encounter
    – or create – in an information system, we will finish this chapter with an overview
    of possible issues with data, patterns of use, possible evolution of data in time,
    and some other general topics that will hopefully provide you with up-to-date
    knowledge about the MDM architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Responsibilities around the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concept of the data referential as a unique point of truth for the data
    entities of a given domain has already been explained globally, but we have not
    formally described what functional responsibilities are contained in it. This
    section will explain each of the main responsibilities and features of the referential.
    Looking at the responsibilities explained in the following subsections, you might
    ask why we’re talking about the data referential instead of simply using the better-known
    expression of a database, but we will see in the second part of this chapter that
    a referential is much more than this.
  prefs: []
  type: TYPE_NORMAL
- en: Persistence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Persistence** is the responsibility that immediately comes to mind when we
    talk about managing data. After all, when we trust an information system’s data,
    the very first demand we have is that the computer does not forget it once it
    has learned about it. This demand is crucial, as even an electricity failure should
    not have an impact on it. This is why databases were invented and why engineers
    went such a long way to ensure the safe travel of data between memory and hard
    disks, both ways.'
  prefs: []
  type: TYPE_NORMAL
- en: Persistence may be often reduced to **CRUD** (which stands for **Create, Read,
    Update, Delete** – the four main operations on data), but this concept is way
    too limited compared to the features encompassed by the data referential, though
    it is enough for most of the standard uses of low-importance data in the information
    system. Since we talk here about primary data used in many places in the information
    system, some other aspects of persistence have to be taken into account. The first
    one was talked about at length in [*Chapter 4*](B21293_04.xhtml#_idTextAnchor121)
    – namely, time. When one includes time in the MDM equation, storing the so-called
    “current” state of the data (which, most of the time, is only the last-known or
    best-known state of the corresponding business reality) suddenly becomes much
    more complicated and means, at least, storing the different states of data over
    time, with an indication of time to follow the history of these successive states.
  prefs: []
  type: TYPE_NORMAL
- en: As we explained in [*Chapter 5*](B21293_05.xhtml#_idTextAnchor164), a good MDM
    is a “know it all” system that, instead of states, should store the actual commands
    modifying the data to enable us to retrace why the state of such an entity has
    evolved. This means that what will be written in the database is not a state with
    a date but, ideally, a “delta” command causing a change from one state to another
    – for example, modifying the zip code in the first address of an author in our
    sample information system. This way, not only can we reconstitute the state of
    a business entity at any time in its life cycle but we also avoid the complexity
    of optimistic/pessimistic locks, transactions, data reconciliation, compensation,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Metadata is also an important addition to the simple CRUD approach. Indeed,
    it is of great importance in the manipulation of master data to be able to retrieve
    and manipulate information linked to the data changes – for example, its author,
    the IP of the machine where the command came from, the identifier of the interaction
    that has caused this change, the actual date of the interaction, maybe also a
    value date if it has been stipulated by the author, and so on. This allows for
    traceability, which becomes more and more important for the main business entities
    in an information system. It also provides powerful insights into the data itself.
    Being able to analyze the history of the data will help you fight fraud (for example,
    by checking which entity changes its bank coordinates often, or limiting how many
    representatives of a given company can change in a given period of time). It can
    also help with some regulatory questions that are becoming more and more common,
    as we will see a bit later when talking about data deletion.
  prefs: []
  type: TYPE_NORMAL
- en: When talking about persistence, we often think of a given entity (and I, for
    one, have only been given examples of such atomic manipulations previously mentioned),
    but the ability to manipulate masses of data is also an important responsibility
    of the data referential. In most cases, this translates into being able to perform
    actions in batches, but the consequences are also in terms of performance management
    and the capacity to handle referential-wide transactions (which are very different
    from business entity-centered translations, which the data referential should
    help eliminate).
  prefs: []
  type: TYPE_NORMAL
- en: The question of identifiers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As soon as a business entity unit is created, the question of how to identify
    it arises, since persistence is the capability of retrieving data that the information
    system has been given, and this naturally means that a deterministic way must
    exist to point at this given entity. At the very least, a system-wide identifier
    should exist to do so. It can take a lot of forms but, for the sake of applicability,
    we will consider the following as a URI, for example, `https://demoeditor.com/authors/202312-007`
    or `urn:com:demoeditor:library:books:978-2409002205`. This kind of identifier
    is supposed to be understood globally, by any module participating in the information
    system. It acts a bit as the ubiquitous language in Domain-Driven Design but allows
    pointing at a given entity instead of defining a business concept.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, local identifiers may exist. For example, the book pointed at by
    `urn:com:demo` **editor:library:books:978-2409002205** could be stored in a MongoDB
    database where its technical `ObjectID` would be `22b2e840-27ed-4315-bb33-dff8e95f1709`.
    This kind of identifier is local to the module it belongs to. Thus, it is generally
    a bad idea to make it known by other modules, as a change in the implementation
    could alter the link and make it impossible for them to retrieve the entity they
    were pointing at.
  prefs: []
  type: TYPE_NORMAL
- en: An entity can also have business identifiers that are not local per se but bear
    no guarantee of being understood anywhere in the information system. The book
    generally identified by `urn:com:demoeditor:library:books:978-2409002205` could
    be retrieved only by its 13-digit ISBN `978-2409002205`; in fact, it is the variable
    part of the unique system identifier. However, other identifiers exist. For example,
    the same book can also be retrieved by its 10-digit ISBN, which is `240900220X`.
    Business identifiers can also be created inside the information system for particular
    uses. In our sample edition company, one could imagine that a serial number is
    applied to a book to keep track at the printing station, where batches are used
    and a single integer might be easier to handle than a full-blown ISBN, without
    risking any confusion as the workshop only prints books of the sample editor.
  prefs: []
  type: TYPE_NORMAL
- en: Additional technical identifiers are more often encountered, particularly in
    information systems with legacy software applications. Indeed, those generally
    insist on having their own identifiers. This way, the accounting system of *DemoEditor*
    might know the `urn:com:demoeditor:` **library:books:978-2409002205** book by
    its local identifier, `BK4648`. The ERP system might have a technical identifier
    of `00000786` if the book is the 786th product that has been entered into it.
    And so on. Of course, the dream would be that all software applications are modern
    and can handle an externally-provided, HTTP-standards-aligned URN. But this is
    rarely the case and even modern web applications seem to forget that interoperating
    with other applications means using the URL that they provide indiscriminately.
  prefs: []
  type: TYPE_NORMAL
- en: To provide a good service and account for this reality of information systems,
    the data referential should provide the capacity to store the business identifiers
    for the other software modules participating in the system. At the very least,
    this should be a dictionary of identifiers associated with an entity, with each
    value pointed at by a key that globally identifies the module in the system. For
    example, `urn:com:demoeditor:accounting` could be the key that points to `BK4648`
    and `urn:com:demoeditor:erp` could point to `00000786`. When defining the keys,
    there is a natural tendency to use the name of the specific software used to implement
    the function, and it would not matter much because the identifier is indeed specific
    to this software. But it still remains a good idea to stay generic in order to
    prepare for any cases. To give just an example, in the fusion of two administrative
    regions in France, it proved very useful to have such a separation. The two existing
    software applications for finance management were competing to have a unique market
    after the merger. It happened that one of the software applications was more customizable
    than the other and could handle external identifiers, which was part of the decision
    to keep it as the new unique finance management application. However, since the
    identifiers used by the abandoned software were prefixed by a vendor mark and
    the key for the software that stayed was not generic but used its name, there
    were some strange associations of identifiers such as `urn:fr:region:VENDOR1=VENDOR2-KEY`.
    Since the two brands were well-known competing companies in France and the merger
    of the two administrative regions caused lots of team modification and change
    management, this additional confusion quickly became an irritant, with people
    not even able to tell which software they should use to manipulate financial data.
    In the end, switching to a generic key such as `urn:fr:region:FINANCE` really
    helped, even if this sounded like a little technical move.
  prefs: []
  type: TYPE_NORMAL
- en: I will finish this review of identifiers with a very special case, which is
    the change of business identifier. Identifiers are, by essence, stable since they
    should be a deterministic way to point at an entity in the information system.
    A documented case of a change of global identifier is when a social security number
    is designated to a person who has not been born yet, typically because surgery
    is necessary on a fetus. As the first digit in French social security numbers
    uses the ISO gender equality standard to specify the gender of the owner, it may
    happen that instead of using 1 (for male) or 2 (for female), a social security
    number starts with 0 (for unknown). The identifier is then changed to a new one
    after the birth of the individual since the first number is then known (or maybe
    unknown in some other conditions – in this case, the norm specifies the number
    should be 9 for an undetermined gender). This is admittedly a very special case
    that provokes the change of the global, system-wide identifier. However, the architecture
    of the system has to be able to handle *any* existing business case (which does
    not mean there cannot be some manual adjustment for these cases) to be considered
    “aligned.”
  prefs: []
  type: TYPE_NORMAL
- en: The single entity reading responsibility
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Persistence really means nothing if data stored somewhere cannot be retrieved
    afterward for subsequent use. This is why reading data is the second responsibility
    of the data referential that we will study. This section details the different
    kinds of read operations and, contrarily to persisting the data, they are actually
    very diverse in their forms.
  prefs: []
  type: TYPE_NORMAL
- en: The first reading act we naturally think of is the retrieval of a unique entity,
    directly using its identifier. In API terms, this sums up as calling a `GET` operation
    on the URL that has been sent back in the response under the `Location` header
    when creating the entity. Or at least, this sends the latest known state of the
    data because parameters can be added to specify which time version of the data
    should be retrieved. This normally raises the question of how to get the state
    of data since we said we would store changes, not states. The response there can
    be simple or complex, depending on the level of detail we go into. If we radically
    apply the “*premature optimization is the root of all evil*” principle made popular
    by Donald Knuth, then it is enough to specify that states can be deduced from
    changes by applying them to the previous state and consider this recursion uses
    the initial state of the date, which is an empty set of attributes designated
    by a unique identifier.
  prefs: []
  type: TYPE_NORMAL
- en: I know very well that most technically minded people (and thus at least 99%
    of you reading this book) will always think a step further and ponder the huge
    performance problem the data referential would have to deal with if each `GET`
    operation caused the iterative application of hundreds of patches to an entity
    in order to find its state at some point of its life cycle. The very least we
    would do would be to cache the calculated states to improve on this. But when
    you think about it, the vast majority of read operations ask for the best-so-far
    state of the entity, which is the latest known state. So, to improve storage while
    still keeping good performance, caching the last known state of the entities is
    the right choice.
  prefs: []
  type: TYPE_NORMAL
- en: But there are, of course, some exceptions and, as has been many times explained
    in this book, business-justified exceptions have to be taken into account – not
    only because it is the goal of the alignment but mostly because these exceptions
    are generally great challenges on the data model and if it can accommodate them
    while staying simple, it means this design is mature and has a much greater chance
    of correctness and, hence, stability. One such exception can be when the data
    is often read using a `date` parameter value. In this case, improving the performance
    might mean storing all calculated states, but this uses lots of storage and wastes
    most of it, as not all states will be called in time. A good compromise might
    be to store only a state calculated every 20, 50, or 100 changes. This way, we
    can start from an existing state all the time and quickly calculate the specified
    state because we need only apply a few limited patches to the data. Depending
    on business constraints, some states that are more often used than others can
    be the milestones that are kept in the cache. For example, in financial applications,
    it is generally interesting to keep the value just before and just after the change
    of fiscal year.
  prefs: []
  type: TYPE_NORMAL
- en: Another detail that has got to be taken into account is the optional possibility
    of inserting modifications in the life cycle of the entity. I understand how this
    may sound weird to “rewrite the history” and insert changes with potential impacts
    on the following ones, but there are some cases where this makes sense. For example,
    I have seen this happen in accounting systems when errors have been made and calculation
    rules were reapplied to find the correct result, inserting correcting operations
    at the time the initial error arose. Again, this is a rare case and it should
    be conditioned by strict authorization rules, but the situation has to be cited
    for the sake of exhaustivity.
  prefs: []
  type: TYPE_NORMAL
- en: Other kinds of reading responsibilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are cases when the unique system-wide identifier of a business entity
    is not known or has been forgotten (which means not stored outside its original
    referential) and, in this case, the responsibility of searching the entities corresponding
    to given criteria has to be used. This responsibility is often called **querying
    data**. Based on the criteria specified in the request, the operation will return
    a set of results, which can be an empty set or one that contains corresponding
    data. There can be cases when the query attributes are such that the results will
    always contain zero or one entity – for example, because the constraint used is
    a unique business identifier. But there can also be cases where the results are
    particularly numerous, and an additional responsibility called **pagination**
    will be quite useful to reduce bandwidth consumption.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pagination can be active (the client specifies which page of data they want)
    but also passive (the server restricts the amount of data and provides a means
    for the client to request the next page of data). A standard way to implement
    the first approach is to use the `$skip` and `$top` attributes, as specified in
    the `$filter` attribute, which is used to specify the constraints reducing the
    query results that have been cited previously, when talking about performance
    in retrieving data. This book is not the place to explain the richness of this
    standard, which is sadly not used as often as it should be. Most API implementers
    indeed chose to use their attribute names, without realizing that they recreate
    functions (such as pagination offset, for example) that have been done so many
    times that they are completely normalized. Lack of interest in standards, and
    a “not invented here” syndrome that many developers suffer, are dragging our whole
    industry back. But enough ranting about this: a complete chapter has been dedicated
    to the importance of norms and standards, so we will just close the subject by
    taking you to the study of the OData standard, or in this case, the GraphQL syntax
    as well, since these two approaches can be seen as competing (though they are
    complementary one to the other, and a great API exposes both protocols).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another type of reading responsibility is reporting: this can sometimes be
    implemented directly by the data referential but this is quite rare, as reporting
    is often done by crossing data coming from several business domains. Even if there
    are only a few of the reporting needs that demand such an external, shared-responsibility
    implementation, then it is better to handle all data for reporting to this entity.
    Depending on the technology you use, this may be a data warehouse, an OLAP cube,
    a data lake, or any other application. Again, the implementation does not really
    matter: as long as you are clean on the interfaces, you may change them any time
    you like with limited impact on the system.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of reporting, these interfaces can be solicited with different
    time-based approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: A synchronous, on-demand, call is always possible but generally not used for
    performance reasons, at least in complex reports (this is the “pull” mode). Indeed,
    if the reporting system needs to wait for all sources to answer and then only
    calculates the aggregations on its side, the results are, of course, as fresh
    as possible, but they may take minutes to come and this is often not acceptable
    by users.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The asynchronous, regular read of data, is the most commonly used pattern. Here,
    data is collected at a given frequency (once a day or more often, sometimes down
    to once an hour), generally by an ETL, and sent to the data warehousing system
    where it is aggregated and prepared for reporting. This way, reports are sent
    to users quicker (sometimes, they are even produced and made available directly
    upon data retrieval). The counterpart is that the data is not as fresh as possible,
    and moving the cursor to a quicker sending of data increases the consumption of
    resources. Optimizations are possible – for example, by reducing the transfer
    to only new or updated data – but this only goes some way into improving the minimal
    time needed to update the whole data warehouse. The greatest technical drawback
    of this approach is that most of the calculations are reproduced even if the source
    data has not changed, which is a waste of resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To go on further in the “push” approach, it is possible to use webhooks to register
    data refresh to an event of source data change. This way, the calculations are
    reproduced only when the data has changed, and the moment is as close as possible
    to the interaction that has changed the data, which means the reports are very
    fresh most of the time. Dealing with large amounts of events is a challenge, but
    grouping the changes into minimum packages (or with a maximum freshness time constraint)
    can help.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A very modern but technically demanding approach is to mix these “push” and
    “calculate on demand” strategies by using a system with queues of messages containing
    data changes and a dedicated architecture to apply fine-grained computations on
    each of these messages as needed. Such implementations of a big data approach
    include Kafka architectures or Apache Spark clusters. The goal here is not to
    detail these approaches but just to explain that they will collect all events
    at the source and then smartly calculate the consequences in aggregated data (the
    smartness being in the fact that they know the consequences and calculate only
    what is needed and they can balance these computations on many machines of a cluster
    and grouping the result in the end). They can even go as far as producing the
    final reports on aggregated data and making them available to end users, achieving
    a complete “push” paradigm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These four approaches are symbolically represented in the following schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Modes of reporting](img/Figure_10.1_B21293.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Modes of reporting
  prefs: []
  type: TYPE_NORMAL
- en: To be exhaustive on these additional reading responsibilities, indexing is another
    function that is used to accelerate data (and some simple aggregates) reading.
    It does not go as far in data transformation as big data and the preceding reporting
    approaches, but can already prepare a few aggregates (such as sums, local joins,
    and so on) and make them available through simple protocols as raw data. Indexing
    engines such as SOLR or Elasticsearch are generally used to accompany the data
    referential on the speed of data retrieval. In this case, the data referential
    itself concentrates on data consistency and validation rules and then handles
    reference data to the indexing system to make it available in quick-read operations.
  prefs: []
  type: TYPE_NORMAL
- en: The complex art of deleting data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If deltas are stored instead of states, there is not much difference between
    `POST`, `PUT`, and `PATCH` operations on a resource, as they all translate into
    a change in the state of the entity, the particular case of a resource creation
    being a change from something completely empty. But as far as the `DELETE` operation
    is concerned, we are in a different situation. Indeed, we could blindly apply
    the same principle and consider that `DELETE` removes all attributes of an entity
    and brings it back to its initial state, but that would not be exactly true as
    the entity still keeps an identifier (otherwise, one would not be able to delete
    it). This means it is not in the same state as when it did not exist and there
    is no way to go back to this situation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The best way to handle the situation is generally to use a particular attribute
    of the date stating that it is not active anymore. When using a `status` attribute
    to keep the value of the calculated position in the life cycle of an entity, this
    attribute may be used with a value such as `archived` to realize a similar operation.
    This is the way the data referential can store the fact that the data has been
    deleted without actually suppressing data (which is incompatible with what has
    been said previously about the data referential and its responsibility for history
    persistence). Of course, this creates a bit of complexity in the referential because
    it has to take this into account in every operation it allows. For example, reading
    some piece of data that is inactive should react as if the data did not exist
    (with a `404` result, in the case of API access), unless in the exceptional case
    that the user accessing the referential has an `archive` role and can read erased
    data. Other questions naturally arise then, such as the possibility of reactivating
    data and continuing its life cycle (hint: it is generally a bad idea, as many
    business rules are not thought to handle this very peculiar case).'
  prefs: []
  type: TYPE_NORMAL
- en: But let’s stop this digression here and come back to the initial idea of data
    conservation even after a deletion command has been issued. The functional rationale
    behind this is mainly regulatory, such as traceability, but also prohibiting data
    erasing for other purposes, such as forensics after a cyber-attack. An interesting
    fact is that some regulations also exist that specify when data should indeed
    be erased for real (and not simply rendered inactive). For example, the European
    GDPR states that personal data should not be kept longer than some legally defined
    periods, depending on the processes they are associated with. In the case of personal
    data collected for marketing reasons (with the consent of the user, of course),
    the delay is generally a year. After this time, without renewal of storage consent,
    the data shall be erased from the information system that has collected it. That
    means the actual removal of the data everywhere it may be (which includes backups).
  prefs: []
  type: TYPE_NORMAL
- en: Relation to specialized links
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As always, the devil lies in details, and links can become a problem when dealing
    with data. Imagine we use a link between a book and an `author` entity. The simplest
    expression of such an RFC link is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The links are often inherited from specialized links – in our case, a specialized
    author link that could contain additional important information in its schema,
    for example, extract restricted to the portion of JSON that has changed, for readability
    purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Having additional information in links is useful when you know this is a piece
    of information that will frequently be used when manipulating the link, as it
    avoids an additional roundtrip to the other API to find this information. Of course,
    there should be a right balance, and including the phone number here is questionable
    because it can be considered volatile data, not changing frequently but on some
    specific occasions in the mass of authors of an editor’s database. The consequence
    is that all links should – in this case – be updated, which accounts for quite
    a large amount of work. When you know it is a piece of data that does not change
    (the author’s name, for example, does not change very often) or even data that
    should never be changed for regulatory reasons (an approved version, for example,
    should not be modified even if further versions appear), there is no such problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the first issue that should be taken care of with links. The second
    one is more subtle: since the `title` attribute (which is not an extended one
    added through inheritance but exists in the standard RFC link definition) has
    been used to store the common designation of the author, as expected from the
    definition of this attribute in the RFC, deleting an author will end up with their
    name still existing in the book’s data referential through these links. This may
    be interesting for archiving reasons (even if we do not deal with this author
    anymore, for example, even though they have died, the books are still in their
    name). However, in some other regulatory contexts, this can be a tough problem:
    if we go back to the example of the European GDPR “right to be forgotten” for
    personal data, that means that when the author is deleted from the database, we
    should also go over all the books they authored and replace the `title` contents
    with something like `N/A (GDPR)`. This is how `DELETE` operations can work under
    specific functional circumstances!'
  prefs: []
  type: TYPE_NORMAL
- en: So-called secondary features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Though we might think we have covered all the responsibilities of the data referential
    since we passed on the four letters of the CRUD acronym, the spectrum of a good
    application is much larger. To be thorough, we should talk about all the functions
    that are generally called “secondary,” even though they are critical and – in
    some cases – equally important as the persistence of the data itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first one of these additional features is **security**. There should be
    no doubt about the importance of this one anymore, but if it is necessary to convince
    anyone, let’s just stress the fact that the four criteria commonly used in security
    categorization are all about data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Availability**: The data should be available to authorized persons, which
    means denial of service (among others) has to be treated. Though unavailable data
    is a good way to prevent leakage or unauthorized access, it remains the primary
    criterion, as the whole idea is to provide a service in a solid way. Availability
    also means that a simple mishap should not get the whole system offline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integrity**: The data should not be tampered with by anyone and its correctness
    should be guaranteed – the consequence being that all functions underlying the
    service have to be secured as well (database, network, source code, etc.).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Confidentiality**: This is the counterpart of the first criterion, as access
    should be forbidden to non-authorized requesters. It is the basis for authorization
    management systems (more on this in the next chapter).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Traceability**: This criterion is a more recent one but becomes more and
    more important with regulations on IT systems; it states that the modification
    and use of data should be stored in a log that cannot be tampered with, allowing
    it to retrieve what happened back in time. Traceability is most important after
    an attack has happened to understand where the vulnerability was and what the
    attackers have done.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance** and **robustness** are also so-called secondary features that
    have a high importance in MDM. They are very much linked to the first criterion
    (availability). Indeed, the robustness of the software underpins its capacity
    to answer requests in time with great confidence, and performance is a quality
    associated with the availability of the data. After all, if someone gets a response
    to their request for data after 5 minutes, they would not think of the service
    as being available, though it could be qualified as such since the data indeed
    arrived… at some point. Rapid availability of data has often been a drive to move
    existing “manual” information systems to a software-oriented approach.'
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with these features is the subject of many books, so we will just leave
    it there for now, since those are indeed responsibilities expected from the data
    referential.
  prefs: []
  type: TYPE_NORMAL
- en: Metadata and special kinds of data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, the data referential should handle data and also metadata. Metadata
    is all the information that sits around the data entities themselves and allows
    for a good comprehension of them. This provides some additional richness to the
    data, but please be aware that metadata should have a different life cycle from
    the data itself. For example, storing information about the history of data is
    not metadata, though it can abide by the definition of metadata just given. As
    has been exposed many times now, the data referential keeps track of every change
    in the entities it hosts. So, information about who changed what at what times
    is data and not metadata for a complete and correct data referential. In the same
    way, dates of changes, indicators of modification, or reading frequency can be
    directly deduced from the series of operations in the data referential, so they
    are not metadata either.
  prefs: []
  type: TYPE_NORMAL
- en: A good example of metadata is the units associated with numeric data. Having
    a number in a named attribute of the entity is often not enough. Sure, the attribute
    can have a name that describes its content and also the units (examples would
    be `populationInMillions`, `lengthInMillimeters`, or `nbDaysBackupRotation`),
    but that does not make it any easier to manipulate the values and, in addition,
    that makes for longer names, which can be a bit cumbersome when the unit sounds
    obvious. Having metadata somewhere in the schema of the referential that states
    that *this* attribute of *this* entity uses *this* unit is a better way to communicate
    the handling of the data, and can also help in some modern database engines to
    directly calculate formulas between attributes that are not on the same scale
    of units, and even provide some warning when the formulas are not safe in terms
    of units definition, such as adding meters and seconds. These new servers generally
    use a standard definition of units that includes the powers in `kN` unit is associated
    with M1K1S-2 as seen previously, but with a multiplier of 10^3 and the name `kiloNewton`.
  prefs: []
  type: TYPE_NORMAL
- en: Geographical attributes are another good example of metadata addition to the
    usual data in a database. Generally, longitude and latitude were expressed as
    double-precision numbers in `lon` and `lat` attributes, but this did not account
    for the kind of world-map projection (which can create some discrepancies in the
    number) and would not prevent silly computations such as adding the two numbers.
    With database or geographical servers able to understand the metadata added to
    the coordinates data, it is now possible to calculate distance, transpose coordinates
    from one projection system to another, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Metadata is the long-forgotten cousin of data. Apart from CMIS, the standard
    for electronic document management systems, where they enjoy first-order citizenship
    (supporting groups of metadata implemented in schemas that can be applied to the
    documents, used in the queries when searching, and sometimes even versioned independently
    of the documents supporting them), there are not that many standards that formalize
    them. The evolution of this depends entirely on engineers who are interested in
    doing their jobs in a professional and clean manner. As long as “quick and dirty”
    tricks are used in the software programming and the structuring of information
    systems, metadata will continue to be set aside. When people – hopefully after
    reading this book and some others advising in the same quality and long-term approach
    – decide that the burden of the coupling is too high and they have to address
    the problem by modernizing their information system, metadata use should naturally
    rise, making it in time as standard and usual as any other practice.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how the data referential should be defined, we will dive into
    how this can be provided by a software system.
  prefs: []
  type: TYPE_NORMAL
- en: The different kinds of data referential applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will not talk about technical aspects in this section (this is the role of
    the following one, called *Some architectural choices*) but about architectural
    strategies to structure the data persistence.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, the metaphor of the flower was introduced to show how
    data can be organized inside an entity. We will follow this idea to represent
    how persistence can be implemented in the data referential that manages instances
    of such an entity. Before we dive into the main architectures, please remember
    that the main criteria of choice should always remain functional, which, in the
    case of data, means that the life cycle in your system is what will drive you
    principally into this or that architectural choice. Also keep in mind that the
    *people* aspect of data management is as important as the *technical* aspect;
    governance, designation of people responsible, and good communication about which
    team owns which pieces of data are essential to the correct use of data in your
    organization.
  prefs: []
  type: TYPE_NORMAL
- en: Centralized architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The centralized (or “unique”) referential is the simplest one (as shown in
    *Figure 10**.2*) that everybody first thinks of and that solves so many problems
    in the information system when it can be applied: it consists of having a single
    storage mechanism for every bit of data concerning a given type of entity (including,
    of course, history, metadata, and so on). This way, all services working in the
    system know that, when needing to read or write something, they have to address
    their request to a single repository service, as the whole “flower” is in one
    well-known place.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Centralized data referential architecture](img/Figure_10.2_B21293.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – Centralized data referential architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'The great thing about this approach is that it simplifies the work for everyone
    in the information system. Of course, this constitutes a **single point of failure**
    (**SPOF**) and, if the implementing application is down, all applications needing
    this referential information will be impacted. But this is just a technical problem,
    with many battle-proven solutions such as active/active synchronization of the
    database, scaling of the application server, redundancy of hardware, and so on.
    By now, you should also be convinced that the functional aspects are always more
    important to take into account than the technical ones. As technicians, we tend
    to focus on low-occurrence problems such as hardware failure or a locked transaction,
    whereas the immensely greater problems in information systems nowadays are duplicates
    of data, poor cleanliness of the inputs, and other commonly observed issues that
    urgently need to be addressed. The SPOF might be more important in the people
    organization: a centralized data referential might mean that a single team or
    even a single person is in charge of the management of this set of data, and some
    drawbacks are always possible with too much centralization (feedback not taken
    into account, the relative obscurity of the changes, etc.).'
  prefs: []
  type: TYPE_NORMAL
- en: Clone architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One way to address this SPOF limitation is to locally copy some of the data
    that is needed by important applications. In this case, some applications will
    keep part of the “flower” in their own persistence system, and it is their choice
    to manage how fresh the data should be compared to the central referential, which
    remains the global single version of the truth.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the data is initially scattered around an information system, it can be
    a first step toward cleaning it, by obeying centralized business rules while still
    keeping the data as it was stored. The advantage is that, for legacy applications,
    nothing changes: they still consume the data locally, so all reading functions
    work as before. With some effort, writing commands can even be kept in the software
    – for example, by using database triggers that will implement a return of data
    to the unique referential. Most of the time, though, and particularly if the application
    is composable and has a unique graphical interface to create entities, it is easier
    to plug the referential GUI into this application instead of the legacy form.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main difficulty with this approach is consistency: as there are several
    copies of data in the system, discrepancies can happen and it is thus important
    to keep them as reduced in time and impact as possible. If applications are well
    separated in function silos, it can end up being very easy, but if the way the
    application has been decomposed is bad, then you may have to implement distributed
    transactions, which can be quite complicated. Eventual consistency will be your
    friend in this situation, but it may not be applicable everywhere.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The most efficient form of the clone architecture is the following one, where
    the cloning of the data (only part of the flower, as only a partial set of the
    petals are normally useful) is synchronously based on events in the data referential
    and the data modification GUI has been replaced by the one coming from the centralized
    data-managing application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Cloned data referential, the most efficient form](img/Figure_10.3_B21293.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – Cloned data referential, the most efficient form
  prefs: []
  type: TYPE_NORMAL
- en: An option in this form is to add a synchronization mechanism for all the data,
    which compensates at night for messages of data change that could be skipped during
    the day due to network micro-failures or such low-frequency but still existing
    incidents if one does not want to put a full-blown **message-oriented middleware**
    (**MOM**) to work for this simple stream.
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative to the first form is when the synchronization connector uses
    an asynchronous, typically time-based mechanism to keep the clone database similar
    to the referential information. The best approach in this case is to call the
    data referential APIs, as they give the best quality of information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Cloned data referential, with asynchronous alternative](img/Figure_10.4_B21293.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – Cloned data referential, with asynchronous alternative
  prefs: []
  type: TYPE_NORMAL
- en: An often-seen alternative (but I really do not recommend it) is to have an ETL
    perform the synchronization, as shown in *Figure 10**.5*. This is often seen in
    companies that have invested lots of money in ETL to keep data in sync with their
    system and use this tool for everything. When there is an API (and every good
    data referential should have one), it is better to not couple ourselves directly
    on the data. Sadly, lots of companies still have this kind of stream in place,
    starting their own “spaghetti dish” of an information system, with all responsibilities
    and streams of data entangled and not clearly defined (see [*Chapter 1*](B21293_01.xhtml#_idTextAnchor014)
    for more explanation on this).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Cloned data referential, using an ETL (not recommended)](img/Figure_10.5_B21293.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – Cloned data referential, using an ETL (not recommended)
  prefs: []
  type: TYPE_NORMAL
- en: 'As explained previously, some implementations cannot be changed and have to
    rely on their legacy GUI. In this case, the only possible approach is to rely
    on specific triggers on the database to get the creation and modification commands
    and send them as requests to the MDM application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – Cloned data referential, with legacy GUI still in place](img/Figure_10.6_B21293.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 – Cloned data referential, with legacy GUI still in place
  prefs: []
  type: TYPE_NORMAL
- en: The difficulty in this approach is when the data changes in the data referential
    due to some business rules, as the change cannot be sent back to the GUI. Indeed,
    most applications will keep the state of the data when they have submitted the
    change to their server. Even for the rare applications that listen to the returned
    data by their back office, the difficulty is that the complete roundtrip will
    not be finished before this reading, and the “updated” data will only be the latest
    in the local database, but not the latest that will come back moments later from
    the webhook callback. When stuck in this situation, it is best to explain to the
    users that this is a temporary situation before reaching the centralized referential
    architecture and that they can refresh their GUI a bit later to see the effects
    of their change. Even better, learn how to use the new centralized referential,
    which will always give them the freshest information, at the price of using two
    graphical interfaces instead of one (which is not such a high price when those
    are web applications that can be opened in two browser tabs).
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 8*](B21293_08.xhtml#_idTextAnchor271), we briefly talked about
    enterprise integration patterns. They are the ideal bricks to construct the synchronization
    connectors that we talked about previously, particularly if a **message-oriented
    middleware** (**MOM**) solution is put in place during the project of information
    system reorganization/data referential structuring.
  prefs: []
  type: TYPE_NORMAL
- en: Consolidated and distributed architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This type of referential consists of exposing, from a central point of view
    (an API, generally), data that is actually placed into different parts of the
    information system. Generally, the core of the flower and some petals are in the
    data referential dedicated persistence. But for other petals, persistence can
    stay in the business applications they are associated with because it is considered
    they know the content of these petals better. In the most collaborative form of
    this approach, the referential exposes the full data for every actor of the information
    system and shares ownership of the petals:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – Consolidated referential architecture](img/Figure_10.7_B21293.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 – Consolidated referential architecture
  prefs: []
  type: TYPE_NORMAL
- en: The data referential can produce an entire flower of data through, and expose
    it in, its API but that means it has to consume the different petals it does not
    own from the business applications (keeping a local cache of these petals is a
    choice of implementation based on freshness, rate of change, and performance but
    does not change the ownership of the data). To expose the whole flower with fresh
    content, the data referential needs to have access to its own database, and also
    to the business applications data (or, again, the cache it may keep locally).
  prefs: []
  type: TYPE_NORMAL
- en: Also, some applications, such as `App2` in *Figure 10**.7*, may not need anything
    other than the petal they own (notice that, of course, everyone has the core of
    the flower, by definition). Some applications, such as `App1`, may need some additional
    petals, and in this case, they have to call the data referential API to obtain
    this data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another difference has been made in *Figure 10**.7* to show that the data referential
    may use a business application API to obtain the data (best case) or may resort
    to direct access to the database of the business application, which causes more
    coupling but is sometimes the only way to go. The alternative shown on the right
    is dangerous and should not be applied: in this case, `App3` is not talked to
    but this is not the main problem. The actual issue is that using an ETL to feed
    the referential database should never be done, as this shortcuts the business
    and validation rules inside the data referential. No application should ever touch
    the referential database but the referential application itself. In fact, this
    rule is so important that, when deploying on-premises, it is a good practice to
    hide, obfuscate, deny access, or use any other possible way to prevent anyone
    from directly accessing a referential database. The results are already bad enough
    when this is a “normal” database, with its trail of coupling and other bad consequences;
    doing so on such an important database is the recipe for problems.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When the data referential exposes all the data possible on an entity (the complete
    “flower”), the architecture is also called “consolidated.” It is possible, in
    some cases, that some bits of data are only useful by the owner application and
    will not be of any use to anyone else. In this case, the term “consolidated” is
    not appropriate as some data is – willingly – not available, and the referential
    should be considered “distributed” only. Such a situation would be schematized
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – Distributed referential architecture](img/Figure_10.8_B21293.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 – Distributed referential architecture
  prefs: []
  type: TYPE_NORMAL
- en: The main difficulty of a distributed referential architecture is to maintain
    performance. Optimizations are of course possible, such as the cache mechanism
    we talked about or the parallelism of calls to the different business applications
    when no cache is used, but all of these technical additions come with a price
    that should not be underestimated, particularly when we know that the situation
    is temporary and that the goal is a centralized architecture. It often happens
    that a “temporary” situation, supposedly cheaper and made as a stepping stone
    to the next architecture, actually costs as much as directly putting in place
    the target architecture. Most of the time, the decision comes from the fact that
    the difficulties of the target vision are well known, but the ones associated
    with the intermediate step are less envisioned, mostly because these unstable
    situations are numerous and thus not as well documented as the final architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Let me give you an example of how hard it can be to set up an intermediate distributed
    system, by talking about the pagination of data. When calling the data referential
    API with a `$top=10` query attribute, if the referential is distributed and consolidates
    data from two business applications, it will have to make two requests to the
    application, but the limiting thing is that, depending on the order of the data
    requested by the `$order` attribute, there may be zero data coming from one source
    and 10 pieces from the other one, or the other way around, or any situation between
    these two extremes. This means that the gateway in charge of merging the data
    will have to take 10 lines from one application and 10 lines from the other, then
    re-apply an ordering algorithm on the 20 lines, finally sending the first 10 to
    the requesting client and discarding the following 10 lines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Do not think it would be easier to use a local cache, as you would have to
    implement the query mechanism on it in addition to the ordering algorithm just
    talked about. Imagine if this has to be done with more applications! With 5 business
    applications, you already cache 50 lines in order to actually use only 10, which
    is an 80% waste of resources. You may think of pre-querying the applications in
    order to know which will provide data out of the filtered values, but that means
    you should already query one application and then adjust the counting querying
    to the other ones, maybe to realize that the optimization will not reduce the
    number of queries but only the number of lines retrieved. The choice of a pivot
    application may be difficult in itself for a resulting improvement that may be
    weak since we deal with reduced sets of data anyway (this is the goal of paginating
    the requests). Wait! We have not talked yet about the worst part of it: when paginating
    for the 10th page of data (between 90 and 100, if we stay on 10-line pages), you
    will not be able to simply call 10 lines from each of the 5 applications, because
    there may be one application that will account for almost all the lines in the
    order applied since the beginning of the pagination, and some others will provide
    nothing in the same range. This means that you may very well have the first result
    coming from an application only when calling the 10th page! You now see it coming,
    don’t you? Yes, we will have to query the 5 applications for 100 lines to extract
    the 10 lines corresponding to the 90 to 100 range of the aggregated data, which
    means a huge waste of 98%… and, the cherry on this sad cake is that, if an application
    does not support dynamic range, you will have to query it several times in order
    to compose the complete range of data needed. Sure, it may be possible with some
    implementations to keep cursors on the database queries in the state, but that
    means that your application is now stateful, and this will account for some other
    technical limitations in terms of scalability. Well, the only thing that will
    save us there is that, generally, the users will stop at the second or third page
    of data, refining their `$filter` attribute to reach quicker results.'
  prefs: []
  type: TYPE_NORMAL
- en: Consistency problems also exist, but they are a bit easier to deal with as long
    as the cutting of data follows a functionally logical order. This is generally
    the case because the distribution of data is done in business applications, so
    the risk that they have duplicate data (apart from the core of the flower, of
    course, which is always shared) is normally very low.
  prefs: []
  type: TYPE_NORMAL
- en: Other types of referential architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A “virtual” data referential is a particular case of a “distributed” referential
    where the central part simply holds no data by itself, and thus has no persistance,
    relying on the surrounding business applications databases. Schematically, this
    is the following state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 – Virtual referential architecture](img/Figure_10.9_B21293.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9 – Virtual referential architecture
  prefs: []
  type: TYPE_NORMAL
- en: Other, more exotic, referential architectures exist but it does not sound really
    useful to expose them here. For those of you who are curious, the French government-issued
    document called *Cadre Commun d’Architecture des Référentiels* (common framework
    for referential architectures, freely available on the internet and in the French
    language) should not be a limitation, as the different possibilities are shown
    using mainly diagrams.
  prefs: []
  type: TYPE_NORMAL
- en: Now the architecture patterns have been shown, we can talk about the implementation
    itself, including what technical choices should be made and how when creating
    the data referential.
  prefs: []
  type: TYPE_NORMAL
- en: Some architectural choices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the first is, of course, the database. By the way, I should even say
    the persistence mechanism because a database is a very well-known persistence
    mechanism, but there are others, as we will see at the end of this section. Some
    other technical considerations will have to be dealt with – in particular, on
    the streams of data.
  prefs: []
  type: TYPE_NORMAL
- en: This section will also be an opportunity for a little rant about the dogmas
    in IT, and how they delay the long-awaited industrialization of information systems.
    Lots of technical choices remain based on the knowledge of the teams available
    rather than on the adequateness of the functional problem at hand. This is not
    to say that competencies should not be taken into account, but training should
    sometimes be forced on technical people who have not changed their way of thinking
    for decades and may hinder your information system development because they simply
    apply the wrong tool to the problem. You have likely heard the proverb “When all
    you have is a hammer, all problems look like nails.” If you have this kind of
    person in your team, a manager’s job is to open their eyes through training, whether
    it be internal, external, formal, or not.
  prefs: []
  type: TYPE_NORMAL
- en: Tabular versus NoSQL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the very first decisions one has to make when implementing the data
    referential is the kind of database paradigm to use. Should it be tabular or document-oriented?
    SQL or NoSQL? Knowing that the natural shape of 99% of business entities is a
    document structure with many levels, like a tree of attributes and arrays with
    varying depth, the obvious choice if you want to reach business / IT alignment
    should be a NoSQL database, adapted to the shape of your data: document-based
    NoSQL if you manage business entities, or graph-based NoSQL if you manipulate
    data entities linked to other entities by many typed relationships, causing a
    network of entities that can be traversed by many paths, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: If one really applies business/IT alignment and looks for a persistence mechanism
    that closely mimics the shape of their data, SQL tabular databases should be used
    for business entities that are naturally tabular… which is almost never! Sure,
    there are cases, just like there are some for key-value pair lists in the NoSQL
    domain, but they are very scarce. In fact, it looks like the main reason SQL is
    still largely used for the data referential is simply history. And this is a justified
    reason when dealing with legacy software… After all, if it has worked for years
    this way, you are better off not touching it. But the real problem is when a new
    data referential, designed during a project of information system modernization,
    also uses a non-efficient approach.
  prefs: []
  type: TYPE_NORMAL
- en: Why do I say *non-efficient*? The history of computer science and databases
    should be invoked in order to explain why… In the old times of data storage, when
    spin disks were used with random-access controllers, data was not randomized in
    the magnetic disks but placed in sequences of blocks (preferably on the outermost
    lines of the hard disk, as the linear speed was higher, providing for quicker
    reads). In order to quickly access the right block, database engines would force
    the size of a line of data in order to quickly jump to the next, knowing the total
    length of each line of data in advance. This is why old types of strings in the
    database required a fixed length, by the way. This is also why the data has to
    be stored in tabular blocks, and structured data decomposed into many tables where
    lines are related to each other by keys, as this was the only way to calculate
    the next block index.
  prefs: []
  type: TYPE_NORMAL
- en: 'These assumptions came with a high price, though: since data was tabular, the
    only way to store multiple values for an attribute of an entity was to create
    another table in the database and join the two lines of data. The consequence
    of this was that complicated mechanisms were necessary to handle global consistency,
    such as transactions. In turn, transactions made it necessary to create the concepts
    of pessimistic and optimistic locks, then manage isolation levels for transactions
    (as the only fully **ACID** ones, which are the serializable transactions, have
    a dramatic impact on performance), then deadlock management and so many other
    complicated things.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When you think about it and realize that hard disk controllers have been providing
    randomized access for decades (and the very concept of a spinning disk does not
    exist in SSD), it is hard to understand why the consequences of this remain so
    pervasive today. One of the reasons is the change management, as nobody likes
    changing. But if there is a job where one should adapt and embrace change, that
    should definitely be a developer. I can also understand that SQL is still used
    in workshops where people only know this as a persistence technique. It is much
    better to start an important work with maybe not the best tool but one that is
    well known by the whole team, and I would not advise starting with a complex technology
    that nobody knows. But in this particular case of not using NoSQL for a business
    entity data referential, there would be two problems:'
  prefs: []
  type: TYPE_NORMAL
- en: First, this would be a training problem, as these technologies have been here
    for more than a decade now, and returns on experience are perfectly established,
    with trustworthy operators.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Second, there are actually few technologies as easy as document-based NoSQL.
    Take MongoDB, for example – writing a full-fledged JSON entity into a MongoDB-compatible
    database is as simple as follows (example in C#):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The equivalent with an SQL-based tabular **RDBMS** (short for, **Relational
    Database Management System**) is the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: And I am not even talking about the **Data Definition Language** (**DDL**) commands
    to create the tables and columns, which would add many lines. MongoDB does not
    need any, as it is schemaless and collections are created as objects are added.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Again, there are cases where SQL is needed. Reporting tools are very numerous
    using this grammar and it is good practice to expose SQL endpoint to access data,
    as it eases its consumption. Big data tools and even NoSQL databases have SQL
    endpoints. This is valuable as there are lots of people who are competent in using
    this way of interrogating data and computing complex aggregations. However, choosing
    a tabular database to store structured data just in order to be able to use a
    well-known query language is a problem, as it will cause lots of unwanted complexity.
    In your next data referential, please consider using NoSQL, as you will gain a
    lot of time with it. And if you know this kind of project will arrive next on
    your project portfolio, start getting training for your team. Only a few days
    are required to understand everything that is needed to be proficient with document-based
    NoSQL servers such as MongoDB, and they are extremely well adapted to storing
    business entities.
  prefs: []
  type: TYPE_NORMAL
- en: CQRS and event sourcing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While we are at it, you may also want to ditch your old data stream architectures
    where reading and writing are handled by the same process. After all, these two
    sets of operations are so different in their frequency (most **Line Of Business**
    (**LOB**) applications have 80% of reads and 20% of writes), functions (no locks
    necessary for reading, consistency needed for writing), and performance (low importance
    for unique writing, very important for massive queries) that it sounds logical
    to separate them.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what **Command and Query Responsibility Segregation** (**CQRS**) is
    about: it separates the storage system receiving the commands for altering or
    creating the data from the system ready to answer queries on the same data. Event
    sourcing is closely associated with this architectural approach as it stores a
    series of business events generated by writing commands and lets the queries use
    this storage to get the aggregated results they need in a highly scalable way,
    thus allowing performance on large data.'
  prefs: []
  type: TYPE_NORMAL
- en: In some way, CQRS could be thought of as a type of referential architecture
    between the distributed and the clone approaches. It does not separate data between
    applications with a criterion that is on the data itself, but rather on the kind
    of operation that is going to be performed on it (mainly, write or different kinds
    of reads). At the same time, the prepared reading servers can be considered as
    clones of the *single version of the truth* data. As their number can rise without
    any limit since the single version of the truth is in the main persistence, the
    performance can always be adjusted, however complex the queries and with high
    volumes as well.
  prefs: []
  type: TYPE_NORMAL
- en: Again, this is not the place to discuss these subjects in detail but they had
    to be cited in a chapter about data referential and MDM, as they are the indisputable
    best approach to implementing high-volume solutions.
  prefs: []
  type: TYPE_NORMAL
- en: One more step beyond databases – storing in memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s come back to our discussion about the origin of the tabular database system
    and even a bit before. Why do we actually need database and storage systems? Mostly
    because hard disks can store more data than only RAM, and because databases would
    not fit in small amounts of RAM. Thus, it is necessary to have a system that is
    good at quickly putting data on disk (in order to keep it safe in case of hardware
    failure, the database first writes in the log files) and good at retrieving some
    parts of data from the disk and putting them back into memory for application
    use (this is the SQL part and, in particular, the role of the `SELECT` and `WHERE`
    keywords).
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this was a major problem when computers had 640 kilobytes of RAM
    and databases would need a few megabytes. But how about today? Sure, there are
    huge databases, but we commonly have databases with a few gigabytes only. And
    how about server RAM? Well, it is very common to have servers with tens of gigabytes,
    and it is easy to acquire online servers with 192 GB RAM. In this case, why is
    there a need for manipulating data in and out of the disks? Sure, SSD disks are
    some kind of memory, but they are still slower than RAM. Also, there is indeed
    this persistence under hardware failure that has to be taken care of. But how
    about the manipulation of data itself? Would the manipulation of queries into
    RAM not go much quicker?
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, it does and there is a rarely-used and scarcely-known technique called
    “object prevalence” that acts as an in-memory database. We are not talking about
    files stored in a RAM disk or a high-speed SSD, but having the data used directly
    in the object-oriented model of your application. How can we be sure not to lose
    any data if there is a hardware failure, you might ask? Well, exactly as a database
    does: by keeping a disk-based log of all the commands sent to the system. The
    difference then is that the reference model for manipulating data and extracting,
    filtering, and aggregating results is not on some tabular writing on the disk
    that has to be accompanied with indexes in order to improve performance, but directly
    in the RAM, and in a binary format that is the one directly used by your application,
    which means nothing can go faster. By doing so, requests in SQL are replaced by
    code in your language of choice – for example, C# with LINQ queries.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is quite astonishing that object prevalence has never reached a wider audience
    but all the people I know who used it were convinced of its high value. Personally,
    when I need to implement a data referential that is limited in volume but has
    one of the following requirements, I always go for this technology:'
  prefs: []
  type: TYPE_NORMAL
- en: High performance required
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Very complex queries that would be hard to write in SQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data model that evolves often
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the best data referential implementations that I have participated in
    was on a project calculating advanced financial simulations and optimizing them
    with genetic algorithms; the performance boost was huge and the ability to write
    extremely complex cases of data manipulations made the whole project a clear win
    for the customer, who was surprised in the first test drives by the sheer velocity
    of the simulations – the old platform this one replaced provided results in minutes,
    whereas the new one responded in no more than a few seconds.
  prefs: []
  type: TYPE_NORMAL
- en: Another example of a successful implementation was in the handling of low-moving
    data such as country codes. In this particular example, people were not feeling
    great with the in-memory approach, even though data is safe in logs on the disk
    (and we even had a backup, as a third set of data for improved reassurance). So,
    testing this quite innovative approach with some data that they could easily feed
    back into the data referential made it more comfortable for a first try of the
    technology. The test went well, but the customer did not expand it to other data.
    Sadly, I do not know more examples of uses of this technology, which is a bit
    sad as the potential was huge.
  prefs: []
  type: TYPE_NORMAL
- en: 'Though this example may not be the best as this technology did not hit it off,
    the message still remains: in order to respect business/IT alignment, which is
    the best way to ensure long-term evolution, always favor a technology that closely
    fits your business needs and data shape.'
  prefs: []
  type: TYPE_NORMAL
- en: In the last section of the book, we are going to talk again about time and how
    it influences what we do with data referential, in our case.
  prefs: []
  type: TYPE_NORMAL
- en: Patterns of data evolution in time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 4*](B21293_04.xhtml#_idTextAnchor121), we studied the importance
    of time management in information systems, and one of the major impacts of time
    handling is on data. Data handled in MDM systems must be taken into account with
    the time factor, and we talked abundantly about data history management. But the
    very act of MDM should also be done according to time.
  prefs: []
  type: TYPE_NORMAL
- en: Data governance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Data governance** is the act of establishing functional responsibilities
    around the management of data referential. Who is responsible for which reference
    data? Who can manipulate and clean the data? Who decides the evolution of the
    model? How are impacted teams and applications informed about changes? What business
    rules should be followed when manipulating the data? When should data be erased
    or archived? Those all are governance questions and they are always related to
    time. In particular, the responses have to be reviewed at regular periods, just
    like business processes, in order for the data to remain under control.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data governance is mostly handled in the second layer of the Cigref map, which
    is the business capability map and usually contains a zone dedicated to reference
    data management. This is where you should draw the different data referentials,
    and store the detailed definition of the entities that are stored, along with
    the versions to prove compatibility between them or document incompatible changes.
    Here, you should also find at least the name and contact of two of the main data
    governance roles:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The data owner**: This person is ultimately responsible for the quality and
    usability of the data inside the information system. They define all the business
    rules around the data: how it must be manipulated, who can access it, in which
    conditions, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The data steward**: Under the delegation of the data owner, this person is
    responsible for the daily maintenance of the data. Following data manipulation
    rules issued by the data owner, they clean data and ensure its availability and
    integrity, as well as the respect for authorization rules.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the obvious consequences of having data governance is that there is a
    clear responsibility for a given data referential. Having shared responsibility
    for a referential is a problem because there can be competing needs that evolve
    in an uncontrolled evolution of the entity’s format or the services provided.
    In the worst case scenario, the IT team does not know who to consider as the decider
    and implements both demands, making the data referential progressively harder
    to use and not fit for its purpose. Having no responsibility is even worse because,
    as the implementation belongs to the IT team, the technical people become, by
    default, the owners of the data, which may be the worst move ever as they do not
    have the best knowledge of the business stakes associated with the data. Sure,
    they basically know what the data is about (after all, we all know in a company
    what a customer is or a product) but again, the devil lies in the details, and
    when the IT team is in charge of defining data, no one should act surprised that
    organizations only support one address, or that there is no distinction between
    a product and an article. Such mistakes would never be made by a specialist in
    the subject, and we all know how destructive a bad entity definition can be. So
    leaving such business-driven decisions to the IT team because nobody wants to
    take ownership is a risky move and everyone should be warned about this.
  prefs: []
  type: TYPE_NORMAL
- en: Progressive implementation of a unique referential
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When presenting the distributed and consolidated data referential architectures,
    it has been stated that, sometimes, these intermediary steps toward a centralized
    referential (which, most of the time, is the ultimate goal) can cost as much as
    directly going to the target state because of hidden efforts or lesser-known shortcomings.
    On the contrary, there are times when directly addressing the final vision is
    impossible, and convergence toward this should be done in several progressive
    steps. This might be because the information system is so coupled that a violent
    move may destroy it; most of the time, the problem is with the human capacity
    to embrace change, and a progressive approach has to be followed for the organization
    itself to be able to adjust.
  prefs: []
  type: TYPE_NORMAL
- en: This has been the case for me in many situations where I consulted for companies
    who, in order to successfully manage their merger or acquisition of another company,
    needed to apply a merging program to the two information systems, incorporating
    them into a single system. This kind of thing generally takes years in big organizations
    (the quickest that I have ever witnessed was done in less than 18 months, but
    all flags were green, which rarely happens). As you will see in the following
    sections, these plans need numerous steps to be realized.
  prefs: []
  type: TYPE_NORMAL
- en: 'For privacy reasons, I will show a mix of two progressive transformations that
    I designed for a public customer (a fusion of two regional councils in France)
    and an agriculture cooperative that was born out of the merger of two giant entities
    in the West of France. Both of them needed to address the MDM of the individuals
    and legal entities that their information systems deal with (customers, agents,
    prospects, farmers, students, etc.). In order to simplify the diagrams, I will
    consider the starting point to be where the two entities each had a consolidated
    data referential, with some applications showing a clone referential pattern.
    This often happens when there are many applications needing referential data:
    the most important are directly plugged into the highest-level data referential
    application, and the secondary applications are simply cloning what happens in
    their leading business application. In the following schema, I have also highly
    reduced the number of applications, again, for simplification reasons. I have
    not drawn the relationships between them and with other software in the information
    system, as they were mostly ERPs with much interoperability.'
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – same infrastructure but no link
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'All this being said, the first step can be schematized as this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.10 – Fusion of two MDM systems – step 1](img/Figure_10.10_B21293.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.10 – Fusion of two MDM systems – step 1
  prefs: []
  type: TYPE_NORMAL
- en: The two companies have completely separate MDM systems, hence data referential
    for their “actors,” if this is the name we should use to describe the entities
    at play. Notice that most applications are different in each case, except for
    `App1`, which is a common ERP between the two companies (this does not mean it
    will be compatible, as versions may differ and customization definitely will,
    but this can make a good candidate to put things in common at some point). The
    very first step is, of course, to connect the two internal networks, even if everything
    that will be shown next could very well be applied by only communicating through
    the Internet.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – providing a common interface
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The second step was to provide all users in the new fusion entity with an API
    to read actors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.11 – Fusion of two MDM systems – step 2](img/Figure_10.11_B21293.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.11 – Fusion of two MDM systems – step 2
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice how symmetrical the diagram is: choosing a neutral pivotal format was
    of utmost importance, as using the proprietary format of one of the companies
    would have been a clear disadvantage to the other one (which would have to change
    all its connectors and mapping code) and this would have caused human problems,
    as tensions are always exacerbated during company fusions, particularly when they
    were previously competitors. We thus spent a lot of time crafting a nice pivotal
    format for the users, using the best data representations coming from both sides.
    At this step, not only is reading the sole operation available but no company
    can read the other’s data! You might wonder how useful this step is since the
    goal is to reach a unique MDM system for both companies and, for now, it does
    not change anything. In fact, it is indeed harder for no functional effect, but
    preparing a common pivotal format is the basis for adequately sharing data. Also,
    it provides a way for all new software functions that would be created during
    the fusion process to read actors in a standardized, fusion-ready way. This means
    we will not have to come back to these new applications, and this is much-appreciated
    news when you know hundreds of applications have to be dealt with in the whole
    project. Finally, it started the work on the mediation connectors (there again,
    this is the kind of thing that is best implemented in Apache Camel, or another
    flavor of enterprise integration patterns), which was an important piece of work,
    better started early in the project.'
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – merging data from the secondary source with the primary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'From now on, we will only represent the difference in streams from the point
    of view of company A, but the opposite is always true. The next step was to start
    obtaining some data from one information system and transferring it to the other.
    Again, this was very progressive: it was only done for the reading operations
    of the data for now and, as shown in the following diagram, the data was first
    read on the system of the person initiating the request, and then only completed
    with data “from the other side of the barrier.” At any time, the data from the
    originating side would win, except if the date of modification clearly showed
    that the data coming from the other information system was fresher.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.12 – Fusion of two MDM systems – step 3](img/Figure_10.12_B21293.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.12 – Fusion of two MDM systems – step 3
  prefs: []
  type: TYPE_NORMAL
- en: For this previous step to work, it was necessary to find a way to look for similar
    actors, for example, with their VAT number or other business identifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – storing identifiers from the other source
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since this is a complex operation to realize, once the correspondence was found,
    the technical identifier from one side was stored in the other, and vice versa,
    which will allow for quicker access next time. This was the first time the system
    would write in the MDM system, but this was limited to storing the identifier
    from the other side:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.13 – Fusion of two MDM systems – step 4](img/Figure_10.13_B21293.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.13 – Fusion of two MDM systems – step 4
  prefs: []
  type: TYPE_NORMAL
- en: However, this opened up an entirely new approach to sharing data because, once
    the *write* authorizations were provided and the “external” identifier known,
    each side was able to share information with the other side.
  prefs: []
  type: TYPE_NORMAL
- en: Step 5 – sending information to the other side
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Every time there was a change in the actor on one side, the other was informed.
    The receiving information system was free to deal with this information at its
    own pace, maybe doing nothing with it the first time, but then choosing which
    pieces of data were interesting and storing them, and so on. At this point, keeping
    the origin of the data change was necessary in order to not start a loop of information,
    sending back to the initial information system the event that the data changed
    because of its initial event. The diagram – once again represented only from A
    to B for simplicity reasons – was as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.14 – Fusion of two MDM systems – step 5](img/Figure_10.14_B21293.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.14 – Fusion of two MDM systems – step 5
  prefs: []
  type: TYPE_NORMAL
- en: Now, since the initial write was started and information systems (and people)
    were starting to trust each other better, the next step was to generalize the
    modification of data.
  prefs: []
  type: TYPE_NORMAL
- en: Step 6 – centralizing the writing of data and extending the perimeter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This means that both sides started to use the centralized API in writing and
    the implementation of this API was to push the data on both sides, in order for
    each information system to know about the latest data. Again, using the data depended
    on whether the receiving end knew the actor (or should record it) but, in some
    cases, data was simply ignored, for example, when this was about a change in a
    supplier that was only used in the other company. As for prospects, though, the
    data was shared because the commercial approach started to get unified between
    the two parts of the slowly emerging fusion company.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.15 – Fusion of two MDM systems – step 6](img/Figure_10.15_B21293.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.15 – Fusion of two MDM systems – step 6
  prefs: []
  type: TYPE_NORMAL
- en: The enterprise integration pattern used in the MOM implementation was a “duplicate
    message” one, sending the data pushed by the initial request in two similar messages
    to the mediation routes and waiting for both acknowledgment messages to come back
    in order to emit its own acknowledgment along the route it was called by, effectively
    creating a robust delivery of the change both sides.
  prefs: []
  type: TYPE_NORMAL
- en: Step 7 – access unified
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This was the time when the old data referential started to act only as gatekeepers
    for the messages, checking that they were related to their side of the information
    system. But, since actors were now largely shared, this was not such an important
    feature, so some applications started to register their actors’ messages directly
    to the top data referential:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.16 – Fusion of two MDM systems – step 7](img/Figure_10.16_B21293.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.16 – Fusion of two MDM systems – step 7
  prefs: []
  type: TYPE_NORMAL
- en: '`App1` (an ERP used on both sides) was a great candidate to start this new
    approach, as the mediation connector to it could directly be shared between the
    two information systems, making for the first common deployment, thus lowering
    the height of the “barrier.” Since this approach worked quite well, it was a kickstart
    for the rest of the applications and some dedicated connectors quickly appeared
    on the other application, which was easy since the common pivotal format had then
    evolved and was easier than the previous ones, also covering more business cases.'
  prefs: []
  type: TYPE_NORMAL
- en: Step 8 – eliminating unnecessary calls
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The situation rapidly evolved to something schematized as this, where the old
    MDM system basically had nothing more to do since all data was coming from the
    new centralized one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.17 – Fusion of two MDM systems – step 8](img/Figure_10.17_B21293.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.17 – Fusion of two MDM systems – step 8
  prefs: []
  type: TYPE_NORMAL
- en: In addition, some applications, such as `App7`, had time to evolve and were
    able to directly take some JSON-representing actors without resorting to a mediation
    connector. Also, some applications started to be used in common between the two
    organizations (which clearly appeared more and more like becoming a single organization
    at this point), and `App4` disappeared in favor of the common use of `App6`.
  prefs: []
  type: TYPE_NORMAL
- en: Step 9 – removing unnecessary intermediate applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Some “low strategy” applications remained under the control of business applications
    such as `App3`, but this was not a problem as their parent application was now
    under the main data referential and would deal with the change of format for them.
    These applications did not see any change in the system, which was great for their
    users, who were not impacted at all by the otherwise major change. The resulting
    information system started to look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.18 – Fusion of two MDM systems – step 9](img/Figure_10.18_B21293.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.18 – Fusion of two MDM systems – step 9
  prefs: []
  type: TYPE_NORMAL
- en: 'Since `App6` was used by all teams, the barrier between the two formerly separated
    companies went one more step down, reaching a point where it was not a problem,
    as it only divided some secondary business applications used in some corner cases
    by a dedicated team on activities that were not part of the fusion process. There
    was now a unique centralized MDM system, with a few important applications acting
    as local referential for actors on which secondary applications would clone some
    parts of data. This took many years in total but the objectives were reached:
    merging the actors used by both sides and doing this in such a progressive way
    that the business was never affected by technical choices.'
  prefs: []
  type: TYPE_NORMAL
- en: Keeping an eye on dogma and useless complexity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I hope I have convinced you in this chapter (and in this book in general) to
    keep a critical eye on technologies and practices for which their use sounds obvious.
    As with SQL databases for data referential storage, or hard disk-based data manipulation,
    there are lots of preconceived approaches in the development itself that do not
    fit the problem very well when you think purely in business/IT alignment terms.
  prefs: []
  type: TYPE_NORMAL
- en: Just one example is data validation. In most programming languages, validators
    are associated with fields or properties of data entities, through attributes,
    for example, in C#. This approach is, in my opinion, very wrong and has proved
    several times in my practice to be a real pain as one will almost always find
    a particular case where these validating attributes are not correct. In the case
    of business identifiers, product owners would sometimes insist on the fact that
    no entity should ever be created without such a value, and then, within a year
    or so, realize that there is this particular case where the identifier is not
    known yet and we should still have the entity in the system. This can, for example,
    be the case with a medical patient database where the product owner will assure
    you that an entity without a social security number would make no sense as it
    is absolutely mandatory before even considering providing medication to them…
    After insisting on putting a strict `NOT NULL` validator on this for data quality
    reasons, the same person may come back a few months later, when the database is
    in production and a major impact change would have a huge cost, telling you that
    they forgot the particular case of a newborn that should be given drugs but they
    do not have a social security number yet.
  prefs: []
  type: TYPE_NORMAL
- en: In this particular example, I have personally taken the habit of never describing
    any entity attribute as mandatory, as only the context of its use makes it mandatory
    or not. And it is so easy to add a business rule or a form behavior blocking the
    `null` value that it really is not a problem to not put it on the entity itself.
    On the other hand, sorting out the mess when this mandatory characteristic has
    been implemented down the lowest levels of your information system is such a pain
    and a cause of errors that it is, in my opinion, never justified to call a field
    “mandatory” (except the one exception for a technical identifier, as there is
    otherwise no way to univocally retrieve an entity once created).
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: I really like it when I read articles such as [https://jonhilton.net/why-use-blazor-edit-forms/](https://jonhilton.net/why-use-blazor-edit-forms/),
    where the author questions there being “too much magic” in the technology exposed.
    Generally, there is, indeed, and such critical eyes are the best reads one can
    have on a given technology, rather than the numerous blog articles that simply
    explain how to use a function without digging into when it is useful and when
    it is actually more of a danger than a real advantage. This article really has
    a great point of view on validation included in forms and data definitions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same goes for cardinalities as for the identifiers cited previously, by
    the way: if you do not have the absolute, definitive, and fully responsible engagement
    of your product owner that an attribute should be with a zero or one cardinality,
    always make it as an array with *N* cardinality. What is the worst that could
    happen? The arrays always being filled with only one item? Well, that does not
    really matter, does it? A developer complaining that, on all these occasions,
    they must type `deliveryAddresses[0]` instead of `deliveryAddress`? Show them
    how to create a property in the language they used and it will be sorted out.
    As far as the GUI is concerned, we will simply display a single piece of information
    for as long as we do not have a use case corresponding to handling several values
    in the array. Only when this new business case appears, where we need to handle
    several pieces of data, will we adjust the GUI with a list instead of a single
    text zone, for example. But the great thing about this approach is that this will
    be done smoothly, as the previously unique data will simply become the first of
    the list, and much more importantly, all the clients of the API will remain compatible
    and will not be broken by this new use. They will even be able to continue using
    only the first piece of data in the list as long as they do not want to use the
    other ones and stick to the old behavior. Since all clients and the server can
    advance at their own pace on the business change, we know we have a low coupling.'
  prefs: []
  type: TYPE_NORMAL
- en: This extends to many other technical approaches that are supposed to help the
    business but, in the end, can hinder it. To name just a last example, most of
    the technical approaches to data robustness actually go against the business concepts.
    The outbox pattern ([https://microservices.io/patterns/data/transactional-outbox.html](https://microservices.io/patterns/data/transactional-outbox.html)),
    for example, should only be used when eventual consistency is not an option. But
    when you know that even banks have always used eventual consistency (and will
    definitely go on doing so in the future), that limits the usefulness of such techniques
    quite a lot. Of course, understanding the business in depth is less fun than using
    the latest technology or pattern that will drop your rate of transaction errors
    to a bare minimum. But it is the only way to win in the long term.
  prefs: []
  type: TYPE_NORMAL
- en: So, once again, because this is such an important message, **think of the business
    functions first and then find the technology that adapts to it**. In order to
    do so, the easiest way is to imagine what would happen in the real world between
    business actors if there were no computers involved.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, the principles of MDM have been applied and implementation
    techniques exposed, not only from the architectural point of view but also with
    technical choices that may prove useful when constructing a data referential.
    The main behaviors of such server applications have been covered and their evolution
    in time has been described with a few examples. This should make you quite knowledgeable
    about how to implement your own data referential.
  prefs: []
  type: TYPE_NORMAL
- en: We will come back to the subject of MDM in [*Chapter 15*](B21293_15.xhtml#_idTextAnchor548),
    where we will go down to the lowest level of implementation, with actual lines
    of code and the design and development of two data referential implementations
    in C#, in order to deal with authors and with books, respectively. This will be
    the final piece where we will join the principles of service management and APIs
    learned about in [*Chapter 8*](B21293_08.xhtml#_idTextAnchor271), the domain-driven
    design of the entities shown in [*Chapter 9*](B21293_09.xhtml#_idTextAnchor318),
    and the architecture approaches described in the present chapter.
  prefs: []
  type: TYPE_NORMAL
- en: But before we reach this point, we will study the two other parts of an ideal
    information system, just like we did here for the MDM part. The next chapter is
    about business process modeling and how we can use **BPMN** (short for, **Business
    Process Modeling Notation**) and BPMN engines in order to implement business processes
    inside our information systems. Some other subjects such as middleware, no-code/low-code
    approaches, and orchestration versus choreography will be exposed in the next
    chapter as well.
  prefs: []
  type: TYPE_NORMAL
