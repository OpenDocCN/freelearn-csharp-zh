- en: Chapter 8. Concurrent and Parallel Programming under .NET
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have been mostly focusing on the GoF design patterns. When the catalog
    appeared, the computing world was mostly sequential, and the bias was reflected
    in the catalog. The world has changed a lot since the publication of the catalog
    in 1994\. There was a shift towards language-level concurrency and parallelism
    due to the arrival of the Java and C# programming languages. The processor designers
    understood the limits of constructing powerful single-core CPUs, and began focusing
    on many core CPUs. This brought its own set of complexity in developing software
    using the existing paradigms. Even a language like C++, which relegated concurrency
    to libraries, added language-level concurrency features in its latest avatar,
    C++ 11/14\. With the advent of functional programming features into the OOP world,
    the programming landscape changed drastically. In this chapter, you will learn
    some techniques for writing concurrent and parallel programs using the C# programming
    language and .NET platform features. In this chapter, we will cover the following
    points:'
  prefs: []
  type: TYPE_NORMAL
- en: Factors that influenced evolution of concurrency and parallelization models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrency versus Parallelism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: .NET Parallel Programming libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some Parallel/Concurrent memes/idioms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embarrassingly parallel
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Fork/join parallelism
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Producer/consumer paradigm
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Days of Free Lunch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The celebrated Moore''s law went on despite microprocessors hitting the clock-speed
    limits in terms of heat dissipation and achieving fabrications beyond nano-level.
    CPU designers found an intelligent workaround for this roadblock-that of leveraging
    the increased chip densities to scale the computation infrastructure horizontally
    as opposed to the traditional vertical way. This principle has deep consequences
    in modern day architecture and design, where application scalability (both vertical
    and horizontal) inherits natural elasticity with respect to the infrastructure
    that powers them. What resulted was a new generation of CPU architectures including
    hyper-threading, multi-core, and many-core. It wasn''t too late before the developers
    realized that the **Free Lunch** (just leveraging the conventional CPU performance
    gains through clock speed, execution optimization, and cache, with no change in
    their programs) was over. Herb Sutter wrote an influential article in Dr. Dobb''s
    Journal about this, aptly titled, *The Free Lunch Is Over: A Fundamental Turn
    toward Concurrency in Software*!'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This realization that software was the gating factor in terms of achieving more
    with most of what was available was the dawn of a new revolution in software architecture
    and design.
  prefs: []
  type: TYPE_NORMAL
- en: This new model helped remove many shortcomings in modeling real-world problem
    scenarios. It seemed though that we (developers) began adding justice to the grand
    concurrent design that went into modelling this world with interactions to be
    parallel as opposed to just being sequential and object-oriented. The prevalent
    parallelization techniques (leveraged mostly by the scientific community for doing
    embarrassingly parallel computations)-that of explicit threading with an affinity
    to hardware threads-was primitive and less scalable. Very few people had expertise
    (or rather felt comfortable) in working with the low-level constructs for leveraging
    multi-threading in their applications. This state of affairs brought in a strong
    imperative for creating better abstractions and the needed APIs to write the next
    generation software that would inherently leverage concurrency and parallelism
    to achieve more with most of what was available.
  prefs: []
  type: TYPE_NORMAL
- en: Days of Sponsored Lunch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microsoft's .Net **Common Language Runtime** (**CLR**) came to the rescue with
    the managed thread pool (which stabilized around version 2.0), which paved the
    way for a strong foundation layer on top of which the concurrency and parallelization
    models subsequently evolved.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most notable ones include **Asynchronous Programming Model** (**APM**),
    **Concurrency and Coordination Runtime** (**CCR**), **Decentralized Software Services**
    (**DSS**), **Parallel LINQ** (**PLINQ**), **Task Parallel Library** (**TPL**),
    and the Task-based Async/Await model.
  prefs: []
  type: TYPE_NORMAL
- en: Certain functional constructs and language features like Anonymous Methods,
    Lambda Expressions, Extension Methods, Anonymous Types, and **Language Integrated
    Query** (**LINQ**) were the core catalysts that aided this evolution. Major contributors
    and SMEs include Erik Meijer for LINQ, Joe Duffy and Jon Skeet for Multithreading
    in .NET), Stephen Toub, Ade Miller, Colin Campbell, and Ralph Johnson for Parallel
    Extensions, and Jeffrey Richter and George Chrysanthakopoulos for CCR.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of the aforementioned notable models, the major ones that matured and are advocated
    today are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Task-based async/await model for concurrency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallel Extensions** (**PFX**) including PLINQ and TPL for parallelization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Days of Sponsored Lunch](img/B05691_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Concurrent versus parallel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These are two key constructs are often confused by developers, and warrant clear
    demystification. The authors strongly feel that a thorough understanding of this
    distinction holds the key to effective software design for achieving more by effective
    utilization of the available infrastructure (processors, cores, hardware, and
    software threads). Let's start with the classical definition by Rob Pike (inventor
    of the Go programming language), and try to decode its meaning.
  prefs: []
  type: TYPE_NORMAL
- en: '|   | *Parallelization is doing multiple tasks (related or non-related) at
    the same time whereas concurrency is about dealing with lots of things at once.
    Concurrency is about structure; parallelism is about execution. Concurrency provides
    a way to structure a solution to solve a problem that may (but not necessarily)
    be parallelizable.* |   |'
  prefs: []
  type: TYPE_TB
- en: '|   | --*Rob Pike* |'
  prefs: []
  type: TYPE_TB
- en: This clearly articulates the difference between these constructs, and goes further
    to illustrate how concurrency, as a model, helps to structure a program by breaking
    it into pieces that can be executed independently. This powerful consequence of
    a concurrent design model facilitates parallelism depending on the hardware support
    available for the program's runtime or compiler infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This further means **write-once**, **run-anywhere**.
  prefs: []
  type: TYPE_NORMAL
- en: The days of Free Lunch are back with this model, where a concurrent program
    can perform better on multiple cores by leveraging parallelization techniques
    (mostly abstracted or under the hood, and is dependent on the framework library
    or runtime that executes these programs). This could be understood very clearly
    as the way an asynchronous programming model works using `async` and `await`.
    This model helps to structure the program by breaking it into pieces that can
    be executed independently. These independent pieces become **tasks** (part of
    the TPL), and are executed simultaneously, leveraging the available hardware (cores
    and threads). Now you see how parallelism becomes an interesting side-effect of
    a concurrent model. This is managed (in terms of the degree of parallelism that
    determines the number of cores to be leveraged for reducing contention) by the
    TPL API and CLR. So, the developer can focus on the core decomposition (with little
    or no shared state) of the process into independent tasks as opposed to getting
    entangled in the low-level constructs (threads and synchronization) for realizing
    concurrency and parallelization. The APIs have evolved quite intelligently to
    support this, thus making the code-base quite expressive, declarative, and readable.
  prefs: []
  type: TYPE_NORMAL
- en: On the contrary, if you explicitly try to leverage basic naive parallelism (`Parallel.For`
    and PLINQ) for executing the decomposed tasks, there is a good chance you would
    end up blocking your threads, thus curtailing scalability and availability (especially
    for server-side programs).
  prefs: []
  type: TYPE_NORMAL
- en: '|   | *Don''t Block your threads, Make Async I/O work for you* |   |'
  prefs: []
  type: TYPE_TB
- en: '|   | --*Stephen Toub/Scott Hanselman* |'
  prefs: []
  type: TYPE_TB
- en: This is the founding principle on which concurrent programming models work.
    The classic example is Node.js, which has gained prominence as a middleware and
    backend system with its inherent support for async I/O. Again, one needs to understand
    that the benefits are seen more in I/O and asynchronous service executions as
    opposed to long-running CPU-bound tasks (which wouldn't give you the benefit you
    typically desire, as these tend to block, and, in turn, cause thread starvation).
    This is a classic scenario, where developers complain that their program takes
    more time to execute (compared to the serial implementation) when task parallelism
    is employed with `Parallel.For` loops.
  prefs: []
  type: TYPE_NORMAL
- en: Again, none of these imply that one shouldn't employ high-and low-level constructs
    for task creation, threading, and parallelism. As long as scalability and availability
    is not a high priority, task parallelism, with appropriate partitioning strategies,
    could be very effectively employed. This is because these libraries effectively
    load balance the work across the CLR threads to minimize contention, and maximize
    throughput through work stealing.
  prefs: []
  type: TYPE_NORMAL
- en: The consequence of both these models is something to be kept in mind when designing
    effective algorithms that can leverage the best of what you have, and yet, scale.
    We will try to illustrate these in the coming sections.
  prefs: []
  type: TYPE_NORMAL
- en: Some common patterns of parallel programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we understand the power that these two models bring, one needs to be
    wary of the responsibility that this power brings forth. Typical abuse of concurrency
    and parallelism in the form of blanket code refactoring are often found counterproductive.
    Patterns become more pertinent in this paradigm, where developers push the limits
    of computing hardware.
  prefs: []
  type: TYPE_NORMAL
- en: '|   | *Issues of races, deadlocks, livelocks, priority inversions, two-step
    dances, and lock convoys typically have no place in a sequential world, and avoiding
    such issues makes quality patterns all the more important* |   |'
  prefs: []
  type: TYPE_TB
- en: '|   | --*Stephen Toub* |'
  prefs: []
  type: TYPE_TB
- en: The authors wish to put in perspective the modelling/decomposition aspects of
    the problem, and illustrate the applicability of some of the key patterns, data
    structures, and synchronization constructs to these so as to aid the programmer
    to leverage concurrency and parallelism to its full potential. For detailed coverage
    in terms of patterns and primitive constructs, the authors strongly recommend
    developers to read *Patterns of Parallel Programming* by Stephen Toub and *Concurrent
    Programming on Windows* by Joe Duffy.
  prefs: []
  type: TYPE_NORMAL
- en: Embarrassingly or conveniently parallel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the foremost pattern we would cover. The candidate programs that can
    leverage these are ones that can be easily decomposed into tasks/operations which
    have little or no dependency. This independence makes parallel execution of these
    tasks/operations very convenient. We will stray a bit from the conventional examples
    (that of a ray tracer and matrix multiplication), plus instill a sense of adventure
    and accomplishment in creating an algorithm, which, in turn, is embarrassingly
    parallel in nature.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We would be articulating this pattern by creating a C# implementation of the
    *Schönhage-Strassen* algorithm for rapidly multiplying large integers.
  prefs: []
  type: TYPE_NORMAL
- en: Problem statement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As indicated, the problem statement seems straightforward in terms of having
    an ability to multiply large numbers (let's push ourselves a bit by further stating
    astronomically large numbers), which cannot even be represented (64-bit computing
    limit). We have consciously reduced the scope by restricting ourselves to just
    one operation (multiplication) for outlining the pattern. Nothing prevents any
    interested reader who is game to go ahead and implement other operations thereby
    devising their own `BigInteger` version with support for all mathematical operations.
  prefs: []
  type: TYPE_NORMAL
- en: Solutions approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s start by outlining the algorithm to multiply two three-digit sequences
    (**456** and **789**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Solutions approach](img/B05691_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The Schönhage-Strassen algorithm depends fundamentally on the convolution theorem,
    which provides an efficient way to compute the cyclic convolution of two sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The authors wish to take a disclaimer here: the cyclic convolution computation
    is not done in the most efficient way here. The prescribed steps include that
    of taking the **Discrete Fourier Transform** (**DFT**) of each sequence, multiplying
    the resulting vectors element by element, and then taking the **Inverse Discrete
    Fourier Transform** (**IDFT**). In contrast to this, we adopt a naïve and computationally
    intensive algorithmic way. This results in a runtime bit complexity of O(n²) in
    Big-O notation for two *n* digit numbers. The core idea here is to demonstrate
    the intrinsic parallel nature of the algorithm!'
  prefs: []
  type: TYPE_NORMAL
- en: As illustrated, the algorithm to compute the product of two three-digit sequences
    comprises of three major steps. We will look at the problem decomposition in detail,
    and interlude the steps with code to see this pattern in action, leveraging some
    of the core .NET parallelization constructs (specifically, the `For` method of
    the `Parallel` class in TPL-`Parallel.For`). In this process, you will understand
    how task decomposition is done effectively, taking into consideration the algorithmic
    and structural aspects of your application.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We will start off multiplying the two numbers 456 (sequence-1) and 789 (sequence-2)
    using long multiplication with base 10 digits, without performing any carrying.
    The multiplication involves three further sub-steps as illustrated.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1.1**'
  prefs: []
  type: TYPE_NORMAL
- en: As part of the long multiplication, we multiply the least significant digit
    (9) from sequence-2 with all the digits of sequence-1, producing a sequence 36,
    45, and 54.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1.2**'
  prefs: []
  type: TYPE_NORMAL
- en: We multiply the next least significant digit (8) from sequence-2 with all the
    digits of sequence-1, producing a sequence 32, 40, and 48.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1.3**'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we multiply the most significant digit (7) from sequence-2 with all
    the digits of sequence-1, producing a sequence 28, 35, and 42.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Add the respective column elements (again without carrying) to obtain the acyclic/linear
    convolution sequence (28, 67, 118, 93, 54) of sequence 1 and 2.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Perform the final step: that of doing the carrying operation (for example,
    in the rightmost column, keep the 4 and add the 5 to the column containing 93).
    In the given example, this yields the correct product, 359,784.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the serial implementation of this algorithm (it faithfully
    follows the preceding steps for clarity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the following `Power` method, we will employ Exponentiation by Squaring Algorithm,
    which relies on the fact that:  *x^y == (x*x)^(y/2)* Using this, we will continuously
    divide the exponent (in this case *y*) by two while squaring the base (in this
    case *x*).That is, in order to find the result of 2^11, we will do [((2*2)*(2*2))*((2*2)*(2*2))]
    * [(2*2)] * [(2)] or, to put it simply, we will do 2^8 * 2^2 * 2^1.This algorithm
    achieves O(log n) efficiency!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you closely evaluate the code, Step 1 and Step 2 in our algorithm are embarrassingly,
    or rather, conveniently parallelizable. Listed next is the equivalent lock-free
    parallel implementation of the same algorithm. This leverages the TPL `Parallel.For`
    parallelization construct.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, to really understand the leverage we got from the `Parallel.For` parallelization
    construct, we have to do a CPU-intensive operation, which would be best achieved
    by computing the power (as opposed to the product) utilizing the multiplication
    algorithm. Imagine solving the wheat and chess problem, or perhaps more, say,
    2^(100,000) (to the power of 100,000) in place of 2^(32). A recursive divide and
    conquer strategy has been applied to compute the exponential (default implementation
    of the `Power` method in the abstract class/product `BigNumOperations`, which
    further uses the overridden, concrete `Multiply` methods of the respective core
    product implementations).
  prefs: []
  type: TYPE_NORMAL
- en: 'Can you really compute 2^(100,000) (given our limit of 64-bit arithmetic operations)?
    Well, take a look at the following invocation code and result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![Step 3](img/B05691_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Yes!!! It computed the values, and the parallel implementation took around half
    the time as compared to the serial one.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The qualifier here, that of taking half the time, is relative and will depend
    on the availability of cores and resources; it will also vary with environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also see how the task granularity seems to utilize the CPU (with all its available
    cores) to the maximum extent possible in the case of parallel execution (towards
    the right-hand side of the usage spectrum in all of the four cores):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Step 3](img/B05691_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is a quick summary of the key applicability of best practices
    and patterns in this implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: This is a classic case, where data parallelism (applying a single operation
    to many data elements/inputs) is exploited to the core, and the parallelization
    construct (`Parallel.For`) we have chosen is best suited for this. We could also
    leverage the synchronization primitive `Barrier` (`System.Threading.Barrier`),
    which would enable various sub-tasks to cooperatively work in parallel through
    multiple phases/tasks. A `Barrier` is recommended when the phases are relatively
    large in number.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose a lock-free task data structure (here, a two dimensional array has been
    utilized to capture the product sequences from each iteration in step 1). The
    operations (reads/writes) are atomic if you examine them closely (including step
    2). This makes the parallelization process very effective, as there wouldn't be
    any synchronization penalties (**locks,** specifically) but a seamless utilization
    of resources (with the inherent load balancing provided by `Parallel.For`). It
    is best to leave `Parallel.For` to calibrate the **degree of parallelism** (**DOP**)
    itself so as to leverage all the available cores, and thereby prevent side-effects
    because of thread starvation or oversubscription. At best, we could specify `ParallelOptions`
    of `Parallel.For` to use `Environment.ProcessorCount` so as to explicitly state
    the usage of one thread per core (a recommended practice in parallelization).
    The biggest limitation would be in terms of the memory required for array allocation
    in this case. You would tend to hit the `OutOfMemory` exception beyond powers
    of 100,000 (again, specific to this algorithm and the associated data structures
    that it employs).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-grained partitioning of tasks, as part of the decomposition process, enables
    throughput (again, it's a balance that needs to be achieved with careful analysis;
    any attempt to overdo can swing the performance pendulum to the other side).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose the data representation in string format to represent really big numbers.
    Of course, you do incur the penalty of data conversion (a necessary evil in this
    case). You could as well create an extension method for string type to support
    these big number operations (perhaps, with a validation for legal numbers).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use of alternate algorithm (reverse long multiplication; that is, reversing
    steps 1.1 through 1.3) to leverage the parallel loop partition counter, which
    is forward only (as its purpose is only to partition, unlike that of a step counter
    in a conventional `for` loop). Restructuring your algorithm is better than tweaking
    the code that was originally designed to run serially.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And finally, leverage the abstract factory GoF design pattern to seamlessly
    support the various implementations (in this case, serial and parallel).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fork/join or master/worker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a pattern that you generally associate with task parallelism. When there
    are distinct asynchronous operations that can run simultaneously, you can temporarily
    fork a program's flow of control with tasks that can potentially execute in parallel.
    You can then wait for these forked tasks to complete.
  prefs: []
  type: TYPE_NORMAL
- en: In the Microsoft® .NET Framework, tasks are implemented by the `Task` class
    in the `System.Threading.Tasks` namespace. Unlike threads, new tasks that are
    forked (using the `StartNew` method) don't necessarily begin executing immediately.
    They are managed internally by a task scheduler, and run based on a FIFO manner
    (from a work queue) as cores become available. The `Wait` (for task) and `WaitAll`
    (for task array) method ensures the join operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if you try to apply this pattern holistically to our original problem
    statement (to compute the power of big numbers), you will see the potential to
    leverage this for executing the tasks within the major phases (Steps 1, 2, and
    3) concurrently (by forking off tasks), and have the phases blocking (joining
    these forked tasks within each phase) to mirror the sequential ordering (steps
    1, 2, and 3) as advocated by the algorithm. See the following code that does lock-free
    parallel implementation of Schönhage-Strassen Algorithm by leveraging the `System.Threading.Tasks` concurrency
    construct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The collective sample output along with the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fork/join or master/worker](img/B05691_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: What we have done here in the preceding code is essentially explicit macro-range
    partitioning with respect to the available cores, and not spinning off in place
    of micro-range partitioning with respect to the outer loop. This is a strategy
    which has to be dealt with carefully, as results would vary with the resources
    available at your disposal. Deliberate calibration can yield much higher throughputs.
    In this context, we come to the next important pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Speculative execution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have seen close to three different implementation strategies of
    the Schönhage-Strassen algorithm, how do we perform deliberate calibration, and
    decide which is the best strategy (now that we understand that it has a close
    co-relation with its environment and associated resources)?
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is where this important pattern really helps us make a decision, when deviations
    against anticipated results are unavoidable, and need to be smartly addressed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We would schedule asynchronous tasks for each of these strategies for execution,
    leverage the `WaitAny` method of the `Task` class to wait for one of the operations
    to complete (one that finishes first), and attempt to cancel all others. On a
    smart-learning front, this could be done periodically to continuously calibrate
    and cache your strategy for mass consumption. It''s an aspect of machine learning
    where the program intelligently adapts to sieve and use effective algorithms.
    See the following code that incorporates options to cancel tasks upon determination
    of the winner by working out who is the fastest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Similarly, concrete products 5 and 6 are created based on constructs employed
    in products 2 and 3\. Please refer the relevant code sections in the companion
    website for these implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have executable parallel code that will respond to user interruptions,
    lets understand how we can do speculative execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s quite interesting, or rather an art, how we can achieve control over
    these constructs. It''s just that you need to see through your algorithm, and
    determine how the decomposition helps you gain coarser or finer control on execution.
    You will see areas that pose limitations once you get into the finer intricacies
    of task parallelization and concurrency. You will also see the power of abstraction
    that these constructs bring to the table, and better appreciate the instrumentation
    and hooks that need to go in for aiding you in gaining better control of your
    program, as opposed to letting **heisenbugs** haunt your programs. Let''s observe
    the output that determined the fastest implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Speculative execution](img/B05691_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Though the `Parallel.For` construct emerged the winner in all the three trials.
    This is not a certainty, as the outcome is determined by the available resources
    and the complexity of the algorithm (in terms of control flow and data flow),
    depending on input data provided. Something interesting has occurred here, which
    warrants an explanation, and will, thereby, demystify certain behaviors. Remember,
    everything ought to have an explanation (unless you are not in control, and have
    absolutely no idea how your code behaves)!
  prefs: []
  type: TYPE_NORMAL
- en: In case you are wondering why the serial implementation got cancelled, before
    it started, only once, it's primarily related to the work load in the machine
    and the precedence/sequence in which the tasks started being executed by the CLR
    thread pool. Also, the reason why the **Tasks Implementation - Cancelled** message
    comes only once is because `Console.WriteLine` blocks until the output has been
    written, as it calls the `Write` method of the underlying stream instance; the
    ones that don't get blocked appear on the console. You also need to ensure that
    the token cancellation detection code (`token.IsCancellationRequested`) is set
    at the required control flow points (forks, joins, and so on) to record near real-time
    cancellations, and throw `TaskCanceledException` via the `token.ThrowIfCancellationRequested`
    method (causing the task to transition to the faulted state). Please inspect the
    highlighted areas in the code to understand this.
  prefs: []
  type: TYPE_NORMAL
- en: The limitation that we noticed in terms of the missing console messages is something
    that we would need to overcome, as, capturing relevant information during program
    execution is an important horizontal concern, irrespective of the execution model
    (synchronous or asynchronous). Ideally, this activity should happen without impacting
    the normal execution flow, or causing any performance penalties (in terms of blocking
    calls). Asynchronous I/O is typically a standard option used by logging libraries
    to capture information (user and system-driven) behind the scenes. We have already
    dealt with the logging library in [Chapter 3](dn-dsnptn_ch03.html "Chapter 3. A
    Logging Library"), *A Logging Library*, and now we will see how to channel data
    and invoke these libraries asynchronously in the next pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Another relevant GoF pattern that could be leveraged here is the visitor pattern,
    where new strategic implementation of the algorithms could be declaratively tried
    out, without flooding consumers with concrete products.
  prefs: []
  type: TYPE_NORMAL
- en: Producer/consumer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a natural pattern that one can easily relate to from the moment you
    start modelling solutions for real-world problems. It is so intuitive that one
    may fail to appreciate the elegance of it, and yet many times we struggle with
    implementations associated with it.
  prefs: []
  type: TYPE_NORMAL
- en: A  producer producing something which a consumer wants is a common scenario
    in software modelling. And this can even happen at multiple levels or stages in
    your data flow. This is a typical pipeline in design parlance, and warrants good
    synchronization between stages. A seamless interplay between the stages in a pipeline
    warrants a regulated handshake, where we don't let consumers starve, and at the
    same time, ensure they are not overfed. Throttling this handshake involves laying
    down a communication protocol (publish-subscribe model, queue-based, and so on),
    which requires some of the boiler-plate concurrency constructs (be it data structures
    or synchronization primitives) to be in place, as opposed to one wiring these
    on their own. We have concurrent data structure starting with .NET 4.0, including
    `BlockingCollection<T>`, `ConcurrentBag<T>`, `ConcurrentDictionary(TKey, TValue)`,
    `ConcurrentQueue<T>,` and `ConcurrentStack<T>`, which help us in this task by
    abstracting out the synchronization pain-points, and giving us just the adequate
    blocking features for a seamless integration of concurrent execution scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: If you really look at our big-number multiplication algorithm, it involves a
    pipeline too, having three stages. The only thing is that our stages aren't concurrent,
    but serial (this is where, irrespective of the cores you have, you tend to reach
    the point of diminishing returns that Amdahl's law predicts). Additionally, our
    data structure (2D Array) gives non-blocking reads/writes for the concurrent producers
    within each stage.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The performance of a pipeline implementation is purely determined by the performance
    of its individual stages, and for efficiency, we need a concurrent model for each
    stage (which was achieved in our case).
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at a producer-consumer model-based implementation for non-blocking
    or asynchronous logging in the case of speculative execution. We want this primarily
    to overcome the limitation of blocking, and console-based stream writes (in production
    you can leverage asynchronous I/O for file or db writes).
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for the consumer is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Here you can see that we are using a blocking collection to record logs. This
    needs to be passed as another parameter to the implementation, and, in turn, collects
    all the log information.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an indicative code for the logger (handled in the respective
    concrete products):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: So, we have seen some of the key patterns that play a major role in modelling
    concurrent tasks that could be run in parallel. Though the narration has been
    primarily based on a single example, we believe that, as a developer, you were
    able to understand the applicability of these in a real-world problem scenario.
    In terms of coverage, there is a lot one needs to learn and prototype. Exception
    handling is a chapter on its own, especially when dealing with concurrent scenarios,
    and that has been avoided for brevity. A sea of threads awaits you. Bon Voyage!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we just touched the surface of concurrent and parallel programming
    under .NET. The topic warrants a book dedicated for itself. Now you have enough
    background to learn about writing advanced software using features of the C# programming
    language, like LINQ, lambda, expression trees, extension methods, async/await,
    and so on. The next chapter will deal with the issue of better state management
    by leveraging these tools.
  prefs: []
  type: TYPE_NORMAL
