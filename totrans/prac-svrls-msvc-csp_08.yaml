- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Practical Microservices Organization with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter is dedicated to a fundamental building block of microservice applications:
    orchestrators! The focus is on **Kubernetes**, but the concepts learned here are
    fundamental for understanding other orchestration options. In particular, **Azure
    Container Apps** is a serverless alternative to Kubernetes, implemented with Kubernetes
    itself, and uses simplified configuration options, but the objects to configure
    and concepts involved are exactly the same. Azure Container Apps is described
    in [*Chapter 9*](Chapter_9.xhtml#_idTextAnchor261)*, Simplifying Containers and
    Kubernetes: Azure Container Apps* *and other Tools*.'
  prefs: []
  type: TYPE_NORMAL
- en: All concepts will be exemplified with small examples and with the car-sharing
    book case study application. After a general description of orchestrators’ role
    and functionalities, we will describe how to configure and interact in practice
    with a Kubernetes cluster. We will use **Minikube**, which is a local simulator
    of a Kubernetes cluster, throughout the chapter. However, we will also explain
    how to create and use a Kubernetes Azure cluster.
  prefs: []
  type: TYPE_NORMAL
- en: We will also describe how to test and debug the interaction of some microservices
    during development with **Docker** first, and then the complete application running
    in a Kubernetes cluster. A .NET-specific alternative for testing a microservices
    application in the development stage is **.NET Aspire**, which will be described
    in [*Chapter 12*](Chapter_12.xhtml#_idTextAnchor345)*, Simplifying Microservices
    with .NET Aspire*.
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, this chapter covers:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to orchestrators and their configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes basics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Interacting with Kubernetes: Kubectl and Minikube'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring your application in Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running your microservices on Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced Kubernetes configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter requires:'
  prefs: []
  type: TYPE_NORMAL
- en: At least the Visual Studio 2022 free *community edition*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An SQL instance accepting TCP/IP requests and user/password authentication,
    and **Docker Desktop** for Windows, the installation for which was explained in
    the *Technical requirements* section of [*Chapter 7*](Chapter_7.xhtml#_idTextAnchor151)*,
    Microservices in Practice*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**If you would like to interact with a Kubernetes cluster on Azure, you need
    Azure CLI. The page at** [https://learn.microsoft.com/bs-latn-ba/cli/azure/install-azure-cli-windows?tabs=azure-cli](https://learn.microsoft.com/bs-latn-ba/cli/azure/install-azure-cli-windows?tabs=azure-cli)
    **contains the links to both the 32-bit and 64-bit Windows installers.**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Minikube**: The easiest way to install Minikube is by using the Windows installer
    you can find on the official installation page: [https://minikube.sigs.k8s.io/docs/start/](https://minikube.sigs.k8s.io/docs/start/).
    During the installation, you will be prompted on the kind of virtualization tool
    to use – please specify Docker. The previous link also gives a PowerShell command
    for adding `minicube.exe` to the Windows path.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Kubectl**: First of all, verify if it is already installed by opening a Windows
    console and issuing this command: `Kubectl -h`. If the response is the list of
    all Kubectl commands, it is already installed. Otherwise, the simplest way to
    install it is through the **Chocolatey** package installer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If Chocolatey is not already installed, you can install it by launching **PowerShell**
    in administrative mode and then issuing the PowerShell command suggested on the
    official Chocolatey page: [https://chocolatey.org/install#individual](https://chocolatey.org/install#individual).
    You can launch PowerShellin administrative mode as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Search **PowerShell** in the Windows search box.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Right-click on the **PowerShell** link and select to execute it as an administrator.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: You can find the sample code for this chapter at [https://github.com/PacktPublishing/Practical-Serverless-and-Microservices-with-Csharp](https://github.com/PacktPublishing/Practical-Serverless-and-Microservices-with-Csharp).
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to orchestrators and their configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Orchestrators were mainly conceived for balancing microservices’ load. Therefore,
    one might ask if they are necessary for all applications. I can’t say they are
    necessary, but, for sure, renouncing them doesn’t mean just manually configuring
    where to place each replica of each microservice. We should also find efficacious
    solutions for dynamically reconfiguring the number of replicas and their locations,
    for balancing the load among several replicas allocated on different servers,
    and for balancing the traffic among the various replicas of each microservice.
  prefs: []
  type: TYPE_NORMAL
- en: 'The above simple considerations show that an efficacious orchestrator should
    offer at least the following services:'
  prefs: []
  type: TYPE_NORMAL
- en: Accepting high-level specifications and translating them into actual allocations
    of microservice replicas on different servers of a given cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Providing a unique virtual address for all replicas of the same microservices
    and automatically splitting the traffic among them. This way, the code of each
    microservice can reference just this unique virtual address without caring where
    each replica is.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recognizing faulty replicas, killing them, and replacing them with newly created
    replicas.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Downloading microservices container images from container registries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Moreover, since microservice replicas are ephemeral and can be destroyed and
    moved from one server to another, they can’t use the disk storage of the servers
    that host them. Instead, they must use network storage. Orchestrators must also
    provide simple ways to allocate disk storage and mount it inside the containers
    where the microservices run. In general, they must provide easy ways of projecting
    everything that can be projected inside a container, namely:'
  prefs: []
  type: TYPE_NORMAL
- en: Disk storage
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Environment variables
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Communication ports
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As a matter of fact, each orchestrator also offers other services, but the seven
    services listed above are the starting point for learning and assessing any orchestrator.
  prefs: []
  type: TYPE_NORMAL
- en: 'The behavior of an orchestrator is controlled with tree-like settings coming
    from various sources: configuration files, command arguments, and so on. Behind
    the curtain, all sources are packaged by a client that communicates with an orchestrator
    web API.'
  prefs: []
  type: TYPE_NORMAL
- en: All possible orchestrator settings are organized like .NET configuration settings
    in a tree data structure. Therefore, analogously to .NET settings, they can be
    provided in JSON format or other equivalent formats. As a matter of fact, all
    orchestrators accept settings either in JSON or in another equivalent format called
    `.yaml`. Some orchestrators accept both formats; others might accept just one
    of them. The `.yaml` format is described in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: .yaml files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`.yaml` files, like JSON files, can be used to describe nested objects and
    collections in a human-readable way, but they do it with a different syntax. You
    have objects and lists, but object properties are not surrounded by `{}`, and
    lists are not surrounded by `[]`. Instead, nested objects are declared by simply
    indenting their content with spaces. The number of spaces can be freely chosen,
    but once they’ve been chosen, they must be used consistently.'
  prefs: []
  type: TYPE_NORMAL
- en: 'List items can be distinguished from object properties by preceding them with
    a hyphen (-). Below, there is an example involving nested objects and collections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In each line, all characters following a `#` character are considered comments.
  prefs: []
  type: TYPE_NORMAL
- en: 'The previous `Person` object has a `Spouse` nested object and a nested collection
    of addresses. The same example in JSON would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the `.yaml` syntax is more readable, since it avoids the overhead
    of parentheses.
  prefs: []
  type: TYPE_NORMAL
- en: '`.yaml` files can contain several sections, each defining a different object,
    that are separated by a line containing the `---` string. Comments are preceded
    by a # symbol, which must be repeated on each comment line.'
  prefs: []
  type: TYPE_NORMAL
- en: Since spaces/tabs contribute to object semantics, YAML is space/tabs sensitive,
    so attention must be paid to add the right number of spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Small collections or small objects can also be specified in-line with the usual
    `[]` and `{}` syntax, that is, after the colon in the same line of the property
    they are the value of.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the basics of orchestrators and `.yaml` files, we are ready to learn about
    the most widespread orchestrator: **Kubernetes**. At the moment, it is also the
    most complete. So, once you’ve learned about it, learning about other orchestrators
    should be very easy.'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Kubernetes orchestrator is distributed software that must be installed on
    all virtual servers of a network. Most of the Kubernetes software is installed
    on just some machines that are called **master nodes**, while all other machines
    run just interface software called **Kubelet** that connects with the software
    running on the master nodes and locally executes tasks decided on by the master
    nodes. All machines in a Kubernetes cluster are called **nodes**.
  prefs: []
  type: TYPE_NORMAL
- en: Actually, all nodes must also run a container runtime in order to be able to
    run containers. As we will see later on, all nodes also run software that handles
    virtual addressing.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes configuration units are abstract objects with properties, subparts,
    and references to other objects. They are referred to as **Kubernetes resources**.
    We have resources that describe a single microservice replica and other resources
    that describe a set of replicas. Resources describe communication settings, disk
    storage, users, roles, and various kinds of security constraints.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cluster nodes and all resources they host are managed by master nodes that
    communicate with human cluster administrators through an API server, as shown
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1: Kubernetes cluster](img/B31916_08_1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: Kubernetes cluster'
  prefs: []
  type: TYPE_NORMAL
- en: Kubectl is the client typically used to send commands and configuration data
    to the API server. The scheduler allocates resources to nodes according to the
    administrator constraints, while the controller manager groups several daemons
    that monitor the cluster’s actual state and try to move it toward the desired
    state declared through the API server. There are controllers for several Kubernetes
    resources, from Microservices replicas to communication facilities. In fact, each
    resource has some target objectives to be maintained while the application runs,
    and the controller verifies these objectives are actually achieved, possibly triggering
    corrective actions, such as moving some pods running too slowly onto less crowded
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: The deployment unit, that is, the unit that can be deployed on a server, started,
    killed, and/or moved to another server, is not a single container, but a set of
    containers called a **Pod**
  prefs: []
  type: TYPE_NORMAL
- en: A Pod is a set of containers that are constrained to run all together on the
    same server..
  prefs: []
  type: TYPE_NORMAL
- en: The concept of the Pod is fundamental since it enables very useful, strong cooperation
    patterns. For instance, we may attach another container to our main container
    whose unique purpose is to read the log files created by the main container and
    send them to a centralized log service.
  prefs: []
  type: TYPE_NORMAL
- en: The **Sidecar** pattern consists of enhancing a main container with a secondary
    container deployed on the same Pod and whose only purpose is to offer some services
    to the main container.
  prefs: []
  type: TYPE_NORMAL
- en: In general, we put several containers together inside the same Pod when we need
    them to communicate through their node file system, or when we need each container
    replica to be somehow associated with a specific replica of other containers.
  prefs: []
  type: TYPE_NORMAL
- en: In Kubernetes, communication between **Pods** is handled by resources called
    **Services** that are assigned virtual addresses by the Kubernetes infrastructure
    and that forward their communications to sets of pods that satisfy some constraints.
    In short, Services are Kubernetes’ way to assign constant virtual addresses to
    sets of **Pods**.
  prefs: []
  type: TYPE_NORMAL
- en: All Kubernetes resources may be assigned name-value pairs called **labels**
    that are used to reference them through a pattern-matching mechanism. Thus, for
    instance, all **Pods** that receive traffic from the same **Service** are selected
    by specifying labels that they must have in the **Service** definition.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes clusters can be on-premises, that is, Kubernetes may be installed
    on any private network. But, more often, they are offered as cloud services. For
    instance, Azure offers **Azure Kubernetes Service (AKS)**.
  prefs: []
  type: TYPE_NORMAL
- en: In the remainder of the book, we will use the **Minikube** Kubernetes simulator
    running on your development machine, since an actual AKS service might quickly
    exhaust all your Azure free credits. However, all operations in our examples can
    be replicated on an actual cluster, and whenever there are differences, we will
    also describe how to perform operations on AKS.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by interacting with a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Interacting with Kubernetes: Kubectl, Minikube, and AKS'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before interacting with a Kubernetes cluster with the Kubectl client, we must
    configure Kubectl and furnish it with both the cluster URL and the necessary credentials.
  prefs: []
  type: TYPE_NORMAL
- en: Once installed, Kubectl creates a different JSON configuration file for each
    computer user, which will contain configuration info for all Kubernetes clusters
    and their users. Kubectl has commands for inserting new Kubernetes cluster configurations
    and for making a cluster configuration the current one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each pair made of a Kubernetes cluster API URL plus a user credential is called
    a **context**. Contexts, credentials, and cluster connections can be defined with
    various `kubectl config` subcommands. Below are the most useful ones:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Viewing the overall configuration file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Adding a new Kubernetes cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'User credentials are based on client certificates. A valid certificate can
    be obtained by creating a certificate request and submitting it to the Kubernetes
    cluster, which will create an approved certificate. The detailed procedure will
    be shown in [*Chapter 10*](Chapter_10.xhtml#_idTextAnchor297)*, Security and Observability
    for Serverless and Microservices Applications*. Once you get an approved certificate,
    the user can be created with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Where `newusername.key` is the complete path to the private key you used to
    create the certificate request, and `newusername.crt` is the complete path of
    the approved certificate file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have both a server and a user, you can create a context for the connection
    of that user to that server, with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once all the contexts you need have been properly defined, you can switch to
    a given context with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After having set a new current context, all Kubectl commands will use both the
    cluster and the user defined in that context.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you are the cluster administrator, your user already exists in the system,
    so you don’t need to create it. However, you need to get the administrator user
    credentials and add them to your configuration file. Each cloud service has a
    login procedure that does this job. For instance, in the case of AKS, the procedure
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Log in to Azure with Azure CLI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The default browser should open, and you should be prompted for your Azure credentials.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If not already installed, install the package for interacting with AKS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Ask to add your AKS credentials to your Kubectl configuration file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If the command is successful, a new cluster, new user, and new context will
    be added to your Kubectl configuration, and the new context will be made the current
    one. Please run `kubectl config view` to see all configuration file modifications.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Minikube comes with a default user, a default cluster name, and a default context,
    which are all called `minikube`. When you start your Minikube cluster with `minikube
    start`, if not already defined, all the above entities will be added to your Kubectl
    configuration file. Moreover, the `minikube` context will be automatically made
    the current one, so no extra actions are needed after you start your cluster.
    Of course, you may define other users and other contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Minikube can be stopped with `minikube stop`, and paused with `minikube pause`.
    Both stopping and pausing do not delete the cluster data and configuration. Other
    useful commands will be shown later on while using Minikube in our examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try some Kubectl commands on Minikube (ensure Minikube has been started):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'It should show all virtual network Kubernetes nodes. As the default, Minikube
    creates a cluster with a single node called `minikube`, so you should see something
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Since we specified Docker as the virtualization tool, the whole cluster will
    be embedded in a Docker container, as you can verify by listing all running containers
    with `docker ps` (remember that all Docker commands must be issued in a Linux
    shell).
  prefs: []
  type: TYPE_NORMAL
- en: 'As the default, this unique node contains 2 CPUs and 4 gigabytes of RAM, but
    we can modify all these parameters, and we can also create clusters with several
    nodes by passing some options to `minikube start`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--nodes <n>`: Specifies the number of nodes in the cluster. Please consider
    that nodes are virtual machines that will run simultaneously, so a large number
    of nodes can be set only on a powerful workstation with several cores and say
    32-64 gigabytes of RAM. The default is 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--cpus <n or no-limits>`: The number of CPUs allocated to Kubernetes, or `no-limits`,
    to let Minikube allocate as many CPUs as needed. The default is 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--memory <string>`: The amount of RAM to be allocated to Kubernetes (format:
    <number>[<unit>], where unit = b, k, m, or g). Use “max” to use the maximum amount
    of memory. Use “no-limit” to not specify a limit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--profile <string>`: The name of the Minikube virtual machine (defaults to
    `minikube`). Useful for having more than one Minikube virtual machine – for instance,
    one with one node and another with two nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--disk-size <string>`: The disk size allocated to the Minikube VM (format:
    <number>[<unit>], where unit = b, k, m, or g). The default is “20000mb”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to change one of the above settings after having created the Minikube
    container with your first `minikube start`, you need either to delete the previous
    container with `minikube delete` or create a new Minikube container with a custom
    name with the `--profile` option.
  prefs: []
  type: TYPE_NORMAL
- en: 'After this short parenthesis, let’s return to Kubectl! Let’s type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'It lists all Kubernetes resources. If you have not created any resources, the
    cluster should contain just a single resource of type ClusterIP, as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: It is part of the Kubernetes infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: In general, `kubectl get <resource type>` lists all resources of a given type.
    Thus, for instance, `kubectl get pods` lists all Pods, and `kubectl get services`
    lists all services.
  prefs: []
  type: TYPE_NORMAL
- en: 'If, instead, we need more detailed information on a given object, we may use
    `kubectl describe <object type> <object name>`. Thus, for instance, if we need
    more information on the Minikube single node called `minikube`, we may issue the
    command below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Please try it!
  prefs: []
  type: TYPE_NORMAL
- en: You will see other Kubectl commands when learning how to define Pods, Services,
    and other Kubernetes resources in other sections of this chapter. The next subsection
    explains how to create an Azure Kubernetes cluster, so if at the moment you don’t
    plan to use Azure Kubernetes, feel free to skip it. You can return to it when
    you need to create one.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an Azure Kubernetes cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To create an AKS cluster, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Type `AKS` into the Azure search box.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Kubernetes services**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then click the **Create** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Kubernetes Cluster**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After that, the following form will appear:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2: AKS creation first form](img/B31916_08_2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: AKS creation first form'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, as usual, you can select one of your Azure subscriptions, an existing
    resource group, or you can create a new one. Let’s move on to the AKS-specific
    configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cluster preset configuration**: Here, you can choose among various preconfigured
    settings that are a good starting point for various situations. In the preceding
    screenshot, I have chosen **Dev/Test**, which is specific for development and
    learning, so it proposes the cheapest options. However, you can also select a
    standard production or an economic production initial setting.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Kubernetes cluster name**: Here, you must select a unique name for your cluster.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For all other settings, you can choose the proposed defaults. In particular,
    the **Region** field should propose the most adequate region for you. **AKS pricing
    tier** should be set to **Free**, meaning you will pay just for the virtual machines
    that make up the cluster. However, you can also select paying options that include
    support and super-big clusters with up to 5,000 nodes. The **Availability zones**
    field enables geographic redundancy in up to 3 different geographic zones.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you selected **Dev/Test**, the cluster will include from 2 to 5 nodes with
    automatic scaling. That is, the number of starting nodes is 2, but it can automatically
    increase up to 5 if the workload increases. Let’s go to the **node pools** tab
    to customize both the node number and type:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3: AKS node pool configuration](img/B31916_08_3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: AKS node pool configuration'
  prefs: []
  type: TYPE_NORMAL
- en: If you selected **Dev/Test**, there should be a unique node pool that will be
    used for both Kubernetes master nodes and standard nodes. Pay attention that the
    **Dev/Test** server type (D4ds-v5) has a high monthly price, so please use the
    price calculator ([https://azure.microsoft.com/en-us/pricing/details/virtual-machines/linux/#pricing](https://azure.microsoft.com/en-us/pricing/details/virtual-machines/linux/#pricing))
    to verify the cost of a machine before choosing it.
  prefs: []
  type: TYPE_NORMAL
- en: The standard production selection, instead, would create two node pools – one
    for master nodes and the other for standard nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Anyway, you can change the node pools and edit each of them. In the case of
    the preceding screenshot, let’s click on **agentpool**. A new form will open.
    Here, you can change both the machine type and the maximum number of nodes. A
    good option for experimenting without wasting too much credit is choosing 3 nodes
    and an `A` family machine. When you have done either, click on update or on cancel
    to return to the previous form.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, you can associate Azure Container Registry with the cluster by going
    to the **Integrations** tab:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4: Connect your cluster to ACR](img/B31916_08_4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4: Connect your cluster to ACR'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you already defined an Azure Container Registry for experimenting in the
    *A few more Docker commands and options* subsection of [*Chapter 3*](Chapter_3.xhtml#_idTextAnchor067)*,
    Setup and Theory: Docker and Onion Architecture*, select that registry; otherwise,
    you can create a new one in a new browser window and select it, or you can associate
    a registry to your cluster at a later time.'
  prefs: []
  type: TYPE_NORMAL
- en: When you associate a registry to your cluster, you enable the cluster to access
    and download all its Docker images.
  prefs: []
  type: TYPE_NORMAL
- en: When you’ve finished, select **Review + Create**.
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve created your cluster, you can connect to it with the login procedure
    we explained earlier in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have learned how to connect with both Minikube and AKS, let’s move
    on to experimenting with Kubernetes resources.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring your application in Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As already mentioned, the simplest Kubernetes resource is the Pod. We will never
    create a single Pod since we will always create several replicas of each microservice,
    but being able to configure a Pod is also fundamental for creating more complex
    resources, so let’s start creating a single Pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Pod can be defined through a `.yaml` file with the content below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: All Kubernetes configuration files start with the name of the API where the
    resources being configured are defined, and its version. In the case of Pods,
    we have just the version since they are defined in the **core API**. Then, `kind`
    defines the type of resource to be configured – in our case, a Pod.
  prefs: []
  type: TYPE_NORMAL
- en: Like types in C#, Kubernetes resources are also organized in namespaces. Therefore,
    together with any resource name, we must also specify a namespace. If no namespace
    is specified, a namespace called `default` is assumed.
  prefs: []
  type: TYPE_NORMAL
- en: Pay attention! While the intent of Kubernetes and C# namespaces is the same,
    there are substantial differences between them. Namely, C# namespaces are hierarchical,
    while Kubernetes namespaces are not. Moreover, namespaces are not applicable to
    all Kubernetes resources since there are cluster-wide resources that belong to
    no specific namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the namespace used in a resource definition doesn’t exist yet, it must be
    defined with the snippet below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Name and namespace are specified as sub-properties of `metadata`, together with
    optional l`abels`. Labels are free name-value pairs we can use to classify the
    object. Typically, they specify information such as the role of the resource in
    the application and the tier or module it belongs to.
  prefs: []
  type: TYPE_NORMAL
- en: As already mentioned in the previous section, other resources can use labels
    to select a set of resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `spec` property specifies the actual content of the Pod, that is, its containers
    and its restart policy (`restartPolicy`). The restart policy specifies when to
    restart a Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '`restartPolicy: Always`: This is the default. The Pod is restarted whenever
    all containers terminate or a container terminates with a failure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`restartPolicy: OnFailure`: The Pod is restarted when at least one container
    exits with a failure'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`restartPolicy: Never`: The Pod is never restarted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Containers are split into two lists: `containers` and `initContainers`. The
    containers in the `containers` list are started only after all containers in `initContainers`
    are **successful**, and each container in the `initContainers` list is started
    only after the previous container is **successful**. In turn, a container in the
    `initContainers` list is considered **successful** in the two circumstances:'
  prefs: []
  type: TYPE_NORMAL
- en: If a container configuration has the `restartPolicy` property set to `Always`,
    then the container is considered successful if it has been successfully started.
    This option is useful for implementing **sidecar** containers. This way, we ensure
    that **sidecars** are ready before the containers they enhance are started. Please
    refer to the Pod definition at the beginning of the *Kubernetes basics* section
    for an explanation of what a **sidecar** is.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If a container configuration doesn’t have the `restartPolicy` property set to
    `Always`, then the container is considered successful if it is successfully terminated.
    This option is useful for performing some startup initialization – for instance,
    for waiting for a database or a message broker to be ready. In a similar situation,
    the container code is a loop that continuously tries a connection with the database/message
    broker, and terminates as soon as it succeeds.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A failed `initContainers` doesn’t cause a whole Pod restart. Instead, it is
    retried with an exponential retry several times before causing a whole Pod failure.
    For this reason, they should be designed as idempotent since their actions might
    be executed more than once.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each container in any of the two above lists is something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We specify both a name for the container and the URL of its image in a container
    registry, which accounts for point 4 of the minimal services any orchestrator
    should offer (see the beginning of the *Introduction to orchestrators and their
    configuration* section). These two properties are obligatory, while all other
    properties are optional. The `command` property, when provided, overwrites the
    `CMD` instruction of the image Docker file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we also account for points 5, 6, and 7 of the minimal services any orchestrator
    should offer, that is, disk storage, environment variables, and communication
    ports. More specifically, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '`volumeMount` specifies how a virtual storage volume specified by `name` is
    mapped to the path specified by `mountPath` in the container file system. If the
    optional `subPath` is provided, just that `subpath` of the volume specified by
    `name` is mounted. Virtual storage volumes are described later on in this chapter
    (in the *Dynamic provisioning of permanent disk space* subsection), together with
    other `volumeMounts` properties.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`env` specifies all container’s environment variables as a list of `name-value`
    pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ports` specifies the list of all ports exposed by the container we would like
    to use in our application. These ports may be mapped to other ports in the actual
    communication between Pods. However, the port mapping is specified in other resources
    called `services` that provide virtual Pod addresses and other communication-related
    options.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the `resource`s section specifies both the minimal computational resources
    needed for starting the container (`requests`) and the maximum computational resources
    it can waste (`limits`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The constraints in the `requests` property are used to choose the virtual machine
    to place a Pod. `limits`, instead, are enforced by the operating system kernel
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: CPU limits are enforced with throttling. That is, containers exceeding the CPU
    limit are delayed, putting them in sleeping mode for enough time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory limits are enforced by throwing an exception when they are exceeded.
    In turn, the exception causes the application of the Pod restart policy, which
    usually causes a Pod restart.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With regard to units of measure, typical memory units of measure are `Ti` (terabytes),
    `Gi` (gigabytes), `Mi` (megabytes), and `Ki` (kilobytes). CPU time, instead, can
    be measured either in millicores (`mi`) or as a fractional number of cores (no
    unit of measure after the value).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try a Pod with a sidecar container, which shows both the practical usage
    of the described syntax and how a sidecar can help in building application-level
    monitoring. The main container will be a fake microservice based on the Alpine
    Linux distribution Docker image, which just puts log messages in a file located
    in a directory shared with the sidecar. In an actual application, the log would
    be organized in several files (for instance, one for each day), and old files
    would be periodically deleted. Moreover, the sidecar would read these files and
    send their content to a log API. Our didactical sidecar, instead, will just periodically
    read the last 10 rows of the file and will display them in its console.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is quite simple. First of all, we define a namespace that encloses
    our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, after a `---` row, we place the actual Pod definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Both containers use a simple Alpine Linux distribution Docker image and confine
    the application-specific code in the `command`, which is a Linux script. This
    technique is used for adapting preexisting images or for very simple tasks such
    as the ones often performed by a sidecar. We also used the same technique for
    the main container because the main container does nothing and has a purely didactical
    purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'Accordingly, with the previously exposed syntax, the sidecar is defined in
    the `initContaines` list with `restartPolicy: Always`.'
  prefs: []
  type: TYPE_NORMAL
- en: The main container command executes an endless loop where it just writes the
    current date and time in the `/opt/logs.txt` file and then sleeps for one second.
  prefs: []
  type: TYPE_NORMAL
- en: The sidecar container command uses `sh -c` to execute a single shell command,
    the `tail` command with the `-f` option on the `/opt/logs.txt` file. This command
    shows the last 10 rows of the file in the container console and updates them whenever
    new rows are added, so that the console always contains the current last 10 rows
    of the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The file processed by both containers is the same because both containers mount
    the same data volume in the same `/opt` directory on their filesystems with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The data volume is defined in a `volumes` list that is a direct descendant
    of the `spec` property, as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '`emptyDir` defines and allocates a volume that is specific to the Pod where
    it is defined. This means that it can’t be accessed by any other Pod. The volume
    is implemented with the disk memory of the node that hosts the Pod. This means
    that if the Pod is deleted or moved to a different node, the volume is destroyed
    and its content is lost. `EmptyDir` is the preferred way to provide temporary
    disk storage that’s used somehow in the Pod computations. It has an optional `sizeLimit`
    property that specifies a maximum disk space the Pod can use. For instance, we
    can set `sizeLimit: 500Mi` to specify 500 mega of maximum disk space.'
  prefs: []
  type: TYPE_NORMAL
- en: Since we have not specified any size limit, the `emptyDir` object has no properties,
    so we are forced to add the empty object value `{}` to get a correct `.yaml` syntax
    (we can’t have a colon followed by nothing).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s create a folder for experimenting with `.yaml` files in Minikube, and
    let’s place the whole example code in a file called `SimplePOD.yaml` inside that
    folder. This file is also available in the `ch08` folder of the book’s GitHub
    repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, right-click on the newly created folder and open a Windows console in
    that directory. After having verified that Minikube is started by issuing a `kubectl
    get all` command, we can apply all our definitions with the `kubectl apply` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if we issue the `kubectl get pods` command, we don’t see a new Pod! This
    is right because that command just lists resources defined in the `default` namespace,
    while our Pod has been defined in a new namespace called `basic-examples`, so
    if we would like to operate on a resource in this namespace, we must add the `-n
    basic-examples` option to our commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to access our sidecar console, we can use the Kubectl `logs` command.
    In fact, all console output of all container Pods is automatically collected by
    Kubernetes and can be inspected with this command. The command needs the Pod name
    and its namespace if different from `default`. Moreover, if the Pod contains several
    containers, it also needs the name of the container we would like to inspect,
    which can be provided with the `-c` option. Summing up, our command is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The command above will show just the current console content and then it will
    exit. If we would like the content to update automatically as the console content
    changes, we must add the `-f` option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This way, our window freezes on the command and automatically updates. The command
    can be exited with `ctrl-c`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also have a console into the `logshipper` container with the Kubectl
    `exec` command. It needs namespace, Pod, and container names, and after the `–`
    characters, the Linux command to execute in the container file system. If you
    need a console, the Linux command is `sh`, and if we would like to interact with
    that console, we need to also specify the `-it` options that stand for “interactive
    tty.” Summing up, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Once in the container, we can move into the `/opt` directory with `cd /opt`,
    and verify if the `logs.txt` file is there, with `ls`.
  prefs: []
  type: TYPE_NORMAL
- en: Once finished, you can exit the container console by issuing the `exit` command.
  prefs: []
  type: TYPE_NORMAL
- en: The `kubectl exec` command is very useful for debugging applications, especially
    when they are already in production or staging.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you have finished with all resources created by a `.yaml` file, you can
    delete all of them with `kubectl deleted <file name>.yaml`. Thus, in our case,
    we can destroy all our example entities with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '`kubectl apply` can also be used for modifying previously created resources.
    It is enough to edit the `.yaml` file used to create the resources and then repeat
    the `apply` command on it.'
  prefs: []
  type: TYPE_NORMAL
- en: We have seen how to create temporary disk space with `emptyDir`. Now let’s see
    the typical way of allocating permanent network disk space and sharing it between
    various Pods.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic provisioning of permanent disk space
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Volume definitions similar to `emptyDir` are called in-tree definitions because
    the instruction that creates the volume is inserted directly into the Pod definition.
    There is no way to share an in-tree definition with other Pod definitions, so
    it is not easy to share in-tree volumes between different Pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Actually, disk space sharing can also be achieved with in-tree definitions
    by adequately configuring the device that provides the disk space. For instance,
    suppose we are using an NFS server connected to our Kubernetes cluster to furnish
    network disk space. We can connect a Pod with it with the instruction below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Where `server` is a server name or an IP address, and path is the directory
    to share. In order to share the same disk space between PodS, it is enough that
    they specify the same server and path.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this technique has two cons:'
  prefs: []
  type: TYPE_NORMAL
- en: The share is not explicitly declared, but it is indirect, thus it undermines
    code maintainability and readability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes is not informed about the Pods that are using a share, so it can’t
    be instructed to release the share when it is not needed anymore.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Therefore, in-tree definitions are more adequate for temporary disk space that
    is not shared among Pods. Luckily, the problem is not the NFS protocol itself,
    but just the in-tree syntax. For this reason, Kubernetes also offers an out-of-tree
    syntax based on two separate objects: **Persistent Volume Claims** (**PVCs**),
    which represent disk space needs, and **Persistent Volumes** (**PVs**), which
    represent actual disk space.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The whole technique works this way:'
  prefs: []
  type: TYPE_NORMAL
- en: We define the disk space specification in a PVC.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All Pods that need to share the same disk space reference the same PVC.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kubernetes, somehow, tries to satisfy each PVC with a compatible PV that is
    then mounted on all Pods sharing that PVC.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When all Pods that share the same PV are destroyed, we can instruct Kubernetes
    to keep the allocated disk space or delete it.
  prefs: []
  type: TYPE_NORMAL
- en: The way a PVC catches the needed disk and returns a PV depends on the driver
    used to serve the PVC. Drivers must be installed in the Kubernetes cluster, but
    all cloud providers furnish predefined drivers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Driver names and related settings are organized in resources called **Storage
    Classes** (`kind: StorageClass`). Together with predefined drivers, all cloud
    providers also offer predefined storage classes based on those drivers. However,
    you can define new storage classes based on the same driver but with different
    settings.'
  prefs: []
  type: TYPE_NORMAL
- en: You can also install drivers and storage classes based on those drivers on on-premises
    Kubernetes clusters (there are a lot of open-source drivers). Minikube has addons
    that install various storage drivers and related storage classes, too.
  prefs: []
  type: TYPE_NORMAL
- en: Drivers that simply match PVCs with PVs that are manually predefined by the
    user are called static. While drivers that dynamically create PV resources, taking
    the needed disk space from a common pool of available disk space, are called dynamic.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will focus just on dynamic storage allocation since it
    is the most relevant in actual microservices applications. You may find more details
    on storage classes and how to define them in the official Kubernetes documentation:
    [https://kubernetes.io/docs/concepts/storage/storage-classes/](https://kubernetes.io/docs/concepts/storage/storage-classes/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step in creating a PVC is the verification of the available storage
    classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Then the details of a specific class can be obtained with `kubectl describe`.
    In Minikube, we obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The “default” after the class name informs us that the `standard` class is the
    default storage class, that is, the one used when no storage class is specified.
  prefs: []
  type: TYPE_NORMAL
- en: 'When using dynamic provisioning, a PVC needs to specify just:'
  prefs: []
  type: TYPE_NORMAL
- en: The storage needed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The storage class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The access modality: `ReadWriteOnce` (only a single node can read and write
    on the storage), `ReadOnlyMany` (several nodes can read), `ReadWriteMany` (several
    nodes can both read and write), `ReadWriteOncePod` (only a single Pod can read
    and write on the storage)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In fact, all the information needed to get a PV is contained in the storage
    class. Since a PVC describes a Pod need and not a specific PV, the provisioned
    storage will provide at least the required access mode, but it can support more
    accesses, too.
  prefs: []
  type: TYPE_NORMAL
- en: If the driver used by the storage class doesn’t support the required modality,
    the operation fails. Therefore, before using a storage class, you must verify
    the operations supported by its driver. `ReadOnlyMany` doesn’t make sense with
    dynamic provisioning, since allocated storage always comes clean, so there is
    nothing to read.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, drivers that support dynamic provisioning always support `ReadWriteOnce`,
    and some of them also support `ReadWriteMany`. Therefore, if you need a volume
    that is shared among several Pods, you must verify that the chosen driver supports
    `ReadWriteMany`; otherwise, all Pods that share the volume will be allocated on
    the same node to ensure that all of them can access the claimed `ReadWriteOnce`
    storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'A PVC is defined as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The needed storage is specified with the same syntax as the RAM required by
    a container. If the storage class is not provided, Kubernetes uses a storage class
    that has been marked as the default storage class, if any.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you’ve defined a PVC, the volume property of the Pod needs to reference
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: However, the PVC and Pod must belong to the same namespace; otherwise, the operation
    fails.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have all the building blocks, we can move on to more complex resources
    built on top of these blocks. Single Pods are not useful since we always need
    several replicas of each microservice, but luckily, Kubernetes already has built-in
    resources for handling both undistinguishable replicas and indexed replicas useful
    for implementing sharding strategies.
  prefs: []
  type: TYPE_NORMAL
- en: ReplicaSets, Deployments, and their services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**ReplicaSets** are resources that automatically create N replicas of a Pod.
    However, they are rarely used because it is more convenient to use **Deployments**,
    which are built on top of ReplicaSets and automatically handle a smooth transition
    when the number of replicas or other parameters are modified.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The definition of a Deployment is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Deployments are not contained in the API core, so their API name (`apps`) must
    be specified. The metadata section is identical to that of a Pod. The `spec` section
    contains the desired number of replicas (`replicas`) and a selector that specifies
    a condition for a Pod to belong to the deployment: it must have all labels with
    the specified values.'
  prefs: []
  type: TYPE_NORMAL
- en: '`template` specifies how to create a Pod for the Deployment. If the cluster
    already contains some Pods that satisfy the selector conditions, then the template
    is used to create just the Pods needed to reach the `replicas` target number.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The template is a complete Pod definition whose syntax is identical to the
    one we use for specifying a single Pod. The only differences being:'
  prefs: []
  type: TYPE_NORMAL
- en: The Pod definition is not preceded by any API specification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Pod metadata section doesn’t contain a Pod name, since we are providing
    a template for creating `replica` Pods. Pod names are automatically created by
    the Deployment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Pod metadata section doesn’t contain a Pod namespace since Pods inherit
    the same namespace as the Deployment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Needless to say, the Pod template must specify labels that match the selector
    `conditions`. Below is a complete example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The Deployment creates two replicas of an **nginx** web server that share a
    common disk space. More specifically, they share the `/usr/share/nginx/html` path
    that is mapped to a common PVC. `/usr/share/nginx/html` is the folder where **nginx**
    looks for static web content, so if we place an `index.html` file there, it should
    be accessible by both web servers.
  prefs: []
  type: TYPE_NORMAL
- en: The code above implements two load-balanced web servers that serve the same
    content. Let’s place the Deployment in a `WebServers.yaml` file. We will use it
    in a short while, after having added the missing code, that is, the PVC definition
    and a Service that forwards traffic from outside of the Kubernetes cluster and
    load-balances it among the replicas.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deployments can be connected to three kinds of services:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ClusterIP**, which forwards traffic from inside the network to the Deployment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LoadBalancer**, which forwards traffic from outside of the cluster to the
    Deployment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NodePort,** which is not fundamental for application developers and will
    not be described'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The definition of a **ClusterIP** is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '`selector` defines the Pods that will receive the traffic from the service.
    The Pods must belong to the same namespace as the service. The `ports` list defines
    the mapping from external ports (`port`) to the ports inside the Pod containers
    (`targetPort`). Each map can also specify an optional name and an optional protocol.
    If no protocol is specified, all protocols will be forwarded to the Pods.'
  prefs: []
  type: TYPE_NORMAL
- en: A **ClusterIP** service is assigned the `<service name>.<namespace>.svc.cluster.local`
    domain name, but it can also be accessed with `<service name>.<namespace>` (or
    simply `<service name>` if the namespace is `default`).
  prefs: []
  type: TYPE_NORMAL
- en: Summing up, all traffic sent to either `<service name>.<namespace>.svc.cluster.local`
    or to `<service name>.<namespace>` is forwarded to the Pods selected by the `selector`.
  prefs: []
  type: TYPE_NORMAL
- en: 'A **LoadBalance**r service is completely analogous, the only difference being
    the two sub-properties of `spec` below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'If you specify an IP address, that IP address must be a static IP address you
    bought somehow; otherwise, in the case of cloud Kubernetes clusters, you can omit
    the `loadBalancerIP` property and a dynamic IP address is automatically assigned
    to the service by the infrastructure. In AKS, you must also specify the resource
    group where the IP address has been allocated in an annotation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Moreover, you must give the “Network Contributor” role on the resource group
    where you defined the static IP address to the managed identity associated to
    the AKS cluster (as a default, a managed identity is automatically assigned to
    any newly created AKS cluster). See the detailed procedure for performing this
    operation here: [https://learn.microsoft.com/en-us/azure/aks/static-ip](https://learn.microsoft.com/en-us/azure/aks/static-ip).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also specify an annotation with a label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: In which case, Azure will automatically associate the `<label>.<location>.cloudapp.azure.com`
    domain name to the LoadBalancer.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to publish the service on a custom domain name, you need to buy
    a domain name, and then you need to create an Azure DNS zone with appropriate
    DNS records. However, in this case, it is better to use an Ingress instead of
    a simple LoadBalancer (see the *Ingresses* subsection).
  prefs: []
  type: TYPE_NORMAL
- en: 'The loadBalancerIP property has been declared obsolete and will be removed
    in future Kubernetes versions. It should be replaced by a platform-dependent annotation.
    In the case of AKS, the annotation is: `service.beta.kubernetes.io/azure-pip-name:
    <your static IP address>`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go back to our nginx example and let’s create a LoadBalancer Service
    to expose our load-balanced web servers on the internet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: We don’t specify an IP address since we are going to test the example in Minikube,
    a simulator that uses a particular procedure to expose LoadBalancer Services.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s place the Service definition in a file named `WebServersService.yaml`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a `WebServersPVC.yaml` file, let’s also place the missing PVC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: We have not specified a storage class because we will use the default one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s also create a `BasicExamples.yaml` file for defining the `basic-examples`
    namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Now let’s copy the `index.html` file contained in the `ch08` folder of the book’s
    GitHub repository, or any other self-contained HTML page with no external references
    to other images/content, in the same folder containing all the above `.yaml` files.
    We will use that page as experimental content to be shown by the web servers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start our experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a console on the folder containing all `.yaml` files (right-click on the
    folder and select the console option).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ensure Minikube is running, and if not, start it with `minikube start`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy all files in the right sequence, that is, ensuring that all resources
    referenced in a file have already been created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we need to copy the `index.html` files in the `/usr/share/nginx/html` folder
    of either of the two created Pods. It will also be seen by the other Pod, since
    they share the same disk storage. For this operation, we need a Pod name. Let’s
    get it with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A file can be copied in a Kubernetes Pod with the `kubectl cp` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In our case, the `cp` command becomes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In Minikube, you can access the cluster through a LoadBalancer service by creating
    a tunnel. Do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open a new console window
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: In this new window, issue the `minikube tunnel` command
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The window will freeze on the command. As long as the window remains open, the
    `LoadBalancer` is accessible through `localhost`. Anyway, you can verify the external
    IP assigned to the LoadBalancer by issuing `kubectl get services -n Basic-Examples`
    in the previous window.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Open your favourite browser and go to `http://localhost`. You should see the
    content of the `index.html` page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once you’ve finished experimenting, let’s destroy all resources in reverse
    order (the opposite order in which you created them):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: You can keep the namespace definition since we will use it in the next example.
  prefs: []
  type: TYPE_NORMAL
- en: All Deployment replicas are identical; they have no identity, so there is no
    way to refer to a specific replica from your code. If a replica goes down, for
    instance, because of a node crash, the system might have a small performance issue,
    but will continue working properly since replicas are just a way to improve performance,
    so no replica is indispensable.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth pointing out that as soon as Kubernetes detects a node fault, it
    recreates all Pods hosted on that node elsewhere. However, this operation might
    take time since the fault might not be detected as soon as it takes place. In
    the meantime, applications might have malfunctions if a Pod hosted by the faulty
    node is indispensable, which is why Deployments must be preferred whenever possible.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, there are situations where identical copies can’t achieve the
    needed parallelism, but we need non-identical sharded copies. If you don’t remember
    what sharding is and why it is necessary in some situations, please refer to the
    *Ensuring that messages are processed in proper order* section of [*Chapter 7*](Chapter_7.xhtml#_idTextAnchor151)*,
    Microservices in Practice*. **StatefulSets** furnish the kind of replication needed
    for sharding.
  prefs: []
  type: TYPE_NORMAL
- en: StatefulSets and Headless Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All replicas of a `StatefulSet` are assigned indexes that go from 0 to N-1,
    where N is the number of replicas. Their Pod names are predictable, too, since
    they are built as `<StatefulSet name>-<replica index>`. Their domain names also
    contain the Pod names, so that each Pod has its own domain name: `<POD name>.<service
    name>.<namespace>.svc.cluster.local`, or simply `<POD name>.<service name>.<namespace>`.'
  prefs: []
  type: TYPE_NORMAL
- en: When a StatefulSet is created, all replicas are created in order of increasing
    index; while when it is destroyed, all replicas are destroyed in decreasing index
    order. The same happens when the number of replicas is changed.
  prefs: []
  type: TYPE_NORMAL
- en: Each `StatefulSet` must have an associated Service that must be declared in
    the `serviceName` property of the `StatefulSet`. The definition of a `StatefulSet`
    is almost identical to that of a `Deployment`; the only difference being that
    `kind` is `StatefulSet` and there is the `serviceName:”<service name>“` property
    immediately under the `spec` section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The service associated to `StatefulSet` must be a so-called `Headless` service,
    which is defined as a ClusterIP service but with a `ClusterIP: None` property
    under `spec`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'It is also worth pointing out that, typically, each replica has its own private
    storage, so, usually, StatefulSet definitions do not have a reference to a PVC,
    but instead use a PVC template that attaches a different PVC to each created Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Where both the `metadata` and `spec` properties are identical to those of a
    PVC resource.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below is an example of a StatefulSet with its associated Headless Service.
    The Pod name is passed to each container through an environment variable, so that
    the code is aware of its index and its possible role in a sharding algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Each Pod contains just the Alpine Linux distribution, and the actual code is
    provided in `command`, which just prints the `MY_POD_NAME` environment variable
    in an endless loop. In turn, the `MY_POD_NAME` environment variable is set with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: This code takes the value from the `metadata.name` field of the Pod. In fact,
    if we did not specify a name in the Pod template metadata section, a name would
    automatically be created by the StatefulSet and added to the resource internal
    representation of the Pod. The Kubernetes component that makes the Pod fields
    available to environment variables definition is called the **downward API**.
  prefs: []
  type: TYPE_NORMAL
- en: The above StatefulSet does nothing useful but just shows how to pass the Pod
    name to your containers.
  prefs: []
  type: TYPE_NORMAL
- en: Put the above code in a `StateFulSetExample.yaml` file and apply it!
  prefs: []
  type: TYPE_NORMAL
- en: 'If you issue the `kubectl get pods -n basic-examples` command, you can verify
    that all 3 replicas were created with the right names based on the StatefulSet
    name and on your indexes. Now let’s verify that `podname-1` correctly received
    its name, by displaying its log:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: You should see several lines with the right Pod name. Great!
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s verify that our code created 3 different PVCs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: You should see three different claims.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you finish experimenting with the example, you can delete everything with
    `kubectl delete -f StateFulSetExample.yaml`. Unluckily, deleting everything does
    not also delete the PVC created by templates, as you can verify at this point.
    The simplest way to delete them is by deleting the `basic-examples` namespace
    with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, if you want, you can recreate it with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Statefulsets are used to deploy RabbitMQ clusters and database clusters in Kubernetes.
    If a master node is needed, then one with a specific index (usually 0) elects
    itself as a master. Each replica uses its own disk storage so that both data sharding
    and data replication strategies can be enforced. It’s likely that you won’t need
    to do this yourself, since the code for deploying clusters of the most famous
    message-broker and database clusters is already available on the web.
  prefs: []
  type: TYPE_NORMAL
- en: Having learned how to create and maintain several replicas of a microservice,
    we have to learn how to set and update the number of replicas, that is, how to
    scale our microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling and autoscaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scaling is fundamental for application performance tuning. We must distinguish
    between scaling the number of replicas of each microservice and scaling the number
    of nodes of the whole Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The number of nodes is usually tuned according to the average CPU busy percentage.
    For instance, one might start with a 50% percentage when the initial application
    traffic is low. Then, as the application traffic increases, we maintain the same
    number of nodes till we are able to keep a good response time, possibly tuning
    the number of microservice replicas. Suppose that performance starts to decrease
    when the CPU busy percentage is 80%. Then, we can target, say, a 75% CPU busy
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Automatic cluster scaling is possible just with cloud clusters, and each cloud
    provider offers some kind of autoscaling.
  prefs: []
  type: TYPE_NORMAL
- en: With regard to AKS, in the *Creating an Azure Kubernetes cluster* section, we
    saw that we can specify both a minimum and a maximum number of nodes, and AKS
    tries to optimize performance for us. You can also fine-tune how AKS decides the
    number of nodes. More details on this customization are given in the references
    in the *Further reading* section.
  prefs: []
  type: TYPE_NORMAL
- en: There are also automatic auto-scalers that integrate with various cloud providers
    ([https://kubernetes.io/docs/concepts/cluster-administration/cluster-autoscaling/](https://kubernetes.io/docs/concepts/cluster-administration/cluster-autoscaling/)).
    As a default, auto-scalers increase the number of nodes when Kubernetes is not
    able to satisfy the resources required by a Pod, which is the sum of the `resource->request`
    fields of all Pod containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scaling microservice replicas, instead, is a more difficult task. You may calculate
    it by measuring the average replica response time and then calculating:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Where the target throughput should be a raw estimate calculated with simple
    calculations. For frontend microservices, it is just the number of requests you
    expect your application will receive for each API call. For Worker services, it
    can depend on the number of requests expected on several frontend services, but
    there is no standard way to compute it. Instead, you need to reason about how
    the application works and how the requests directed to that Worker microservice
    are created.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, you should monitor the system performance, looking for bottlenecks, according
    to the following procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: Look for a microservice that is a bottleneck
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Increase its number of replicas till it stops being a bottleneck
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat point 1 till there are no evident bottlenecks
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then optimize the number of cluster nodes to achieve good performance
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Store the average CPU utilization memory occupation of all Deployments and StatefulSets,
    and the average number of requests reaching the whole application. You may use
    this data for setting auto-scalers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While StatefulSets are difficult to scale automatically, Deployments can be
    automatically scaled without causing problems. Therefore, you may use a Kubernetes
    Pod auto-scaler to scale them automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Pod auto-scaler targets are either average per Pod resource consumption or metrics
    somehow connected with the traffic. In the first case, the auto-scaler chooses
    the number of replicas that makes the resource consumption closest to a specified
    target. In the second case, the number of replicas is set to the actual value
    of the traffic metric divided by the target value of the metric, that is, the
    traffic target is interpreted as the target traffic sustained by each Deployment
    Pod.
  prefs: []
  type: TYPE_NORMAL
- en: If several target types are specified, the maximum number of replicas proposed
    by each of them is taken.
  prefs: []
  type: TYPE_NORMAL
- en: 'An auto-scaler can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: We specify the type of resource to control and the API where it is defined,
    and its name. Both the controlled resource and the auto-scaler must be defined
    in the same namespace. You can set `scaleTargetRef->kind` also to `StatefulSet`,
    but you need to verify that the change in the number of replicas doesn’t break
    your sharding algorithm, both in the long run and during transitions between different
    numbers of replicas.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we specify the maximum and minimum number of replicas. If the computed
    number of replicas exceeds this interval, it is cut to either `minReplicas` or
    `maxReplicas`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we have the list of criteria, where each criterion may refer to three
    types of **metrics**: `resource`, `pod`, or `object`. We will describe each of
    them in a separate subsection.'
  prefs: []
  type: TYPE_NORMAL
- en: Resource metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Resource metrics are based on the average memory and CPU resources wasted by
    each Pod. The target consumption may be an absolute value such as 100Mb, or 20mi
    (millicores), in which case the number of replicas is computed as `<actual average
    consumption>/<target consumption>`. Resource metrics based on absolute values
    are declared as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The target can also be specified as a percentage of the total Pod `resource->request`
    declared (sum of all Pod containers). In this case, Kubernetes first computes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Then, the number of replicas is computed as `<utilization>/<target utilization>`.
    For instance, if the target CPU utilization is 50 on average, each Pod must waste
    50% of the CPU millicores declared in the request. Therefore, if the average CPU
    wasted by all Pods of a Deployment is 30Mi, while the CPU required by each Pod
    is 20mi, we compute the utilization as 100*30/20= 150\. So, the number of replicas
    is 150/50 = 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the code is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Pod metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Pod metrics are not standard but depend on the metrics actually computed by
    each specific cloud platform or on-premise installation. Pod metric constraints
    are declared as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Where we suppose that the `packets-per-second` metric exists in the platform
    and computes the average communication packets received per second by a Pod. The
    calculation of the number of replicas is done as in the case of `averageValue`
    for resource metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Object metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Object metrics refer to metrics computed on objects outside of the controlled
    Pods but inside the Kubernetes cluster. Like Pod metrics, object metrics are also
    not standard but depend on the metrics actually computed by each specific platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the *Advanced Kubernetes configuration* section, we will describe Kubernetes
    resources called **Ingresses** that interface the Kubernetes cluster with the
    external world. Typically, all Kubernetes input traffic transits through a single
    Ingress, so we can measure the total input traffic by measuring the traffic inside
    that Ingress. Once a cluster has been empirically optimized, and we need to just
    adapt it to temporary peaks, the easiest way to do it is by connecting the number
    of replicas of each frontend microservice and also of some Worker microservice
    to the total application input traffic. This can be done with Object metric constraints
    that reference the unique application Ingress:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we have a `value` since we don’t average on several objects, but
    the number of replicas is computed as for the Pod metrics. Moreover, in this case,
    we must be sure that the `requests-per-second` metric is actually computed by
    the infrastructure on all Ingresses.
  prefs: []
  type: TYPE_NORMAL
- en: Personally, I always use CPU and memory metrics since they are available on
    all platforms, and since using the procedure sketched in this subsection, it is
    reasonably easy to find good target values for them.
  prefs: []
  type: TYPE_NORMAL
- en: Though all cloud providers offer useful Kubernetes metrics, there are open-source
    metric servers that can also be installed on on-premises Kubernetes clusters through
    `.yaml` files. See the *Further reading* section for an example.
  prefs: []
  type: TYPE_NORMAL
- en: Minikube has a metrics-server addon that can be installed with `minikube addons
    enable metrics-server`. You also need it to use standard resource metrics like
    CPU and memory.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will analyze how to test and deploy a microservice application
    and will put these concepts into practice by running and debugging the Worker
    microservice we implemented in [*Chapter 7*](Chapter_7.xhtml#_idTextAnchor151)*,
    Microservices in Practice*, on Minikube.
  prefs: []
  type: TYPE_NORMAL
- en: Running your microservices on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will test the routes-matching worker microservice in Minikube,
    but we will also describe how to organize the various environments your microservices
    application will be deployed to: development, staging, and production. Each environment
    has its own peculiarities, such as an easy way to test each change in development
    and maximizing performance in production.'
  prefs: []
  type: TYPE_NORMAL
- en: Organizing all deployment environments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is also worth pointing out that the simplest test in Minikube requires a
    not-negligible setup time. Therefore, most development simply uses Docker, that
    is, a few containerized microservices organized into a unique Visual Studio solution
    that starts all of them when you launch the solution.
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, we don’t test the whole application but just a few tightly interacting
    microservices, possibly simulating the remainder of the application with stubs.
    If communication is handled through a message broker, it is enough to launch all
    microservices and the message broker to test everything; otherwise, if we rely
    on direct communication between microservices, we must connect all microservices
    in a virtual network.
  prefs: []
  type: TYPE_NORMAL
- en: Docker offers the possibility to both create a virtual network and connect running
    containers to it. The virtual network created by Docker also includes your development
    machine, which gets the **host.docker.internal** hostname. Therefore, all microservices
    can use various services running on the development machine, such as RabbitMQ,
    SQL Server, and Redis.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can create a test virtual network in Docker with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, attaching all running microservices to this network is super easy. It
    is enough to modify their project files as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Then, you can also add other `docker run` arguments, such as a volume mount.
  prefs: []
  type: TYPE_NORMAL
- en: Testing on Minikube can be performed at the end of the working day or simply
    after the complete implementation of a feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next subsections, we will compare all deployment environments on the
    following axes:'
  prefs: []
  type: TYPE_NORMAL
- en: Database engine and database installation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Container registries
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Message broker installation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Debugging techniques
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Database engine and database installation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Development tests with Docker or Minikube may all use a database engine running
    directly on the development machine. You may use either an actual installation
    or an engine running as a Docker container. The advantage is that the database
    is also accessible from Visual Studio, so you can pass all migrations while you
    develop them.
  prefs: []
  type: TYPE_NORMAL
- en: You can also use fresh Docker containers running the database engine to start
    databases from scratch and perform unit tests, or to test the overall migration
    set.
  prefs: []
  type: TYPE_NORMAL
- en: If you installed Minikube with the Docker driver, a database running on your
    development machine can be reached from inside your Minikube containers by using
    either the **host.minikube.internal** or **host.docker.internal** hostnames. Therefore,
    if you use **host.docker.internal**, you will be able to reach your host machine
    from both Minikube and from your containerized applications directly launched
    by Visual Studio.
  prefs: []
  type: TYPE_NORMAL
- en: On both staging and production, you can use database cloud services that ensure
    good performance, are scalable, and offer clustering, replication, geographic
    redundancy, and so on. It’s also possible to deploy the database inside your Kubernetes
    cluster, but in this case, you must buy a license, you should dedicate ad hoc
    Kubernetes nodes for the database (virtual machines that ensure optimal database
    performance), and you should fine-tune the database configuration. Therefore,
    if there are no compelling reasons for a different choice, it is more convenient
    to opt for cloud services.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, both in production and staging, you can’t configure your Deployments
    to automatically apply migrations when they start; otherwise, all replicas will
    attempt to apply them. It’s better to extract a database script from your migrations
    and apply it with a database DBO user privilege, while leaving the microservice
    replicas with a less privileged database user.
  prefs: []
  type: TYPE_NORMAL
- en: 'A database script can be extracted from all migrations with the migration command
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Let’s move on to container registries.
  prefs: []
  type: TYPE_NORMAL
- en: Container registries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As far as staging and production are concerned, they can both use the same container
    registry since containers are versioned. So, for instance, production can use
    `v1.0`, while staging can use `v2.0-beta1`. It is better if registries belong
    to the same cloud subscription of the Kubernetes cluster to simplify credential
    handling. For instance, in the case of AKS, it is enough to associate a registry
    to an AKS cluster once and for all to grant access to the cluster to the registry
    (see the *Creating an Azure Kubernetes cluster* subsection of this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: 'As far as development is concerned, each developer can use the same registry
    used by the staging environment for the containers they are not working on, but
    each developer should have a private registry for the containers they are working
    on, so they can experiment with no risk of dirtying the “official image” registries.
    Therefore, the simplest solution is to install a local registry in your Docker
    Desktop. You can do this with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Once the container has been created with the instruction above, you can stop
    and restart it from the Docker Desktop graphical user interface.
  prefs: []
  type: TYPE_NORMAL
- en: Unluckily, as a default, both Docker and Minikube do not accept interacting
    with insecure registries, that is, with registries that do not support HTTPS with
    a certificate signed by a public authority, so we must instruct both Docker and
    Minikube to accept insecure interaction with the local registry.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s open the Docker Desktop graphical user interface and click on the settings
    image in the top-right corner:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5: Docker settings](img/B31916_08_5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5: Docker settings'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, select **Docker Engine** from the left menu, and edit the big text box
    that contains Docker configuration information, and add the entry shown below
    to the existing JSON content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The above settings add the 5000 ports of both hostnames that point to your
    host computer to the allowed insecure registries. The result should be something
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6: Adding a local registry to Docker allowed insecure registries](img/B31916_08_6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.6: Adding a local registry to Docker allowed insecure registries'
  prefs: []
  type: TYPE_NORMAL
- en: 'As far as Minikube is concerned, you have to destroy your current Minikube
    VM with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you need to create a new VM image with the right insecure registry settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Please execute all the above steps because we will need a local registry for
    testing the route-planning microservice.
  prefs: []
  type: TYPE_NORMAL
- en: 'If Minikube also needs to access other password-protected registries, you must
    configure and enable the **registry-creds** addon:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Once you issue the above command, you will be asked to configure Google, AWS,
    Azure, or Docker private registries and enter your credentials.
  prefs: []
  type: TYPE_NORMAL
- en: 'After a successful configuration, you can enable the credential usage with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Let’s move on to the message broker.
  prefs: []
  type: TYPE_NORMAL
- en: Message broker installation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RabbitMQ can be installed both locally and in the cloud, and works on all clouds,
    so it really is a good option. You can run a single RabbitMQ server or a server
    cluster. A RabbitMQ cluster can also be installed on the Kubernetes cluster itself.
    During development, you may install it on Minikube, but it is more convenient
    to run it outside of Minikube, so it can also be easily reached by applications
    running outside of Minikube, which, in turn, facilitates application debugging,
    as we will see in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: 'In staging and production, the simplest way to install a RabbitMQ cluster is
    by installing the so-called **RabbitMQ Cluster Operator** with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'The RabbitMQ operator defines the **RabbitmqCluster** custom resource that
    represents a RabbitMQ Cluster. You can create and configure **RabbitmqCluster**
    as you configure any other Kubernetes resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'The persistence section specifies the options for persisting queues on persistent
    storage. If you omit it, all default values will be taken. If you omit the number
    of replicas, a cluster with a single server will be created. More options are
    available in the official documentation: [https://www.rabbitmq.com/kubernetes/operator/using-operator](https://www.rabbitmq.com/kubernetes/operator/using-operator).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can get the username and password of your RabbitMQ cluster default user
    by printing the `<cluster name>-default-user` secret where they are stored, as
    shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Both username and password are base-64 encoded. The simplest way to decode
    them is by copying each of them from the console output, opening a Linux console,
    and using the `base64` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: If you want, you may also install the RabbitMQ cluster operator in Minikube,
    but in this case, it is better to start Minikube with at least 4 CPUs and 6-8
    gigabytes of run.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you need to connect to the RabbitMQ cluster from outside of the Kubernetes
    cluster for debugging purposes, you can use the `kubectl port-forward` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: The above instruction freezes the console and forwards port 5672 of the `service/<cluster
    name>` ClusterIP service that is part of the RabbitMQ cluster to port 5672 of
    localhost. The port-forwarding remains active while the console window is open
    or `ctrl-c` is issued to abort the instruction.
  prefs: []
  type: TYPE_NORMAL
- en: 'The general `kubectl port-forward` syntax is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: In our case, the service name is equal to the cluster name.
  prefs: []
  type: TYPE_NORMAL
- en: The service <cluster name> is the ClusterIP service you must use to access the
    RabbitMQ cluster from inside the Kubernetes cluster. Therefore, the RabbitMQ hostname
    to specify in the connection is `<cluster name>.<cluster namespace>`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also access the RabbitMQ management UI with your browser by forwarding
    the 15672 port:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: Then, the UI will be available at `localhost:15672`. There, you must use the
    credentials you previously extracted from the `cluster name>-default-user` secret.
  prefs: []
  type: TYPE_NORMAL
- en: The port forwarding is safe and doesn’t expose RabbitMQ to the outside world
    since the connection between localhost and the service is mediated by the Kubernetes
    API server. It can be safely used to connect test code running on the development
    machine with the RabbitMQ cluster, as we will see in more detail in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you launch all containers from Visual Studio, you can debug your code without
    performing any further configuration. However, if you need to debug some microservices
    running either in Minikube, in staging, or in production, you need some supplementary
    configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of trying to attach the debugger inside of your Kubernetes cluster,
    a simpler approach is to use the so-called bridge: you select a specific microservice
    to debug, and instead of debugging it in Kubernetes, you redirect its traffic
    to a replica of your microservice running in Visual Studio, then you redirect
    all local microservice output traffic again inside the cluster. This way, you
    debug just a local copy that has been compiled in debug mode, overcoming both
    the need to replace the release code with debug code, and the difficulty of attaching
    a debugger inside of your Kubernetes cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The image below exemplifies the bridge idea:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7: Bridging](img/B31916_08_7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.7: Bridging'
  prefs: []
  type: TYPE_NORMAL
- en: 'If both inputs and outputs are handled by a message broker, bridging is easy:
    it is enough to connect the local copy to the same RabbitMQ queues of the in-cluster
    replicas. This way, part of the traffic will be automatically forwarded to the
    local copy. If the RabbitMQ cluster runs inside the Kubernetes cluster, you need
    to forward its ports on localhost as explained in the previous section.'
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, if the microservice is connected to a database, we must also connect
    the local copy to the same database. If you are in production, this might require
    the definition of a firewall rule to enable access of your development machine
    to the database.
  prefs: []
  type: TYPE_NORMAL
- en: If some input and output are handled by services instead of message brokers,
    bridging becomes more complex. More specifically, forwarding the output to a service
    inside the Kubernetes cluster is quite easy since it requires just port-forwarding
    the target service on localhost with `kubectl port-forward`. However, forwarding
    traffic from a service to the local microservice copy requires some kind of hack
    on the service.
  prefs: []
  type: TYPE_NORMAL
- en: Services compute the Pods they must route the traffic to and then create resources
    called `EndpointSlice` containing the IP addresses where they must route the traffic.
    Therefore, in order to route all service traffic to your local machine, you need
    to override the `EndpointSlices` of that service. This can be done by removing
    the selector of the target service so that all `EndpointSlices` will be deleted,
    and then manually adding an `EndpointSlice` that points to your development machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can do this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Get the target service definition with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Remove the selector, and apply the new definition.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you are working on a remote cluster, add the `EndpointSlice` below:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If, instead, you are working on a Minikube local cluster, add the `EndpointSlice`
    below:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When you finish debugging, reapply the original service definition. Your custom
    `EndpointSlice` will be automatically destroyed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As you can see, using message brokers simplifies a lot of the debugging. It
    is the advised option when implementing applications. Services are a better option
    when implementing tools, such as database clusters, or message brokers that run
    inside your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: There are tools that automatically handle all needed service hacking, such as
    **Bridge to Kubernetes** ([https://learn.microsoft.com/en-us/visualstudio/bridge/bridge-to-kubernetes-vs](https://learn.microsoft.com/en-us/visualstudio/bridge/bridge-to-kubernetes-vs)),
    but unluckily, Microsoft announced that it will stop supporting it. Microsoft
    will advise a valid alternative.
  prefs: []
  type: TYPE_NORMAL
- en: Now we are finally ready to test an actual Microservice on Minikube.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the route-matching worker microservice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will test the route-matching worker microservice implemented in [*Chapter
    7*](Chapter_7.xhtml#_idTextAnchor151)*, Microservices in Practice*, together with
    two stub microservices. The first one will send test input to it, while the other
    will collect all its output and will write it in its console, so that we may access
    this output with the `kubectl logs` command. This is a typical way to perform
    a preliminary test. Then, more complex tests may also involve other application
    services.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a copy of our route-matching worker microservice solution, then
    add two more **Worker service** projects, and call them respectively `FakeSource`
    and `FakeDestination`. For each of them, enable container support for Linux as
    shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8: Worker services project settings](img/B31916_08_8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.8: Worker services project settings'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, let’s also add all needed EasyNetQ packages to enable both services to
    interact with a RabbitMQ cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '`EasyNetQ`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`EasyNetQ.Serialization.NewtonsoftJson`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`EasyNetQ.Serialization.SystemTextJson`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select at least version 8, also if it is still a prerelease.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then you must add RabbitMQ to the services in the `Program.cs` of both projects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'The RabbitMQ connection string must be added in the environment variables defined
    in `Properties->launchSettings.json`, as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: Finally, refer to the `SharedMessages` project from both `FakeSource` and `FakeDestination`,
    so they can use all application communication messages.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we are ready to code our stub services. In the `Worker.cs` file
    scaffolded by Visual Studio in the `FakeDestination` project, replace the existing
    class with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: The hosted service adds a subscription named `FakeDestination` to the `RouteExtensionProposalsMessage`
    event. This way, it receives all matching proposals between an existing route
    and some requests. Once the subscription handler receives a proposal, it just
    logs the message in JSON format, so we can verify that the right match proposal
    events are generated by exploring the `FakeDestination` logs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `Worker.cs` file scaffolded by Visual Studio in the FakeSource project,
    we will replace the existing class with simple code that does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Creates three town messages: Phoenix, Santa Fe, and Cheyenne.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sends a request going from Phoenix to Santa Fe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sends a route offer passing from Phoenix, Santa Fe, and Cheyenne. As soon as
    this message is received by the route planning worker microservice, it should
    create a proposal to match this offer with the previous request. This proposal
    should be received by `FakeDestination` and logged.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sends a request going from Santa Fe to Cheyenne. As soon as this message is
    received by the routes planning worker microservice, it should create a proposal
    to match this request with the previous offer. This proposal should be received
    by `FakeDestination` and logged.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After 10 seconds, it simulates that both previous proposals have been accepted
    and creates a route extension event based on the previous offer and containing
    both the matched requests. As soon as this message is received by the route planning
    worker microservice, it should both update the offer and should add the two requests
    to the offer. As a result, the `RouteId` field of both requests should point to
    the offer `Id`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The code of the `Worker.cs` class is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: The code that defines all messages has been omitted. You can find the full code
    in the `ch08->CarSharing->FakeSource->Worker.cs` file of the GitHub repository
    associated with the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s prepare to execute all microservices in Docker by performing the
    following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Right-click on the solution line in Visual Studio Solution Explorer and select
    **Configure Startup Projects…**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then select **Multiple startup projects**, and change the name of the launch
    option to **AllMicroservices**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, select all three `FakeDestination`, `FakeSource`, and `RoutesPlanning`
    projects, and for each of them, choose **Start** for **Action** and **Container
    (Docker file)** for **Debug Target**, as shown below:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.9: Launch settings](img/B31916_08_9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.9: Launch settings'
  prefs: []
  type: TYPE_NORMAL
- en: Now you can launch all projects simultaneously by choosing **AllMicroservices**
    in Visual Studio Debug Launcher.
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that both the application’s SQL Server and the RabbitMQ server are running.
    Then, build the project and launch it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Containers tab that appears, select `FakeDestination`, so you can inspect
    its logs. After a few seconds, you should see the two match proposal messages,
    as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.10: FakeDestination logs](img/B31916_08_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.10: FakeDestination logs'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, in the SQL Server Object Explorer pane, select the application database,
    if already there; otherwise, connect to it, and then show its tables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.11: Application database](img/B31916_08_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.11: Application database'
  prefs: []
  type: TYPE_NORMAL
- en: 'Right-click on both **dbo.RouteOffers** and **dbo.RouteRequests** and select
    **View Data** to see all their data. You should see that the offer’s `Timestamp`
    changed to 2 because the offer was updated once the two matching proposals were
    accepted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.12: Updated offer](img/B31916_08_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.12: Updated offer'
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, you should see that the two requests have been associated with the
    offer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.13: Updated requests](img/B31916_08_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.13: Updated requests'
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s stop debugging and delete all records in the **dbo.RouteOffers** and
    **dbo.RouteRequests** tables.
  prefs: []
  type: TYPE_NORMAL
- en: It’s time to deploy our Microservices in Minikube!
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the same RabbitMQ and SQL Servers running on the development machine.
    However, there are some preliminary steps to perform before we start deploying
    our `.yaml` files in Minikube:'
  prefs: []
  type: TYPE_NORMAL
- en: We must create adequate Docker images, since the debug images created by Visual
    Studio can’t run outside of Visual Studio. They all have a `dev` version. Go to
    the Docker files of the three `FakeDestination`, `FakeSource`, and `RoutesPlanning`
    projects in Visual Studio Explorer, right-click on them, and select **Build Docker
    Image**. These actions will create three Docker images with the latest version.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Launch the local registry container from inside the Docker UI. If you have not
    yet created a registry container, please refer to the *Container registries* subsection
    for installation instructions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Push our newly created images in this registry so they can be downloaded by
    Minikube (remember that you need a Linux console to issue the commands below):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We need to create 3 deployments, one for each of our three microservices. Let’s
    create a `Kubernetes` folder in the `CarSharing` solution folder. We will place
    our deployment definitions there.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below `FakeSource.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: It contains just a single environment variable for the RabbitMQ connection string
    – the same one we defined in `launchSettings.json`. The resource request is minimal.
    Labels are a documentation tool, too. Therefore, they define both the application
    name, the role in the application, and the fact that this microservice is a stub.
  prefs: []
  type: TYPE_NORMAL
- en: We designed the `car-sharing` namespace to host the whole application.
  prefs: []
  type: TYPE_NORMAL
- en: '`host.docker.internal:5000` is the hostname of our local registry as seen from
    inside Minikube.'
  prefs: []
  type: TYPE_NORMAL
- en: Our deployments don’t need services since they communicate through RabbitMQ.
  prefs: []
  type: TYPE_NORMAL
- en: '`FakeDestination.yaml` is completely analogous:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '`RoutesPlanning.yaml` differs from the other just because it contains a lot
    more environment variables and because it exposes the `8080` port, which we might
    exploit to check the service’s health state (see the *Readiness, liveness, and
    startup probes* subsection in the next section).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s open a Windows console on the `Kubernetes` folder, and start deploying
    our application:'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start Minikube with `minikube start`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s create the `car-sharing` namespace with `kubectl create namespace car-sharing`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s deploy `FakeDestination.yaml` first: `kubectl apply -f FakeDestination.yaml`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now let’s verify all Pods are okay and ready with `kubectl get all -n car-sharing`.
    If they’re not ready, please repeat the command until they are ready.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s copy the name of the created Pod. We need it to access its logs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, let’s deploy `RoutesPlanning.yaml`: `kubectl apply -f RoutesPlanning.yaml`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Again, let’s verify all Pods are okay and ready with `kubectl get all -n car-sharing`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, let’s deploy `FakeSource.yaml`: `kubectl apply -f FakeSource.yaml`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Again, let’s verify all Pods are okay and ready with `kubectl get all -n car-sharing`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now let’s check the `FakeDestination` logs to verify it received the match
    proposals with: `kubectl logs <FakeDestination POD name> -n car-sharing`. Where
    `<FakeDestination POD name>` is the name that we got in *step5.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Also check the database table to verify that the applications work properly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When you’ve finished experimenting, delete everything by simply deleting the
    `car-sharing` namespace: `kubectl delete namespace car-sharing`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Also delete the records in the **dbo.RouteOffers** and **dbo.RouteRequests**
    database tables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Stop Minikube with: `minikube stop`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, if you would like to experiment with debugging with the bridge technique,
    repeat the above steps, but replace points 6 and 7, which deploy the `RoutePlanning`
    microservice with the launch of the single `RoutePlanning` project inside of Visual
    Studio (just replace `AllMicroservices` with `RoutePlanning` in the Visual Studio
    debug widget, and then start the debugger).
  prefs: []
  type: TYPE_NORMAL
- en: Since all containers are attached to the same RabbitMQ server, the container
    running in Visual Studio will receive all input messages created from within Minikube,
    and all its output messages will be routed inside of Minikube. Let’s place a breakpoint
    wherever you would like to analyze the code before continuing the Kubernetes deployment.
    A few seconds after the deployment of the `FakeSource.yaml` file, the breakpoint
    should be hit!
  prefs: []
  type: TYPE_NORMAL
- en: Advanced Kubernetes configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section describes advanced Kubernetes resources that play a fundamental
    role in application design. Other advanced resources and configurations related
    specifically to security and observability will be described in [*Chapter 10*](Chapter_10.xhtml#_idTextAnchor297)*,
    Security and Observability for Serverless and Microservices Applications*.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with secrets.
  prefs: []
  type: TYPE_NORMAL
- en: Secrets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes allows various kinds of Secrets. Here, we will describe just `generic`
    and `tls` secrets, which are the ones used in the practical development of applications
    based on microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Each generic Secret contains a collection of entry-name/entry-value pairs. Secrets
    can be defined with .`yaml` files, but since it is not prudent to mix sensitive
    information with code, they are usually defined with `kubectl` commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below is how to define a Secret, taking the entry values from file contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: The file names become entry names (just the file name with its extension – the
    path information is removed), while file contents become the associated entry
    values. Each entry is defined with a different `--from-file=…` option.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creates two files with the above names in a directory, put some content in
    them, then open a console on that directory, and finally try the above command.
    Once created, you can see it in `.yaml` format with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: In the data section, you will see the two entries, but the entry values appear
    encrypted. Actually, they are not encrypted but just base64-encoded. Needless
    to say, you can prevent some Kubernetes users from accessing Secret resources.
    We will see how in [*Chapter 10*](Chapter_10.xhtml#_idTextAnchor297)*, Security
    and Observability for Serverless and Microservices Applications*.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Secret can be deleted with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of using files, one can specify the entry values in line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: As usual, we can specify the Secret namespace with the `-n` option.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once defined, generic Secrets can be mounted as volumes on Pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: Each entry is seen as a file whose name is the entry name and whose content
    is the entry value.
  prefs: []
  type: TYPE_NORMAL
- en: Do not forget that entry values are base64-encoded, so they must be decoded
    before usage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Secrets can also be passed as environment variables:'
  prefs: []
  type: TYPE_NORMAL
- en: 'env:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: In this case, Secret values are automatically base64-decoded before passing
    them as environment variables.
  prefs: []
  type: TYPE_NORMAL
- en: '`Let’s try Secrets on the routes-matching worker microservices. Let’s create
    a Kubernetes Secret that contains the RabbitMQ connection string and correct FakeDestination.yaml,
    FakeSource.yaml, and RoutesPlanning.yaml, to use this Secret.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`tls` Secrets are designed for storing web servers’ certificates. We will see
    how to use them in the *Ingresses* subsection. `tls` secrets take as input both
    the private key certificate (.key) and the public key approved certificate (.crt):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: The next important topic concerns how our container code may help Kubernetes
    verify both whether each container is ready to interact with the remainder of
    the application and if it is in good health.
  prefs: []
  type: TYPE_NORMAL
- en: Readiness, liveness, and startup probes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Liveness probes inform Kubernetes when containers are in an unrecoverable faulty
    state, so Kubernetes must kill and restart them. If a container has no liveness
    probe defined for it, Kubernetes restarts it just in case it crashes due to some
    unpredictable exception or because it exceeded its memory limits. Liveness probes
    must be carefully designed to detect actual unrecoverable error situations; otherwise,
    the container might end up in an endless loop of restarts.
  prefs: []
  type: TYPE_NORMAL
- en: Temporary failures, instead, are connected to readiness probes. When a readiness
    probe fails, it informs Kubernetes that the container is not able to receive traffic.
    Accordingly, Kubernetes removes the failed container from all the lists of matching
    services that could send traffic to it. This way, traffic is split only among
    the ready containers. The faulty container is not restarted and is reinserted
    in the services list as soon as the readiness probe succeeds again.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, a startup probe informs Kubernetes that the container has completed
    its startup procedure. Its only purpose is avoiding Kubernetes killing and restarting
    the container during startup because of liveness probe failures. In fact, similar
    occurrences might move the container into an endless loop of restarts.
  prefs: []
  type: TYPE_NORMAL
- en: Put simply, Kubernetes starts liveness and readiness probes only after the startup
    probe succeeds. Since both liveness and readiness probes already have initial
    delays, startup probes are necessary only in case of very long startup procedures.
  prefs: []
  type: TYPE_NORMAL
- en: 'All probes have a **probe operation** that may either fail or succeed, with
    the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`failureThreshold`: The number of consecutive times the probe operation must
    fail to consider the probe as failed. If not provided, it defaults to 3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`successThreshold`: Used only for readiness probes. This is the minimum number
    of consecutive successes for the probe to be considered successful after having
    failed. It defaults to 1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`initialDelaySeconds`: The time in seconds Kubernetes must wait after the container
    starts before trying the first probe. The default value is 0.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`periodSeconds`: The time in seconds between two successive probes. The default
    is 10 seconds.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`timeoutSeconds`: The number of seconds after which the probe times out. The
    default is 1 second.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Often, liveness and readiness probes are implemented with the same probe operation,
    but the liveness probe has a greater failure threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Probes are container-level properties, that is, they are on the same level as
    container ports, and `name`.
  prefs: []
  type: TYPE_NORMAL
- en: Probe operations may be based on shell commands, HTTP requests, or TCP/IP connection
    attempts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Probes based on shell commands are defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: The `command` list contains the command and all its arguments. The operation
    succeeds if it is completed with a `0` status code, that is, if the command completes
    with no errors. In the example above, the command succeeds if the `/tmp/healthy`
    file exists.
  prefs: []
  type: TYPE_NORMAL
- en: 'Probes based on TCP/IP connections are defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: The operation succeeds if a TCP/connection is successfully established.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, probes based on HTTP requests are defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '`path` and `port` specify the endpoint path and port. The optional `httpHeaders`
    section lists all HTTP headers that Kubernetes must provide in its request. The
    operation succeeds if the response returns a status code satisfying: `200<=status<400`.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s add a liveness probe to the `RoutesPlanning.yaml` deployment of the *Testing
    the route-matching worker microservice* section. We don’t need a readiness probe,
    since readiness probes only affect services, and we don’t use services since all
    communications are handled by RabbitMQ.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, let’s define the following API in the `Program.cs` file of the
    `RoutesPlanning` project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: The code returns an error status if there were at least 6 consecutive failed
    attempts to communicate with RabbitMQ.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `RoutesPlanning.yaml` deployment, we must add the code below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: After this change, if you want, you can retry the whole Minikube test from the
    *Testing the route-matching worker microservice* section.
  prefs: []
  type: TYPE_NORMAL
- en: The next section describes a structured, modular, and efficient way to handle
    the interaction between our cluster and the external world.
  prefs: []
  type: TYPE_NORMAL
- en: Ingresses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most microservices applications have several frontend microservices, so exposing
    them with LoadBalancer services would require a different IP address for each
    of them. Moreover, inside of our Kubernetes cluster, we don’t need the burden
    of HTTPS and certificates for each microservice, so the best solution is a unique
    entry point for the whole cluster with a unique IP address that takes care of
    HTTPS communication with the external world while forwarding HTTP communication
    to the services inside of the cluster. Both functionalities are typical of web
    servers.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, each IP address has several domain names attached, and a web server
    splits the traffic between several applications according to both the domain name
    and the request path inside each domain. This web server functionality is called
    **virtual hosting**.
  prefs: []
  type: TYPE_NORMAL
- en: The translation between HTTPS and HTTP is a peculiarity of web servers, too.
    It is called **HTTPS termination**.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, web servers furnish further services, such as request filtering to
    prevent various kinds of attacks. More generally, they understand the HTTP protocol
    and offer HTTP-related services such as access to static files, and various kinds
    of protocol and content negotiations with the client.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, LoadBalancer services just handle the lower-level TCP/IP
    protocol and perform some load balancing. Therefore, it would be great to use
    an actual web server to interface our Kubernetes cluster with the external world
    instead of several LoadBalancer services.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes offers the possibility to run actual web servers inside of resources
    called **Ingresses**. Ingresses act as interfaces between an actual web server
    and the Kubernetes API, and enable us to configure most web server services with
    a common interface that doesn’t depend on the specific web server that is behind
    the Ingress.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram exemplifies how an Ingress splits traffic among all frontend
    microservices inside a Kubernetes cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.14: Ingress](img/B31916_08_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.14: Ingress'
  prefs: []
  type: TYPE_NORMAL
- en: Ingresses can be created in a cluster only after an **Ingress controller** has
    been installed in the cluster. Each Ingress controller installation supplies both
    a specific web server, such as NGINX, and the code that interfaces it with the
    Kubernetes API.
  prefs: []
  type: TYPE_NORMAL
- en: The information about the Ingress controller and its settings is provided in
    a resource called `IngressClass`, which is referenced in the actual Ingress definition.
    However, often, Ingress controller installations already define a default `IngressClass`
    class, so there is no need to specify its name inside the ingress definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below is how to define an IngressClass:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: Each class specifies just the controller’s name (`controller`), if it is the
    default class (`…/is-default-class` annotation), and some optional parameters
    that depend on the specific controller.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below is how to define an Ingress:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: Some controllers, such as the NGINX-based controller, use annotations placed
    in the metadata section to configure the web server.
  prefs: []
  type: TYPE_NORMAL
- en: 'HTTPS termination rules (`tls`) are pairs made of a collection of domain names
    and an HTTPS certificate associated to them, where each certificate must be packaged
    as a `tls` secret (see the *Secrets* subsection):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: In the example above, each certificate applies just to a single domain, but
    if that domain has subdomains that are secured by the same certificate, we may
    add them to the same certificate list.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a virtual hosting rule for each domain, and each of these rules has
    subrules for various paths:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: Domain segments may be replaced by wildcards (`*`). Each `path` subrule specifies
    a service name, and all traffic matching that rule will be sent to that service,
    at the port specified in the rule. The service, in turn, forwards the traffic
    to all matching Pods.
  prefs: []
  type: TYPE_NORMAL
- en: If `pathType` is prefix, it will match all request paths that have the specified
    path as a subsegment. Otherwise, a perfect match is required. In the example above,
    the first rule matches all paths since all paths have the empty segment`/`as subsegment.
  prefs: []
  type: TYPE_NORMAL
- en: If an input request matches more paths, the more specific one (the one containing
    more segments) is preferred.
  prefs: []
  type: TYPE_NORMAL
- en: In the next subsection, we will put into practice what we have learned about
    Ingresses with a very simple example in Minikube.
  prefs: []
  type: TYPE_NORMAL
- en: Testing Ingresses with Minikube
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The easiest way to install an NGINX-based Ingress controller in Minikube is
    to enable the `ingress` addon. Therefore, after having started Minikube, let’s
    enable this addon:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: As a result, some Pods are created in the `ingress-nginx` namespace. Let’s check
    it with `kubectl get pods -n ingress-nginx`!
  prefs: []
  type: TYPE_NORMAL
- en: 'The addon installs the same NGINX-based ingress controller used by most Kubernetes
    environments ([https://github.com/kubernetes/ingress-nginx?tab=readme-ov-file](https://github.com/kubernetes/ingress-nginx?tab=readme-ov-file)).
    The installation also automatically creates an `IngressClass` called `nginx`.
    The annotations supported by this controller are listed here: [https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/](https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/).'
  prefs: []
  type: TYPE_NORMAL
- en: The `ch08` folder of the GitHub book repository contains `IngressExampleDeployment.yaml`
    and the `IngressExampleDeployment2.yaml` files. They define two Deployments with
    their associated ClusterIP services. They deploy two different versions of a very
    simple web application that creates a simple HTML page.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, let’s copy the two `.yaml` files in a folder and open a console on
    that folder. As the first step, let’s apply these files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will create an ingress that connects the first version of the application
    to `/` and the second version of the application to `/v2`. The names of the ClusterIP
    services of the two deployments are `helloworldingress-service` and `helloworldingress2-service`,
    and both receive on the `8080` port. Therefore, we need to bind the `helloworldingress-service`
    `8080` port to `/` and the `helloworldingress2-service` `8080` port to `/v2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: It is worth pointing out that the host property is empty, so the Ingress doesn’t
    perform any selection based on the domain name, but the microservice selection
    is based just on the path. This was a forced choice since we are experimenting
    on an isolated development machine without the support of a DNS, so we can’t associate
    domain names to IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s put the above code in a file named `IngressConfiguration.yaml` and let’s
    apply it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: In order to connect with the Ingress, we need to open a tunnel with the Minikube
    virtual machine. As usual, open another console and issue the `minikube tunnel`
    command in it. Remember that the tunnel works as long as this window remains open.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now open the browser and go to `http://localhost`. You should see something
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hello, world!**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Version: 1.0.0**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hostname: ……**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then go to `http://localhost/v2`. You should see something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hello, world!**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Version: 2.0.0**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hostname: ……**'
  prefs: []
  type: TYPE_NORMAL
- en: We were able to split the traffic between the two applications according to
    the request path!
  prefs: []
  type: TYPE_NORMAL
- en: 'When you have finished experimenting, let’s clean up the environment with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let’s stop Minikube with: `minikube stop`.'
  prefs: []
  type: TYPE_NORMAL
- en: The next subsection explains how to install the same Ingress controller on AKS.
  prefs: []
  type: TYPE_NORMAL
- en: Using an NGNIX-based Ingress in AKS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can manually install the NGNIX-based Ingress on AKS either with a .`yaml`
    file or with a package manager called Helm. However, then, you should handle complex
    permissions-related configurations to associate a static IP and an Azure DNS zone
    to your AKS cluster. The interested reader can find the complete procedure here:
    [https://medium.com/@anilbidary/domain-name-based-routing-on-aks-azure-kubernetes-service-using-ingress-cert-manager-and-9b4028d762ed](https://medium.com/@anilbidary/domain-name-based-routing-on-aks-azure-kubernetes-service-using-ingress-cert-manager-and-9b4028d762ed).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily, you can let Azure do all of this job for you, because Azure has an
    AKS application routing addon that automatically installs the Ingress for you
    and facilitates all permission configuration. This addon can be enabled on an
    existing cluster with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: The addon creates `webapprouting.kubernetes.azure.com` `IngressClass`, which
    you must reference in all your Ingresses.
  prefs: []
  type: TYPE_NORMAL
- en: An IP address is created whenever you create a new Ingress and remains allocated
    for the lifetime of the Ingress. Moreover, if you create an Azure DNS zone and
    associate it to the addon, the addon will automatically add all needed records
    for all domains defined in the rules of your Ingresses.
  prefs: []
  type: TYPE_NORMAL
- en: 'You just need to create an Azure DNS zone with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to associate this zone to the addon, you need the zone’s unique ID,
    which you can get with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can attach the zone with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: After this command, all domain names used in your Ingress’s rules will be automatically
    added to the zone with adequate records. Obviously, you must update your domain
    data in the provider where you bought your domain names. More specifically, you
    must force them to point to the names of the Azure DNS servers that handle your
    zone. You can easily get these DNS server names by going to the newly created
    DNS zone in the Azure portal.
  prefs: []
  type: TYPE_NORMAL
- en: We have finished our amazing Kubernetes trip. We will return to most of the
    concepts learned about here in most of the remaining chapters, and in particular
    in [*Chapter 11*](Chapter_11.xhtml#_idTextAnchor332)*,* *The Car Sharing App*.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter shows how to start a new microservices application smoothly
    and with low costs with the help of Azure Container Apps.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about the basics of orchestrators and then learned
    how to install and configure a Kubernetes cluster. More specifically, you learned
    how to interact with a Kubernetes cluster through Kubectl and Kubectl’s main commands.
    Then you learned how to deploy and maintain a microservices application, and how
    to test it locally with the help of Docker and Minikube.
  prefs: []
  type: TYPE_NORMAL
- en: You also learned how to interface your Kubernetes cluster with a LoadBalancer
    and with an Ingress, and how to fine-tune it to optimize performance.
  prefs: []
  type: TYPE_NORMAL
- en: All concepts were put into practice with both simple examples and with a more
    complete example taken from the car-sharing case study.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why do Kubernetes applications need network disk storage?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Because PODs can’t rely on the disk storage of the nodes where they run, since
    they might be moved to different nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Is it true that if a node containing a Pod of a Deployment with 10 replicas
    crashes, your application will continue running properly?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Yes.
  prefs: []
  type: TYPE_NORMAL
- en: Is it true that if a node containing a Pod of a StatefulSet with 10 replicas
    crashes, your application will continue running properly?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Not necessarily.
  prefs: []
  type: TYPE_NORMAL
- en: Is it true that if a Pod crashes, it is always automatically restarted?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Yes.
  prefs: []
  type: TYPE_NORMAL
- en: Why do StatefulSets need persistent volume claim templates instead of persistent
    volume claims?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Because each POD of the StatefulSet needs a different volume.
  prefs: []
  type: TYPE_NORMAL
- en: What is the utility of persistent volume claims?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They enable Kubernetes users to request and manage storage resources dynamically,
    decoupling storage provisioning from application deployment.
  prefs: []
  type: TYPE_NORMAL
- en: What is more adequate for interfacing an application with three different frontend
    services, a LoadBalancer or an ingress?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An Ingress. LoadBalancers are adequate just when there is an unique Frontend
    service.
  prefs: []
  type: TYPE_NORMAL
- en: What is the most adequate way of passing a connection string to a container
    running in a Pod of a Kubernetes cluster?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By using a Kubernetes Secret since it contains sensitive information.
  prefs: []
  type: TYPE_NORMAL
- en: How are HTTPS certificates installed in Ingresses?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Through a specific type of secret.
  prefs: []
  type: TYPE_NORMAL
- en: Does standard Kubernetes syntax allow the installation of an HTTPS certificate
    on a LoadBalancer service?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: No.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes official documentation: [https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AKS official documentation: [https://learn.microsoft.com/en-us/azure/aks/](https://learn.microsoft.com/en-us/azure/aks/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Minikube official documentation: [https://minikube.sigs.k8s.io/docs/](https://minikube.sigs.k8s.io/docs/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AKS autoscaling: [https://learn.microsoft.com/en-us/azure/aks/cluster-autoscaler?tabs=azure-cli](https://learn.microsoft.com/en-us/azure/aks/cluster-autoscaler?tabs=azure-cli
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cloud-independent cluster auto-scalers: [https://kubernetes.io/docs/concepts/cluster-administration/cluster-autoscaling/](https://kubernetes.io/docs/concepts/cluster-administration/cluster-autoscaling/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Storage classes: [https://kubernetes.io/docs/concepts/storage/storage-classes/](https://kubernetes.io/docs/concepts/storage/storage-classes/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Assigning a static Azure IP address to a LoadBalancer: [https://learn.microsoft.com/en-us/azure/aks/static-ip](https://learn.microsoft.com/en-us/azure/aks/static-ip)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example metrics server: [https://github.com/kubernetes-sigs/metrics-server](https://github.com/kubernetes-sigs/metrics-server).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NGINX-based Ingress controller: [https://github.com/kubernetes/ingress-nginx?tab=readme-ov-file](https://github.com/kubernetes/ingress-nginx?tab=readme-ov-file)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Manual installation of NGINX-based Ingress of AKS: [https://medium.com/@anilbidary/](https://www.medium.com/@anilbidary/domain-name-based-routing-on-aks-azure-kubernetes-service-using-ingresscert-manager-and-9b4028d762ed)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[domain-name-based-routing-on-aks-azure-kubernetes-service-using-ingresscert-manager-and-9b4028d762ed](https://www.medium.com/@anilbidary/domain-name-based-routing-on-aks-azure-kubernetes-service-using-ingresscert-manager-and-9b4028d762ed)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using RabbitMQ Cluster operator: [https://www.rabbitmq.com/kubernetes/operator/using-operator](https://www.rabbitmq.com/kubernetes/operator/using-operator)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Installing a RabbitMQ Cluster on Kubernetes: [https://www.rabbitmq.com/kubernetes/operator/install-operator](https://www.rabbitmq.com/kubernetes/operator/install-operator).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/PSMCSharp](https://packt.link/PSMCSharp)'
  prefs: []
  type: TYPE_NORMAL
- en: '![A qr code with black squares  AI-generated content may be incorrect.](img/B31916_Discord-QR-Code.png)'
  prefs: []
  type: TYPE_IMG
