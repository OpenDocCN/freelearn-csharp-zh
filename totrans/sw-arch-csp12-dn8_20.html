<html><head></head><body>
<div><h1 class="chapterNumber">20</h1>
<h1 class="chapterTitle" id="_idParaDest-411">Kubernetes</h1>
<p class="normal">This chapter is dedicated to describing the Kubernetes container orchestrator and its implementation in Azure, called <strong class="keyWord">Azure Kubernetes Service </strong>(<strong class="keyWord">AKS</strong>). We discussed the importance and the<a id="_idIndexMarker1438"/> tasks handled by orchestrators in the <em class="italic">Which tools are needed to manage microservices?</em> section of <em class="italic">Chapter 11, Applying a Microservice Architecture to Your Enterprise Application</em>. Here, it is worth recalling just that Kubernetes is the de facto standard for orchestrators.</p>
<p class="normal">We will show also how to install and use minikube on your local machine, which is a one-node Kubernetes simulator you can use to try out all of the examples in this chapter, and also to test your own applications. Simulators are useful both to avoid wasting too much money on an actual cloud-based Kubernetes cluster, and to provide a different Kubernetes cluster to each developer.</p>
<p class="normal">This chapter explains the fundamental Kubernetes concepts and then focuses on how to interact with a Kubernetes cluster and how to deploy a Kubernetes application. All concepts are put into practice with simple examples. We recommend reading <em class="italic">Chapter 11</em>, <em class="italic">Applying a Microservice Architecture to Your Enterprise Application</em>, before reading this chapter, since we will use concepts explained in previous chapters.</p>
<p class="normal">More specifically, in this chapter, we will cover the following topics:</p>
<ul>
<li class="bulletList">Kubernetes basics</li>
<li class="bulletList">Interacting with Azure Kubernetes clusters</li>
<li class="bulletList">Advanced Kubernetes concepts</li>
</ul>
<p class="normal">By the end of this chapter, you will have learned how to implement and deploy a complete solution using Azure Kubernetes Service.</p>
<h1 class="heading-1" id="_idParaDest-412">Technical requirements</h1>
<p class="normal">In this chapter, you will require the following:</p>
<ul>
<li class="bulletList">Visual Studio 2022 free Community Edition or better, with all the database tools installed, or any other <code class="inlineCode">.yaml</code> file editor, such as Visual Studio Code.</li>
<li class="bulletList">A free Azure account. The <em class="italic">Creating an Azure account</em> section in <em class="italic">Chapter 1</em>, <em class="italic">Understanding the Importance of Software Architecture</em>, explains how to create one.</li>
<li class="bulletList">An optional minikube installation. Installation instructions will be given in the <em class="italic">Using minikube</em> section of this chapter.</li>
</ul>
<p class="normal">The code for this chapter is available at <a href="https://github.com/PacktPublishing/Software-Architecture-with-C-Sharp-12-and-.NET-8-4E">https://github.com/PacktPublishing/Software-Architecture-with-C-Sharp-12-and-.NET-8-4E</a>.</p>
<h1 class="heading-1" id="_idParaDest-413">Kubernetes basics</h1>
<p class="normal">Kubernetes is an advanced open source software for managing distributed applications running on a computer network. Kubernetes can be used on your private machine’s cluster, or you can use hardware-scalable<a id="_idIndexMarker1439"/> Kubernetes offerings from all main cloud providers. This kind of software is called an <strong class="keyWord">orchestrator</strong> since it dynamically allocates microservices to the available hardware resources in order to maximize performance. Moreover, orchestrators <a id="_idIndexMarker1440"/>like Kubernetes provide stable virtual addresses to microservices that they move around from one machine to another, thus changing their physical addresses. At the time of writing, Kubernetes is the most widespread orchestrator and the <em class="italic">de facto</em> standard for cluster orchestration that can be used with a wide ecosystem of tools and applications. While not being tied to specific languages or frameworks, Kubernetes is a fundamental tool for managing hardware resources and communications in .NET distributed applications based on microservices. This section introduces the basic Kubernetes concepts and entities.</p>
<p class="normal">A Kubernetes cluster is a cluster of virtual machines running the Kubernetes orchestrator.</p>
<figure class="mediaobject"><img alt="" role="presentation" src="img/B19820_20_01.png"/></figure>
<p class="packt_figref">Figure 20.1: Computer network equipped with Kubernetes</p>
<p class="normal">Generally, Kubernetes is installed on specific machines <a id="_idIndexMarker1441"/>referred to as <strong class="keyWord">master nodes</strong>, while all other computers <a id="_idIndexMarker1442"/>simply run an interface software that connects with the software running on the master nodes.</p>
<p class="normal">The virtual machines composing the <a id="_idIndexMarker1443"/>cluster are called <strong class="keyWord">nodes</strong>. The smallest software unit we can deploy on Kubernetes is not a single application, but an aggregate of containerized applications called <strong class="keyWord">Pod</strong>. While <a id="_idIndexMarker1444"/>Kubernetes supports various types of containers, the most commonly used container type is Docker, which we analyzed in <em class="italic">Chapter 11</em>, <em class="italic">Applying a Microservice Architecture to Your Enterprise Application</em>, so we will confine our discussion here to Docker. Pods are aggregates of Docker images, each containing one of your .NET microservices or microservices implemented with other technologies.</p>
<p class="normal">More specifically, Pods are sets of Docker images constrained to be placed together on the same node during the overall life of the application. They can be moved to other nodes, but they must be moved together. This means that they can easily communicate through localhost ports. Communication between different Pods, however, is more complex since the IP addresses of Pods are ephemeral resources because Pods have no fixed node where they run, but rather are moved from one node to another by the orchestrator. Moreover, Pods may be replicated to increase performance, so, in general, it makes no sense to address a message to a specific Pod; instead, we address it to any of the identical replicas of the same Pod.</p>
<p class="normal">Cluster nodes and Pods are managed by master nodes that communicate with cluster administrators through <a id="_idIndexMarker1445"/>an API server, as shown in the following diagram:</p>
<figure class="mediaobject"><img alt="" role="presentation" src="img/B19820_20_02.png"/></figure>
<p class="packt_figref">Figure 20.2: Kubernetes cluster</p>
<p class="normal">The scheduler allocates Pods to nodes according to the administrator constraints, while the controller manager groups several daemons that monitor the cluster’s actual state and try to move it toward the desired state declared through the API server. There are controllers for several Kubernetes resources, from Pod replicas to communication facilities. In fact, each resource has some target objectives to be maintained while the application runs, and the controller verifies these objectives are actually achieved, possibly triggering corrective actions if not, such as moving some Pods running too slowly to less crowded nodes.</p>
<p class="normal">The kubelet manages the interaction of each non-master node with the master nodes.</p>
<p class="normal">In Kubernetes, communication<a id="_idIndexMarker1446"/> between Pods is handled by resources called <strong class="keyWord">Services</strong> that are <a id="_idIndexMarker1447"/>assigned virtual addresses by the Kubernetes infrastructure and that forward their communications to sets of identical Pods. In short, Services are Kubernetes’ way of assigning consistent virtual addresses to sets of Pod replicas.</p>
<p class="normal">All Kubernetes entities may be assigned name-value<a id="_idIndexMarker1448"/> pairs called <strong class="keyWord">labels</strong> that are used to reference them through a pattern-matching mechanism. More specifically, Selectors select Kubernetes entities by listing labels they must have.</p>
<p class="normal">Thus, for instance, all Pods that receive traffic from the same Service are selected by specifying labels that they must have in the Service definition.</p>
<p class="normal">The way a Service routes its traffic to all connected Pods depends on the way Pods are organized. Stateless Pods are organized in so-called <code class="inlineCode">ReplicaSets</code>. <code class="inlineCode">ReplicaSets</code> have a unique virtual address assigned to the whole group and traffic is split equally among all Pods of the group.</p>
<p class="normal">Stateful Kubernetes Pod replicas are organized into so-called <code class="inlineCode">StatefulSets</code>. <code class="inlineCode">StatefulSets</code> use sharding to split the traffic between all their Pods. For this reason, Kubernetes Services assign a different name to each Pod of the <code class="inlineCode">StatefulSet</code> they are connected to. These names look like the following: <code class="inlineCode">basename-0.&lt;base URL&gt;</code>, <code class="inlineCode">basename-1.&lt;base URL&gt;</code>, ..., <code class="inlineCode">basename-n.&lt;base URL&gt;</code>. This way, message sharding is easily accomplished as follows:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Each time a message must be sent to a <code class="inlineCode">StatefulSet</code> composed of <em class="italic">N</em> replicas, you compute a hash between <code class="inlineCode">0</code> and <code class="inlineCode">N-1</code>, say <code class="inlineCode">X</code>.</li>
<li class="numberedList">Add the postfix <code class="inlineCode">X</code> to a base name to get a cluster address, such as <code class="inlineCode">basename-x.&lt;base URL&gt;</code>.</li>
<li class="numberedList">Send the message to the <code class="inlineCode">basename-x.&lt;base URL&gt; cluster address</code>.</li>
</ol>
<p class="normal">Kubernetes has no predefined storing facilities, and you can’t use node disk storage since Pods are moved between the available nodes, so long-term storage must be provided with sharded cloud databases or with other kinds of cloud storage. While each Pod in a StatefulSet can access a sharded cloud database with the usual connection string technique, Kubernetes offers a technique to abstract disk-like cloud storage provided by the external Kubernetes cluster environment. We will describe this storage in the <em class="italic">Advanced Kubernetes concepts</em> section.</p>
<p class="normal">All Kubernetes entities <a id="_idIndexMarker1449"/>mentioned in this short introduction can be defined in a <code class="inlineCode">.yaml</code> file, which, once deployed to a Kubernetes cluster, causes the creation of all entities defined in the file. The subsection that follows describes <code class="inlineCode">.yaml</code> files, while the other subsections thereafter describe in detail all the basic Kubernetes objects mentioned so far, and explain how to define them in a <code class="inlineCode">.yaml</code> file. Other Kubernetes objects will be described throughout the chapter.</p>
<h2 class="heading-2" id="_idParaDest-414">.yaml files</h2>
<p class="normal">The desired configuration of a <a id="_idIndexMarker1450"/>cluster and the structure of Kubernetes objects are described by the <a id="_idIndexMarker1451"/>developer with a language called YAML, and are packaged in files with a <code class="inlineCode">.yaml</code> extension.</p>
<p class="normal"><code class="inlineCode">.yaml</code> files, like JSON files, can be used to describe nested objects and collections in a human-readable way, but they do it with a different syntax. You have objects and lists, but object properties are not surrounded by <code class="inlineCode">{}</code>, and lists are not surrounded by <code class="inlineCode">[]</code>. Instead, nested objects are declared by simply indenting their content with spaces. The number of spaces can be freely chosen, but once they’ve been chosen, they must be used consistently.</p>
<p class="normal">List items can be distinguished from object properties by preceding them with a hyphen (<code class="inlineCode">-</code>).</p>
<p class="normal">Here is an example involving nested objects and collections:</p>
<pre class="programlisting code"><code class="hljs-code">Name: John
Surname: Smith
Spouse:
Name: Mary
Surname: Smith
Addresses:
- Type: home
Country: England
Town: London
Street: My home street
- Type: office
Country: England
Town: London
Street: My home street
</code></pre>
<p class="normal">The <a id="_idIndexMarker1452"/>preceding <code class="inlineCode">Person</code> object has a <code class="inlineCode">Spouse</code> nested object and a nested collection of addresses.</p>
<p class="normal">The same example<a id="_idIndexMarker1453"/> in JSON would be:</p>
<pre class="programlisting code"><code class="hljs-code">{
Name: John
Surname: Smith
Spouse:
{
  Name: Mary
Surname: Smith
}
Addresses:
[
 {
  Type: home
Country: England
Town: London
Street: My home street
 },
 {
  Type: office
Country: England
Town: London
Street: My home street
 }
]
}
</code></pre>
<p class="normal">As you can see, the syntax is more readable, since it avoids the overhead of parentheses.</p>
<p class="normal"><code class="inlineCode">.yaml</code> files can contain several sections, each defining a different entity, that are separated by a line containing the <code class="inlineCode">---</code> string. Comments are preceded by a <code class="inlineCode">#</code> symbol, which must be repeated on each comment line.</p>
<p class="normal">Each section starts with the declaration of the Kubernetes API group and version. In fact, not all objects belong to the same API group. For objects that belong to the <code class="inlineCode">core</code> API group, we can specify just the <a id="_idIndexMarker1454"/>API version, as in the following example:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: v1
</code></pre>
<p class="normal">While objects belonging<a id="_idIndexMarker1455"/> to different API groups must also specify the API name, as in the following example:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: apps/v1
</code></pre>
<p class="normal">In the next subsection, we analyze ReplicaSets and the Deployments that are built on top of them.</p>
<h2 class="heading-2" id="_idParaDest-415">ReplicaSets and Deployments</h2>
<p class="normal">The most important building block of<a id="_idIndexMarker1456"/> Kubernetes applications is the ReplicaSet, that is, a Pod replicated <em class="italic">N</em> times. Usually, however, you use a more complex object that is built on top of the ReplicaSet – the Deployment. Deployments not only create a ReplicaSet, but also monitor them to ensure that the number of replicas is kept constant regardless of hardware faults and other events that might involve the ReplicaSets. In other words, they are a declarative way of defining ReplicaSets and Pods.</p>
<p class="normal">Replicating the same functionalities, and thus the same Pods, is the simplest operation to optimize for performance: the more replicas we create of the same Pod, the more hardware resources and threads must be made available for the functionality encoded by that Pod. Thus, when we discover that a functionality becomes a bottleneck in the system, we may just increase the number of replicas of the Pod that encodes that functionality.</p>
<p class="normal">Each Deployment has a name (<code class="inlineCode">metadata-&gt;name</code>), an attribute that specifies the desired number of replicas (<code class="inlineCode">spec-&gt;replicas</code>), a key-value pair (<code class="inlineCode">spec -&gt; selector-&gt; matchLabels</code>) that selects the Pods to monitor, and a template (<code class="inlineCode">spec-&gt;template</code>) that specifies how to build the Pod replicas:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: apps/v1
kind: Deployment
metadata:
name: my-deployment-name
namespace: my-namespace #this is optional
spec:
replicas: 3
selector:
matchLabels:
my-pod-label-name: my-pod-label-value
...
template:
...
</code></pre>
<p class="normal"><code class="inlineCode">namespace</code> is optional and, if not provided, a namespace called <code class="inlineCode">default</code> is assumed. Namespaces are a way of keeping separate the objects of a Kubernetes cluster. For instance, a cluster can host <a id="_idIndexMarker1457"/>the objects of two completely independent applications, each placed in a separate <code class="inlineCode">namespace</code> in order to prevent possible name collisions. In a few words, Kubernetes namespaces have the same purpose as .NET namespaces: preventing name collisions.</p>
<p class="normal">Indented inside the template is the definition of the Pod to replicate. Complex objects such as Deployments can also contain other kinds of templates, for instance, a template of disk-like memory required by the external environment. We will discuss this in more detail in the <em class="italic">Advanced Kubernetes concepts</em> section.</p>
<p class="normal">In turn, the Pod template contains a <code class="inlineCode">metadata</code> section with the labels used to select the Pods, and a <code class="inlineCode">spec</code> section with a list of all of the containers:</p>
<pre class="programlisting code"><code class="hljs-code">metadata:
labels:
my-pod-label-name: my-pod-label-value
...
spec:
containers:
...
- name: my-container-name
image: &lt;Docker imagename&gt;
resources:
requests:
cpu: 100m
memory: 128Mi
limits:
cpu: 250m
memory: 256Mi
ports:
- containerPort: 6379
env:
- name: env-name
value: env-value
...
</code></pre>
<p class="normal">Each container has a name and must specify the name of the Docker image to use to create the containers. If the Docker image is not contained in the public Docker registry, the name must be a URI that also includes the repository’s location.</p>
<p class="normal">Then, containers must <a id="_idIndexMarker1458"/>specify the memory and CPU resources that they need to be created in the <code class="inlineCode">resources-&gt;requests</code> object. A Pod replica is created only if these resources are currently available. The <code class="inlineCode">resources-&gt;limits</code> object, instead, specifies the maximum resources a container replica can use. If, during the container execution, these limits are exceeded, action is taken to limit them. More specifically, if the CPU limit is exceeded, the container is throttled (its execution is stopped to restore its CPU consumption), while, if the memory limits are exceeded, the container is restarted. <code class="inlineCode">containerPort</code> must be the port exposed by the container. Here, we can also specify further information, such as the protocol used.</p>
<p class="normal">CPU time is expressed in millicores; <code class="inlineCode">1,000</code> millicores means <code class="inlineCode">100%</code> of the CPU time, while memory is expressed in mebibytes (<code class="inlineCode">1Mi</code> <code class="inlineCode">=</code> <code class="inlineCode">1,024*1,024 bytes</code>), or other units. <code class="inlineCode">env</code> lists all the operating system environment variables to pass to the containers with their values.</p>
<p class="normal">Both containers and Pod templates can contain other fields, such as properties that define virtual files, and properties that define commands that return the readiness and the health state of the container. We will analyze these fields in the <em class="italic">Advanced Kubernetes concepts</em> section.</p>
<p class="normal">The following subsection describes Pod sets conceived to store state information.</p>
<h2 class="heading-2" id="_idParaDest-416">StatefulSets</h2>
<p class="normal">StatefulSets are very similar<a id="_idIndexMarker1459"/> to ReplicaSets, but while the Pods of a ReplicaSet are indistinguishable <a id="_idIndexMarker1460"/>processors that contribute in parallel to the same workload through load-balancing strategies, Pods in a StatefulSet have a unique identity and can contribute to the same workload only through sharding. This is because StatefulSets were conceived to store information, and information cannot be stored in parallel, merely split among several stores through sharding.</p>
<p class="normal">For the same reason, each Pod instance is always kept tied to any virtual disk space it requires (see the <em class="italic">Advanced Kubernetes concepts</em> section) so that each Pod instance is responsible for writing to a specific store.</p>
<p class="normal">Moreover, StatefulSets’ Pod instances have ordinal numbers attached to them. They are started in sequence according to these<a id="_idIndexMarker1461"/> numbers, and they are stopped in reverse order. If the StatefulSet contains <em class="italic">N</em> replicas, these numbers go from <code class="inlineCode">0</code> to <code class="inlineCode">N-1</code>. Moreover, a unique name for each instance is obtained by chaining the Pod name specified in the template with the instance <a id="_idIndexMarker1462"/>ordinal, in the following way – <code class="inlineCode">&lt;pod name&gt;-&lt;instance ordinal&gt;</code>. Thus, instance names will be something like <code class="inlineCode">mypodname-0</code>, <code class="inlineCode">mypodname-1</code>, and so on. As we will see in the <em class="italic">Services</em> subsection, instance names are used to build unique cluster network URIs for all instances, so that other Pods can communicate with a specific instance of a StateFulSet Pod.</p>
<p class="normal">Since Pods in a StateFulSet have memory, each of them can only serve the requests that can be processed with the data contained in them. Therefore, in order to take advantage of several Pods in a StatefulSets, we must share the whole data space in easy-to-compute subsets. This<a id="_idIndexMarker1463"/> technique is called sharding. For instance, Pods of a StatefulSet that handle customers could each be assigned a different set of customer names according to their first letters. One could handle all customers whose names start with letters in the interval A-C, another the names in the interval D-F, and so on.</p>
<p class="normal">Here is a typical StatefulSet definition:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: apps/v1
kind: StatefulSet
metadata:
name: my-stateful-set-name
spec:
selector:
matchLabels:
my-pod-label-name: my-pod-label-value
...
serviceName: "my-service-name"
replicas: 3
template:
...
</code></pre>
<p class="normal">The template part is the same as that of Deployments. The main difference between StatefulSets and Deployments is the <code class="inlineCode">serviceName</code> field. This specifies the name of a service that must be connected with the StatefulSet to provide unique network addresses for all Pod instances. We will discuss this subject in more detail in the <em class="italic">Services </em>subsection. Moreover, usually, StatefulSets use some form of storage. We will discuss this in detail in the <em class="italic">Advanced Kubernetes concepts</em> section.</p>
<p class="normal">It is also worth pointing out<a id="_idIndexMarker1464"/> that the default order of the creation and stop strategy of StatefulSets can be changed <a id="_idIndexMarker1465"/>by specifying an explicit <code class="inlineCode">Parallel</code> value for the <code class="inlineCode">spec-&gt;podManagementPolicy</code> property (the default value is <code class="inlineCode">OrderedReady</code>).</p>
<p class="normal">The following table summarizes the<a id="_idIndexMarker1466"/> differences between StatefulSets and<a id="_idIndexMarker1467"/> ReplicaSets:</p>
<table class="table-container" id="table001-5">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Features</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">StatefulSets</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">ReplicaSets</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">Unique address for the whole set</p>
</td>
<td class="table-cell">
<p class="normal">No. Each Pod in the set has a different address and takes care of a different kind of requests. </p>
</td>
<td class="table-cell">
<p class="normal">Yes. Pods in ReplicaSets are indistinguishable so each request can be served by any of them.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">Number of replicas can be increased during application lifetime</p>
</td>
<td class="table-cell">
<p class="normal">No. Since each Pod is in charge of a specific kind of requests and has a unique address, we can't add more Pods.</p>
</td>
<td class="table-cell">
<p class="normal">Yes. Since Pods are indistinguishable, more Pods can't cause problems, but just improve the performance of the whole set.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">Pods can store permanent data inside of them</p>
</td>
<td class="table-cell">
<p class="normal">Yes, they are designed for this. Requests are issued to Pods with the sharding technique. </p>
</td>
<td class="table-cell">
<p class="normal">No, because they are designed to be undistinguishable, and storing a specific datum in a specific Pod would make a Pod different from the others in the set.</p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 20.1: StataefulSets versus ReplicaSets</p>
<p class="normal">The following subsection describes how to provide stable network addresses to both ReplicaSets and StatefulSets.</p>
<h2 class="heading-2" id="_idParaDest-417">Services</h2>
<p class="normal">Since Pod instances can be moved between nodes, they have no stable IP address attached to them. Services <a id="_idIndexMarker1468"/>take care of assigning a unique and stable virtual address to a whole ReplicaSet and of load balancing the traffic to all its instances. Services are not <a id="_idIndexMarker1469"/>software objects created in the cluster, but just an abstraction of the various settings and activities needed to implement their functionalities.</p>
<p class="normal">Services work at level 4 of the protocol stack, so they understand protocols such as TCP, but they aren’t able to perform, for instance, HTTP-specific actions/transformations, such as ensuring a secure HTTPS connection. Therefore, if you need to install HTTPS certificates on the Kubernetes cluster, you need a more complex object that is capable of interacting at level 7 of the protocol stack. The <code class="inlineCode">Ingress</code> object was conceived for this. We will discuss this in<a id="_idIndexMarker1470"/> the next subsection.</p>
<p class="normal">Services also handle assigning a unique virtual address to each instance of a StatefulSet. In fact, there are various kinds of Services; some were conceived for ReplicaSets and others for StatefulSets.</p>
<p class="normal">A <code class="inlineCode">ClusterIP</code> service type is assigned a unique cluster internal IP address. It specifies the ReplicaSets or Deployments it is connected to through label pattern matching. It uses tables maintained by the Kubernetes infrastructure to load balance the traffic it receives between all the Pod instances to which it is connected.</p>
<p class="normal">Therefore, other Pods can communicate with the Pods connected to a Service by interacting with this Service that is assigned the stable network name <code class="inlineCode">&lt;service name&gt;.&lt;service namespace&gt;.svc.cluster.local</code>. Since they are just assigned local IP addresses, a <code class="inlineCode">ClusterIP</code> service can’t be accessed from outside the Kubernetes cluster.</p>
<div><p class="normal">A ClusterIP is the usual communication choice for Deployments and ReplicaSets that do not communicate with anything outside of their Kubernetes cluster.</p>
</div>
<p class="normal">Here is the definition of a typical <code class="inlineCode">ClusterIP</code> service:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: v1
kind: Service
metadata:
name: my-service
namespace: my-namespace
spec:
selector:
my-selector-label: my-selector-value
...
ports:
- name: http
protocol: TCP
port: 80
targetPort: 9376
- name: https
protocol: TCP
port: 443
targetPort: 9377
</code></pre>
<p class="normal">Each Service can work on several ports and can route any port (<code class="inlineCode">port</code>) to the ports exposed by the containers (<code class="inlineCode">targetPort</code>). However, it is very often the case that <code class="inlineCode">port = targetPort</code>. Ports can be given names, but these names are optional. Also, the specification of the protocol is <a id="_idIndexMarker1471"/>optional; when not explicitly specified, all supported level 4 protocols are allowed. The <code class="inlineCode">spec-&gt;selector</code> property specifies all the name/value pairs that select the Pods for the<a id="_idIndexMarker1472"/> Service to which to route the communications it receives.</p>
<p class="normal">Since a <code class="inlineCode">ClusterIP</code> service can’t be accessed from outside the Kubernetes cluster, we need other Service types to expose a Kubernetes application on a public IP address.</p>
<p class="normal"><code class="inlineCode">NodePort</code>-type Services are the simplest way to expose Pods to the outside world. In order to implement a <code class="inlineCode">NodePort</code> service, the same port <code class="inlineCode">x</code> is opened on all nodes of the Kubernetes cluster and each node routes the traffic it receives on this port to a newly created <code class="inlineCode">ClusterIP</code> service.</p>
<p class="normal">In turn, the <code class="inlineCode">ClusterIP</code> service routes its traffic to all Pods selected by the service:</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="img/B19820_20_03.png"/></figure>
<p class="packt_figref">Figure 20.3: NodePort service</p>
<p class="normal">Therefore, you can simply<a id="_idIndexMarker1473"/> communicate with port <code class="inlineCode">x</code> through a public IP of any cluster node in order to access<a id="_idIndexMarker1474"/> the Pods connected to the <code class="inlineCode">NodePort</code> service. Of course, the whole process is completely automatic and hidden from the developer, whose only preoccupation is getting the port number <code class="inlineCode">x</code> so they know where to forward the external traffic.</p>
<p class="normal">The definition of a <code class="inlineCode">NodePort</code> service is similar to the definition of a <code class="inlineCode">ClusterIP</code> service, the only difference being that they specify a value of <code class="inlineCode">NodePort</code> for the <code class="inlineCode">spec-&gt;type</code> property:</p>
<pre class="programlisting code"><code class="hljs-code">...
spec:
type: NodePort
selector:
...
</code></pre>
<p class="normal">As a default, a node port <code class="inlineCode">x</code> in the range 30000-32767 is automatically chosen for each <code class="inlineCode">targetPort</code> specified by the <code class="inlineCode">Service</code>. The <code class="inlineCode">port</code> property associated with each <code class="inlineCode">targetPort</code> is meaningless for <code class="inlineCode">NodePort</code> Services since all traffic passes through the selected node port <code class="inlineCode">x</code>, and, by convention, is set to the same value as the <code class="inlineCode">targetPort</code>.</p>
<p class="normal">The developer can also set the NodePort <code class="inlineCode">x</code> directly through a <code class="inlineCode">nodePort</code> property:</p>
<pre class="programlisting code"><code class="hljs-code">...
ports:
- name: http
protocol: TCP
port: 80
targetPort: 80
nodePort: 30007
- name: https
protocol: TCP
port: 443
targetPort: 443
nodePort: 30020
...
</code></pre>
<p class="normal">When the Kubernetes cluster is hosted on a cloud, the more convenient way to expose some Pods to the outside world is through a <code class="inlineCode">LoadBalancer</code> service, in which case the Kubernetes cluster is<a id="_idIndexMarker1475"/> exposed to the outside world through a level 4 load balancer of the selected cloud provider.</p>
<div><p class="normal">A LoadBalancer is the usual communication choice for Deployments and ReplicaSets that do communicate outside of their Kubernetes cluster but don’t need advanced HTTP features.</p>
</div>
<p class="normal">The definition of a <code class="inlineCode">LoadBalancer</code> service<a id="_idIndexMarker1476"/> is similar to that of a <code class="inlineCode">ClusterIp</code> service, the only difference being that the <code class="inlineCode">spec-&gt;type</code> property must be set to <code class="inlineCode">LoadBalancer</code>:</p>
<pre class="programlisting code"><code class="hljs-code">...
spec:
type: LoadBalancer
selector:
...
</code></pre>
<p class="normal">If no further specification is added, a dynamic public IP is randomly assigned. However, if a specific public IP address is required, it can be set as a public IP address for the cluster load balancer by specifying it in the <code class="inlineCode">spec-&gt;loadBalancerIP</code> property:</p>
<pre class="programlisting code"><code class="hljs-code">...
spec:
type: LoadBalancer
loadBalancerIP: &lt;your public ip&gt;
selector:
...
</code></pre>
<p class="normal">In <strong class="keyWord">Azure Kubernetes Service</strong> (<strong class="keyWord">AKS</strong>), you must also specify the resource group where the IP address was<a id="_idIndexMarker1477"/> allocated in an annotation:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: v1
kind: Service
metadata:
annotations:
service.beta.kubernetes.io/azure-load-balancer-resource-group: &lt;IP resource group name&gt;
name: my-service-name
...
</code></pre>
<p class="normal">In AKS, you can remain with a dynamic IP address, but you can get a public static domain name of the type <code class="inlineCode">&lt;my-service-label&gt;.&lt;location&gt;.cloudapp.azure.com</code>, where <code class="inlineCode">&lt;location&gt;</code> is the geographic label <a id="_idIndexMarker1478"/>you have chosen for your resources. <code class="inlineCode">&lt;my-service-label&gt;</code> is a label that you have verified that makes the previous domain name unique. The chosen label must be declared in an annotation of your service, as shown here:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: v1
kind: Service
metadata:
annotations:
service.beta.kubernetes.io/azure-dns-label-name: &lt;my-service-label&gt;
name: my-service-name
...
</code></pre>
<p class="normal">StatefulSets don’t need any load balancing since each Pod instance has its own identity, but do require a unique URL address for each Pod instance. This unique URL is provided by the so-called <strong class="keyWord">headless Services</strong>. Headless <a id="_idIndexMarker1479"/>Services are defined like <code class="inlineCode">ClusterIP</code> services, the only difference <a id="_idIndexMarker1480"/>being that they have their <code class="inlineCode">spec-&gt;clusterIP</code> property set to <code class="inlineCode">none</code>:</p>
<pre class="programlisting code"><code class="hljs-code">...
spec:
clusterIP: none
selector:
...
</code></pre>
<p class="normal">All StatefulSets handled by a headless Service must place the Service name in their <code class="inlineCode">spec-&gt; serviceName</code> property, as already stated in the <em class="italic">StatefulSets</em> subsection.</p>
<p class="normal">The unique name provided by a headless Service to all <code class="inlineCode">StatefulSets</code> Pod instances it handles is <code class="inlineCode">&lt;unique pod name&gt;.&lt;service name&gt;.&lt;namespace&gt;.svc.cluster.local</code>.</p>
<p class="normal">Services only understand low-level protocols, such as TCP/IP, but most web applications are situated on the more sophisticated HTTP protocol. That’s why Kubernetes offers higher-level entities called I<strong class="keyWord">ngresses</strong> that are<a id="_idIndexMarker1481"/> built on top of services.</p>
<p class="normal">Ingresses are<a id="_idIndexMarker1482"/> fundamental in the implementation of all web-based applications that need support for HTTP. Moreover, since at the moment, a substantial amount of applications are web applications, ingresses are a <strong class="keyWord">must</strong> for all microservices applications. In particular, they are needed by all microservices based on ASP.NET Core, which we will discuss in the remainder of the book.</p>
<p class="normal">The following subsection describes these and explains how to expose a set of Pods through a level 7 protocol load balancer, which can be used to get access to typical HTTP services, instead of through a <code class="inlineCode">LoadBalancer</code> Service.</p>
<h2 class="heading-2" id="_idParaDest-418">Ingresses</h2>
<p class="normal">Ingresses were conceived to enable each application running in a Kubernetes cluster to expose an HTTP-based interface. This is <a id="_idIndexMarker1483"/>a fundamental requirement for any orchestrator, since nowadays all microservices applications are web applications that interact with their clients through HTTP-based protocols. Moreover, Ingresses must be very <a id="_idIndexMarker1484"/>efficient since all communications with the Kubernetes cluster will pass through them.</p>
<p class="normal">Accordingly, ingresses offer all of the typical services offered by an advanced and efficient web server. They provide the following services:</p>
<ul>
<li class="bulletList">HTTPS termination. They accept HTTPS connections and route them in HTTP format to any service in the cluster.</li>
<li class="bulletList">Name-based virtual hosting. They associate several domain names with the same IP address and route each domain, or <code class="inlineCode">&lt;domain&gt;/&lt;path prefix&gt;</code>, to a different cluster Service.</li>
<li class="bulletList">Load balancing.</li>
</ul>
<div><p class="normal">An ingress is the usual communication choice for Deployments and ReplicaSets that do communicate outside of their Kubernetes cluster and need advanced HTTP features.</p>
</div>
<p class="normal">Since rewriting all functionalities of an advanced web server from scratch would be substantially impossible, Ingresses rely on existing web servers to offer their services. More specifically, Kubernetes offers the possibility to add an interface module called Ingress Controllers to connect each Kubernetes cluster with an existing web server, such as NGINX and Apache.</p>
<p class="normal">Ingress Controllers are custom<a id="_idIndexMarker1485"/> Kubernetes objects that must be installed in the cluster. They handle the interface between Kubernetes and the pre-existing web server software, which can be either an external web server or a web server that is part of the Ingress Controller installation.</p>
<p class="normal">We will describe the installation of an Ingress Controller based on the NGINX web server software in the <em class="italic">Advanced Kubernetes concepts</em> section, as an example of the use of Helm. However, there are Ingress Controllers for all main web servers. The <em class="italic">Further reading</em> section contains information on how to install also an Ingress Controller that interfaces an external Azure Application Gateway.</p>
<p class="normal">HTTPS termination and name-based virtual hosting (see the explanation of these terms at the beginning of this subsection) can be configured in the Ingress definition in a way that is independent of the chosen Ingress Controller, while the way load balancing is achieved depends on the specific Ingress Controller chosen and on its configuration. Some Ingress Controller <a id="_idIndexMarker1486"/>configuration data can be passed in the <code class="inlineCode">metadata-&gt; annotations</code> field of the Ingress definition.</p>
<p class="normal">Name-based virtual hosting is defined in the <code class="inlineCode">spec-&gt; rules</code> section of the Ingress definition:</p>
<pre class="programlisting code"><code class="hljs-code">...
spec:
...
rules:
- host: *.mydomain.com
http:
paths:
- path: /
pathType: Prefix
backend:
service:
name: my-service-name
port:
number: 80
- host: my-subdomain.anotherdomain.com
...
</code></pre>
<p class="normal">Each rule specifies an optional hostname that can contain the <code class="inlineCode">*</code> wildcard. If no hostname is provided, the rule matches all hostnames. For each rule, we can specify several paths, each redirected to a different service/port pair, where the service is referenced through its<a id="_idIndexMarker1487"/> name. The way the match with each <code class="inlineCode">path</code> is carried out depends on the value of <code class="inlineCode">pathType</code>; if this value is <code class="inlineCode">Prefix</code>, the specified <code class="inlineCode">path</code> must be a prefix of any matching path. Otherwise, if this value is <code class="inlineCode">Exact</code>, the match must be exact. Matches are case-sensitive.</p>
<p class="normal">HTTPS termination on a specific hostname is specified by associating it with a certificate encoded in a Kubernetes secret:</p>
<pre class="programlisting code"><code class="hljs-code">...
spec:
...
tls:
- hosts:
- www.mydomain.com
secretName: my-certificate1
- my-subdomain.anotherdomain.com
secretName: my-certificate2
...
</code></pre>
<p class="normal">HTTPS certificates can be obtained free of charge at <a href="https://letsencrypt.org/">https://letsencrypt.org/</a>. The procedure is explained on the website, but basically, as with all certificate authorities, you provide a key and they return the certificate based on that key. It is also possible to install a <strong class="keyWord">certificate manager</strong> that takes <a id="_idIndexMarker1488"/>care of automatically installing and<a id="_idIndexMarker1489"/> renewing the certificate. The way a key/certificate pair is encoded in a Kubernetes secret string is detailed in the <em class="italic">Advanced Kubernetes concepts</em> section.</p>
<p class="normal">The whole Ingress definition is as follows:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
name: my-example-ingress
namespace: my-namespace
spec:
tls:
...
rules:
...
</code></pre>
<p class="normal">Here, the <code class="inlineCode">namespace</code> is optional, and if not specified, is assumed to be <code class="inlineCode">default</code>.</p>
<p class="normal">In the next section, we will <a id="_idIndexMarker1490"/>put into practice some of the concepts explained here by defining an Azure Kubernetes cluster and deploying a simple application.</p>
<h1 class="heading-1" id="_idParaDest-419">Interacting with Kubernetes clusters</h1>
<p class="normal">In this section, we will explain<a id="_idIndexMarker1491"/> both how to create an Azure Kubernetes cluster, and how to install minikube, a Kubernetes simulator, on your local machine. All examples can be run on both Azure Kubernetes and your local minikube instance.</p>
<h2 class="heading-2" id="_idParaDest-420">Creating an Azure Kubernetes cluster</h2>
<p class="normal">To create an AKS cluster, do <a id="_idIndexMarker1492"/>the following:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Type <code class="inlineCode">AKS</code> into the Azure search box.</li>
<li class="numberedList">Select <strong class="keyWord">Kubernetes services</strong>.</li>
<li class="numberedList">Then click the <strong class="keyWord">Create</strong> button.</li>
</ol>
<p class="normal">After that, the following form will appear:</p>
<figure class="mediaobject"><img alt="" role="presentation" src="img/B19820_20_04.png"/></figure>
<p class="packt_figref">Figure 20.4: Creating a Kubernetes cluster</p>
<p class="normal">It is worth mentioning that you can get help simply by hovering over any <a id="_idIndexMarker1493"/><img alt="" role="presentation" src="img/B19820_20_001.png"/> with the mouse.</p>
<p class="normal">As usual, you are required to specify a subscription, resource group, and region. Then, you can choose a unique name (<strong class="keyWord">Kubernetes cluster name</strong>) and the version of Kubernetes you would<a id="_idIndexMarker1494"/> like to use. For computational power, you are asked to select a machine template for each node (<strong class="keyWord">Node size</strong>) and the number of nodes. While for an actual application, it is recommended to select at least three nodes, let’s select just two nodes for our exercise in order to save our free Azure credit. </p>
<p class="normal">Moreover, the default virtual machine should also be set to a cheap one, so click <strong class="keyWord">Change size</strong> and select <strong class="keyWord">DS2 v2</strong>. Finally, set <strong class="keyWord">Scale method</strong> to <strong class="keyWord">Manual</strong> to prevent the number of nodes from being automatically changed, which might quickly burn through your free Azure credit.</p>
<p class="normal">The <strong class="keyWord">Availability zones</strong> setting allows you to spread your nodes across several geographic zones for better fault tolerance. The default is three zones. Please change it to two zones since we have just two nodes.</p>
<p class="normal">After implementing the preceding changes, you should see the following settings:</p>
<figure class="mediaobject"><img alt="Graphical user interface, text, application, email  Description automatically generated" src="img/B19820_20_05.png"/></figure>
<p class="packt_figref">Figure 20.5: Chosen settings</p>
<p class="normal">Now you can create your cluster by clicking the <strong class="keyWord">Review + create</strong> button. A review page should appear. Confirm and create the cluster.</p>
<p class="normal">If you click <strong class="keyWord">Next</strong> instead of <strong class="keyWord">Review + create</strong>, you can also define other node types, and then you can provide security information, namely, a <em class="italic">service principal</em>, and specify whether you wish to enable role-based access control. In Azure, service principals are accounts that are associated with services you may use to define resource access policies. You may also change the default network settings and other settings.</p>
<p class="normal">Deployment may take a little while (10-20 minutes). After that time, you will have your first Kubernetes cluster! At the end of the chapter, when the cluster is no longer required, please don’t forget to delete it in order to avoid wasting your free Azure credit.</p>
<p class="normal">In the next subsection, you <a id="_idIndexMarker1495"/>will learn how to install and use minikube, a single-node Kubernetes simulator, on your local machine.</p>
<h2 class="heading-2" id="_idParaDest-421">Using minikube</h2>
<p class="normal">The easiest way to<a id="_idIndexMarker1496"/> install minikube is the usage of the Windows installer you can find <a id="_idIndexMarker1497"/>in the official installation page: <a href="https://minikube.sigs.k8s.io/docs/start/">https://minikube.sigs.k8s.io/docs/start/</a>.</p>
<p class="normal">During the installation you <a id="_idIndexMarker1498"/>will be prompted on the kind of virtualization tool to use. If you already installed Docker Desktop and WSL, please specify Docker.</p>
<p class="normal">If you have a different operating system, please follow the default choices, instead.</p>
<p class="normal">The installation of Docker Desktop is explained in the technical requirements of <em class="italic">Chapter 11, Applying a Microservice Architecture to Your Enterprise Application</em>. Please note that both WSL and Docker Desktop must be installed and Docker must be configured to use Linux containers by default.</p>
<p class="normal">Once you have minikube installed, you must add its binary to your computer <code class="inlineCode">PATH</code>. The easiest way to do it is to open a PowerShell console and run the following command:</p>
<pre class="programlisting con"><code class="hljs-con">$oldPath=[Environment]::GetEnvironmentVariable('Path', [EnvironmentVariableTarget]::Machine)
if ($oldPath.Split(';') -inotcontains 'C:\minikube'){ '
 [Environment]::SetEnvironmentVariable('Path', $('{0};C:\minikube' -f $oldPath), [EnvironmentVariableTarget]::Machine) '
}
</code></pre>
<p class="normal">Once installed, your cluster can be run with:</p>
<pre class="programlisting con"><code class="hljs-con">minikube start
</code></pre>
<p class="normal">When you have finished working with the cluster, it can be stopped with:</p>
<pre class="programlisting con"><code class="hljs-con">minikube stop
</code></pre>
<p class="normal">In the next subsection, you<a id="_idIndexMarker1499"/> will learn how to interact with your minikube instance or Azure cluster through <a id="_idIndexMarker1500"/>Kubernetes’ official client, kubectl.</p>
<h2 class="heading-2" id="_idParaDest-422">Using kubectl</h2>
<p class="normal">Once you have created your <a id="_idIndexMarker1501"/>Azure Kubernetes cluster, you <a id="_idIndexMarker1502"/>can interact with it via the Azure Cloud Shell. Click on the console icon in the top right of your Azure portal page. The following screenshot shows the Azure Shell icon:</p>
<figure class="mediaobject"><img alt="" role="presentation" src="img/B19820_20_06.png"/></figure>
<p class="packt_figref">Figure 20.6: Azure Shell icon</p>
<p class="normal">When prompted, select the <strong class="keyWord">Bash Shell</strong>. Then you will be prompted to create a storage account, so confirm and create it.</p>
<p class="normal">We will use this shell to interact with our cluster. At the top of the shell there is a file icon that we will use to upload our <code class="inlineCode">.yaml</code> files:</p>
<figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" src="img/B19820_20_07.png"/></figure>
<p class="packt_figref">Figure 20.7: How to upload files in Azure Cloud Shell</p>
<p class="normal">It is also possible to download a client called Azure CLI and to install it on your local machine (see <a href="https://docs.microsoft.com/en-US/cli/azure/install-azure-cli">https://docs.microsoft.com/en-US/cli/azure/install-azure-cli</a>), but, in this case, you also need to install all the<a id="_idIndexMarker1503"/> tools needed to interact with the Kubernetes cluster (kubectl and Helm) that are pre-installed in Azure Cloud Shell.</p>
<p class="normal">Once you’ve created a Kubernetes cluster, you can interact with it through the <code class="inlineCode">kubectl</code> command-line tool. <code class="inlineCode">kubectl</code> is integrated into Azure Cloud Shell, so you just need to activate your cluster credentials <a id="_idIndexMarker1504"/>to use it. You can do this with the following Azure Cloud Shell command:</p>
<pre class="programlisting con"><code class="hljs-con">az aks get-credentials --resource-group &lt;resource group&gt; --name &lt;cluster name&gt;
</code></pre>
<p class="normal">The preceding command stores the credentials that were automatically created to enable your interaction<a id="_idIndexMarker1505"/> with the cluster in a <code class="inlineCode">/.kube/config</code> configuration file. From now on, you can issue your <code class="inlineCode">kubectl</code> commands with no further authentication.</p>
<p class="normal">If, instead, you need to interact with your local minikube cluster, you need a local installation of <code class="inlineCode">kubectl</code>, but minikube installs it automatically for you.</p>
<p class="normal">In order to use the automatically-installed <code class="inlineCode">kubectl</code>, all <code class="inlineCode">kubectl</code> commands must be preceded by the <code class="inlineCode">minikube</code> command and <code class="inlineCode">kubectl</code> must be followed by --. Thus, for instance, if you wanted to run the following command:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl get all
</code></pre>
<p class="normal">Then you would have to write the following:</p>
<pre class="programlisting con"><code class="hljs-con">minkube kubectl -- get all
</code></pre>
<p class="normal">In the remainder of the chapter, we will write commands that work on actual Kubernetes clusters such as Azure Kubernetes. Therefore, when using minikube, remember to replace <code class="inlineCode">kubectl</code> with <code class="inlineCode">minikube kubectl</code> -- in your commands.</p>
<p class="normal">If you issue the <code class="inlineCode">kubectl get nodes</code> command, you get a list of all your Kubernetes nodes. In general, <code class="inlineCode">kubectl get &lt;object type&gt;</code> lists all objects of a given type. You can use it with <code class="inlineCode">nodes</code>, <code class="inlineCode">pods</code>, <code class="inlineCode">statefulset</code>, and so on. <code class="inlineCode">kubectl get all</code> shows a list of all the objects created in your cluster. If you also add the name of a specific object, you will get information on just that specific object, as shown here:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl get &lt;object type&gt;&lt;object name&gt;
</code></pre>
<p class="normal">If you add the <code class="inlineCode">--watch</code> option, the object list will be continuously updated, so you can see the state of all the selected objects changing over time. You can leave this watch state by hitting <em class="italic">Ctrl</em> + <em class="italic">C</em>.</p>
<p class="normal">The following command shows a detailed report on a specific object:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl describe &lt;object name&gt;
</code></pre>
<p class="normal">All objects described in a <code class="inlineCode">.yaml</code> file, say <code class="inlineCode">myClusterConfiguration.yaml</code>, can be created with the following command:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl create -f myClusterConfiguration.yaml
</code></pre>
<p class="normal">Then, if you modify the <code class="inlineCode">.yaml</code> file, you can reflect all the modifications in your cluster with the <code class="inlineCode">apply</code> command, as shown<a id="_idIndexMarker1506"/> here:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl apply -f myClusterConfiguration.yaml
</code></pre>
<p class="normal"><code class="inlineCode">apply</code> does the same job as <code class="inlineCode">create</code> but, if the resource already exists, <code class="inlineCode">apply</code> overrides it, while <code class="inlineCode">create</code> exits with <a id="_idIndexMarker1507"/>an error message.</p>
<p class="normal">You can destroy all objects that were created with a <code class="inlineCode">.yaml</code> file by passing the same file to the <code class="inlineCode">delete</code> command, as shown here:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl delete -f myClusterConfiguration.yaml
</code></pre>
<p class="normal">The <code class="inlineCode">delete</code> command can also be passed an object type and a list of names of objects of that type to destroy, as shown in the following example:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl delete deployment deployment1 deployment2...
</code></pre>
<p class="normal">The preceding <code class="inlineCode">kubectl</code> commands should suffice for most of your practical needs. For more details, the <em class="italic">Further reading</em> section contains a link to the official documentation.</p>
<p class="normal">In the next subsection, we will use <code class="inlineCode">kubectl create</code> to install a simple demo application.</p>
<h2 class="heading-2" id="_idParaDest-423">Deploying the demo Guestbook application</h2>
<p class="normal">The Guestbook application is a <a id="_idIndexMarker1508"/>demo application used in the<a id="_idIndexMarker1509"/> examples in the official Kubernetes documentation. We will use it as an example of a Kubernetes application since its Docker images are available in the public Docker repository, so we don’t need to write software.</p>
<p class="normal">The Guestbook application stores the opinions of customers who visit a hotel or a restaurant.</p>
<p class="normal">It is composed of a UI, and an in-memory database, based on Redis. Moreover, updates are sent to the master copy <a id="_idIndexMarker1510"/>of the Redis database, which is automatically replicated in <em class="italic">N</em> read-only Redis replicas.</p>
<figure class="mediaobject"><img alt="" role="presentation" src="img/B19820_20_08.png"/></figure>
<p class="packt_figref">Figure 20.8: Architecture of the Guestbook application</p>
<p class="normal">The UI application can be deployed in Kubernetes as a Deployment, since it is memoryless.</p>
<p class="normal">The Redis master store is <a id="_idIndexMarker1511"/>deployed as a single pod <code class="inlineCode">Deployment</code>. We can’t implement it with an <em class="italic">N</em>-pods <code class="inlineCode">Deployment</code> since we need sharding for parallelizing updates. However, we might have used a <code class="inlineCode">StatefulSet</code> assigning a different data shard to each different master Pod. However, since this is your first Kubernetes exercise and since write operations should not be predominant, a single master database should suffice in the practical case of a single restaurant/hotel.</p>
<p class="normal">Since all slave copies contain the same data and consequently are undistinguishable, they can be implemented with a <code class="inlineCode">Deployment</code>, too.</p>
<p class="normal">The whole application is <a id="_idIndexMarker1512"/>composed of three <code class="inlineCode">.yaml</code> files that you can find in the GitHub repository associated with this book (<a href="https://github.com/PacktPublishing/Software-Architecture-with-C-Sharp-12-and-.NET-8-4E">https://github.com/PacktPublishing/Software-Architecture-with-C-Sharp-12-and-.NET-8-4E</a>).</p>
<p class="normal">Here is the code for the<a id="_idIndexMarker1513"/> master storage based on Redis that is contained in the <code class="inlineCode">redis-master.yaml</code> file:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: apps/v1
kind: Deployment
metadata:
name: redis-master
labels:
app: redis
spec:
selector:
matchLabels:
app: redis
role: master
tier: backend
replicas: 1
template:
metadata:
labels:
app: redis
role: master
tier: backend
spec:
containers:
- name: master
image: docker.io/redis:6.0.5
resources:
requests:
cpu: 100m
memory: 100Mi
ports:
- containerPort: 6379
---
apiVersion: v1
kind: Service
metadata:
name: redis-leader
labels:
app: redis
role: master
tier: backend
spec:
ports:
- port: 6379
targetPort: 6379
selector:
app: redis
role: master
tier: backend
</code></pre>
<p class="normal">The file is composed of two object definitions separated by a line containing just <code class="inlineCode">---</code>, that is, the object definition <a id="_idIndexMarker1514"/>separator of <code class="inlineCode">.yaml</code> files. It is common to group related objects, such as a Deployment with its associated Service, in the same file separated by the <code class="inlineCode">---</code> objects separator<a id="_idIndexMarker1515"/> symbol in order to increase code readability.</p>
<p class="normal">The first object is a <code class="inlineCode">Deployment</code> with a single replica, and the second object is a <code class="inlineCode">ClusterIP</code> Service that exposes the <code class="inlineCode">Deployment</code> on the <code class="inlineCode">6379</code> port at the internal <code class="inlineCode">redis-leader.default.svc.cluster.local</code> network address. The Deployment pod template defines the three <code class="inlineCode">app</code>, <code class="inlineCode">role</code>, and <code class="inlineCode">tier</code> labels with values that are used in the <code class="inlineCode">selector</code> definition of the Service to connect the Service with the unique Pod defined in the <code class="inlineCode">Deployment</code>.</p>
<p class="normal">Let’s upload the <code class="inlineCode">redis-master.yaml</code> file to Azure Cloud Shell, and then deploy it in the cluster with the following command:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl create -f redis-master.yaml
</code></pre>
<p class="normal">Once the operation is complete, you can inspect the contents of the cluster with <code class="inlineCode">kubectl get all</code>.</p>
<p class="normal">The slave storage is defined in the <code class="inlineCode">redis-slave.yaml</code> file and is created in the same way, the only difference being that this time we have two replicas, and a different Docker image. The full code is in the GitHub repository associated with this book.</p>
<p class="normal">Let’s upload this file as well and deploy it with the following command:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl create -f redis-slave.yaml
</code></pre>
<p class="normal">The code for the UI tier is contained in the <code class="inlineCode">frontend.yaml</code> file. <code class="inlineCode">Deployment</code> has three replicas and a different<a id="_idIndexMarker1516"/> Service type. Let’s upload and deploy this file with the following command:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl create -f frontend.yaml
</code></pre>
<p class="normal">It is worthwhile analyzing the Service code in the <code class="inlineCode">frontend.yaml</code> file:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: v1
kind: Service
metadata:
name: frontend
labels:
app: guestbook
tier: frontend
spec:
type: LoadBalancer
ports:
- port: 80
selector:
app: guestbook
tier: frontend
</code></pre>
<p class="normal">Again, the full code is in the GitHub repository associated with the book.</p>
<p class="normal">This Service is of the <code class="inlineCode">LoadBalancer</code> type. Since this Pod is the application interface with the world outside of the Kubernetes <a id="_idIndexMarker1517"/>cluster, its service must have a fixed IP and must be load balanced. Therefore, we must use a <code class="inlineCode">LoadBalancer</code> service since this is the unique service type that satisfies those requirements. (See the <em class="italic">Services</em> section of this chapter for more information.)</p>
<p class="normal">If you are in Azure Kubernetes or any other cloud Kubernetes service, in order to get the public IP address assigned to the service, and then to the application, use the following command:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl get service
</code></pre>
<p class="normal">The preceding command should display information on all the installed services. You should find the public IP in the <code class="inlineCode">EXTERNAL-IP</code> column of the list. If you see only <code class="inlineCode">&lt;none&gt;</code> values, please repeat the command until the public IP address is assigned to the load balancer.</p>
<p class="normal">If no IP is assigned after<a id="_idIndexMarker1518"/> a few minutes, please verify whether there is some error or warning in any of the service descriptions. If not, please check whether all deployments are actually running using the following command:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl get deployments
</code></pre>
<p class="normal">If, instead, you are on minikube, <code class="inlineCode">LoadBalancer</code> services can be accessed by issuing this command:</p>
<pre class="programlisting con"><code class="hljs-con">minikube service &lt;service name&gt;
</code></pre>
<p class="normal">Thus, in our case:</p>
<pre class="programlisting con"><code class="hljs-con">minikube service frontend
</code></pre>
<p class="normal">The command should automatically open the browser.</p>
<p class="normal">Once you get the IP address, navigate with the browser to this address. The application’s home page should now appear!</p>
<p class="normal">If the page doesn’t appear, verify <a id="_idIndexMarker1519"/>whether any service has an error by issuing the following command:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl get service
</code></pre>
<p class="normal">If not, also verify that all deployments are in the running state with the following:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl get deployments
</code></pre>
<p class="normal">If you find problems, please look for errors in the <code class="inlineCode">.yaml</code> files, correct them, and then update the object defined in the file with:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl update -f &lt;file name&gt;
</code></pre>
<div><p class="normal">Once you have finished experimenting with the application, make sure to remove the application from the cluster to avoid wasting your free Azure credit (public IP addresses cost money) with the following commands:</p>
<p class="normal"><code class="inlineCode">kubectl delete deployment frontend redis-master redis-slave</code></p>
<p class="normal"><code class="inlineCode">kubectl delete service frontend redis-leader redis-follower</code></p>
</div>
<p class="normal">In the next section, we will <a id="_idIndexMarker1520"/>analyze other important Kubernetes features.</p>
<h1 class="heading-1" id="_idParaDest-424">Advanced Kubernetes concepts</h1>
<p class="normal">In this section, we will discuss other important Kubernetes features, including how to assign permanent storage to<a id="_idIndexMarker1521"/> StatefulSets; how to store secrets such as passwords, connection strings, or certificates; how a container can inform Kubernetes about its health state; and how to handle complex Kubernetes packages with Helm. All of these subjects are organized into dedicated subsections. We will start with the problem of permanent storage.</p>
<h2 class="heading-2" id="_idParaDest-425">Requiring permanent storage</h2>
<p class="normal">Since Pods are moved between <a id="_idIndexMarker1522"/>nodes, they can’t store data on the disk storage offered by the current node where they are running, or they would lose that storage as soon as they are moved to a different node. This leaves us with two options:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Using external databases</strong>: With the help of databases, ReplicaSets can also store information. However, if we need better performance in terms of write/update operations, we should use distributed sharded databases based on non-SQL engines such as Cosmos DB or MongoDB (see <em class="italic">Chapter 12</em>, <em class="italic">Choosing Your Data Storage in the Cloud</em>). In this case, in order to take maximum advantage of table sharding, we need StatefulSets, where each Pod instance takes care of a different table shard.</li>
<li class="bulletList"><strong class="keyWord">Using cloud storage</strong>: Not being tied to a physical cluster node, cloud storage can be associated permanently with specific Pod instances of StatefulSets. Cloud storage is discussed in the <em class="italic">Redis</em> and <em class="italic">Azure storage accounts</em> sections of <em class="italic">Chapter 12</em>.</li>
</ul>
<p class="normal">Since access to external databases doesn’t require any Kubernetes-specific techniques but can be done with the usual connection strings, we will concentrate on cloud storage.</p>
<p class="normal">Kubernetes offers an abstraction of <a id="_idIndexMarker1523"/>storage called <strong class="keyWord">PersistentVolumeClaim</strong> (<strong class="keyWord">PVC</strong>) that is independent of the underlying storage provider. More specifically, PVCs are allocation requests that are either matched to predefined resources or allocated dynamically. When the Kubernetes cluster is in the cloud, typically, you use dynamic allocation carried out by dynamic providers installed by the cloud provider. For more information on cloud storage, please refer to <em class="italic">Chapter 12</em>.</p>
<p class="normal">Cloud providers such as Azure offer different storage classes with different performance and different costs. Moreover, the PVC can also specify the <code class="inlineCode">accessMode</code>, which can be:</p>
<ul>
<li class="bulletList"><code class="inlineCode">ReadWriteOnce</code>: The volume can be mounted as read-write by a single Pod.</li>
<li class="bulletList"><code class="inlineCode">ReadOnlyMany</code>: The volume can be mounted as read-only by many Pods.</li>
<li class="bulletList"><code class="inlineCode">ReadWriteMany</code>: The<a id="_idIndexMarker1524"/> volume can be mounted as read-write by many Pods.</li>
</ul>
<p class="normal">Volume claims can be added to StatefulSets in a specific <code class="inlineCode">spec-&gt;volumeClaimTemplates</code> object:</p>
<pre class="programlisting code"><code class="hljs-code">volumeClaimTemplates:
- metadata:
name: my-claim-template-name
spec:
resources:
request:
storage: 5Gi
volumeMode: Filesystem
accessModes:
- ReadWriteOnce
storageClassName: my-optional-storage-class
</code></pre>
<p class="normal">The <code class="inlineCode">storage</code> property contains the storage requirements. <code class="inlineCode">volumeMode</code> set to <code class="inlineCode">Filesystem</code> is a standard setting that means the storage will be available as a file path. The other possible value is <code class="inlineCode">Block</code>, which allocates the memory as <code class="inlineCode">unformatted</code>. <code class="inlineCode">storageClassName</code> must be set to an existing storage class offered by the cloud provider. If it’s omitted, the default storage class will be assumed.</p>
<p class="normal">All available storage classes can be listed with the following command:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl get storageclass
</code></pre>
<p class="normal">Once <code class="inlineCode">volumeClaimTemplates</code> has defined how to create permanent storage, then each container must specify which file path to attach that permanent storage to in the <code class="inlineCode">spec-&gt;containers-&gt;volumeMounts</code> property:</p>
<pre class="programlisting code"><code class="hljs-code">...
volumeMounts
- name: my-claim-template-name
mountPath: /my/requested/storage
readOnly: false
...
</code></pre>
<p class="normal">Here, <code class="inlineCode">name</code> must correspond to the name given to the PVC.</p>
<p class="normal">The following subsection<a id="_idIndexMarker1525"/> shows how to use Kubernetes secrets.</p>
<h2 class="heading-2" id="_idParaDest-426">Kubernetes secrets</h2>
<p class="normal">Some data, such as passwords and connection strings, cannot be exposed but need to be protected by some kind of<a id="_idIndexMarker1526"/> encryption. Kubernetes handles private sensitive data that need encryption through specific objects called <strong class="keyWord">secrets</strong>. <strong class="keyWord">Secrets</strong> are sets of<a id="_idIndexMarker1527"/> key-value pairs that are encrypted to protect them. They can be created by putting each value in a file, and then invoking the following <code class="inlineCode">kubectl</code> command:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl create secret generic my-secret-name \
  --from-file=./secret1.bin \
  --from-file=./secret2.bin
</code></pre>
<p class="normal">In this case, the filenames become the keys and the files’ contents are the values.</p>
<p class="normal">When the values are strings, they can be specified directly in the <code class="inlineCode">kubectl</code> command, as shown here:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl create secret generic dev-db-secret \
  --from-literal=username=devuser \
  --from-literal=password='$dsd_weew1'
</code></pre>
<p class="normal">In this case, keys and values are listed one after the other, separated by the <code class="inlineCode">=</code> character. In the previous example, the actual password is enclosed between single quotes to escape special characters like <code class="inlineCode">$</code> that are usually required to build strong passwords.</p>
<p class="normal">Once defined, secrets can<a id="_idIndexMarker1528"/> be referred to in the <code class="inlineCode">spec-&gt;volume</code> property of a Pod (Deployment or StatefulSet template), as shown here:</p>
<pre class="programlisting code"><code class="hljs-code">...
volumes:
- name: my-volume-with-secrets
secret:
secretName: my-secret-name
...
</code></pre>
<p class="normal">After that, each container can specify on which path to mount them in the <code class="inlineCode">spec-&gt;containers-&gt;volumeMounts</code> property:</p>
<pre class="programlisting code"><code class="hljs-code">...
volumeMounts:
- name: my-volume-with-secrets
mountPath: "/my/secrets"
readOnly: true
...
</code></pre>
<p class="normal">In the preceding example, each key is seen as a file with the same name as the key. The content of the file is the<a id="_idIndexMarker1529"/> secret value, base64-encoded. Therefore, the code that reads each file must decode its content (in .NET, <code class="inlineCode">Convert.FromBase64</code> will do the job).</p>
<p class="normal">When secrets contain strings, they can also be passed as environment variables in the <code class="inlineCode">spec-&gt;containers-&gt;env</code> object:</p>
<pre class="programlisting code"><code class="hljs-code">env:
- name: SECRET_USERNAME
valueFrom:
secretKeyRef:
name: dev-db-secret
key: username
- name: SECRET_PASSWORD
valueFrom:
secretKeyRef:
name: dev-db-secret
key: password
</code></pre>
<p class="normal">Here, the <code class="inlineCode">name</code> property must match the secret’s name. Passing secrets as environment variables is very convenient when containers host ASP.NET Core applications, since, in this case, environment <a id="_idIndexMarker1530"/>variables are all immediately available in the configuration object (see the <em class="italic">Loading configuration data and using it with the options framework</em> section of <em class="italic">Chapter 17</em>, <em class="italic">Presenting ASP.NET Core</em>).</p>
<p class="normal">Secrets can also encode the key/certificate pair of an HTTPS certificate with the following <code class="inlineCode">kubectl</code> command:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl create secret tls test-tls --key="tls.key" --cert="tls.crt"
</code></pre>
<p class="normal">Secrets defined in this way can be used to enable HTTPS termination in Ingresses. You can do this by placing the secret names in the <code class="inlineCode">spec-&gt;tls-&gt;hosts-&gt;secretName</code> properties of an Ingress.</p>
<h2 class="heading-2" id="_idParaDest-427">Liveness and readiness checks</h2>
<p class="normal">Kubernetes automatically monitors all containers to ensure they are still alive and that they keep their resource<a id="_idIndexMarker1531"/> consumption within the limits declared in the <code class="inlineCode">spec-&gt;containers-&gt;resources-&gt;limits</code> object. When some conditions are violated, the container is either throttled, or restarted, or the whole Pod instance is restarted on a different node. How does Kubernetes know that a container is in a healthy state? While it can use the operating system to check the healthy state of nodes, it has no universal check that works with all containers.</p>
<p class="normal">Therefore, the containers themselves must inform Kubernetes of their health, otherwise Kubernetes cannot verify them. Containers can inform Kubernetes of their health in two ways: either by declaring a console command that returns their health, or by declaring an endpoint that provides the same information.</p>
<p class="normal">Both declarations are provided in the <code class="inlineCode">spec-&gt; containers-&gt; livenessProb</code> object. The console command check is declared as shown here:</p>
<pre class="programlisting code"><code class="hljs-code">...
livenessProbe:
exec:
command:
- cat
- /tmp/healthy
initialDelaySeconds: 10
periodSeconds: 5
...
</code></pre>
<p class="normal">If <code class="inlineCode">command</code> returns <code class="inlineCode">0</code>, the container is considered healthy. In the preceding example, the software running in the container records its state of health in the <code class="inlineCode">/tmp/healthy</code> file, so the <code class="inlineCode">cat/tmp/healthy</code> command returns it. <code class="inlineCode">PeriodSeconds</code> is the time between checks, while <code class="inlineCode">initialDelaySeconds</code> is the initial delay before performing the first check. An initial delay is always necessary so as to give the container time to start.</p>
<p class="normal">The endpoint check is quite similar:</p>
<pre class="programlisting code"><code class="hljs-code">...
livenessProbe:
exec:
httpGet:
path: /healthz
port: 8080
httpHeaders:
- name: Custom-Health-Header
value: container-is-ok
initialDelaySeconds: 10
periodSeconds: 5
...
</code></pre>
<p class="normal">The test is successful if the HTTP <a id="_idIndexMarker1532"/>response contains the declared header with the declared value. You may also use a pure TCP check, as shown here:</p>
<pre class="programlisting code"><code class="hljs-code">...
livenessProbe:
exec:
tcpSocket:
port: 8080
initialDelaySeconds: 10
periodSeconds: 5
...
</code></pre>
<p class="normal">In this case, the check succeeds if Kubernetes is able to open a TCP socket to the container on the declared port.</p>
<p class="normal">Similarly, the readiness of containers once they are installed is monitored with a readiness check. The readiness check is defined in a similar way as the liveness check, the only difference being that <code class="inlineCode">livenessProbe</code> is replaced with <code class="inlineCode">readinessProbe</code>.</p>
<p class="normal">The following subsection explains how to autoscale Deployments.</p>
<h2 class="heading-2" id="_idParaDest-428">Autoscaling</h2>
<p class="normal">Instead of manually modifying the <a id="_idIndexMarker1533"/>number of replicas in a Deployment to adapt it to a decrease or increase in load, we can let Kubernetes decide for itself the number of replicas needed to keep a declared resource’s consumption constant. Thus, for instance, if we declare a target of 10% CPU consumption, then when the average resource consumption of each replica exceeds this limit, a new replica will be created. If the average CPU consumption falls below this limit, a replica is destroyed. The typical resource used to monitor replicas is CPU consumption, but we can also use memory consumption.</p>
<p class="normal">In actual high-traffic production systems, autoscaling is a <strong class="keyWord">must</strong>, because it is the only way to adapt quickly the system to changes in the load.</p>
<p class="normal">Autoscaling is achieved by defining a <code class="inlineCode">HorizontalPodAutoscaler</code> object. Here is an example of the <code class="inlineCode">HorizontalPodAutoscaler</code> definition:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
name: my-autoscaler
spec:
scaleTargetRef:
apiVersion: extensions/v1beta1
kind: Deployment
name: my-deployment-name
minReplicas: 1
maxReplicas: 10
metrics:
- type: Resource
resource:
name: cpu
targetAverageUtilization: 25
</code></pre>
<p class="normal"><code class="inlineCode">spec-&gt; scaleTargetRef-&gt;name</code> specifies the name of the Deployment to autoscale, while <code class="inlineCode">targetAverageUtilization</code> specifies the target resource (in our case, <code class="inlineCode">cpu</code>) percentage usage (in our case, 25%).</p>
<p class="normal">The following subsection gives a short introduction to the Helm package manager and Helm charts and explains how to install Helm charts on a Kubernetes cluster. An example of how to install an <a id="_idIndexMarker1534"/>Ingress Controller is given as well.</p>
<h2 class="heading-2" id="_idParaDest-429">Helm – installing an Ingress Controller</h2>
<p class="normal">Helm charts are a way to organize the installation of complex Kubernetes applications that contain several <code class="inlineCode">.yaml</code> files. A Helm <a id="_idIndexMarker1535"/>chart is a set of <code class="inlineCode">.yaml</code> files<a id="_idIndexMarker1536"/> organized into folders and subfolders. Here is a typical folder structure of a Helm chart taken from the official documentation:</p>
<figure class="mediaobject"><img alt="Text  Description automatically generated" src="img/B19820_20_09.png"/></figure>
<p class="packt_figref">Figure 20.9: Folder structure of a Helm chart</p>
<p class="normal">The <code class="inlineCode">.yaml</code> files specific to the application are placed in the top <code class="inlineCode">templates</code> directory, while the <code class="inlineCode">charts</code> directory may contain other Helm charts used as helper libraries. The top-level <code class="inlineCode">Chart.yaml</code> file contains general<a id="_idIndexMarker1537"/> information on the package (name and description), together <a id="_idIndexMarker1538"/>with both the application version and the Helm chart version. The following is a typical example:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: v2
name: myhelmdemo
description: My Helm chart
type: application
version: 1.3.0
appVersion: 1.2.0
</code></pre>
<p class="normal">Here, <code class="inlineCode">type</code> can be either <code class="inlineCode">application</code> or <code class="inlineCode">library</code>. Only <code class="inlineCode">application</code> charts can be deployed, while <code class="inlineCode">library</code> charts are utilities for developing other charts. <code class="inlineCode">library</code> charts are placed in the <code class="inlineCode">charts</code> folder of other Helm charts.</p>
<p class="normal">In order to configure each specific application installation, Helm chart <code class="inlineCode">.yaml</code> files contain variables that are specified when Helm charts are installed. Moreover, Helm charts also provide a simple templating language that allows some declarations to be included only if some conditions depending on the input variables are satisfied. The top-level <code class="inlineCode">values.yaml</code> file declares default values for the input variables, meaning that the developer needs to specify just the few variables for which they require values different from the defaults. We will not describe the Helm chart template language because it would be too extensive, but you can find it in the official Helm documentation referred to in the <em class="italic">Further reading</em> section.</p>
<p class="normal">Helm charts are usually organized in public or private repositories in a way that is similar to Docker images. There is a Helm client that you can use to download packages from a remote repository and to install charts in Kubernetes clusters. The Helm client is immediately<a id="_idIndexMarker1539"/> available in Azure Cloud Shell, so you can start using Helm for your Azure Kubernetes cluster <a id="_idIndexMarker1540"/>without needing to install it.</p>
<p class="normal">A remote repository must be added before using its packages, as shown in the following example:</p>
<pre class="programlisting con"><code class="hljs-con">helm repo add &lt;my-repo-local-name&gt; https://charts.helm.sh/stable
</code></pre>
<p class="normal">The preceding command makes the packages of a remote repository available and gives a local name to that remote repository. After that, any package from the remote repository can be installed with a command such as the following:</p>
<pre class="programlisting con"><code class="hljs-con">helm install &lt;instance name&gt;&lt;my-repo-local-name&gt;/&lt;package name&gt; -n &lt;namespace&gt;
</code></pre>
<p class="normal">Here, <code class="inlineCode">&lt;namespace&gt;</code> is the namespace in which to install the application. As usual, if it’s not provided, the <code class="inlineCode">default</code> namespace is assumed. <code class="inlineCode">&lt;instance name&gt;</code> is the name that you give to the installed application. You need this name to get information about the installed application with the following command:</p>
<pre class="programlisting con"><code class="hljs-con">helm status &lt;instance name&gt;
</code></pre>
<p class="normal">You can get also information about all applications installed with Helm with the help of the following command:</p>
<pre class="programlisting con"><code class="hljs-con">helm ls
</code></pre>
<p class="normal">The application name is also needed to delete the application from the cluster by means of the following command:</p>
<pre class="programlisting con"><code class="hljs-con">helm delete &lt;instance name&gt;
</code></pre>
<p class="normal">When we install an application, we may also provide a <code class="inlineCode">.yaml</code> file with all the variable values we want to override. We can also specify a specific version of the Helm chart, otherwise the most recent version is used. Here is an example with both the version and values overridden:</p>
<pre class="programlisting con"><code class="hljs-con">helm install &lt;instance name&gt;&lt;my-repo-local-name&gt;/&lt;package name&gt; -f  values.yaml --version &lt;version&gt;
</code></pre>
<p class="normal">Finally, value overrides can also be provided in-line with the <code class="inlineCode">--set</code> option, as shown here:</p>
<pre class="programlisting con"><code class="hljs-con">...--set &lt;variable1&gt;=&lt;value1&gt;,&lt;variable2&gt;=&lt;value2&gt;...
</code></pre>
<p class="normal">We can also upgrade an <a id="_idIndexMarker1541"/>existing installation with the <code class="inlineCode">upgrade</code> command, as shown here:</p>
<pre class="programlisting con"><code class="hljs-con">helm upgrade &lt;instance name&gt;&lt;my-repo-local-name&gt;/&lt;package name&gt;...
</code></pre>
<p class="normal">The <code class="inlineCode">upgrade</code> command <a id="_idIndexMarker1542"/>may be used to specify new value overrides with the <code class="inlineCode">–f</code> option or with the <code class="inlineCode">--set</code> option, and a new version with <code class="inlineCode">--version</code>.</p>
<p class="normal">Let’s use Helm to provide an Ingress for the Guestbook demo application. More specifically, we will use Helm to install an Ingress Controller based on Nginx. The detailed procedure to be observed is as follows:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Add the remote repository:
        <pre class="programlisting con"><code class="hljs-con">helm repo add gcharts https://charts.helm.sh/stable
</code></pre>
</li>
<li class="numberedList">Install the Ingress Controller:
        <pre class="programlisting con"><code class="hljs-con">helm install ingress gcharts/nginx-ingress
</code></pre>
</li>
<li class="numberedList">When the installation is complete, you should see an entry for the installed Ingress Controller among the installed services if you type <code class="inlineCode">kubectl get service</code>. The entry should contain a public IP. Please make a note of this IP since it will be the public IP of the application.</li>
<li class="numberedList">Open the <code class="inlineCode">frontend.yaml</code> file and remove the <code class="inlineCode">type: LoadBalancer</code> line. Save and upload this to Azure Cloud Shell. We changed the service type of the frontend application from <code class="inlineCode">LoadBalancer</code> to <code class="inlineCode">ClusterIP</code> (the default). This service will be connected to the new Ingress you are going to define.</li>
<li class="numberedList">Deploy <code class="inlineCode">redis-master.yaml</code>, <code class="inlineCode">redis-slave.yaml</code>, and <code class="inlineCode">frontend.yaml</code> with <code class="inlineCode">kubectl</code>, as detailed in the <em class="italic">Deploying the demo Guestbook application</em> subsection. Create a <code class="inlineCode">frontend-ingress.yaml</code> file and place the following code in it:
        <pre class="programlisting code"><code class="hljs-code">apiVersion: extensions/v1beta1
kind: Ingress
metadata:
name: simple-frontend-ingress
spec:
rules:
- http:
paths:
- path:/
backend:
serviceName: frontend
servicePort: 80
</code></pre>
</li>
<li class="numberedList">Upload <code class="inlineCode">frontend-ingress.yaml</code> to Azure Cloud Shell and deploy it with the following command:
        <pre class="programlisting con"><code class="hljs-con">kubectl apply -f frontend-ingress.yaml
</code></pre>
</li>
<li class="numberedList">Open the browser and navigate to the public IP you made a note of in <em class="italic">step 3</em>. There, you should see the application running.</li>
</ol>
<p class="normal">The public IP allocated to the <code class="inlineCode">Ingress-Controller</code> at <em class="italic">Step 3</em> is listed also in the <strong class="keyWord">Azure Public IP Addresses</strong> section of Azure. You <a id="_idIndexMarker1543"/>can find it by searching for this<a id="_idIndexMarker1544"/> section in the Azure search box. Once in this section, you should see this IP address listed. There you can also assign it a hostname of the type <code class="inlineCode">&lt;a name you can choose&gt;.&lt;your Azure region&gt;.cloudeapp.com</code>.</p>
<p class="normal">We recommend studying the <a href="https://letsencrypt.org/">https://letsencrypt.org/</a> documentation on how to require a certificate, assign a hostname to the application’s public IP, and then use this hostname to get a free HTTPS certificate from <a href="https://letsencrypt.org/">https://letsencrypt.org/</a>. Unfortunately, we can’t give more details since the procedure to require a certificate is too extensive. Once you get a certificate, you can generate a secret from it with the following command:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl create secret tls guestbook-tls --key="tls.key" --cert="tls.crt"
</code></pre>
<p class="normal">Then you can add the preceding secret to your <code class="inlineCode">frontend-ingress.yaml Ingress</code> by adding the following <code class="inlineCode">spec-&gt;tls</code> section to it:</p>
<pre class="programlisting code"><code class="hljs-code">...
spec:
...
tls:
- hosts:
- &lt;chosen name&gt;.&lt;your Azure region&gt;.cloudeapp.com
secretName: guestbook-tls
</code></pre>
<p class="normal">Following the correction, upload the file to your Azure Cloud Shell instance and update the previous Ingress definition with the following:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl apply –f frontend-ingress.yaml
</code></pre>
<p class="normal">At this point, you should be <a id="_idIndexMarker1545"/>able to access the Guestbook application with HTTPS.</p>
<p class="normal">When you are done <a id="_idIndexMarker1546"/>experimenting, please don’t forget to delete everything from your cluster to avoid wasting your free Azure credit. You can do this by means of the following commands:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl delete –f frontend-ingress.yaml
kubectl delete –f frontend.yaml
kubectl delete –f redis-slave.yaml
kubectl delete –f redis-master.yaml
helm delete ingress
</code></pre>
<h1 class="heading-1" id="_idParaDest-430">Summary</h1>
<p class="normal">In this chapter, we described Kubernetes’ basic concepts and objects, and then we explained how to create an AKS cluster. We also showed how to deploy applications and how to monitor and inspect the state of your cluster with a simple demo application.</p>
<p class="normal">The chapter also described more advanced Kubernetes features that have fundamental roles in practical applications, including how to provide persistent storage to the containers running on Kubernetes, how to inform Kubernetes of the health state of your containers, and how to offer advanced HTTP services, such as HTTPS and name-based virtual hosting.</p>
<p class="normal">Finally, we reviewed how to install complex applications with Helm, and gave a short description of Helm and Helm commands.</p>
<p class="normal">Up next, we have the book’s case study.</p>
<h1 class="heading-1" id="_idParaDest-431">Questions</h1>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Why are Services needed?</li>
<li class="numberedList">Why is an Ingress needed?</li>
<li class="numberedList">Why is Helm needed?</li>
<li class="numberedList">Is it possible to define several Kubernetes objects in the same <code class="inlineCode">.yaml</code> file? If yes, how?</li>
<li class="numberedList">How does Kubernetes detect container faults?</li>
<li class="numberedList">Why are persistent volume claims needed?</li>
<li class="numberedList">What is the difference between a ReplicaSet and a StatefulSet?</li>
</ol>
<h1 class="heading-1" id="_idParaDest-432">Further reading</h1>
<ul>
<li class="bulletList">A good book for extending the knowledge acquired in this chapter is the following: <a href="https://www.packtpub.com/product/hands-on-kubernetes-on-azure-second-edition/9781800209671">https://www.packtpub.com/product/hands-on-kubernetes-on-azure-second-edition/9781800209671</a>.</li>
<li class="bulletList">The official documentation for Kubernetes and <code class="inlineCode">.yaml</code> files can be found here: <a href="https://kubernetes.io/docs/home/">https://kubernetes.io/docs/home/</a>.</li>
<li class="bulletList">More information on Helm and Helm charts can be found in the official documentation. This is extremely well written and contains some good tutorials: <a href="https://helm.sh/">https://helm.sh/</a>.</li>
<li class="bulletList">The official documentation for Azure Kubernetes can be found here: <a href="https://docs.microsoft.com/en-US/azure/aks/">https://docs.microsoft.com/en-US/azure/aks/</a>.</li>
<li class="bulletList">The official documentation on the Azure Application Gateway-based Ingress Controller is available here: <a href="https://github.com/Azure/application-gateway-kubernetes-ingress">https://github.com/Azure/application-gateway-kubernetes-ingress</a>.</li>
<li class="bulletList">Ingress certificate release and renewal can be automated as explained here: <a href="https://docs.microsoft.com/azure/application-gateway/ingress-controller-letsencrypt-certificate-application-gateway">https://docs.microsoft.com/azure/application-gateway/ingress-controller-letsencrypt-certificate-application-gateway</a>. While the procedure specifies an Azure Application Gateway-based ingress controller, it is adequate for any Ingress Controller.</li>
</ul>
<h1 class="heading-1">Leave a review!</h1>
<p class="normal">Enjoying this book? Help readers like you by leaving an Amazon review. Scan the QR code below for a 20% discount code.</p>
<p class="normal"><img alt="" role="presentation" src="img/Leave_a_review_QR.png"/></p>
<p class="normal"><em class="italic">*Limited Offer</em></p>
</div>
</body></html>