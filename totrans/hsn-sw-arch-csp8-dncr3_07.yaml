- en: Applying a Microservice Architecture to Your Enterprise Application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter is dedicated to describing highly scalable architectures based
    on small modules called microservices. The microservices architecture allows for
    fine-grained scaling operations where every single module can be scaled as required
    without it affecting the remainder of the system. Moreover, they allow for better
    **Continuous Integration/Continuous Deployment** (**CI/CD**) by permitting every
    system subpart to evolve and be deployed independently of the others.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What are microservices?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When do microservices help?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does .NET Core deal with microservices?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which tools are needed to manage microservices?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case – logging a microservice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have learned how to implement a microservice
    in .NET Core based on this chapter's use case.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you will require the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Visual Studio 2017 or 2019 free Community Edition or better with all the database
    tools installed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A free Azure account. The *Creating an Azure account* section in [Chapter 1](14b5c5da-4042-439e-9e5a-2e19ba4c4930.xhtml),
    *Understanding the Importance of Software Architecture*, explains how to create
    one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A local emulator for Azure Service Fabric to debug your microservices in Visual
    Studio. It is free and can be downloaded from [https://www.microsoft.com/web/handlers/webpi.ashx?command=getinstallerredirect&appid=MicrosoftAzure-ServiceFabric-CoreSDK](https://www.microsoft.com/web/handlers/webpi.ashx?command=getinstallerredirect&appid=MicrosoftAzure-ServiceFabric-CoreSDK).
    To avoid installation issues, ensure your version of Windows is up to date. Moreover,
    the emulator uses PowerShell high-privilege-level commands that, by default, are
    blocked by PowerShell. To enable them, you need to execute the following command
    in the Visual Studio Package Manager Console or in any PowerShell console. Visual
    Studio or an external PowerShell console must be started as an *administrator*
    for the following command to be successful:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Docker CE for Windows if you want to debug Docker containerized microservices
    in Visual Studio ([https://store.docker.com/editions/community/docker-ce-desktop-windows?tab=description](https://store.docker.com/editions/community/docker-ce-desktop-windows?tab=description)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are microservices?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microservice architectures allow each module that makes up a solution to be
    scaled independently from the others to achieve the maximum throughput with minimal
    cost. In fact, scaling whole systems instead of their current bottlenecks inevitably
    results in a remarkable waste of resources, so a fine-grained control of subsystem
    scaling has a considerable impact on the system's overall cost.
  prefs: []
  type: TYPE_NORMAL
- en: However, microservices are more than scalable components – they are software
    building blocks that can be developed, maintained, and deployed independently
    of each other. Splitting development and maintenance among modules that can be
    independently developed, maintained, and deployed improves the overall system's
    CI/CD cycle (the *CI/CD* concept was explained in more detail in the *Organizing
    your work using Azure DevOps* section in [Chapter 3](bc26065f-b001-4123-9524-3bbfa87bfadd.xhtml),
    *Documenting Requirements with Azure DevOps*).
  prefs: []
  type: TYPE_NORMAL
- en: 'The CI/CD improvement is due to microservice *independence* because it enables
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Scaling and distributing microservices on different types of hardware.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since each microservice is deployed independently from the others, there can't
    be binary compatibility or database structure compatibility constraints. Therefore,
    there is no need to align the versions of the different microservices that compose
    the system. This means that each of them can evolve, as needed, without being
    constrained by the others.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assigning their development to completely separate smaller teams, thus simplifying
    job organization and reducing all the inevitable coordination inefficiencies that
    arise when handling large teams.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing each microservice with more adequate technologies and in a more
    adequate environment, since each microservice is an independent deployment unit.
    This means choosing tools that best fit your requirements and an environment that
    minimizes development efforts and/or maximizes performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since each microservice can be implemented with different technologies, programming
    languages, tools, and operating systems, enterprises can use all available human
    resources by matching environments with developers' competences. For instance,
    underused Java developers can also be involved in .NET projects if they implement
    microservices in Java with the same required behavior.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Legacy subsystems can be embedded in independent microservices, thus enabling
    them to cooperate with newer subsystems. This way, companies may reduce the time
    to market of new system versions. Moreover, this way, legacy systems can evolve
    slowly toward more modern systems with an acceptable impact on costs and the organization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next subsection explains how the concept of microservices was conceived.
    Then, we will continue this introductory section by exploring basic microservice
    design principles and analyzing why microservices are often designed as Docker
    containers.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices and the evolution of the concept of modules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For a better understanding of the advantages of microservices, as well as their
    design techniques, we must keep the two-folded nature of software modularity,
    and of software modules, in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: Code modularity refers to a code organization that makes it easy for us to modify
    a chunk of code without it affecting the remainder of the application. It is usually
    enforced with object-oriented design, where *modules* can be identified with classes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment modularity depends on what your deployment units are and which properties
    they have. The simplest deployment units are executable files and libraries. Thus,
    for instance, **dynamic link libraries** (**DLL**) are, for sure, more modular
    than static libraries since they must not be linked with the main executable before
    being deployed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While the fundamental concepts of code modularity have reached stasis, the concept
    of deployment modularity is still evolving and microservices are currently state
    of the art along this evolution path.
  prefs: []
  type: TYPE_NORMAL
- en: As a short review of the main milestones on the path that led to microservices,
    we can say that, first, monolithic executables were broken into static libraries.
    Later on, dynamic link libraries replaced static libraries.
  prefs: []
  type: TYPE_NORMAL
- en: A great change took place when .NET (and other analogous frameworks, such as
    Java) improved the modularity of executables and libraries. In fact, with .NET,
    they can be deployed on different hardware and on different operating systems
    since they are deployed in an intermediary language that's compiled when the library
    is executed for the first time. Moreover, they overcome some versioning issues
    of previous DLLs since any executable brings with it a DLL with a version that
    differs from the version of the same DLL that is installed in the operating system.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, .NET can''t accept two referenced DLLs – let''s say, A and B – using
    two different versions of a common dependency – let''s say, C. For instance, suppose
    there is a newer version of A with a lot of new features we would like to use
    that, in turn, rely on a newer version of C that''s not supported by B. In a similar
    situation, we should renounce the newer version of A because of the incompatibility
    of C with B. This difficulty has led to two important changes:'
  prefs: []
  type: TYPE_NORMAL
- en: The development world moved from DLLs and/or single files to package management
    systems such as NuGet and npm, which automatically check version compatibility
    with the help of *semantic versioning*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service-Oriented Architecture** (**SOA**). Deployment units started being
    implemented as XML and then as REST web services. This solves the version compatibility
    problem since each web service runs in a different process and can use the most
    adequate version of each library with no risk of causing incompatibilities with
    other web services. Moreover, the interface that''s exposed by each web service
    is platform-agnostic, that is, web services can connect with applications using
    any framework and run on any operating system since web service protocols are
    based on universally accepted standards. SOAs and protocols will be discussed
    in more detail in [Chapter 12](3ec4db7b-e570-41d0-94fb-2bdbd6dc4a9a.xhtml), *Applying
    Service-Oriented Architectures with .NET Core*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microservices are an evolution of SOA and add more features and more constraints
    that improve scalability and the modularity of services to improve the overall
    CI/CD cycle. It's sometimes said that *microservices are SOA done well*.
  prefs: []
  type: TYPE_NORMAL
- en: Microservice design principles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To sums things up, the microservice architecture is an SOA that maximizes independence
    and fine-grained scaling. Now that we've clarified all the advantages of microservice
    independence and fine-grained scaling, as well as the very nature of independence,
    we are in a position to look at microservice design principles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let start with principles that arise from the independence constraint:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Independence of design choices**: The design of each microservice must not
    depend on the design choices that were made in the implementation of other microservices.
    This principle enables the full independence of each microservice CI/CD cycle
    and leaves us with more technological choices on how to implement each microservice.
    This way, we can choose the best available technology to implement each microservice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another consequence of this principle is that different microservices can't
    connect to the same shared storage (database or filesystem) since sharing the
    same storage also means sharing all the design choices that determined the structure
    of the storage subsystem (database table design, database engine, and so on).
    Thus, either a microservice has its own data storage or it has no storage at all
    and communicates with other microservices that take care of handling storage.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here, having dedicated data storage doesn't mean that the physical database
    is distributed within the process boundary of the microservice itself, but that
    the microservice has exclusive access to a database or set of database tables
    that are handled by an external database engine. In fact, for performance reasons,
    database engines must run on dedicated hardware and with OS and hardware features
    that are optimized for their storage functionalities. Usually, *independence of
    design choices* is interpreted in a lighter form by distinguishing between logical
    and physical microservices. More specifically, a logical microservice is implemented
    with several physical microservices that use the same data storage but that are
    load-balanced independently. That is, the logical microservice is designed as
    a logical unity and then split into more physical microservices to achieve better
    load balance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Independence from the deployment environment**: Microservices are scaled
    out on different hardware nodes and different microservices can be hosted on the
    same node. Therefore, the less a microservice relies on the services offered by
    the operating system and on other installed software, the more available hardware
    nodes it can be deployed on. More node optimization can also be performed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the reason why microservices are usually containerized and use Docker.
    Containers will be discussed in more detail in the *Containers and Docker* subsection
    of this chapter, but basically, containerization is a technique that allows each
    microservice to bring its dependencies with it so that it can run anywhere.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Loose coupling**: Each microservice must be loosely coupled with all the
    other microservices. This principle has a two-folded nature. On the one hand,
    this means that, according to object-oriented programming principles, the interface
    that''s exposed by each microservice must not be too specific, but as general
    as possible. However, it also means that communications among microservices must
    be minimized in order to reduce communication costs since microservices don''t
    share the same address space and run on different hardware nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No chained requests/responses**: When a request reaches a microservice, it
    must not cause a recursive chain of nested requests/responses to other microservices
    since a similar chain would result in an unacceptable response time. Chained requests/responses
    can be avoided if the private data models of all the microservices synchronize
    with push notifications each time they change. In other words, as soon as the
    data that''s handled by a microservice changes, those changes are sent to all
    the microservices that may need them to serve their requests. This way, each microservice
    has all the data it needs to serve all its incoming requests in its private data
    storage, with no need to ask other microservices for the data that it lacks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In conclusion, every microservice must contain all the data it needs to serve
    incoming requests and ensure fast responses. To keep their data models up to date
    and ready for incoming requests, microservices must communicate their data changes
    as soon as they take place. These data changes should be communicated through
    asynchronous messages since synchronous nested messages cause unacceptable performance
    because they block all the threads involved in the call tree until a result is
    returned.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It is worth pointing out that the first constraint we mentioned is substantially
    the Bounded Context principle of domain-driven design, which we will talk about
    in detail in [Chapter 10](2a42483c-2193-4bd4-91b4-0fdce94f6ed1.xhtml), *Understanding
    the Different Domains in Software Solutions*. In this chapter, we will see that,
    often, a full domain-driven design approach is useful for the *update* subsystem
    of each microservice.
  prefs: []
  type: TYPE_NORMAL
- en: It's not trivial that the opposite is also true, that is, that systems that
    have been developed according to the Bounded Context principle are better implemented
    with a microservice architecture. In fact, once a system has been decomposed into
    several completely independent and loosely coupled parts, it is very likely that
    these different parts need to be scaled independently because of different traffic
    and different resources requirements.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding constraints are some best practices for building a reusable SOA.
    More details on these best practices will be given in [Chapter 12](3ec4db7b-e570-41d0-94fb-2bdbd6dc4a9a.xhtml),
    *Applying Service-Oriented Architectures with .NET Core*, but nowadays, most SOA
    best practices are automatically enforced by tools and frameworks that are used
    to implement web services.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-grained scaling requires that microservices are small enough to isolate
    well-defined functionalities, but this also requires a complex infrastructure
    that takes care of automatically instantiating microservices, allocating instances
    on nodes, and scaling them as needed. These kinds of structure will be discussed
    in the *Which tools are needed to manage Microservices?* section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, fine-grained scaling of distributed microservices that communicate
    through asynchronous communication requires each microservice to be resilient.
    In fact, communication that's directed to a specific microservice instance may
    fail due to a hardware fault or for the simple reason that the target instance
    was killed or moved to another node during a load balancing operation.
  prefs: []
  type: TYPE_NORMAL
- en: Temporary failures can be overcome with exponential retries. This is where we
    retry the same operation after each failure with a delay that increases exponentially
    until a maximum number of attempts is reached. For instance, first, we would retry
    after 10 milliseconds, and if this retried operation results in a failure, a new
    attempt is done after 20 milliseconds, then after 40 milliseconds, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, long-term failures often cause an explosion of retry operations
    that may saturate all system resources in a way that is similar to a Denial Of
    Service Attack. Therefore, usually, exponential retries are used together with
    a *circuit break strategy*: after a given number of failures, a long-term failure
    is assumed and access to the resource is prevented for a given time by returning
    an immediate failure without attempting the communication operation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also fundamental that the congestion of some subsystems, due to either
    failure or to a requests peak, does not propagate to other system parts, in order
    to prevent overall system congestion. **Bulkhead isolation** avoids congestion
    propagation in the following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Only a maximum number of similar simultaneous outbound requests are allowed,
    let's say, 10\. This is similar to putting an upper bound on thread creation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requests exceeding the previous bound are queued.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the maximum queue length is reached, any further requests result in exceptions
    being thrown to abort them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retry policies may make it so that the same message is received and processed
    several times because the sender has received no confirmation that the message
    has been received or simply because it has timed-out the operation, while the
    receiver actually received the message. The only possible solution to this problem
    is designing all messages so that they're idempotent, that is, designing messages
    in such a way that processing the same message several times has the same effect
    as processing it once.
  prefs: []
  type: TYPE_NORMAL
- en: 'Updating a database table field to a value, for instance, is an idempotent
    operation since repeating it once or twice has exactly the same effect. However,
    incrementing a decimal field is not an idempotent operation. Microservice designers
    should make an effort to design the overall application with as many idempotent
    messages as possible. The remaining non-idempotent messages must be transformed
    into idempotent ones in the following ways, or with some other similar technique:'
  prefs: []
  type: TYPE_NORMAL
- en: Attach both a time and some identifier that uniquely identify each message.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Store all the messages that have been received in a dictionary that's been indexed
    by the unique identifier attached to the message mentioned in the previous point.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reject old messages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When a message that may be a duplicate is received, verify whether it's contained
    in the dictionary. If it is, then it has already been processed, so reject it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since old messages are rejected, they can be periodically removed from the dictionary
    to avoid it growing exponentially.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will use this technique in the example at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In the next subsection, we will talk about microservice containerization based
    on Docker.
  prefs: []
  type: TYPE_NORMAL
- en: Containers and Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ve already discussed the advantages of having microservices that don''t
    depend on the environment where they run: better hardware usage, the ability to
    mix legacy software with newer modules, the ability to mix several development
    stacks in order to use the best stack for each module implementation, and so on.
    Independence on the hosting environment can be easily achieved by deploying each
    microservice with all its dependencies on a private virtual machine.'
  prefs: []
  type: TYPE_NORMAL
- en: However, starting a virtual machine with its private copy of the operating system
    takes a lot of time, and microservices must be started and stopped quickly to
    reduce load balancing and fault recovery costs. In fact, new microservices may
    be started either to replace faulty ones or because they were moved from one hardware
    node to another to perform load balancing. Moreover, adding a whole copy of the
    operating system to each microservice instance would be an excessive overhead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily, microservices can rely on a lighter form of technology: containers.
    Containers are a kind of light virtual machine. They do not virtualize a full
    machine – they just virtualize the **operating system** (**OS**) filesystem level
    that sits on top of the OS kernel. They use the OS of the hosting machine (kernel,
    DLLs, and drivers) and rely on the OS''s native features to isolate processes
    and resources to ensure an isolated environment for the images they run.'
  prefs: []
  type: TYPE_NORMAL
- en: As a consequence, containers are tied to a specific operating system but they
    don't suffer the overhead of copying and starting a whole OS in each container
    instance.
  prefs: []
  type: TYPE_NORMAL
- en: On each host machine, containers are handled by a runtime that takes care of
    creating them from *images* and creating an isolated environment for each of them.
    The most famous container runtime is Docker, which is a *de facto* standard for
    containerization.
  prefs: []
  type: TYPE_NORMAL
- en: Images are files that specify what is put in each container and which container
    resources, such as communication ports, to expose outside the container. None
    of the images need to explicitly specify their full content, but they can reference
    other images. This way, images are built by adding new software and configuration
    information on top of existing images.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if you want to deploy a .NET Core application as a Docker image,
    it is enough to just add your software and files to your Docker image and then
    reference an already existing .NET Core Docker image.
  prefs: []
  type: TYPE_NORMAL
- en: 'To allow for easy image referencing, images are grouped into registries that
    may be either public or private. They are similar to NuGet or npm registries.
    Docker offers a public registry ([https://hub.docker.com/_/registry](https://github.com/Particular/Workshop/tree/master/demos/asp-net-core))
    where you can find most of the public images you may need to reference in your
    own images. However, each company can define private registries. For instance,
    Azure offers a private container registry service: `https://azure.microsoft.com/en-us/services/container-registry/`.'
  prefs: []
  type: TYPE_NORMAL
- en: Before instantiating each container, the Docker runtime must solve all the recursive
    references. This cumbersome job is not performed each time a new container is
    created since the Docker runtime has a cache where it stores the fully assembled
    images that correspond to each input image and that it's already processed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since each application is usually composed of several modules to be run in
    different containers, Docker also allows `.yml` files, also known as composition
    files, that specify the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: Which images to deploy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How the internal resources that are exposed by each image must be mapped to
    the physical resources of the host machine. For instance, how communication ports
    that are exposed by Docker images must be mapped to the ports of the physical
    machine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will analyze Docker images and `.yml` files in the *How does .NET Core deal
    with Microservices?* section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The Docker runtime handles images and containers on a single machine but, usually,
    containerized microservices are deployed and load-balanced on clusters that are
    composed of several machines. Clusters are handled by pieces of software called
    **Orchestrators**. Orchestrators will be discussed in the *Which tools are needed
    to manage microservices?* section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have understood what microservices are, what problems they can solve,
    and their basic design principles, we are ready to analyze when and how to use
    them in our system architecture. The next section analyzes when we should use
    them.
  prefs: []
  type: TYPE_NORMAL
- en: When do microservices help?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The answer to this question requires us to understand the roles microservices
    play in modern software architectures. We will look at this in the following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: Layered architectures and microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enterprise systems are usually organized in logical independent layers. The
    first layer is the one that interacts with the user and is called the presentation
    layer, while the last layer takes care of storing/retrieving data and is called
    the data layer. Requests originate in the presentation layer and pass through
    all the layers until they reach the data layer, and then come back, traversing
    all the layers in reverse until they reach the presentation layer, which takes
    care of presenting the results to the user/client. Layers can't be *jumped*.
  prefs: []
  type: TYPE_NORMAL
- en: Each layer takes data from the previous layer, processes it, and passes it to
    the next layer. Then, it receives the results from its next layer and sends them
    back to its previous layer. Also, thrown exceptions can't jump layers – each layer
    must take care of intercepting all the exceptions and either *solving them* somehow
    or transforming them into other exceptions that are expressed in the language
    of its previous layer. The layer architecture ensures the complete independence
    of the functionalities of each layer from all the other layers of their functionalities.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, we can change the database engine without affecting all the layers
    that are above the data layer. In the same way, we can completely change the user
    interface, that is, the presentation layer, without affecting the remainder of
    the system.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, each layer implements a different kind of system specification. The
    data layer takes care of what the system *must remember*, the presentation layer
    takes care of the system-user interaction protocol, and all the layers that are
    in the middle implement the domain rules, which specify how data must be processed
    (for instance, how an employed paycheck must be computed). Typically, the data
    and presentation layers are separated by just one domain rule layer, called the
    business or application layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each layer *speaks* a different language: the data layer *speaks* the language
    of the chosen storage engine, the business layer speaks the language of domain
    experts, and the presentation layer speaks the language of users. So, when data
    and exceptions pass from one layer to another, they must be translated into the
    language of the destination layer.'
  prefs: []
  type: TYPE_NORMAL
- en: A detailed example of how to build a layered architecture will be given in the
    *Use case - Logging Microservices* section in [Chapter 10](2a42483c-2193-4bd4-91b4-0fdce94f6ed1.xhtml),
    *Understanding the Different Domains in Software Solutions*, which is dedicated
    to domain-driven design.
  prefs: []
  type: TYPE_NORMAL
- en: That being said, how do microservices fit into a layered architecture? Are they
    adequate for the functionalities of all the layers or of just some layers? Can
    a single microservice span several layers?
  prefs: []
  type: TYPE_NORMAL
- en: 'The last question is the easiest to answer: yes! In fact, we''ve already stated
    that microservices should store the data they need within their logical boundaries.
    Therefore, there are microservices that span the business and data layers. Some
    others take care of encapsulating shared data and remain confined in the data
    layer. Thus, we may have business layer microservices, data layer microservices,
    and microservices that span both layers. So, what about the presentation layer?'
  prefs: []
  type: TYPE_NORMAL
- en: The presentation layer can also fit into a microservice architecture if it is
    implemented on the server-side. Single-page applications and mobile applications
    run the presentation layer on the client machine, so they either connect directly
    to the business microservices layer or, more often, to an *API Gateway* that exposes
    the public interface and takes care of routing requests to the right microservices.
  prefs: []
  type: TYPE_NORMAL
- en: In a microservices architecture, when the presentation layer is a website, it
    can be implemented with a set of microservices. However, if it requires heavy
    web servers and/or heavy frameworks, containerizing them may not be convenient.
    This decision must also consider the loss of performance that happens when containerizing
    the web server and the possible need for hardware firewalls between the web server
    and the remainder of the system.
  prefs: []
  type: TYPE_NORMAL
- en: ASP.NET Core is a lightweight framework that runs on the light Kestrel web server,
    so it can be containerized efficiently and used in a microservice for intranet
    applications. However, public high-traffic websites require dedicated hardware/software
    components that prevent them from being deployed together with other microservices.
    In fact, while Kestrel is an acceptable solution for an intranet website, public
    websites need a more complete web server such as IIS. In this case, security requirements
    are more compelling and require specialized hardware/software components.
  prefs: []
  type: TYPE_NORMAL
- en: Monolithic websites can be easily broken into load-balanced smaller subsites
    without microservice-specific technologies, but a microservice architecture can
    bring all the advantages of microservices into the construction of a single HTML
    page. More specifically, different microservices may take care of different areas
    of each HTML page. Unfortunately, at the time of writing, such a similar scenario
    is not easy to implement with the available .NET and .NET Core technology.
  prefs: []
  type: TYPE_NORMAL
- en: 'A proof of concept that implements a website with ASP.NET Core-based microservices
    that cooperate in the construction of each HTML page can be found here: [https://github.com/Particular/Workshop/tree/master/demos/asp-net-core](https://github.com/Particular/Workshop/tree/master/demos/asp-net-core).
    The main limit of this approach is that microservices cooperate just to generate
    the data that''s needed to generate the HTML page and not to generate the actual
    HTML page. Instead, this is handled by a monolithic gateway. In fact, at the time
    of writing, frameworks such as ASP.NET Core MVC don''t provide any facilities
    for the distribution of HTML generation. We will return to this example in [Chapter
    13](003ee8cb-5995-4364-8772-73d73df29cf8.xhtml), *Presenting ASP.NET Core MVC*.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've clarified which parts of a system can benefit from the adoption
    of microservices, we are ready to state the rules when it comes to deciding how
    they're adopted.
  prefs: []
  type: TYPE_NORMAL
- en: When is it worth considering microservice architectures?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Microservices can improve the implementation of both the business and data
    layer, but their adoption has some costs:'
  prefs: []
  type: TYPE_NORMAL
- en: Allocating instances to nodes and scaling them has a cost in terms of cloud
    fees or internal infrastructures and licenses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting a unique process into smaller communicating processes increases communication
    costs and hardware needs, especially if the microservices are containerized.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing and testing software for a microservice requires more time and increases
    human resources costs. In particular, making microservices resilient and ensuring
    that they adequately handle all possible failures, as well as verify these features
    with integration tests, can increase the development time by more than one order
    of magnitude.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, when are microservices worth the cost of using them? Are there functionalities
    that must be implemented as microservices?
  prefs: []
  type: TYPE_NORMAL
- en: 'A rough answer to the first question is: yes, when the application is big enough
    in terms of traffic and/or software complexity. In fact, as an application grows
    in complexity and its traffic increases, it''s recommended that we pay the costs
    connected to scaling it since this allows for more scaling optimization and better
    handling when it comes to the development team. The costs we pay for these would
    soon exceed the cost of microservice adoption.'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, if fine-grained scaling makes sense for our application, and if we are
    able to estimate the savings that fine-grained scaling and development give us,
    we can easily compute an overall application throughput limit that makes the adoption
    of microservices convenient.
  prefs: []
  type: TYPE_NORMAL
- en: Microservice costs can also be justified by the market value of our products/services
    increasing. Since the microservice architecture allows us to implement each microservice
    with a technology that has been optimized for its use, the quality that's added
    to our software may justify all or part of the microservice costs.
  prefs: []
  type: TYPE_NORMAL
- en: However, scaling and technology optimizations are not the only parameters to
    consider. Sometimes, we are forced to adopt a microservice architecture without
    being able to perform a detailed cost analysis.
  prefs: []
  type: TYPE_NORMAL
- en: If the size of the team that takes care of the CI/CD of the overall system grows
    too much, the organization and coordination of this big team cause difficulties
    and inefficiencies. In this type of situation, it is desirable to move to an architecture
    that breaks the whole CI/CD cycle into independent parts that can be taken care
    of by smaller teams.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, since these development costs are only justified by a high volume
    of requests, we probably have high traffic being processed by independent modules
    that have been developed by different teams. Therefore, scaling optimizations
    and the need to reduce interaction between development teams makes the adoption
    of a microservice architecture very convenient.
  prefs: []
  type: TYPE_NORMAL
- en: From this, we may conclude that, if the system and the development team grows
    too much, it is necessary to split the development team into smaller teams, each
    working on an efficient Bounded Context subsystem. It is very likely that, in
    a similar situation, a microservices architecture is the only possible option.
  prefs: []
  type: TYPE_NORMAL
- en: Another situation that forces the adoption of a microservice architecture is
    the integration of newer subparts with legacy subsystems based on different technologies
    since containerized microservices are the only way to implement an efficient interaction
    between the legacy system and the new subparts in order to gradually replace the
    legacy subparts with newer ones. Similarly, if our team is composed of developers
    with experience in different development stacks, an architecture based on containerized
    microservices may become a *must*.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will analyze building blocks and tools that are available
    so that we can implement .NET Core-based microservices.
  prefs: []
  type: TYPE_NORMAL
- en: How does .NET Core deal with microservices?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: .NET Core was conceived as a multi-platform framework that was light and fast
    enough to implement efficient microservices. In particular, ASP.NET Core is the
    ideal tool for implementing REST APIs to communicate with a microservice, since
    it can run efficiently with light web servers such as Kestrel and is itself light
    and modular.
  prefs: []
  type: TYPE_NORMAL
- en: The whole .NET Core framework evolved with microservices as a strategic deployment
    platform in mind and has facilities and packages for building efficient and light
    HTTP communication to ensure service resiliency and to handle long-running tasks.
    The following subsections describe some of the different tools or solutions that
    we can use to implement a .NET Core-based microservice architecture.
  prefs: []
  type: TYPE_NORMAL
- en: .NET Core communication facilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Microservices need two kinds of communication channel:'
  prefs: []
  type: TYPE_NORMAL
- en: A communication channel to receive external requests, either directly or through
    an API Gateway. HTTP is the usual protocol for external communication due to available
    web services standards and tools. .NET Core's main HTTP communication facility
    is ASP.NET Core since it's a lightweight HTTP framework, which makes it ideal
    for implementing Web APIs in small microservices. We will describe ASP.NET Core
    App in detail in [Chapter 12](3ec4db7b-e570-41d0-94fb-2bdbd6dc4a9a.xhtml), *Applying
    Service-Oriented Architectures with .NET Core*, which is dedicated to HTTP services.
    .NET Core also offers an efficient and modular HTTP client solution that is able
    to pool and reuse heavy connection objects. Also, the `HttpClient` class will
    be described in more detail in [Chapter 12](3ec4db7b-e570-41d0-94fb-2bdbd6dc4a9a.xhtml),
    *Applying Service-Oriented Architectures with .NET Core*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A different type of communication channel to push updates to other microservices.
    In fact, we have already mentioned that intra-microservice communication cannot
    be triggered by an on-going request since a complex tree of blocking calls to
    other microservices would increase request latency to an unacceptable level. As
    a consequence, updates must not be requested immediately before they're used and
    should be pushed whenever state changes take place. Ideally, this kind of communication
    should be asynchronous to achieve acceptable performance. In fact, synchronous
    calls would block the sender while they are waiting for the result, thus increasing
    the idle time of each microservice. However, synchronous communication that just
    puts the request in a processing queue and then returns confirmation of the successful
    communication instead of the final result is acceptable if communication is fast
    enough (low communication latency and high bandwidth). A publisher/subscriber
    communication would be preferable since, in this case, the sender and receiver
    don't need to know each other, thus increasing the microservices' independence.
    In fact, all the receivers that are interested in a certain type of communication merely need
    to register to receive a specific *event*, while senders just need to publish
    those events. All the wiring is performed by a service that takes care of queuing
    events and dispatching them to all the subscribers. The publisher/subscriber pattern
    will be described in more detail in [Chapter 9](a2d50e08-6698-47f6-a9b5-188de08134c0.xhtml),
    *Design Patterns and .NET Core Implementation*, along with other useful patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While .NET Core doesn't directly offer tools that may help in asynchronous communication
    or client/server tools that implement a publisher/subscriber communication, Azure
    offers a similar service with *Azure Service Bus*. Azure Service Bus handles both
    queued asynchronous communication through Azure Service Bus *queues* and publisher/subscriber
    communication through Azure Service Bus *topics*.
  prefs: []
  type: TYPE_NORMAL
- en: Once you've configured an Azure Service Bus on the Azure portal, you can connect
    to it in order to send messages/events and to receive messages/events through
    a client contained in the `Microsoft.Azure.ServiceBus` NuGet package.
  prefs: []
  type: TYPE_NORMAL
- en: 'Azure Service Bus has two types of communication: queue-based and topic-based.
    In queue-based communication, each message that''s placed in the queue by a sender
    is removed from the queue by the first receiver that pulls it from the queue.
    Topic-based communication, on the other hand, is an implementation of the publisher/subscriber
    pattern. Each topic has several subscriptions and a different copy of each message
    sent to a topic can be pulled from each topic subscription.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The design flow is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Define an Azure Service Bus private namespace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the root connection strings that were created by the Azure portal and/or
    define new connection strings with fewer privileges.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define queues and/or topics where the sender will send their messages in binary
    format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each topic, define names for all the required subscriptions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the case of queue-based communication, the sender sends messages to a queue
    and the receivers pull messages from the same queue. Each message is delivered
    to one receiver. That is, once a receiver gains access to the queue, it reads
    and removes one or more messages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the case of topic-based communication, each sender sends messages to a topic,
    while each receiver pulls messages from the private subscription associated with
    that topic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There are also other commercial alternatives to Azure Service Bus, such as
    NServiceBus, MassTransit, Brighter, and ActiveMQ. There is also a free open source
    option: RabbitMQ . RabbitMQ can be installed locally, on a virtual machine, or
    in a Docker container. Then, you can connect with it through the client contained
    in the `RabbitMQ.Client` NuGet package.'
  prefs: []
  type: TYPE_NORMAL
- en: The functionalities of RabbitMQ are similar to the ones offered by Azure Service
    Bus but you have to take care of all the implementation details, confirmations
    of performed operations, and so on, while Azure Service Bus takes care of all
    the low-level tasks and offers you a simpler interface. Azure Service Bus and
    RabbitMQ will be described alongside Publisher/Subscriber-based communication
    in [Chapter 9](a2d50e08-6698-47f6-a9b5-188de08134c0.xhtml), *Design Patterns and
    .NET Core Implementation*.
  prefs: []
  type: TYPE_NORMAL
- en: If microservices are published to Azure Service Fabric, which will be described
    in the next section, we can use a built-in reliable binary communication. Communication
    is resilient since communication primitives automatically use a retry policy.
    This communication is synchronous, but this is not a big limitation since microservices
    in Azure Service Fabric have built-in queues; thus, once the receiver has received
    a message, they can just put it in a queue and return it immediately, without
    blocking the sender.
  prefs: []
  type: TYPE_NORMAL
- en: The messages in the queue are then processed by a separate thread. The main
    limitation of this built-in communication is that it is not based on the publisher/subscriber
    pattern; the senders and receivers must know each other. When this is not acceptable,
    you should use Azure Service Bus. We will learn how to use Service Fabric's built-in
    communication in the *Use case - logging microservices* section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Resilient task execution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Resilient communication and, in general, resilient task execution can be implemented
    easily with the help of a .NET Core library called Polly, which is maintained
    by the .NET Foundation. Polly is available through the Polly NuGet package.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Polly, you define policies, and then execute tasks in the context of that
    policy, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The first part of each policy specifies the exceptions that must be handled.
    Then, you specify what to do when one of those exceptions is captured. In the
    preceding code, the `Execute` method is retried up to three times if a failure
    is reported either by an `HttpRequestException` exception or by an `OperationCanceledException`
    exception.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the implementation of an exponential retry policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The first argument of `WaitAndRetry` specifies that a maximum of six retries
    is performed in case of failure. The lambda function passed as second argument
    specifies how much time to wait before the next attempt. In the specific example,
    this time grows exponentially with the number of the attempt with a power of 2
    (2 seconds for the first retry, 4 seconds for the second retry, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a simple Circuit Breaker policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: After six failures, the task can't be executed for 1 minute since an exception
    is returned.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the implementation of the Bulkhead Isolation policy (see the
    *Microservices design principles* section for more information):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: A maximum of 10 parallel executions is allowed in the `Execute` method. Further
    tasks are inserted in an execution queue. This has a limit of 15 tasks. If the
    queue limit is exceeded, an exception is thrown.
  prefs: []
  type: TYPE_NORMAL
- en: For the Bulkhead policy to work properly and, in general, for every strategy
    to work properly, task executions must be triggered through the same policy instance;
    otherwise, Polly is unable to count how many executions of a specific task are
    active.
  prefs: []
  type: TYPE_NORMAL
- en: 'Policies can be combined with the `Wrap` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Polly offers several more options, such as generic methods for tasks that return
    a specific type, timeout policies, task result caching, the ability to define
    custom policies, and so on. The link to the official Polly documentation is in
    the *Further reading* section.
  prefs: []
  type: TYPE_NORMAL
- en: Using generic hosts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each microservice may need to run several independent threads, each performing
    a different operation on requests received. Such threads need several resources,
    such as database connections, communication channels, specialized modules that
    perform complex operations, and so on. Moreover, all processing threads must be
    adequately initialized when the microservice is started and gracefully stopped
    when the microservice is stopped as a consequence of either load balancing or
    errors.
  prefs: []
  type: TYPE_NORMAL
- en: All of these needs led the .NET Core team to conceive and implement *hosted
    services* and *hosts*. A host creates an adequate environment for running several
    tasks, known as **hosted services**, and provides them with resources, common
    settings, and graceful start/stop.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of a web host was mainly conceived to implement the ASP.NET Core
    web framework, but, with effect from .NET Core 2.1, the host concept was extended
    to all .NET applications. All features related to the concept of "host" are contained
    in the `Microsoft.Extensions.Hosting` NuGet package.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you need to configure the host with a fluent interface, starting with
    a `HostBuilder` instance. The final step of this configuration is calling the
    `Build` method, which assembles the actual host with all the configuration information
    we provided:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Host configuration includes defining the common resources, defining the default
    folder for files, loading the configuration parameters from several sources (JSON
    files, environment variables, and any arguments that are passed to the application),
    and declaring all the hosted services.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the host can be started, which causes all the hosted services to be started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The program remains blocked on the preceding instruction until the host is shutdown.
    The host can be shutdown either by one of the hosted services or externally by
    calling `await host.StopAsync(timeout)`. Here, `timeout` is a time span defining
    the maximum time to wait for the hosted services to stop gracefully. After this
    time, all the hosted services are aborted if they haven't been terminated.
  prefs: []
  type: TYPE_NORMAL
- en: Often, the fact that a microservice is being shutdown is signaled by a `CancelationToken`
    being passed when the microservice is started by the orchestrator. This happens
    when microservices are hosted in Azure Service Fabric.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, instead of using `host.Start()`, we can use the `RunAsync` method
    and pass it the `CancelationToken` that we received from the orchestrator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This way of shutting down is triggered as soon as the `cancelationToken` enters
    a canceled state. By default, the host has a 5-second timeout for shutting down;
    that is, it waits 5 seconds before exiting once a shutdown has been requested.
    This time can be changed within the `ConfigureServices` method, which is used
    to declare *hosted services* and other resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: However, increasing the host timeout doesn't increase the orchestrator timeout,
    so if the host waits too long, the whole microservice is killed by the orchestrator.
  prefs: []
  type: TYPE_NORMAL
- en: Hosted services are implementations of the `IHostedService` interface, whose
    only methods are `StartAsync(CancellationToken)` and `StopAsync(CancellationToken`).
    Both methods are passed a `CancelationToken`. The `CancelationToken` in the `StartAsync`
    method signals that a shutdown was requested. The `StartAsync` method periodically
    checks this `CancelationToken` while performing all operations needed to start
    the host, and if it is signaled the host start process is aborted. On the other
    hand, the `CancelationToken` in the `StopAsync` method signals that the shutdown
    timeout expired.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hosted services must be declared in the same `ConfigureServices` method that''s
    used to define host options, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Most declarations inside `ConfigureServices` require the addition of the following
    namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Usually, the `IHostedService` interface isn't implemented directly but can be
    inherited from the `BackgroundService` abstract class, which exposes the easier-to-implement
    `ExecuteAsync(CancellationToken)` method, which is where we can place the whole
    logic of the service. A shutdown is signaled by passing `CancellationToken` as
    an argument, which is easier to handle. We will look at an implementation of `IHostedService`
    in the example at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'To allow a hosted service to shutdown the host, we need to declare an `IApplicationLifetime`
    interface as its constructor parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'When the hosted service is created, it will be automatically passed an implementation
    of `IApplicationLifetime`, whose `StopApplication` method will trigger the host
    shutdown. This implementation is handled automatically, but we can declare custom
    resources whose instances will be automatically passed to all the host service
    constructors that declare them as parameters. There are several ways to define
    these resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: When we use `AddTransient`, a different instance is created and passed to all
    the constructors that require an instance of that type. On the other hand, with
    `AddSingleton`, a unique instance is created and passed to all the constructors
    that require the declared type. The overload with two generic types allows you
    to pass an interface and a type that implements that interface. This way, a constructor
    requires the interface and is decoupled from the specific implementation of that
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: If resource constructors contain parameters, they will be automatically instantiated
    with the types declared in `ConfigureServices` in a recursive fashion. This pattern
    of interaction with resources is called **dependency injection** (**DI**) and
    will be discussed in detail in [Chapter 9](a2d50e08-6698-47f6-a9b5-188de08134c0.xhtml),
    *Design Patterns and .NET Core Implementation*.
  prefs: []
  type: TYPE_NORMAL
- en: '`HostBuilder` also has a method we can use to define the default folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'It also has methods that we can use to add logging targets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The preceding example shows a console-based logging source, but we can also
    log into Azure targets with adequate providers. The *Further reading* section
    contains links to some Azure logging providers that can work with microservices
    that have been deployed in Azure Service Fabric. Once you've configured logging,
    you can enable your hosted services and log custom messages by adding an `ILoggerFactory`
    parameter in their constructors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, `HostBuilder` has methods we can use to read configuration parameters
    from various sources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The way parameters can be used from inside the application will be explained
    in more detail in [Chapter 13](003ee8cb-5995-4364-8772-73d73df29cf8.xhtml), *Presenting
    ASP.NET Core MVC*, which is dedicated to ASP.NET Core.
  prefs: []
  type: TYPE_NORMAL
- en: Visual Studio support for Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Visual Studio offers support for creating, debugging, and deploying Docker images.
    Docker deployment requires us to install *Docker CE for Windows* on our development
    machine so that we can run Docker images. The download link can be found in the
    *Technical requirements* section at the beginning of this chapter. Before we start
    any development activity, we must ensure it is installed and running (you should
    see a Docker icon in the window notification bar when the Docker runtime is running).
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker support will be described with a simple ASP.NET Core MVC project. Let''s
    create one. To do so, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Name the project `MvcDockerTest`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For simplicity, disable authentication.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You are given the option to add Docker support when you create the project,
    but please don't check the Docker support checkbox. You can test how Docker support
    can be added to any project after it has been created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you have your ASP.NET Core MVC application scaffolded and running, right-click
    on its project icon in the Solution Explorer and select Container Orchestrator
    Support | Docker Compose. This will enable not only the creation of a Docker image
    but also the creation of a Docker Compose project, which helps you configure Docker
    Compose files so that they run and deploy several Docker images simultaneously.
    In fact, if you add another MVC project to the solution and enable container orchestrator
    support for it, the new Docker image will be added to the same Docker Compose
    file.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of enabling Docker Compose instead of just `docker` is that you
    can manually configure how the image is run on the development machine, as well
    as how Docker image ports are mapped to external ports by editing the Docker Compose
    files that are added to the solution.
  prefs: []
  type: TYPE_NORMAL
- en: If your Docker runtime has been installed properly and is running, you should
    be able to run the Docker image from Visual Studio.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s analyze the Docker file that was created by Visual Studio. It is a sequence
    of image creation steps. Each step enriches an existing image with something else
    with the help of the `From` instruction, which is a reference to an already existing
    image. The following is the first step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The first step uses the `microsoft/dotnet:x.x-aspnetcore-runtime` ASP.NET Core
    runtime that was published by Microsoft in the Docker public repository (where
    `x.x` is the ASP.NET Core version that was selected in your project).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `WORKDIR` command creates the directory that follows the current directory
    within the image that is going to be created. If the directory doesn''t exist
    yet, it is created in the image. The two `EXPOSE` commands declare which ports
    of the image ports will be exposed outside the image and mapped to the actual
    hosting machine. Mapped ports are decided in the deployment stage either as command-line
    arguments of a Docker command or within a Docker Compose file. In our case, there
    are two ports: one for HTTP (80) and another for HTTPS (443).'
  prefs: []
  type: TYPE_NORMAL
- en: This intermediate image is cached by Docker, which doesn't need to recompute
    it since it doesn't depend on the code we write on the selected version of the
    ASP.NET Core runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second step produces a different image that will not be used to deploy.
    Instead, it will be used to create application-specific files that will be deployed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This step starts from the ASP.NET SDK image, which contains parts we don't need
    to add for deployment; these are needed to process the project code. The new `src`
    directory is created in the `build` image and made the current image directory.
    Then, the project file is copied into `/src/MvcDockerTest`.
  prefs: []
  type: TYPE_NORMAL
- en: The `RUN` command executes an operating system command on the image. In this
    case, it calls the `dotnet` runtime, asking it to restore the NuGet packages that
    were referenced by the previously copied project file.
  prefs: []
  type: TYPE_NORMAL
- en: Then, the `COPY..` command copies the whole project file tree into the `src`
    image directory. Finally, the project directory is made the current directory
    and the `dotnet` runtime is asked to build the project in release mode and copy
    all the output files into the new `/app` directory. Finally, a new image called
    **publish** executes the `publish` command on the output files.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final step starts from the image that we created in the first step, which
    contains the ASP.NET Core runtime, and adds all the files that were published
    in the previous step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The `ENTRYPOINT` command specifies the operating system command that's needed
    to execute the image. It accepts an array of strings. In our case, it accepts
    the `dotnet` command and its first command-line argument, that is, the DLL we
    need to execute.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we right-click on our project and click Publish, we are presented with several
    options:'
  prefs: []
  type: TYPE_NORMAL
- en: Publish the image to an existing or new web app (automatically created by Visual
    Studio)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Publish to one of several Docker registries, including a private Azure Container
    Registry that, if it doesn't already exist, can be created from within Visual
    Studio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Publish to an Azure Virtual machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Compose support allows you to run and publish a multi-container application
    and add further images, such as a containerized database that is available everywhere.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following Docker Compose file adds two ASP.NET Core applications to the
    same Docker image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code references existing Docker files. Any environment-dependent
    information is placed in the `docker-compose.override.yml` file, which is merged
    with the `docker-compose.yml` file when the application is launched from Visual
    Studio:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: For each image, the file defines some environment variables, which will be defined
    in the image when the application is launched, the port mappings, and some host
    files.
  prefs: []
  type: TYPE_NORMAL
- en: The files in the host are directly mapped into the images, so if the image isn't
    projected to a host containing those files, the image won't run properly. Each
    declaration contains the path in the host, how the path is mapped in the image,
    and the desired access rights. In our case, `volumes` are used to map the self-signed
    https certificate that's used by Visual Studio and the user secrets (encrypted
    settings) that are used by ASP.NET Core.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, suppose we want to add a containerized SQL Server instance. We would need
    something like the following instructions split between `docker-compose.yml` and
    `docker-compose.override.yml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the preceding code specifies the properties of the SQL Server container,
    as well as the SQL server''s configuration and installation parameters. More specifically,
    the preceding code contains the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sql.data` is the name that''s given to the container.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image` specifies where to take the image from. In our case, the image is contained
    in a public Docker registry.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`environment` specifies the environment variables that are needed by SQL Server,
    that is, the administrator password and the acceptance of a SQL Server license.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As usual, `ports` specifies the port mappings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker-compose.override.yml` is used to run the images from within Visual
    Studio. If you need to specify parameters for either the production environment
    or the testing environment, you can add further `docker-compose-xxx.override.yml`
    files, such as `docker-compose-staging.override.yml` and `docker-compose-production.override.yml`,
    and then launch them manually in the target environment with something like the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you can destroy all the containers with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: While `docker-compose` has a limited capability when it comes to handling node
    clusters, it is mainly used in testing and development environments. For production
    environments, more sophisticated tools are needed, as we will see in the *Which
    tools are needed to manage microservices?* section.
  prefs: []
  type: TYPE_NORMAL
- en: Azure and Visual Studio support for microservice orchestration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Visual Studio has a specific project template for microservice applications,
    based on the Service Fabric platform, where you can define various microservices,
    configure them, and deploy them to Azure Service Fabric, which is a microservice
    orchestrator. Azure Service Fabric will be described in more detail in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will describe the various types of microservice you can
    define within a Service Fabric Application. A complete code example will be provided
    in the last section of this chapter. If you want to debug microservices on your
    development machine, you need to install the Service Fabric emulator listed in
    this chapter's technical requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Service Fabric Applications can be found by selecting *cloud in Visual Studio
    project type drop-down filter* . Once you''ve selected the project, you can choose
    from a variety of services:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/daf2b3ce-48b4-4c21-9e48-d2a24156449d.png)'
  prefs: []
  type: TYPE_IMG
- en: All projects under .NET Core use a microservice model that is specific to Azure
    Service Fabric. The Guest executable adds a wrapper around an existing Windows
    application to turn it into a microservice that can run in Azure Service Fabric.
    The Container application enables the addition of any Docker image in the Service
    Fabric Application. All the other choices scaffold a template that allows you
    to code a microservice with a Service Fabric-specific pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you select any of the choices in the preceding screenshot and you fill
    in all the request information, Visual Studio creates two projects: an application
    project that contains configuration information for the overall application and
    a project for the specific service you have chosen that contains both the service
    code and service-specific configuration. If you want to add more microservices
    to your application, right-click on the application project and select Add | New
    Service Fabric Service.'
  prefs: []
  type: TYPE_NORMAL
- en: If you right-click on the solution and select Add | New project, a new Service
    Fabric application will be created instead of a new service being added to the
    already existing application.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you select Guest Executable, you need to provide the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A folder containing the main executable file, along with all the files it needs
    to work properly. You need this if you want to create a copy of this folder in
    your project or simply to link to the existing folder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main executable file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arguments to pass on the command line to that executable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which folder to use as a working folder on Azure. You want to use the folder
    containing the main executable (`CodeBase`), the folder where Azure Service Fabric
    will package the whole microservice (`CodePackage`), or a new subfolder named
    `Work`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you select Container, you need to provide the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The complete name of a Docker image in your private Azure Container Registry.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The username that will be used to connect to Azure Container Registry. The password
    will be specified manually in the same `RepositoryCredentials` XML element of
    the application configuration file that was automatically created for the username.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The port where you can access your service (Host Port) and the port inside the
    container the Host Port must be mapped to (Container Port). The Container Port must
    be the same port that was exposed in the Docker file and used to define the Docker
    image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Afterward, you may need to add further manual configuration to ensure that your
    Docker application works properly. The *Further reading* section contains links
    to the official documentation where you can find more details.
  prefs: []
  type: TYPE_NORMAL
- en: There are five types of .NET Core native Service Fabric services. The Actor
    service pattern is an opinionated pattern that was conceived several years ago
    by Carl Hewitt. We will not discuss it here, but the *Further reading* section
    contains some links that provide more information on this.
  prefs: []
  type: TYPE_NORMAL
- en: The remaining four patterns refer to the usage (or not) of ASP.NET Core as the
    main interaction protocol and to the fact that the service has or hasn't got an
    internal state. In fact, Service Fabric allows microservices to use distributed
    queues and dictionaries that are globally accessible to all instances of the microservice
    that declares them, independent of the hardware node where they are running (they
    are serialized and distributed to all available instances when they're needed).
  prefs: []
  type: TYPE_NORMAL
- en: 'Stateful and stateless templates differ mainly in terms of their configuration.
    All native services are classes that specify just two methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The `CreateServiceReplicaListeners` method specifies a list of listeners that
    are used by the microservice to receive messages and the code that handles those
    messages. Listeners may use any protocol, but they are required to specify an
    implementation of the relative socket.
  prefs: []
  type: TYPE_NORMAL
- en: '`RunAsync` contains the code for background threads that asynchronously run
    tasks that are triggered by received messages. Here, you can build a host that
    runs several hosted services.'
  prefs: []
  type: TYPE_NORMAL
- en: ASP.NET Core templates follow the same pattern; however, they use a unique ASP.NET
    Core-based listener and no `RunAsync` implementation since background tasks can
    be launched from inside ASP.NET Core. However, you may add further listeners to
    the array of listeners returned by the `CreateServiceReplicaListeners` implementation
    created by Visual Studio, and also a custom `RunAsync` override.
  prefs: []
  type: TYPE_NORMAL
- en: More details on Service Fabric's native services pattern will be provided in
    the *Which tools are needed to manage microservices?* section, while a complete
    code example will be provided in the *Testing the application* section of this
    chapter, which is dedicated to this book's use case.
  prefs: []
  type: TYPE_NORMAL
- en: While this section presented the tools we can use to build the code for our
    microservices, the next section describes the tools we can use to define and manage
    the clusters where our microservices will be deployed.
  prefs: []
  type: TYPE_NORMAL
- en: Which tools are needed to manage microservices?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Effectively handling microservices in your CI/CD cycles requires both a private
    Docker image registry and a state of-the-art microservice orchestrator that''s
    capable of doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Allocating and load-balancing microservices on available hardware nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring the health state of services and replacing faulty services if hardware/software
    failures occur
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging and presenting analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allowing the designer to dynamically change requirements such as hardware nodes
    allocated to a cluster, the number of service instances, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following subsection describes the Azure facilities we can use to store
    Docker images and to orchestrate microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Defining your private Docker registry in Azure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Defining your private Docker registry in Azure is easy. Just type `Container
    registries` into the Azure search bar and select Container registries. On the
    page that appears, click on the Add button.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following form will appear:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ebb61911-66fb-490c-ab9b-760c77695884.png)'
  prefs: []
  type: TYPE_IMG
- en: The name you select is used to compose the overall registry URI: `<name>.azurecr.io`.
    As usual, you can specify the subscription, resource group, and location. The
    SKU dropdown lets you choose from various levels of offerings that differ in terms
    of performance, available memory, and a few other auxiliary features.
  prefs: []
  type: TYPE_NORMAL
- en: If you enable Admin user, an admin user will be created whose username is `<name>`
    and whose password is created automatically by the portal; otherwise, the user
    will log in with your Azure portal credentials. Once Admin user has been selected,
    their login information will be available under the resource *Access key* menu
    item.
  prefs: []
  type: TYPE_NORMAL
- en: 'Whenever you mention image names in Docker commands or in a Visual Studio publish
    form, you must prefix its name with the registry URI: `<name>.azurecr.io/<my imagename>`.'
  prefs: []
  type: TYPE_NORMAL
- en: If images are created with Visual Studio, then they can be published by following
    the instructions that appear once you've published the project. Otherwise, you
    must use `docker` commands to push them into your registry.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say you have the image in another registry. The first step pulls the
    image onto your local machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'If there are several versions of the preceding image, the latest will be pulled
    since no version was specified. The version of the image can be specified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the following command, you should see `myimage` within the list of local
    images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, log in to Azure by typing in the following command and providing your
    credentials:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, tag the image with the path you want to assign in the Azure registry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Both the name and destination tag may have versions (`:<version name>`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, push it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: In this case, you can specify a version; otherwise, the latest version is pushed.
  prefs: []
  type: TYPE_NORMAL
- en: 'By doing this, you can remove the image from your local computer using the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Azure Service Fabric
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Azure Service Fabric is the main Microsoft orchestrator that can host Docker
    containers, native .NET applications, and a distributed computing model called
    **reliable services**. We've already explained how we can create and publish applications
    that contain these three types of service in the *Azure and Visual Studio support
    for microservice orchestration* subsection. In this section, we will explain how
    to create an Azure Service Fabric cluster in the Azure portal and provide some
    more details on **reliable services**. More practical details regarding *reliable
    services* will be provided in the example described in the *Use case - logging
    microservices* section.
  prefs: []
  type: TYPE_NORMAL
- en: You can enter the Service Fabric section of Azure by typing `Service Fabric`
    into the Azure search bar and selecting Service Fabric Cluster. A multi-step wizard
    will appear. The following subsections describe the available steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Basic information'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following screenshot shows the creation of Azure Service Fabric:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f3a6d357-a608-41a5-8a8d-64a8106434a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, you can choose the operating system, resource group, subscription, location,
    and username and password you want to use to connect the remote desktop to all
    the cluster nodes. You are required to choose a cluster name, which will be used
    to compose the cluster URI as `<cluster name>.<location>.cloudapp.azure.com`,
    where `location` is a name associated with the datacenter location you have chosen.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Cluster configuration'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the second step, you can configure the number of nodes and their features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/06c97604-b097-4c3f-92eb-942c3e65149e.png)'
  prefs: []
  type: TYPE_IMG
- en: You can specify up to three node types. Nodes of a different node type can be
    scaled independently, and node type 1, called the **primary node** type, is where
    Azure Service Fabric runtime services will be hosted. For each node type, you
    can specify the type of machine (durability tier), machine dimensions (CPU and
    RAM), and the initial number of nodes.
  prefs: []
  type: TYPE_NORMAL
- en: You can also specify all the ports that will be visible from outside the cluster
    (Custom endpoints).
  prefs: []
  type: TYPE_NORMAL
- en: The services that are hosted on the different nodes of a cluster can communicate
    through any port since they are part of the same local network. Therefore, *Custom
    endpoints* must declare the ports that need to accept traffic from outside the
    cluster. The port that's exposed in *Custom endpoints* is the cluster's public
    interface, which can be reached through the cluster URI, that is, `<cluster name>.<location>.cloudapp.azure.com`.
    Their traffic is automatically redirected to all the microservices that have had
    the same ports opened by the cluster load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: To understand the *enable reverse proxy* option, we must explain how communications
    are sent to several instances of services whose physical addresses change during
    their lifetimes. From within the cluster, services are identified with a URI such
    as `fabric://<application name>/<service name>`. That is, this name allows us
    to access one of the several load-balanced instances of `<service name>`. However,
    these URIs can't be used directly by communication protocols. Instead, they are
    used to get the physical URI of the required resource, along with all its available
    ports and protocols from the Service Fabric naming service.
  prefs: []
  type: TYPE_NORMAL
- en: Later, we will learn how to perform this operation with *reliable services*.
    However, this model is not adequate for Dockerized services that weren't conceived
    to run specifically on Azure Service Fabric since they are not aware of Service
    Fabric-specific naming services and APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, Service Fabric provides two more options that we can use to standardize
    URLs instead of interacting directly with its naming service:'
  prefs: []
  type: TYPE_NORMAL
- en: '**DNS**: Each service can specify its `hostname` (also known as its **DNS name**).
    The DNS service takes care of translating it into the actual service URL. For
    example, if a service specifies an `order.processing` DNS name and it has an HTTP
    endpoint on port `80` and a `/purchase` path, we can reach this endpoint with
    `http://order.processing:80/purchase`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reverse proxy:** Service Fabric''s Reverse Proxy intercepts all the calls
    that have been directed to the cluster address and uses the name service to send
    them to the right application and service within that application. Addresses that
    are resolved by the reverse proxy service have the following structure: `<cluster
    name>.<location>.cloudapp.azure.com: <port>//<app name>/<service name>/<endpoint
    path>?PartitionKey=<value> & PartitionKind=value`. Here, partition keys are used
    to optimize state, fully reliable services and will be explained at the end of
    this subsection. This means that stateless services lack the query string part
    of the previous address. Thus, a typical address that''s solved by reverse proxy
    may be something similar to `myCluster.eastus.cloudapp.azure.com: 80//myapp/myservice/<endpoint
    path>?PartitionKey=A & PartitionKind=Named`. If the preceding endpoint is called
    from a service hosted on the same cluster, we can specify `localhost` instead
    of the complete cluster name (that is, from the same cluster, not from the same
    node): `localhost: 80//myapp/myservice/<endpoint path>?PartitionKey=A & PartitionKind=Named`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By default, the DNS service is activated but the reverse proxy isn't. Therefore,
    we must enable it by checking the *Enable reverse proxy* checkbox in the second
    step of Service Fabric's configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Security configuration'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we''ve submitted the second step, we come to a security page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f0decbde-2744-4755-86b1-dce6c85fa72b.png)'
  prefs: []
  type: TYPE_IMG
- en: If we choose the basic option, the wizard creates an X509 certificate to secure
    our communication with the cluster. Otherwise, we can select an existing one from
    the Azure Key Vault. If you don't have a Key Vault, the wizard will make you create
    one so that you can store the newly created certificate. In the certificate options,
    locate the certificate usage option and select publishing/deploying. If you don't,
    you will receive an error message, along with some instructions telling you what
    to do to fix the issue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the certificate is ready, download it onto your machine (by following
    the wizard''s instructions) and double-click on it to install it in your local
    machine. The certificate will be used to deploy applications from your machine.
    Specifically, you are required to insert the following information into the Cloud
    Publish Profile of your Visual Studio Service Fabric applications (see this chapter''s
    *Use case – logging microservices* section for more details):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Since both the client (Visual Studio) and the server use the same certificate
    for authentication, the server and client thumbprint are the same. The certificate
    thumbprint can be copied from your Azure Key Vault. It is worth mentioning that
    you can add also client-specific certificates with the main server certificate
    by selecting the Custom option in *step 3*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you submit your certificate, you are presented with a summary of your
    configuration. Submitting your approval will create the cluster. Pay attention
    to this: a cluster may spend your Azure free credit in a short time, so just keep
    your cluster on when you''re testing. After, you should delete it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we mentioned in the *Azure and Visual Studio support for microservices orchestration*
    subsection, Azure Service Fabric supports two kinds of *reliable service*: stateless
    and stateful. Stateless services either don''t store permanent data or they store
    it in external supports such as the Redis Cache or databases (see [Chapter 7](77cdecb5-cef4-4b02-80a1-052ad366b9f3.xhtml),
    *How to Choose Your Data Storage in the Cloud*, for the main storage options offered
    by Azure).'
  prefs: []
  type: TYPE_NORMAL
- en: Stateful services, on the other hand, use Service Fabric-specific distributed
    dictionaries and queues. Each distributed data structure is accessible from all
    the *identical* replicas of a service, but only one replica, called the primary
    replica, is allowed to write on them to avoid synchronized access to those distributed
    resources, which may cause bottlenecks. All the other replicas, known as secondary
    replicas, can only be read from these distributed data structures.
  prefs: []
  type: TYPE_NORMAL
- en: You can check if a replica is primary by looking at the context object your
    code receives from the Azure Service Fabric runtime, but usually, you don't need
    to do this. In fact, when you declare your service endpoints, you are required
    to declare those that are read-only. A read-only endpoint is supposed to receive
    requests so that it can read data from the shared data structures. Therefore,
    since only read-only endpoints are activated for secondary replicas, if you implement
    them correctly, write/update operations should be automatically prevented on stateful
    secondary replicas with no need to perform further checks.
  prefs: []
  type: TYPE_NORMAL
- en: In stateful services, secondary replicas enable parallelism on read operations,
    so in order to get parallelism on write/update operations, stateful services are
    assigned different data partitions. More specifically, for each stateful service,
    Service Fabric creates a primary instance for each partition. Then, each partition
    may have several secondary replicas.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed data structures are shared between the primary instance of each
    partition and its secondary replicas. The whole extent of data that can be stored
    in a stateful service is split among the chosen number of partitions, according
    to a partition key that is generated by a hashing algorithm on the data to be
    stored.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, partition keys are integers that belong to a given interval that
    is split among all the available partitions. For instance, a partition key can
    be generated by calling the .NET `GetHashCode()` method on one or more string
    fields to get integers that are then processed to get a unique integer (using,
    for instance, an exclusive or operation on the integer bits). Then, this integer
    can be constrained to the integer interval that was chosen for the partition key
    by taking the remainder of an integer division (for instance, the remainder of
    a division for 1,000 will be an integer in the 0-999 interval).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say we want four partitions, which will be selected with an integer
    key in the 0-999 interval. Here, Service Fabric will automatically create four
    primary instances of our stateful service and assign them the following four partition
    key subintervals: 0-249, 250-499, 500-749, 750-999.'
  prefs: []
  type: TYPE_NORMAL
- en: From within your code, you are required to compute the partition key of the
    data you send to a stateful service. Then, Service Fabric's runtime will select
    the right primary instance for you. The *Use case – logging microservices* section
    at the end of this chapter provides more practical details on this and how to
    use reliable services in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Kubernetes Service (AKS)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is an advanced open source orchestrator that you can install locally
    on your private machine's cluster. At the time of writing, it is the most widespread
    orchestrator, so Microsoft offers it as an alternative to Azure Service Fabric.
    Also, if you prefer Azure Service Fabric, you may be forced to use the **Azure
    Kubernetes Service** (**AKS**) since some advanced solutions (for instance, some
    big data solutions) are built on top of Kubernetes. This subsection provides a
    short introduction to AKS, but more details can by found in the official documentation,
    which is referenced in the *Further reading* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create an AKS cluster, type `AKS` into Azure search, select Kubernetes services,
    and then click the Add button. The following form will appear:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/230c287a-5924-48e3-842a-167e84e49f0c.png)'
  prefs: []
  type: TYPE_IMG
- en: As usual, you are required to specify a subscription, resource group, and location.
    Then, you can choose a unique name (Kubernetes cluster name), the prefix of the
    cluster URI (DNS name prefix), and the version of Kubernetes you would like to
    use. For computational power, you are asked to select a machine template for each
    node (Node size) and the number of nodes. If you click Next, you can provide security
    information, namely a *service principal*, and specify whether you wish to enable
    role-based access control. In Azure, service principals are accounts that are
    associated with services you may use to define resource access policies. If you
    have no experience with this concept and/or if you don't have any preexisting
    service principals, you can let the wizard create one for you.
  prefs: []
  type: TYPE_NORMAL
- en: There are other settings you can change too, but the default values work well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you''ve created the Kubernetes cluster, you can interact with it through
    the `kubectl` command-line tool. `kubectl` is integrated into the Azure console,
    so you need just to activate your cluster credentials. Select the Azure console
    at the top of the page portal and then type in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command downloads the credentials that were automatically created
    to enable your interaction with the cluster and configures the Kubernetes CLI
    so that it can use them.
  prefs: []
  type: TYPE_NORMAL
- en: Then, if you write `kubectl get nodes`, you should get a list of available Kubernetes
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker images can be loaded into the cluster and configured by writing a `.yaml`
    configuration file, such as `myClusterConfiguration.yaml`, and typing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: You can create and edit this file by writing `nano` on the Azure console to
    launch the nano editor. Once you're in the editor, you can paste content from
    a local file and then save it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding command deploys the application and runs it. The deployment state
    can be monitored with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Here, `MyDeployment` is the name that's given to deployment in the `.yaml` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the cluster is no longer needed, you can delete it with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The application state can be monitored by selecting Insights from the resource
    Azure menu. Here, you can apply filters and select the information you need.
  prefs: []
  type: TYPE_NORMAL
- en: '`.yaml` files have the same structure as JSON files but they have a different
    syntax. You have objects and lists but object properties are not surrounded by
    `{}` and lists are not surrounded by `[]`. Instead, nested objects are declared
    by simply indenting their content with spaces. The number of spaces can be freely
    chosen, but once they''ve been chosen, they must be used coherently.'
  prefs: []
  type: TYPE_NORMAL
- en: List items can be distinguished from object properties by preceding them with
    a hyphen (`-`). `.yaml` files can contain several sections that are separated
    by a line containing the `---` string. Typically, you define a `Deployment` that
    describes which images to deploy and how many replicas they must have. Each deployment
    groups a set of images to be deployed together on the same node, which means that
    each replica that's deployed in any node must have all those images installed.
    A set of images to be deployed together is called a **pod**.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, the following configuration deploys two replicas of a single
    image pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The initial header declares the Kubernetes API version to use and the kind of
    object we are going to define (a deployment), and assigns a name to the object.
    The deployment name can be used at a later time to modify the cluster with deployment
    edit commands.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: On the other hand, the `spec` attribute under the template lists all the containers
    that will compose each replica of the pod.
  prefs: []
  type: TYPE_NORMAL
- en: In turn, each container has a name and specifies the Docker image to be used
    to instantiate it. Then, it specifies the average computational resources that
    are needed and their maximum limits. Finally, it specifies the ports that are
    exposed externally. These ports are not forwarded to a different port and are
    exposed as they are. This port setting overrides the EXPOSE Docker file setting.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can specify some environment variables to set inside each container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since there are several replicas of the same services located on different
    nodes, and since allocating services to nodes may change dynamically, there is
    a problem when it comes to internal communication among pods and internal-to-external
    communication. This problem is solved by defining services that offer a unique
    entry point for all instances of a pod. A service definition can be added to the
    same `.yaml` file, separated by `---`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The preceding definition creates a service that's exposed on port `8080`, which
    redirects all requests to port `80` of a `MyApplication` replica. The pod that's
    served by the service is selected by the `selector` property. The service IP is
    internally visible, but client pods don't need to know the service IP since the
    services can be reached through their names, just like hosts in a classic network.
    Thus, in this case, `MyApplication-service:8080` does the job.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we need a publicly accessible IP, we need to add `type: LoadBalancer` under
    `spec` before `ports`. AKS will select a public IP for you. We can get the chosen
    public IP by watching the deployment process with `kubectl get service MyDeployment
    --watch` until the IP is selected. If we bought an IP address in the same resource
    group as AKS, we can specify this IP address by adding `clusterIP: <your IP>`
    under the service `spec`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pods can be organized into namespaces if we create namespaces in our `.yaml`
    files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you can target objects (services or deployments) in a namespace by adding
    `namespace: <your namespace>` after its name in its definition metadata. Similarly,
    you can target `kubectl` commands in a specific namespace by adding them with
    the `-- namespace=<your namespace>` option.'
  prefs: []
  type: TYPE_NORMAL
- en: The use case in the next section will provide more details when it comes to
    defining a Service Fabric application. More details on Kubernetes clusters can
    be found in the references listed in the *Further reading* section.
  prefs: []
  type: TYPE_NORMAL
- en: Use case – logging microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will take a look at a microservice-based system that logs
    data about purchases relating to various destinations in our WWTravelClub use
    case. In particular, we will design microservices that takes care of computing
    daily revenues per location. Here, we're assuming that these microservices receive
    data from other subsystems hosted in the same Azure Service Fabric application.
    More specifically, each purchase log message is composed of the location name,
    the overall package cost, and the date and time of the purchase.
  prefs: []
  type: TYPE_NORMAL
- en: As a first step, let's ensure that the Service Fabric emulator that we mentioned
    in the *Technical requirements* section of this chapter has been installed and
    is running on your development machine. Now, we need so switch it so that it runs
    **5 nodes**.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can follow the steps we explained in the *Azure and Visual Studio support
    for microservice orchestration* section to create a Service Fabric project named
    `PurchaseLogging`. Select a .NET Core stateful reliable service and name it `LogStore`.
  prefs: []
  type: TYPE_NORMAL
- en: The solution that's created by Visual Studio is composed of a `PurchaseLogging`
    project, which represents the overall application, and a `LogStore` project, which
    will contain the implementation of the first microservice that's included in the
    `PurchaseLogging` application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Under the `PackageRoot` folder, the `LogStore` service and each reliable service
    contain the `ServiceManifest.xml` configuration file and a `Settings.xml` folder
    (under the `Config` subfolder). The `Settings.xml` folder contains some settings
    that you can read from the service code. The initial file contains predefined
    settings that are needed by the Service Fabric runtime. Let''s add a new settings
    section, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: We will use the value of `MessageMaxDelaySeconds` to configure the system component
    and ensure message idempotency. The setting value is empty, because most of the
    settings are overridden by the overall application settings contained in the `PurchaseLogging`
    project.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ServiceManifest.xml` file contains some configurations tags that are automatically
    handled by Visual Studio, as well as a list of endpoints. Two endpoints are preconfigured
    since they are used by the Service Fabric runtime. Here, we must add the configuration
    details of all the endpoints our microservice will listen to. Each endpoint definition
    has the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: If `Type` is `Internal`, the port will be opened just inside the cluster's local
    network; otherwise, the port will be available from outside the cluster as well.
    In the preceding case, we must declare that port in the configuration of the Azure
    Service Fabric cluster as well, otherwise the cluster load balancer/firewall will
    not forward messages to it.
  prefs: []
  type: TYPE_NORMAL
- en: Public ports can be reached directly from the cluster URI (`<cluster name>.<location
    code>.cloudapp.azure.com`) since the load balancer that interfaces each cluster
    will forward the input traffic it receives to them.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we won't define endpoints since we are going to use remoting-based
    communication, which has already been defined, for all internal interactions,
    but we will show you how to use them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `PurchaseLogging` project contains a reference to the `LogStore` project
    under the *services* Solution Explorer node and contains various folders with
    various XML configuration files. More specifically, we have the following folders:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ApplicationPackageRoot`, which contains the overall application manifest named
    `ApplicationManifest.xml`. This file contains some initial parameter definitions
    and then further configurations. Parameters have the following format:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Once defined, parameters can replace any value in the remainder of the file.
    Parameter values are referenced by enclosing the parameter name between square
    brackets, as shown in the following code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Some parameters define the number of replicas and partitions for each service
    and are automatically created by Visual Studio:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Let's replace the initial values suggested by Visual Studio with those in the
    preceding code. We will use just two partitions to show you how partitions work,
    but you can increase this value to improve write/update parallelism. Each partition
    of the `LogStore` service doesn't need several replicas since replicas improve
    performance on read operations and this service is not designed to offer read
    services. Therefore, you may choose just one replica, or at most two, to make
    the system redundant and more robust to failures.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding parameters are used to define the role of the `LogStore` service
    inside the overall application. This definition is generated automatically by
    Visual Studio in the same file, below the initial definition created by Visual
    studio, with just the partition interval changed to 0-1,000:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '`ApplicationParameters` contains possible overrides for parameters defined
    in `ApplicationManifest.xml` for various deployment environments: the cloud (that
    is, the actual Azure Service Fabric cluster) and local emulators with one or five
    nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PublishProfiles` contains the settings that are needed to publish the application
    in the same environments handled by the `ApplicationParameters` folder. You just
    need to customize the cloud publish profile with the actual name of your Azure
    Service Fabric URI and with the authentication certificate you downloaded during
    the Azure cluster configuration process:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: The remaining steps that need to be followed in order to complete the application
    have been organized into several subsections. Let's start by looking at ensuring
    message idempotency.
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring message idempotency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Messages can become lost because of failures or small timeouts caused by load
    balancing. Here, we will use a predefined remoting-based communication that performs
    automatic message retries in the case of failures. However, as we explained in
    the *Microservice design principles* subsection, this may cause the same messages
    to be received twice. Since we are summing up the revenues of purchase orders,
    we must protect ourselves from summing up the same purchase several times.
  prefs: []
  type: TYPE_NORMAL
- en: To do this, we will implement a library containing the necessary tools to ensure
    that message replicas are discarded.
  prefs: []
  type: TYPE_NORMAL
- en: Let's add a new .NET Standard 2.0 library project called **IdempotencyTools**
    to our solution. Now, we can remove the initial class scaffolded by Visual studio.
    This library needs a reference to the same version of the `Microsoft.ServiceFabric.Services`
    NuGet package referenced by `LogStore`, so let's verify the version number and
    add the same NuGet package reference to the `IdempotencyTools` project.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main tool that ensures message idempotency is the `IdempotentMessage` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: We added the `DataContract` and `DataMember` attributes since they are needed
    by the remoting communication serializer we are going to use for all internal
    messages. Basically, the receding class is a wrapper that adds a `Guid` and a
    time mark to the message class instance that's passed to its constructor.
  prefs: []
  type: TYPE_NORMAL
- en: The `IdempotencyFilter` class uses a distributed dictionary to keep track of
    the messages it's already received. To avoid the indefinite growth of this dictionary,
    older entries are periodically deleted. Messages that are too old to be found
    in the dictionary are automatically discarded.
  prefs: []
  type: TYPE_NORMAL
- en: 'The time interval entries are kept in the dictionary and are passed in the
    `IdempotencyFilter` static factory method, which creates new filter instances,
    along with the dictionary name and the `IReliableStateManager` instance, which
    are needed to create the distributed dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The dictionary contains each message time mark indexed by the message `Guid`
    and is created by invoking the `GetOrAddAsync` method of the `IReliableStateManager`
    instance with the dictionary type and name. `lastClear` contains the time of the
    removal of all old messages.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a new message arrives, the `NewMessage` method checks whether it must
    be discarded. If the message must be discarded, it returns `null`; otherwise,
    it adds the new message to the dictionary and returns the message without the
    `IdempotentMessage` wrapper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'As a first step, the method verifies whether it''s time to clear the dictionary
    and whether the message is too old. Then, it starts a transaction to access the
    dictionary. All distributed dictionary operations must be enclosed in a transaction,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: If the message `Guid` is found in the dictionary, the transaction is aborted
    since the dictionary doesn't need to be updated and the method returns `default(T)`,
    which is actually `null` since the message must not be processed. Otherwise, the
    message entry is added to the dictionary and the unwrapped message is returned.
  prefs: []
  type: TYPE_NORMAL
- en: The code of the `Clear` method can be found in the GitHub repository associated
    with this book.
  prefs: []
  type: TYPE_NORMAL
- en: The Interaction library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are some types that must be shared among all microservices. If the internal
    communication is implemented with either remoting or WCF, each microservice must
    expose an interface with all the methods other microservices call. Such interfaces
    must be shared among all microservices. Moreover, with all communication interfaces,
    the classes that implement the messages must also be shared among all microservices
    (or among some subsets of them). Therefore, all of these structures are declared
    in external libraries that are referenced by the microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's add a new .NET Standard 2.0 library project called `Interactions`
    to our solution. Since this library must use the `IdempotentMessage` generic class,
    we must add it as a reference to the `IdempotencyTools` project. We must also
    add a reference to the remoting communication library contained in the `Microsoft.ServiceFabric.Services.Remoting`
    NuGet package since all interfaces that are used to expose the microservice's
    remote methods must inherit from the `IService` interface defined in this package.
  prefs: []
  type: TYPE_NORMAL
- en: '`IService` is an empty interface that declares the communication role of the
    inheriting interface. The `Microsoft.ServiceFabric.Services.Remoting` NuGet package
    version must match the version of the `Microsoft.ServiceFabric.Services` package
    declared in the other projects.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows the declarations of the interface that need to be
    implemented by the `LogStore` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the code of the `PurchaseInfo` message class, which is referenced
    in the `ILogStore` interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Now, we are ready to implement our main `LogStore` microservice.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the receiving side of communication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To implement the `LogStore` microservice, we must add a reference to the `Interaction`
    library, which will automatically create references to the remoting library and
    to the `IdempotencyTools` project. Then, the `LogStore` class must implement the
    `ILogStore` interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Once the service receives a `LogPurchase` call from the remoting runtime, it
    puts the message in the `LogQueue` to avoid the caller remaining blocked, waiting
    for message processing completion. This way, we achieve both the reliability of
    a synchronous message passing protocol (the caller knows that the message has
    been received) and the performance advantages of asynchronous message processing
    that are typical of asynchronous communication.
  prefs: []
  type: TYPE_NORMAL
- en: '`LoqQueue`, as a best practice for all distributed collections, is created
    in the `RunAsync` method, so `LogQueue` may be null if the first call arrives
    before the Azure Service Fabric runtime has called `RunAsync`. In this case, the
    method returns `false` to signal that the service isn''t ready yet. Otherwise,
    a transaction is created to enqueue the new message.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, our service will not receive any communication if we don''t furnish
    an implementation of `CreateServiceReplicaListeners()` that returns all the listeners
    that the service would like to activate. In the case of remoting communications,
    there is a predefined method that performs the whole job, so we just need to call
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Here, `CreateServiceRemotingReplicaListeners` is an extension method defined
    in the remoting communication library. It creates listeners for both primary replicas
    and secondary replicas (for read-only operations). When creating the client, we
    can specify whether its communications are addressed just to primary replicas
    or also to secondary replicas.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you would like to use different listeners, you must create an `IEnumerable`
    of `ServiceReplicaListener` instances. For each listener, you must invoke the
    `ServiceReplicaListener` constructor with three arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: A function that receives the reliable service context object as its input and
    returns an implementation of the `ICommunicationListener` interface.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The name of the listener. This second argument becomes obligatory when the service
    has more than one listener.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Boolean that is true if the listener must be activated on secondary replicas.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For instance, if we would like to add both custom and HTTP listeners, the code
    becomes something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '`MyCustomHttpListener` is a custom implementation of `ICommunicationListener`,
    while `KestrelCommunicationListener` is a predefined HTTP listener based on Kestrel
    and ASP.NET Core. The following is the full code that defines the `KestrelCommunicationListener`
    listener:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Usually, `ICommunicationListener` implementations accept the node context and
    an endpoint name in their constructors and are responsible for reading the endpoint
    data defined in the `ServiceManifest.xml` service, as well as creating a listening
    endpoint that satisfies the specification contained there. They do this in their
    `CommunicationListener.OpenAsync` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '`<computedURISchema>` is the URI with the IP address replaced by a "+". Once
    returned by `OpenAsync`, it is published in the Service Fabric naming service
    and used to compute the actual service address from the cluster node IP address
    it''s been deployed in.'
  prefs: []
  type: TYPE_NORMAL
- en: '`ICommunicationListener` implementations must also have a `Close` method, which
    must close the opened communication channel, and an `Abort` method, which must
    **immediately** close the communication channel (ungracefully, that is, without
    informing connected clients and so on).'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have turned communications on, we can implement the service logic.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing service logic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Service logic is executed by the tasks that are launched as independent threads
    when `RunAsync` is invoked by the Service Fabric runtime. It's good practice to
    create an `IHost` and design all the tasks as `IHostedService` implementations
    when you only need to implement one task. In fact, `IHostedService` implementations
    are independent chunks of software that are easier to unit-test. `IHost` and `IHostedService`
    were discussed in detail in the *Using generic hosts* subsection.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will implement the logic that computes daily revenues for
    each location into an `IHostedservice` named `ComputeStatistics`, which uses a
    distributed dictionary whose keys are the location names and whose values are
    instances of a class called `RunningTotal`. This class stores the current running
    total and the day that is being computed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'This class has an `Update` method that updates the instance when a new purchase
    message is received. First of all, the incoming message time is normalized to
    universal time. Then, the day part of this time is extracted and compared with
    the current `Day` of the running total, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'If it''s a new day, we assume that the running total computation of the previous
    day has finished, so the `Update` method returns it in a new `RunningTotal` instance
    and resets `Day` and `Count` so that it can compute the new day running total.
    Otherwise, the new value is added to the running `Count` and the method returns
    `null`, meaning that the day total isn''t ready yet. This implementation can be
    seen in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The `IHostedService` implementation of `ComputeStatistics` needs some parameters
    to work properly, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The queue containing all the incoming messages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `IReliableStateManager` service, so that it can create the distributed dictionary
    where it stores data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `ConfigurationPackage` service, so that it can read the settings defined
    in the `Settings.xml` service file and possibly those overridden in the application
    manifest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The preceding parameters must be passed in the `ComputeStatistics` constructor
    when a `ComputeStatistics` instance is created by `IHost` through dependency injection.
    We will return to the `IHost` definition in the next subsection. For now, let''s
    concentrate on the `ComputeStatistics` constructor and its fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'All the constructor parameters are stored in private fields so that they can
    be used when `ExecuteAsync` is called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Before entering its loop, the `ComputeStatistics` service prepares some structures
    and parameters. It declares that the queue isn''t empty so that it can start dequeuing
    messages. Then, it extracts `MessageMaxDelaySeconds` from the service settings
    and turns it into an integer. The value of this parameter was left empty in the
    `Settings.xml` file. Now, it''s time to override it and define its actual value
    in `ApplicationManifest.xml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '`ServiceManifestImport` imports the service manifest in the application and
    overrides some configuration. Its version number must be changed every time its
    content and/or the service definition is changed and the application is redeployed
    in Azure because version number changes tell the Service Fabric runtime what to
    change in the cluster. Version numbers also appear in other configuration settings.
    They must be changed every time the entities they refer to change.'
  prefs: []
  type: TYPE_NORMAL
- en: '`MessageMaxDelaySeconds` is passed to the instance of the idempotency filter,
    along with a name for the dictionary of the already received messages, and with
    the instance of the `IReliableStateManager` service. Finally, the main distributed
    dictionary that''s used to store running totals is created.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After this, the service enters its loop and finishes when `stoppingToken` is
    signaled, that is, when the Service Fabric runtime signals that the service is
    going to be stopped:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'The inner loop runs until the queue isn''t empty and then exits and waits 100
    milliseconds before verifying whether new messages have been enqueued:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the code for the inner loop, which is enclosed in a transaction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the service is trying to dequeue a message. If the queue is empty, it
    sets `queueEmpty` to `true` to exit the loop; otherwise, it passes the message
    through the idempotency filter. If the message survives this step, it uses it
    to update the running total of the location referenced in the message. However,
    correct operation of the distributed dictionary requires that the old counter
    is replaced with a new counter each time an entry is updated. Accordingly, the
    old counter is copied into a new `RunningTotal` object. This new object can be
    updated with the new data if we call the `Update` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the transaction is committed, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'When the `Update` method returns a complete computation result, that is when
    the `total != null` method is called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: The `SendTotal` method sends the total to a service that publicly exposes all
    the statistics through an HTTP endpoint. After reading [Chapter 12](3ec4db7b-e570-41d0-94fb-2bdbd6dc4a9a.xhtml),
    *Applying Service-Oriented Architectures with .NET Core*, which is dedicated to
    the Web API, you may want to implement a similar service with a stateless ASP.NET
    Core microservice connected to a database. The stateless ASP.NET Core service
    template automatically creates an ASP.NET Core-based HTTP endpoint for you.
  prefs: []
  type: TYPE_NORMAL
- en: However, since this service must receive data from the `SendTotal` method, it
    also needs remote-based endpoints. Therefore, we must create them, just like we
    did for the `LogStore` microservice, and concatenate the remote-based endpoint
    array with the preexisting array containing the HTTP endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the microservice's host
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we have everything in place to define the microservice''s `RunAsync` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Here, the method verifies whether the cancellation token was signaled, in which
    case we throw an exception to abort the method. Then, the service queue is created,
    and the service settings are saved in `configurationPackage`.
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, we can create the `IHost` service, as we explained in the *Using
    generic hosts* subsection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '`ConfigureServices` defines all singletons instances that may be needed by
    `IHostedService` implementations, so they are injected into the constructor of
    all the implementations that reference their types. Then, `AddHostedService` declares
    the unique `IHostedService` of the microservice. Once the `IHost` is built, we
    run it until the `RunAsync` cancellation token is signaled. When the cancellation
    token is signaled, the request to shutdown is passed to all `IHostedService` implementations.'
  prefs: []
  type: TYPE_NORMAL
- en: Communicating with the service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since we haven't implemented the whole purchase logic yet, we will implement
    a stateless microservice that sends random data to the `LogStore` service. Right-click
    on the `PurchaseLogging` project in the Solution Explorer and select Add | Service
    Fabric Service. Then, select the .NET Core stateless template and name the new
    microservice project `FakeSource`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s add a reference to the `Interaction` project. Before moving on
    to the service code, we need to update the replica count of the newly created
    service in `ApplicationManifest.xml` and in all the other environment-specific
    parameter overrides (the cloud, one local cluster node, five local cluster nodes):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'This fake service needs no listeners and its `RunAsync` method is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'In each loop, a random message is created and sent to the counting microservices.
    Then, the thread sleeps for a second and starts a new loop. The code that sends
    the created messages is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Here, a key in the 0-9,999 interval is computed from the location string. This
    integer is passed to the `ServicePartitionKey` constructor. Then, a service proxy
    is created, and the URI of the service to call and the partition key are passed.
    The proxy uses this data to ask the naming service for a physical URI for a primary
    instance for the given partition value.
  prefs: []
  type: TYPE_NORMAL
- en: '`ServiceProxy.Create` also accepts a third optional argument that specifies
    whether messages that are sent by the proxy can also be routed to secondary replicas.
    The default is that messages are routed just to primary instances. If the message
    target returns `false`, meaning that it''s not ready (remember that `LogPurchase`
    returns `false` when the `LogStore` message queue hasn''t been created yet), the
    same transmission is attempted after 100 milliseconds.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sending messages to a remoting target is quite easy. However, other communication
    listeners require that the sender interacts manually with the naming service to
    get a physical service URI. This can be done with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: Moreover, in the case of generic communication protocols, we must manually handle
    failures and retries with a library such as Polly (see the *Resilient task execution*
    subsection for more information).
  prefs: []
  type: TYPE_NORMAL
- en: Testing the application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To test that the application actually computes running purchase totals, let''s
    place a breakpoint in the `ComputeStatistics.cs` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: Each time the breakpoint is hit, look at the content of `newCounter` to verify
    how the running totals of all the locations change.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we described what microservices are and how they have evolved
    from the concept of a module. Then, we talked about the advantages of microservices
    and when it's worth using them, as well as general criteria for their design.
    We also explained what Docker containers are and analyzed the strong connection
    between containers and microservice architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we took on a more practical implementation by describing all the tools
    that are available in .NET Core so that we can implement microservice-based architectures.
    We also described infrastructures that are needed by microservices and how the
    Azure cluster offers Azure Kubernetes Services and Azure Service Fabric.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we put these concepts into practice by implementing a Service Fabric
    application. Here, we looked at the various ways in which Service Fabric applications
    can be implemented.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter focuses on how to use ORMs and Entity Framework Core to interact
    with various kinds of database while keeping our code independent from the database
    engine we've selected.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the two-folded nature of the module concept?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is scaling optimization the only advantage of microservices? If not, list some
    further advantages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is Polly?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is `ConfigureServices`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What Docker support is offered by Visual Studio?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'What Docker application method is more powerful: the one based on `.yml` files
    or the one based on `.yaml` files?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What kinds of port must be declared during the definition of an Azure Service
    Fabric cluster?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why are partitions of reliable stateful services needed?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we declare that a remoting communication must be addressed by secondary
    replicas? What about for other types of communication?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are links to the official documentation for Azure Service Bus
    and RabbitMQ, two event bus technologies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Azure Service Bus**: [https://docs.microsoft.com/en-us/azure/service-bus-messaging/](https://docs.microsoft.com/en-us/azure/service-bus-messaging/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RabbitMQ**: [https://www.rabbitmq.com/getstarted.html](https://www.rabbitmq.com/getstarted.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The documentation for Polly, a tool for reliable communication/tasks, can be
    found here: [https://github.com/App-vNext/Polly](https://github.com/App-vNext/Polly).'
  prefs: []
  type: TYPE_NORMAL
- en: 'More information on Docker can be found on Docker''s official website: [https://docs.docker.com/](https://docs.docker.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The official documentation for Kubernetes and `.yaml` files can be found here:
    [https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The official documentation for Azure Kubernetes can be found here: [https://docs.microsoft.com/en-US/azure/aks/](https://docs.microsoft.com/en-US/azure/aks/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The official documentation for Azure Service Fabric can be found here: [https://docs.microsoft.com/en-US/azure/service-fabric/](https://docs.microsoft.com/en-US/azure/service-fabric/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The official documentation for Azure Service Fabric''s reliable services can
    be found here: [https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-reliable-services-introduction](https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-reliable-services-introduction).'
  prefs: []
  type: TYPE_NORMAL
- en: 'More information about the Actor model can be found here: [https://www.researchgate.NET/publication/234816174_Actors_A_conceptual_foundation_for_concurrent_object-oriented_programming](https://www.researchgate.net/publication/234816174_Actors_A_conceptual_foundation_for_concurrent_object-oriented_programming).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The official documentation for Actor models that can be implemented in Azure
    Service Fabric can be found here: [https://docs.microsoft.com/en-US/azure/service-fabric/service-fabric-reliable-actors-introduction](https://docs.microsoft.com/en-US/azure/service-fabric/service-fabric-reliable-actors-introduction).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Microsoft has also implemented an advanced actor model that is independent
    of Service Fabric. This is known as the Orleans framework. More information about
    Orleans can be found at the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Orleans – Virtual Actors**: [https://www.microsoft.com/en-us/research/project/orleans-virtual-actors/?from=https%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fprojects%2Forleans%2F](https://www.microsoft.com/en-us/research/project/orleans-virtual-actors/?from=https%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fprojects%2Forleans%2F)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Orleans** **Documentation**: [http://dotnet.github.io/orleans/Documentation/](http://dotnet.github.io/orleans/Documentation/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
