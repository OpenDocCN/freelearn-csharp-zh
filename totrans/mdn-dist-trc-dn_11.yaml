- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Instrumenting Messaging Scenarios
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置消息传递场景
- en: Messaging and asynchronous processing improve distributed system scalability
    and reliability by reducing coupling between services. However, they also increase
    complexity and introduce a new failure mode, which makes observability even more
    important.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 通过减少服务之间的耦合，消息和异步处理提高了分布式系统的可扩展性和可靠性。然而，它们也增加了复杂性，并引入了一种新的故障模式，这使得可观察性变得更加重要。
- en: In this chapter, we’ll work on instrumenting a messaging producer and consumer
    with traces and metrics and cover individual and batch message processing.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用跟踪和指标来配置消息生产者和消费者，并涵盖单个和批量消息处理。
- en: 'In this chapter, you’ll learn how to do the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习以下内容：
- en: Trace individual messages as they are created and published
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟踪消息在创建和发布时的单个消息
- en: Instrument receiving and processing operations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仪器接收和处理操作
- en: Instrument batches
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置批次
- en: Use instrumentation to diagnose common messaging problems
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用配置来诊断常见的消息传递问题
- en: By the end of this chapter, you should be able to instrument your messaging
    application from scratch or tune the existing messaging instrumentation to your
    needs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '到本章结束时，你应该能够从头开始配置你的消息传递应用程序，或者根据需要调整现有的消息传递配置。 '
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code for this chapter is available in the book’s repository on GitHub at
    [https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter11](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter11).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可在GitHub上找到，位于书籍仓库的[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter11](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter11)。
- en: 'To run the samples and perform analysis, we’ll need the following tools:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行示例并执行分析，我们需要以下工具：
- en: .NET SDK 7.0 or later.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: .NET SDK 7.0或更高版本。
- en: Docker and `docker-compose`.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker和`docker-compose`。
- en: Any HTTP benchmarking tool, for example, `bombardier`. You can install it with
    `$ go get -u github.com/codesenberg/bombardier` if you have Go tools, or download
    bits directly from its GitHub repository at [https://github.com/codesenberg/bombardier/releases](https://github.com/codesenberg/bombardier/releases).
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何HTTP基准测试工具，例如，`bombardier`。如果你有Go工具，可以使用`$ go get -u github.com/codesenberg/bombardier`安装它，或者直接从其GitHub仓库[https://github.com/codesenberg/bombardier/releases](https://github.com/codesenberg/bombardier/releases)下载。
- en: We will also be using the Azure Storage emulator in Docker. No setup or Azure
    subscription is necessary.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用Docker中的Azure存储模拟器。不需要设置或Azure订阅。
- en: Observability in messaging scenarios
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 消息传递场景中的可观察性
- en: In [*Chapter 10*](B19423_10.xhtml#_idTextAnchor161), *Tracing Network Calls*,
    we just started scratching the surface of tracing support for asynchronous processing.
    There, we saw how the client and server can send a stream of potentially independent
    messages to each other.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第10章*](B19423_10.xhtml#_idTextAnchor161) *跟踪网络调用*中，我们刚刚开始探索异步处理的支持。在那里，我们看到了客户端和服务器如何相互发送一系列可能独立的消息。
- en: 'In the case of messaging, things get even more complicated: in addition to
    asynchronous communication, the producer and consumer interact through an intermediary
    – a messaging **broker**.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在消息传递的情况下，事情变得更加复杂：除了异步通信之外，生产者和消费者通过一个中介——消息**代理**进行交互。
- en: Operation on the producer completes once the message is published to the broker
    without waiting for the consumer to process this message. Depending on the scenario
    and application health, the consumer may process it right away, in a few seconds,
    or in several days.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦消息发布到代理，生产者的操作就完成了，而不需要等待消费者处理这条消息。根据场景和应用的健康状况，消费者可能立即处理它，几秒钟后处理，或者几天后处理。
- en: In some cases, producers get a notification that the message was processed,
    but this usually happens through another messaging queue or a different communication
    channel.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，生产者会收到消息已处理的通知，但这通常是通过另一个消息队列或不同的通信渠道完成的。
- en: Essentially, the producer does not know whether the consumer exists – failures
    or delays in the processing pipeline are not visible on the producer side. This
    changes how we should look at latency, throughput, or error rate from an observability
    standpoint – now we need to think about end-to-end flows that consist of multiple
    independent operations.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，生产者不知道消费者是否存在——处理管道中的故障或延迟在生产者端是不可见的。这改变了我们应该从可观察性角度看待延迟、吞吐量或错误率的方式——现在我们需要考虑由多个独立操作组成的端到端流程。
- en: 'For example, when using HTTP calls only, the latency of the original request
    covers almost everything that happened with the request. Once we introduce messaging,
    we need means to measure the end-to-end latency and identify failures between
    different components. An example of an application that uses messaging is shown
    in *Figure 11**.1*:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当仅使用HTTP调用时，原始请求的延迟几乎涵盖了请求过程中发生的所有事情。一旦我们引入消息传递，我们需要手段来测量端到端延迟并识别不同组件之间的故障。一个使用消息传递的应用示例在*图11.1*中显示：
- en: '![Figure 11.1 – Application using messaging to run tasks in the background
    ](img/B19423_11_01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图11.1 – 使用消息传递在后台运行任务的应用](img/B19423_11_01.jpg)'
- en: Figure 11.1 – Application using messaging to run tasks in the background
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 – 使用消息传递在后台运行任务的应用
- en: In such an application, when the user sends a request to the frontend, they
    receive a response once the backend finishes processing and publishes a message
    to a topic. The indexer, replicator, archiver, and any other services that post-process
    the data run at their own speed. The indexer usually processes the latest messages,
    while the archiver would only look at the messages published days ago.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这样的应用中，当用户向前端发送请求时，一旦后端完成处理并向主题发布消息，他们就会收到响应。索引器、复制器、归档器和任何其他后处理数据的其他服务以它们自己的速度运行。索引器通常处理最新的消息，而归档器只会查看几天前发布的消息。
- en: Some of these components can fail without affecting user scenarios directly,
    while others impact how soon the data the user published shows up in other parts
    of the system and therefore can be critical.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些组件可能会失败而不会直接影响用户场景，而其他组件则影响用户发布的数据在其他系统部分显示的速度，因此可能是关键的。
- en: Let’s explore how we can instrument such applications.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索如何对这样的应用进行工具化。
- en: Before writing our own instrumentation from scratch, we should always check
    whether there are existing instrumentation libraries we can start with, and if
    there are none available, we should consult with OpenTelemetry semantic conventions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在从头开始编写自己的工具之前，我们应始终检查是否已有可从中开始的现有工具库，如果没有，则应咨询OpenTelemetry语义约定。
- en: We’re going to instrument Azure Queue Storage as an example. The existing instrumentation
    does not cover the messaging aspects of queues because of the reasons we’ll see
    in the next couple of sections. So, we’ll have to write our own; we’ll do it according
    to OpenTelemetry conventions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以Azure Queue Storage为例进行工具化。由于我们将在下一两个部分中看到的原因，现有的工具化没有涵盖队列的消息传递方面，因此我们不得不自己编写；我们将根据OpenTelemetry约定进行编写。
- en: Messaging semantic conventions
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 消息传递语义约定
- en: The messaging conventions for tracing are available at [https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/trace/semantic_conventions/messaging.md](https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/trace/semantic_conventions/messaging.md).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪的消息传递约定可在[https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/trace/semantic_conventions/messaging.md](https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/trace/semantic_conventions/messaging.md)找到。
- en: They currently have experimental status and are very likely to change. There
    are no general metrics conventions available yet, but you can find ones specific
    to Kafka.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 它们目前处于实验状态，并且很可能发生变化。目前还没有通用的度量约定，但你可以找到针对Kafka的特定度量。
- en: 'Conventions provide some considerations on context propagation (we’ll discuss
    this in the *Trace context propagation* section) and define generic attributes
    to describe messaging operations. Here are a few essential ones we’re going to
    use:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 约定提供了一些关于上下文传播的考虑（我们将在*跟踪上下文传播*部分讨论），并定义了通用属性来描述消息传递操作。以下是我们将要使用的一些基本属性：
- en: '`messaging.system`: Indicates that the span follows messaging semantics and
    describes the specific messaging system used, such as `kafka` or `rabbitmq`. In
    our sample, we’ll use `azqueues`.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`messaging.system`：表示跨度遵循消息传递语义，并描述了使用的特定消息传递系统，例如`kafka`或`rabbitmq`。在我们的示例中，我们将使用`azqueues`。'
- en: '`messaging.operation`: Identifies one of the standard operations: `publish`,
    `receive`, or `process`.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`messaging.operation`：标识标准操作之一：`publish`、`receive`或`process`。'
- en: '`messaging.destination.name` and `messaging.source.name`: Describe a queue
    or topic name within a broker. The term `destination` is used on the producer
    and `source` is used on the consumer.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`messaging.destination.name`和`messaging.source.name`：描述代理内部的一个队列或主题名称。术语`destination`用于生产者，而`source`用于消费者。'
- en: '`net.peer.name`: Identifies the broker domain name.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`net.peer.name`：标识代理域名。'
- en: Let’s see how we can use the conventions to add observability signals that can
    help us document the application behavior or detect and resolve a new class of
    issues happening in messaging scenarios.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用约定来添加可观察性信号，这些信号可以帮助我们记录应用程序行为或检测和解决在消息场景中发生的新类别问题。
- en: Instrumenting the producer
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 仪表化生成器
- en: 'The producer is the component responsible for publishing messages to a broker.
    The publishing process itself is usually synchronous: we send a request to the
    broker and get a response from it indicating whether the message was published
    successfully.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器是负责向代理发布消息的组件。发布过程本身通常是同步的：我们向代理发送请求，并从它那里获得响应，指示消息是否成功发布。
- en: Depending on the messaging system and producer needs, one publish request may
    carry one or more messages. We’ll discuss batching in the *Instrumenting batching
    scenarios* section. For now, let’s focus on a single message case.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 根据消息系统和生成器需求，一个发布请求可能携带一条或多条消息。我们将在“*仪表批处理场景*”部分讨论批处理。现在，让我们专注于单条消息的情况。
- en: To trace it, we need to make sure we create an activity when we publish a message,
    so we can track the call duration and status and debug individual requests. We’d
    also be interested in metrics for duration, throughput, and failure rate – it’s
    important to budget cloud messaging solutions or scale self-hosted brokers.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟踪它，我们需要确保在发布消息时创建一个活动，这样我们就可以跟踪调用持续时间、状态，并调试单个请求。我们也会对持续时间、吞吐量和失败率等指标感兴趣——这对于云消息解决方案的预算或自托管代理的扩展非常重要。
- en: Another essential part of producer instrumentation is context propagation. Let’s
    stop here for a second and discuss it.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器仪表的一部分是上下文传播。让我们在这里稍作停顿，讨论一下。
- en: Trace context propagation
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跟踪上下文传播
- en: When we instrument HTTP calls, context is propagated via HTTP request headers,
    which are part of the request. In messaging, the context is carried via a transport
    call to the broker and is not propagated to a consumer. Transport call trace context
    identifies the request, but not the message(s) it carries.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们仪表化HTTP调用时，上下文通过HTTP请求头传播，这是请求的一部分。在消息中，上下文通过一个传输调用到代理，并且不会传播到消费者。传输调用跟踪上下文标识请求，但不标识它携带的消息。
- en: 'So, we need to propagate context inside the message to make sure it goes all
    the way to the consumer. But which context should we inject? We have several options:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要在消息内部传播上下文，以确保它能够到达消费者。但我们应该注入哪种上下文？我们有几个选项：
- en: '**Use context from the current activity**: For instance, when we publish messages
    in the scope of an incoming HTTP request, we may use the context of the activity
    representing this HTTP server call. This works only if we send one message per
    incoming request. If we send more than one (each in an individual publish call),
    we’d not be able to tell which message the consumer call processed or identify
    whether we sent messages to the right queues.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用当前活动的上下文**：例如，当我们在一个传入HTTP请求的范围内发布消息时，我们可以使用代表这个HTTP服务器调用的活动的上下文。这只有在每个传入请求只发送一条消息的情况下才有效。如果我们发送多条（每条都在单独的发布调用中），我们就无法确定消费者调用处理了哪条消息，或者确定我们是否向正确的队列发送了消息。'
- en: '**Create an activity per message and inject its context**: Unique context allows
    us to trace messages individually and works in batching scenarios as well where
    we send multiple messages in one publish call. It also adds the overhead of creating
    an additional activity per message.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**为每条消息创建一个活动并注入其上下文**：独特的上下文允许我们单独跟踪消息，在发送多条消息在一个发布调用中的批处理场景中也有效。这也增加了为每条消息创建额外活动的开销。'
- en: '**Reuse the publish activity**: When we publish one message in one call to
    the broker, we can uniquely identify a message and publish call with one activity.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重用发布活动**：当我们向代理发送一条消息时，我们可以通过一个活动唯一地识别一条消息和一个发布调用。'
- en: The first option goes against OpenTelemetry messaging semantic conventions,
    which allow us to pick a suitable option from the last two. In our example, we’re
    using Azure Queue Storage, which does not support batching when publishing messages.
    So, we’re going to use the last option and create one activity to trace a publish
    call and inject its context into the message.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个选项违反了OpenTelemetry消息语义约定，这允许我们从最后两个选项中选择一个合适的选项。在我们的例子中，我们使用Azure Queue Storage，它不支持发布消息时的批处理。因此，我们将使用最后一个选项，创建一个活动来跟踪发布调用，并将它的上下文注入到消息中。
- en: Note
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When forking or routing messages from one queue to another, the message might
    have pre-existing trace context injected in the upstream service. The default
    behavior in such a case should be to keep the message context intact. To correlate
    all operations that happen with the message, we can always add a link to an existing
    trace context in the message when publishing or receiving it.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当从一个队列将消息分叉或路由到另一个队列时，消息可能在上游服务中已经注入了预存在的跟踪上下文。在这种情况下，默认行为应该是保持消息上下文完整。为了关联与消息发生的所有操作，我们可以在发布或接收消息时始终添加一个链接到现有的跟踪上下文。
- en: Another interesting aspect of Azure Queue Storage is that it doesn’t support
    message metadata – the message is an opaque payload without any prescribed structure
    or format that the service carries over. So, similarly to gRPC streaming, which
    we covered in [*Chapter 10*](B19423_10.xhtml#_idTextAnchor161), *Tracing Network
    Calls*, we’ll need to define our own message structure or use one of the well-known
    event formats available out there, such as **CloudEvents**.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Queue Storage的另一个有趣方面是它不支持消息元数据——消息是一个不透明的有效载荷，没有任何规定的结构或格式，服务会携带它。因此，类似于我们在[*第10章*](B19423_10.xhtml#_idTextAnchor161)中讨论的gRPC流，*跟踪网络调用*，我们需要定义自己的消息结构或使用可用的知名事件格式之一，例如**CloudEvents**。
- en: Note
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: CloudEvents ([https://cloudevents.io](https://cloudevents.io)) is an open standard
    that defines event structure in a vendor- and technology-agnostic way. It’s commonly
    used by cloud providers to notify applications about infrastructure changes or
    when implementing data change feeds. CloudEvents have distributed tracing extensions
    to carry W3C Trace Context as well as general-purpose metadata that can be used
    for other formats. OpenTelemetry also provides semantic conventions for CloudEvents.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: CloudEvents ([https://cloudevents.io](https://cloudevents.io)) 是一个开放标准，以供应商和技术无关的方式定义事件结构。它通常被云提供商用于通知应用程序有关基础设施更改或实现数据更改馈送时使用。CloudEvents具有分布式跟踪扩展，可以携带W3C跟踪上下文以及可用于其他格式的通用元数据。OpenTelemetry还提供了CloudEvents的语义约定。
- en: 'For demo purposes, we’ll keep things simple and define our own tiny message
    model in the following way:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示目的，我们将保持简单，并以下述方式定义我们自己的小型消息模型：
- en: producer/Message.cs
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: producer/Message.cs
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Message.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Message.cs)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Message.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Message.cs)'
- en: We’ll use the `Headers` property to propagate the trace context and will keep
    the payload in the `Text` property.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`Headers`属性来传播跟踪上下文，并将有效载荷保留在`Text`属性中。
- en: 'Similarly to the gRPC streaming examples we saw in [*Chapter 10*](B19423_10.xhtml#_idTextAnchor161),
    *Tracing Network Calls*, we can inject context into this message using the OpenTelemetry
    propagator with the following code snippet:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在[*第10章*](B19423_10.xhtml#_idTextAnchor161)中看到的gRPC流示例类似，*跟踪网络调用*，我们可以使用以下代码片段通过OpenTelemetry传播器将上下文注入此消息：
- en: producer/Controllers/SendController.cs
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: producer/Controllers/SendController.cs
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs)'
- en: Now we have all we need to instrument a publish call – let’s do it.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了所有需要的工具来对发布调用进行仪器化——让我们来做吧。
- en: Tracing a publish call
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跟踪发布调用
- en: We’ll need to create a new activity and put common messaging attributes on it
    to identify the broker, queue operation, and add other information. In the case
    of Azure Queue Storage, we can use the account name as the broker identifier (as
    they are unique within a public cloud).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要创建一个新的活动，并在其上放置常见的消息属性以标识代理、队列操作，并添加其他信息。在Azure Queue Storage的情况下，我们可以使用账户名称作为代理标识符（因为它们在公共云中是唯一的）。
- en: Then, we’ll inject context into the message and proceed with publishing. After
    the message is published successfully, we can also record the information returned
    by the broker, such as the message ID and other details we might consider useful.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将向消息中注入上下文并继续发布。消息成功发布后，我们还可以记录代理返回的信息，例如消息ID和其他可能认为有用的细节。
- en: 'Here’s the corresponding code:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这是相应的代码：
- en: producer/Controllers/SendController.cs
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: producer/Controllers/SendController.cs
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs)'
- en: Here, we injected the context of `Activity.Current` with the `Inject` method
    we implemented before. This may be useful if you want to turn off per-message
    activities. In such a case, per-message tracing will be limited, but consumer
    and producer calls will still be correlated. We also record metrics here – stay
    tuned for the details; we’re going to cover them in the next section.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用之前实现的 `Inject` 方法注入了 `Activity.Current` 的上下文。如果你想要关闭按消息的活动，这可能很有用。在这种情况下，按消息的跟踪将被限制，但消费者和生产者的调用仍然会关联。我们在这里也记录了指标——请保持关注细节；我们将在下一节中介绍它们。
- en: 'Here’s the `StartPublishActivity` method implementation:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是 `StartPublishActivity` 方法的实现：
- en: producer/Controllers/SendController.cs
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: producer/Controllers/SendController.cs
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs)'
- en: The activity here has a `producer` kind, which indicates the start of an async
    flow. The name follows OpenTelemetry semantic conventions, which recommend using
    the `{queue_name} {operation}` pattern. We can also cache it to avoid unnecessary
    string formatting.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的活动有一个 `producer` 类型，这表示异步流程的开始。名称遵循 OpenTelemetry 语义约定，建议使用 `{queue_name}
    {operation}` 模式。我们也可以将其缓存以避免不必要的字符串格式化。
- en: This is it; we’ve covered producer tracing – let’s look at metrics now.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样；我们已经涵盖了生产者跟踪——现在让我们看看指标。
- en: Producer metrics
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生产者指标
- en: Messaging-specific metrics come as an addition to resource utilization, .NET
    runtime, HTTP, and other metrics you might want to expose.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 特定于消息的指标作为资源利用率、.NET 运行时、HTTP 等其他你可能想要公开的指标的补充。
- en: To some extent, we can use HTTP metrics to monitor calls to Azure Queue Storage
    since they work on top of HTTP. This would allow us to monitor duration, success
    rate, and throughput for individual HTTP calls to storage, but won’t allow us
    to distinguish queues within one storage account.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在一定程度上，我们可以使用 HTTP 指标来监控对 Azure Queue Storage 的调用，因为它们基于 HTTP。这将允许我们监控对存储的个别
    HTTP 调用的持续时间、成功率以及吞吐量，但无法区分同一存储账户内的队列。
- en: So, if we rely on metrics, we should record some messaging-specific ones that
    cover common indicators such as publish call duration, throughput, and latency
    for each queue we use.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们依赖于指标，我们应该记录一些特定于消息的指标，这些指标涵盖了常见的指标，例如每个队列的发布调用持续时间、吞吐量和延迟。
- en: 'We can report all of them using a duration histogram, as we saw in [*Chapter
    7*](B19423_07.xhtml#_idTextAnchor115), *Adding Custom Metrics*. First, let’s initialize
    the duration histogram, as shown in the following code snippet:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用持续时间直方图来报告所有这些指标，就像我们在 [*第 7 章*](B19423_07.xhtml#_idTextAnchor115) 中看到的，*添加自定义指标*。首先，让我们初始化持续时间直方图，如下面的代码片段所示：
- en: producer/Controllers/SendController.cs
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: producer/Controllers/SendController.cs
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs)'
- en: '`Meter` and `Histogram` are static since we defined them in the controller.
    The controller lifetime is scoped to a request, so we keep them static to stay
    efficient.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`Meter` 和 `Histogram` 是静态的，因为我们是在控制器中定义它们的。控制器的生命周期是针对请求的，所以我们保持它们为静态以保持效率。'
- en: 'As we saw in the tracing example, every time we publish a message, we’re also
    going to record a publish duration. Here’s how it’s implemented:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在跟踪示例中看到的，每次我们发布消息时，我们也会记录发布持续时间。以下是它的实现方式：
- en: producer/Controllers/SendController.cs
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: producer/Controllers/SendController.cs
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs)'
- en: Here, we used the same attributes to describe the queue and added a custom status
    attribute. Keep in mind that we need it to have low cardinality, so we only use
    `ok` and `fail` statuses when we call this method.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了相同的属性来描述队列，并添加了一个自定义状态属性。请注意，我们需要它具有低基数，所以我们只在使用此方法时使用`ok`和`fail`状态。
- en: We’re done with the producer. Having basic tracing and metrics should give us
    a good starting point to diagnose and debug most of the issues and monitor overall
    producer health, as we’ll see in the *Performance analysis in messaging scenarios*
    section later. Let’s now explore instrumentation on consumers.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了生产者的监控。拥有基本的跟踪和指标应该为我们提供一个良好的起点，以诊断和调试大多数问题，并监控整体生产者健康，正如我们将在后面的*消息场景中的性能分析*部分中看到的。现在，让我们探索消费者上的监控。
- en: Instrumenting the consumer
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控消费者
- en: While you might be able to get away without custom instrumentation on the producer,
    consumer instrumentation is unavoidable.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你可能可以在生产者上不使用自定义监控的情况下逃脱，但消费者监控是不可避免的。
- en: Some brokers push messages to consumers using synchronous HTTP or RPC calls,
    and the existing framework instrumentation can provide the bare minimum of observability
    data. In all other cases, messaging traces and metrics are all we have to detect
    consumer health and debug issues.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一些代理使用同步HTTP或RPC调用将消息推送到消费者，现有的框架监控可以提供最基本的可观察性数据。在所有其他情况下，消息跟踪和指标是我们检测消费者健康和调试问题的全部。
- en: Let’s start by tracing individual messages – recording when they arrive in the
    consumer and how they are processed. This allows us to debug issues by answering
    questions such as “Where is this message now?” or “Why did it take so long to
    process the data?”
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们追踪单个消息——记录它们在消费者中的到达时间和处理方式。这使我们能够通过回答诸如“这条消息现在在哪里？”或“为什么处理数据花了这么长时间？”等问题来调试问题。
- en: Tracing consumer operations
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跟踪消费者操作
- en: When using Azure Queue Storage, applications request one or more messages from
    the queue. Received messages stay in the queue but become invisible to other consumers
    for configurable visibility timeout. The application processes messages and, when
    done, deletes them from the queue. If processing fails with a transient issue,
    applications don’t delete messages. The same flow is commonly used when working
    with AWS SQS.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 Azure Queue Storage 时，应用程序从队列请求一条或多条消息。接收到的消息仍然留在队列中，但对其他消费者来说是不可见的，这是可配置的可见性超时。应用程序处理消息，完成后，从队列中删除它们。如果处理因暂时性问题失败，应用程序不会删除消息。当与
    AWS SQS 一起工作时，通常使用相同的流程。
- en: RabbitMQ- and AMQP-based messaging flows look similar, except messages can be
    pushed to the consumer so that the application reacts to the client library callback
    instead of polling the queue.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 RabbitMQ 和 AMQP 的消息流看起来很相似，除了消息可以被推送到消费者，这样应用程序就可以对客户端库回调做出反应，而不是轮询队列。
- en: Callback-based delivery allows us to implement instrumentation in client libraries
    or provide a shared instrumentation library, and with a poll-based model, we essentially
    are forced to write at least some custom instrumentation for processing. Let’s
    do it.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 基于回调的交付允许我们在客户端库中实现监控，或者提供一个共享的监控库，而在基于轮询的模型中，我们实际上被迫至少编写一些自定义监控来处理。让我们这样做。
- en: 'First, let’s instrument message processing in isolation from receiving. We’ll
    need to create an activity to track processing that will capture everything that
    happens there, including message deletion:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们在接收之外单独对消息处理进行监控。我们需要创建一个活动来跟踪处理，这将捕获那里发生的所有事情，包括消息删除：
- en: consumer/SingleReceiver.cs
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: consumer/SingleReceiver.cs
- en: '[PRE6]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs)'
- en: Here, all the processing logic happens in the `ProcessMessage` method. When
    it completes successfully, we delete the message from the queue. Otherwise, we
    update its visibility to reappear in the queue after the backoff timeout.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，所有处理逻辑都在 `ProcessMessage` 方法中完成。当它成功完成时，我们从队列中删除消息。否则，我们更新其可见性，以便在回退超时后再次出现在队列中。
- en: 'Here’s the `StartProcessActivity` implementation:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是 `StartProcessActivity` 的实现：
- en: consumer/SingleReceiver.cs
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: consumer/SingleReceiver.cs
- en: '[PRE7]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs
    )'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs)'
- en: Here, we extracted the context from the message and used it as a parent of the
    processing activity. It has the `consumer` kind, which indicates the continuation
    of the asynchronous flow. We also kept `Activity.Current` as a link to preserve
    correlation. We also added messaging attributes.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们从消息中提取了上下文，并将其用作处理活动的父级。它具有 `consumer` 类型，这表示异步流的延续。我们还保留了 `Activity.Current`
    作为关联的链接。我们还添加了消息属性。
- en: 'Message deletion and updates are traces by HTTP or Azure Queue SDK instrumentations.
    They don’t have messaging semantics, but should give us reasonable observability.
    Corresponding activities become children of the processing one, as shown in *Figure
    11**.2*:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 消息删除和更新是通过 HTTP 或 Azure 队列 SDK 仪器跟踪的。它们没有消息语义，但应该提供合理的可观察性。相应活动成为处理活动的子活动，如图
    *11.2* 所示：
- en: '![Figure 11.2 – Message trace from producer to consumer](img/B19423_11_02.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.2 – 从生产者到消费者的消息跟踪](img/B19423_11_02.jpg)'
- en: Figure 11.2 – Message trace from producer to consumer
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 – 从生产者到消费者的消息跟踪
- en: 'The message was published and then we see two attempts to process it on the
    consumer: the first attempt failed. The second try was successful, and the message
    was deleted.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 消息被发布后，我们看到消费者尝试处理它两次：第一次尝试失败。第二次尝试成功，消息被删除。
- en: 'What’s missing in the preceding screenshot? We don’t see how and when the message
    was received. This might not be important on this trace, but look at another one
    in *Figure 11**.3*:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 上一张截图缺少了什么？我们没有看到消息是如何和何时被接收的。这可能在这个跟踪中并不重要，但看看 *11.3* 中的另一个跟踪：
- en: '![Figure 11.3 – Message trace with a nine-minute gap between producer and consumer](img/B19423_11_03.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.3 – 生产者和消费者之间有九分钟间隔的消息跟踪](img/B19423_11_03.jpg)'
- en: Figure 11.3 – Message trace with a nine-minute gap between producer and consumer
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3 – 生产者和消费者之间有九分钟间隔的消息跟踪
- en: Here, nothing has happened for almost nine minutes. Was the message received
    by a consumer during that time? Were the consumers alive? What were they doing?
    Were there any problems in the Azure Queue service that prevented messages from
    being received?
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，几乎九分钟内没有发生任何事情。在这段时间内，消费者是否收到了消息？消费者是否存活？他们在做什么？Azure 队列服务中是否有任何问题阻止了消息的接收？
- en: We’ll see how to answer these questions later. Now, let’s focus on tracing the
    receive operation.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在稍后回答这些问题。现在，让我们专注于跟踪接收操作。
- en: The challenge with the receive operation is that the message trace context is
    available after the message is received and the corresponding operation is about
    to end. We could add links to message trace contexts then, but it’s currently
    only possible to add them at activity start time.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 接收操作中的挑战在于，在收到消息并即将结束相应操作之后，消息跟踪上下文才可用。那时我们可以添加链接到消息跟踪上下文，但目前只能在活动开始时添加它们。
- en: 'This is likely to change, but for now, we’ll work around it by tracing the
    receive-and-process iteration and adding an attribute with the received message
    ID so we can find all spans that touched this message:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能会改变，但到目前为止，我们将通过跟踪接收和处理迭代，并添加一个带有接收消息 ID 的属性来解决这个问题，这样我们就可以找到所有接触过这条消息的跨度：
- en: consumer/SingleReceiver.cs
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: consumer/SingleReceiver.cs
- en: '[PRE8]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs)'
- en: Here, we receive at most one message from the queue. If a message was received,
    we process it.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们从队列中接收最多一条消息。如果收到了消息，我们就处理它。
- en: 'One iteration is tracked with the `ReceiveAndProcess` activity, which becomes
    a parent to the receiving operation. The message processing activity is created
    in the `ProcessAndSettle` method and links to the `ReceiveAndProcess` activity,
    as shown in *Figure 11**.4*:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 一个迭代通过 `ReceiveAndProcess` 活动进行跟踪，该活动成为接收操作的父级。消息处理活动在 `ProcessAndSettle` 方法中创建，并链接到
    `ReceiveAndProcess` 活动如图 11.4 所示：
- en: '![Figure 11.4 – Link from processing to outer loop activity](img/B19423_11_04.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.4 – 从处理到外部循环活动的链接](img/B19423_11_04.jpg)'
- en: Figure 11.4 – Link from processing to outer loop activity
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4 – 从处理到外部循环活动的链接
- en: 'If we follow the link, we’ll see an outer loop trace like the one shown in
    *Figure 11**.5*:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们跟随链接，我们将看到类似于 *图 11.5* 中所示的循环跟踪：
- en: '![Figure 11.5 – Trace representing the receive and process iteration](img/B19423_11_05.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.5 – 表示接收和处理迭代的跟踪图](img/B19423_11_05.jpg)'
- en: Figure 11.5 – Trace representing the receive and process iteration
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5 – 表示接收和处理迭代的跟踪图
- en: Since more and more observability backends are providing better support for
    links, it can be more convenient to use them in your backend.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 由于越来越多的可观察性后端提供了更好的链接支持，因此在后端使用它们可能更加方便。
- en: With iteration instrumented, we can now correlate receiving and processing or
    see how long a full loop cycle takes. This can help us understand whether consumers
    are alive and trying to receive and process something.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 通过迭代进行仪表化，我们现在可以关联接收和处理，或者看到完整循环周期需要多长时间。这有助于我们了解消费者是否活跃并尝试接收和处理某些内容。
- en: We’re stamping the `messaging.message.id` attribute on all spans to simplify
    finding all operations related to any given message.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在所有跨度上标记了 `messaging.message.id` 属性，以简化查找与任何给定消息相关的所有操作。
- en: Now, back to the nine-minute gap we saw in *Figure 11**.3*. What happened there
    is that we got too many messages in the queue – they were produced faster than
    we consumed them. By looking at gaps in individual traces, we can suspect that
    message spent time in the queue, but can’t tell for sure. What we need is to see
    the rate at which messages are published, processed, and deleted. We should also
    understand how long messages spend in the queue and how big the queue is. Let’s
    see how we can record and use such metrics.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，回到我们在 *图 11.3* 中看到的九分钟间隔。那里发生的事情是队列中的消息太多——它们的生产速度超过了我们的消费速度。通过查看单个跟踪中的间隔，我们可以怀疑消息在队列中花费了时间，但无法确定。我们需要看到消息发布的速率、处理的速率和删除的速率。我们还应该了解消息在队列中花费的时间和队列的大小。让我们看看我们如何记录和使用这些指标。
- en: Consumer metrics
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 消费者指标
- en: Similar to producers, we should enable common runtime and process metrics so
    we know the resource utilization for consumer processes. We should also record
    the processing loop duration, which will give us the error rate and throughput.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 与生产者类似，我们应该启用常见的运行时和进程指标，以便我们知道消费者进程的资源利用率。我们还应该记录处理循环的持续时间，这将给我们提供错误率和吞吐量。
- en: 'From a messaging perspective, we’d also want to cover the following things:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 从消息传递的角度来看，我们还想涵盖以下内容：
- en: The amount of time messages spend in the queue is a great indicator of consumer
    health and scale. When there are not enough consumers, the amount of time spent
    in the queue will grow and can be used to scale consumers up. When it decreases
    consistently, it could serve as a signal to scale consumers down.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消息在队列中花费的时间是消费者健康状况和规模的良好指标。当消费者不足时，队列中花费的时间会增长，可以用来扩展消费者。当它持续下降时，这可以作为一个信号来缩小消费者规模。
- en: The number of messages in the queue provides similar data, but in real time.
    It includes messages that have not yet been processed. Queue size metric can also
    be recorded on the producer side without ever depending on the consumer.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 队列中的消息数量提供了类似的数据，但实时。它包括尚未处理的消息。队列大小指标也可以在生产者端记录，而无需依赖消费者。
- en: These metrics, or similar ones you can come up with, and their trends over time
    provide a great indication of consumer health.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标，或者你可以想到的类似指标及其随时间的变化趋势，为消费者健康状况提供了很好的指示。
- en: These metrics increase if consumer performance degrades or the error rate increases.
    They won’t be helpful if consumers fail to process messages but immediately delete
    them from the queue, but this will manifest in high error rate. So, let’s go ahead
    and instrument our application with these metrics.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果消费者性能下降或错误率增加，这些指标会增加。如果消费者未能处理消息但立即将其从队列中删除，则它们将没有帮助，但这将表现为高错误率。所以，让我们继续用这些指标来仪表化我们的应用程序。
- en: Duration, throughput, and failure rate
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 持续时间、吞吐量和故障率
- en: We’re going to measure the processing loop duration, which includes trying to
    receive a message and its processing. Measuring the receiving and processing duration
    independently would be even more precise and is something to consider in your
    production applications.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要测量处理循环的持续时间，这包括尝试接收消息及其处理。独立测量接收和处理持续时间将更加精确，这也是你在生产应用中需要考虑的事情。
- en: 'At the beginning of the loop, we’ll start a stopwatch to measure operation
    duration, and once processing completes, we’ll report it as a histogram along
    with queue information and the status. Let’s first create the histogram instrument:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在循环开始时，我们将启动一个计时器来测量操作持续时间，一旦处理完成，我们将将其作为直方图与队列信息和状态一起报告。让我们首先创建直方图仪表：
- en: consumer/SingleReceiver.cs
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: consumer/SingleReceiver.cs
- en: '[PRE9]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs)'
- en: We create meter and duration instruments here as instance variables, which we
    dispose of along with the `SingleReceiver` instance. The receiver extends the
    `BackgroundService` interface and is registered in the dependency injection container
    as a singleton, so they are all disposed of once the application shuts down.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将创建计米和持续时间仪表作为实例变量，我们将与`SingleReceiver`实例一起丢弃它们。接收器扩展了`BackgroundService`接口，并在依赖注入容器中注册为单例，因此一旦应用程序关闭，它们都会被丢弃。
- en: 'The processing loop instrumentation can be done in the following way:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 处理循环的仪表化可以按以下方式进行：
- en: consumer/SingleReceiver.cs
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: consumer/SingleReceiver.cs
- en: '[PRE10]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs)'
- en: 'Here, we record the duration of each iteration along with the queue information
    and status. The status can have the following values: `ok`, `fail`, or `empty`
    (if no messages were received). In real applications, you probably want to be
    more precise and add a few more statuses to indicate the failure reason. For example,
    it would be important to record why the receive operation failed, whether there
    was a serialization or validation error, processing timed out, or it failed with
    a terminal or transient error.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们记录了每次迭代的持续时间以及队列信息和状态。状态可以有以下值：`ok`、`fail`或`empty`（如果没有收到消息）。在实际应用中，你可能希望更加精确，并添加更多状态以指示失败原因。例如，记录接收操作失败的原因很重要，无论是序列化或验证错误、处理超时，还是以终端或暂时性错误失败。
- en: 'The `RecordLoopDuration` method implementation is shown in this snippet:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`RecordLoopDuration`方法实现如下所示：'
- en: consumer/SingleReceiver.cs
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: consumer/SingleReceiver.cs
- en: '[PRE11]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs)'
- en: We’ll see how we can use this metric later in this chapter. Let’s first implement
    consumer lag and queue size.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面看到如何使用这个指标。首先，让我们实现消费者延迟和队列大小。
- en: Consumer lag
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 消费者延迟
- en: In the code sample showing metrics in the processing loop, we called into the
    `RecordLag` method as soon as we received a message. Consumer lag records the
    approximate time a message spent in the queue – the delta between the receive
    and enqueue time.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在显示处理循环中指标的代码示例中，我们一收到消息就调用了`RecordLag`方法。消费者延迟记录了消息在队列中花费的大致时间——接收和入队时间之间的差值。
- en: 'The enqueue time is recorded by the Azure Queue service and is exposed as a
    property on the `QueueMessage` instance. We can record the metric with the following
    code:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 入队时间由Azure队列服务记录，并作为`QueueMessage`实例上的属性公开。我们可以使用以下代码记录指标：
- en: consumer/SingleReceiver.cs
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: consumer/SingleReceiver.cs
- en: '[PRE12]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs)'
- en: Here, we create a histogram that represents the lag (in seconds) and record
    it for every received message as the difference between the current time and the
    time at which the message was received by the broker.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建一个表示延迟（以秒为单位）的直方图，并将它记录在每条接收到的消息上，作为当前时间和消息被代理接收的时间之间的差异。
- en: Note that these timestamps usually come from two different computers – the difference
    can be negative and is not precise due to clock skew. The margin of error can
    reach seconds but may, to some extent, be corrected within your system.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这些时间戳通常来自两台不同的计算机——差异可能是负数，并且由于时钟偏差而不精确。误差范围可能达到秒，但在某种程度上，可以在您的系统中进行纠正。
- en: Clock skew should be expected, but sometimes things can go really wrong. I once
    was involved in investigating an incident that took our service down in one of
    the data centers. It happened because of the wrong time server configuration,
    which moved the clock on one of the services back a few hours. It broke authentication
    – the authentication tokens had timestamps from hours ago and were considered
    expired.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 时钟偏差是可以预期的，但有时事情可能会真的出错。我曾经参与调查一起事件，导致我们的服务在数据中心中断。这是由于错误的时间服务器配置，将其中一个服务的时间表向后移动了几小时。它破坏了认证——认证令牌的时间戳来自几小时前，被认为是过期的。
- en: Despite being imprecise, consumer lag should give us an idea of how long messages
    spend in a queue. We record it every time a message is received, so it also reflects
    redeliveries. Also, we record it before we know whether processing was successful,
    so it does not have any status.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管不够精确，消费者延迟应该能让我们了解消息在队列中花费的时间。我们每次收到消息时都会记录它，因此它也反映了重新投递。此外，我们在知道处理是否成功之前就记录它，因此它没有任何状态。
- en: Before we record lag on the consumer, we first need to receive a message. When
    we see a huge lag, it’s a good signal that something is not right, but it does
    not tell us how many messages have not yet been received.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们记录消费者延迟之前，我们首先需要接收一条消息。当我们看到巨大的延迟时，这是一个很好的信号，表明某些事情可能不正常，但它并没有告诉我们还有多少消息尚未接收。
- en: For example, when the load is low and the queue is empty, there might be a few
    invalid messages that are stuck there. It’s a bug, but it can be fixed during
    business hours. To detect how big the issue is, we also need to know how many
    messages are in the queue. Let’s see how to implement it.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当负载低且队列空时，可能会有一些无效的消息卡在那里。这是一个错误，但可以在工作时间内修复。为了检测问题的严重性，我们还需要知道队列中有多少消息。让我们看看如何实现它。
- en: Queue size
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 队列大小
- en: Azure Queue Storage as well as Amazon SQS allow us to retrieve an approximate
    count of messages. We can register another `BackgroundService` implementation
    to retrieve the count periodically. This can be done on the consumer or producer.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 队列存储以及 Amazon SQS 允许我们检索消息的大致数量。我们可以注册另一个 `BackgroundService` 实现来定期检索计数。这可以在消费者或生产者上完成。
- en: 'We’ll use a gauge instrument to report it, as shown in this code snippet:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个量度仪表来报告它，如下面的代码片段所示：
- en: consumer/QueueSizeReporter.cs
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: consumer/QueueSizeReporter.cs
- en: '[PRE13]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/QueueSizeReporter.cs
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/QueueSizeReporter.cs
- en: 'We passed a callback that returns a `_currentQueueSize` instance variable.
    We’re going to update it every several seconds as we retrieve the size from the
    queue:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们传递了一个返回 `_currentQueueSize` 实例变量的回调。我们将每隔几秒钟从队列中检索大小来更新它：
- en: consumer/QueueSizeReporter.cs
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: consumer/QueueSizeReporter.cs
- en: '[PRE14]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/QueueSizeReporter.cs
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/QueueSizeReporter.cs
- en: That’s it – now we measure the queue size. This number alone does not tell the
    entire story, but if it’s significantly different from the baseline or grows fast,
    this is a great indication of a problem.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样——现在我们测量队列大小。这个数字本身并不能告诉我们整个故事，但如果它与基线有显著差异或增长迅速，这将是问题的一个很好的迹象。
- en: Once the load grows, the queue size will also go up and we may try to add more
    consumers or optimize them. One of the typical optimizations is batching – it
    helps reduce the number of network calls and would utilize consumer instances
    better. Let’s see how we can instrument it.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦负载增加，队列大小也会增加，我们可能会尝试添加更多消费者或优化它们。一种典型的优化是批处理——它有助于减少网络调用的数量，并更好地利用消费者实例。让我们看看我们如何对其进行仪器化。
- en: Instrumenting batching scenarios
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批处理场景的仪器化
- en: Instrumentation for batching scenarios can be different depending on the use
    case – transport-level batching needs a slightly different approach compared to
    batch processing.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 根据用例的不同，批处理场景的仪器化可能不同——与批处理相比，传输级别的批处理需要稍微不同的方法。
- en: Batching on a transport level
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 传输级别的批处理
- en: Messages can be batched together to minimize the number of network calls. It
    can be used by producers or consumers, and systems such as Kafka, Amazon SQS,
    or Azure Service Bus support batching on both sides.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 消息可以一起批处理以最小化网络调用的数量。它可以由生产者或消费者使用，例如Kafka、Amazon SQS或Azure Service Bus等系统都支持两端的批处理。
- en: On the consumer, when multiple messages are received together but processed
    independently, everything we had for single message processing still applies.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在消费者端，当一起接收多条消息但独立处理时，对于单条消息处理的所有内容仍然适用。
- en: From a tracing perspective, the only thing we’d want to change is to add attributes
    that record all received message identifiers and batch size on the outer iteration
    activity.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 从跟踪的角度来看，我们唯一想要改变的是在外部迭代活动上添加记录所有接收到的消息标识符和批次大小的属性。
- en: From the metrics side, we’d also want to measure individual message processing
    duration, error rate, and throughput. We can track them all by adding a message
    processing duration histogram.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 从指标方面来看，我们还想测量单个消息处理持续时间、错误率和吞吐量。我们可以通过添加消息处理持续时间直方图来跟踪它们。
- en: When we send multiple messages in a batch, we still need to trace these messages
    independently . To do so, we’d have to create an activity per message and inject
    unique trace context into each message. The publish activity then should be linked
    to all the messages being published.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在一批中发送多条消息时，我们仍然需要独立跟踪这些消息。为了做到这一点，我们必须为每条消息创建一个活动并将唯一的跟踪上下文注入到每条消息中。然后，发布活动应该链接到所有正在发布的消息。
- en: The main question here is when to create a per-message activity and inject context
    into the message. Essentially, the message trace context should continue the operation
    that created the message. So, if we buffer messages from different unrelated operations
    and then send them in a background thread, we should create message activities
    when the message is created. Then, a batch publish operation will link to independent,
    unrelated trace contexts.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的主要问题是何时创建每条消息的活动并向消息中注入上下文。本质上，消息跟踪上下文应该继续创建消息的操作。因此，如果我们从不同的无关操作中缓冲消息，然后在后台线程中发送它们，我们应该在消息创建时创建消息活动。然后，批量发布操作将链接到独立的、无关的跟踪上下文。
- en: The duration metric for the publish operation remains the same as for the single-message
    case we implemented before, but we should consider adding another metric to describe
    the batch size and the exact number of sent messages – we won’t be able to figure
    it out from the publish duration.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 发布操作的持续时间指标与之前我们实现的单条消息案例相同，但我们应该考虑添加另一个指标来描述批次大小和发送的确切消息数量——我们无法从发布持续时间中得出这些信息。
- en: Processing batches
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理批次
- en: In some cases, we process messages in batches, for example, when aggregating
    data for analytic purposes, replicating or archiving received data. In such cases,
    it’s just not possible to separate individual messages. Things get even more complicated
    in scenarios such as routing or sharding, when a received batch is split into
    several new batches and sent to the next destination.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们以批处理的方式处理消息，例如，为了分析目的聚合数据、复制或存档接收到的数据。在这种情况下，根本无法分离单个消息。在路由或分片等场景中，接收到的批次被分成几个新的批次并发送到下一个目的地时，事情变得更加复杂。
- en: We can record relationships using links – this will allow us to tell whether
    (when and how many times) a message was received, and which processing operation
    it contributed to.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用链接来记录关系——这将使我们能够了解消息何时以及多少次被接收，以及它对哪个处理操作做出了贡献。
- en: Essentially, we create a batch-processing activity with links to all messages
    being processed. Links have attributes and there we can put important message
    metadata, such as the delivery count, message ID, or insertion time.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们创建了一个批量处理活动，其中包含所有正在处理的消息的链接。链接有属性，我们可以在那里放置重要的消息元数据，例如投递次数、消息ID或插入时间。
- en: From a metrics perspective, consumer lag (measured per message), queue size,
    and processing duration (throughput and failure rate) still apply. We might also
    want to report the batch size as a histogram.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 从指标的角度来看，消费者延迟（按消息衡量）、队列大小和处理持续时间（吞吐量和失败率）仍然适用。我们可能还想将批处理大小作为直方图报告。
- en: Note
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Message and batch processing are frequently done outside of messaging client
    library control, by application code or integration frameworks. It’s rarely possible
    for auto-instrumentation to trace or measure processing calls. These scenarios
    vary a lot from application to application, requiring custom instrumentations
    tuned to specific use cases and messaging systems.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 消息和批量处理通常在消息客户端库控制之外进行，由应用程序代码或集成框架完成。自动仪表化很难跟踪或测量处理调用。这些场景因应用程序而异，需要针对特定用例和消息系统进行定制仪表化。
- en: Now that we have an idea of how to instrument messaging scenarios, let’s see
    how we can use it in practice.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何对消息场景进行仪表化，让我们看看我们如何在实践中使用它。
- en: Performance analysis in messaging scenarios
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 消息场景的性能分析
- en: 'We’re going to use our demo application to simulate a few common problems and
    use signals we have to detect and debug issues. Let’s start the application with
    the following command:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用我们的演示应用程序来模拟一些常见问题，并使用我们拥有的信号来检测和调试问题。让我们使用以下命令启动应用程序：
- en: '[PRE15]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: It will run one producer and three consumers along with the observability stack.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 它将运行一个生产者和三个消费者，以及可观察性堆栈。
- en: You can now send a request to the producer at `http://localhost:5051/send`,
    which sends one message to the queue and returns receipt information as a response.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以向生产者发送请求到`http://localhost:5051/send`，该请求向队列发送一条消息，并作为响应返回收据信息。
- en: 'Now you need to add some load with the tool of your choice. If you use `bombardier`,
    you can do it with the following command:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您需要使用您选择的工具添加一些负载。如果您使用`bombardier`，可以使用以下命令：
- en: '[PRE16]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: It sends requests to the producer in one connection. You can play with a different
    number of connections and consumers in the `docker-compose` command to see how
    the metrics change.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 它在一个连接中向生产者发送请求。您可以在`docker-compose`命令中尝试不同的连接数和消费者数，以查看指标如何变化。
- en: You might also want to install Grafana and import the dashboard from the book’s
    repository (https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/grafana-dashboard.json)
    to look at all metrics at once.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还希望安装Grafana，并从本书的存储库（https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/grafana-dashboard.json）导入仪表板，以一次性查看所有指标。
- en: 'How do we check whether the consumers are working properly? We can start with
    consumer lag and queue size metrics. *Figure 11**.6* shows the 95th percentile
    for consumer lag obtained with the following query:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何检查消费者是否正常工作？我们可以从消费者延迟和队列大小指标开始。*图11.6*显示了以下查询获得的消费者延迟的第95百分位数：
- en: '[PRE17]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Figure 11.6 – Consumer lag grows over time](img/B19423_11_06.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图11.6 – 消费者延迟随时间增长](img/B19423_11_06.jpg)'
- en: Figure 11.6 – Consumer lag grows over time
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6 – 消费者延迟随时间增长
- en: 'Consumer lag grows almost to 600 seconds, and if we look at the queue size,
    as shown in *Figure 11**.7*, we’ll see there were up to about 11,000 messages
    in the queue:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者延迟几乎增长到600秒，如果我们查看队列大小，如图*图11.7*所示，我们将看到队列中最多有大约11,000条消息：
- en: '![Figure 11.7 – Queue size grows and then slowly goes down](img/B19423_11_07.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图11.7 – 队列大小先增长然后缓慢下降](img/B19423_11_07.jpg)'
- en: Figure 11.7 – Queue size grows and then slowly goes down
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.7 – 队列大小先增长然后缓慢下降
- en: 'Here’s the query for the queue size:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这是队列大小的查询：
- en: '[PRE18]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Consumer lag stays high for a long time until all messages are processed at
    around 19:32, but we can judge by the queue size that things started to improve
    at 19:27.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者延迟在长时间内保持较高水平，直到大约19:32所有消息都处理完毕，但我们可以通过队列大小判断，事情开始改善是在19:27。
- en: The trend changed and the queue quickly shrunk because I stopped the application
    and restarted it with 15 consumers.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我停止了应用程序并重新启动了它，使用15个消费者，趋势发生了变化，队列迅速缩小。
- en: But now we have too many consumers and are wasting resources. We can check the
    average batch size we retrieve – if it’s consistently and noticeably lower than
    the configured batch size, we may slowly start decreasing the number of consumers,
    leaving some buffer for bursts of load.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在我们有太多的消费者，正在浪费资源。我们可以检查我们检索的平均批次大小——如果它持续并且明显低于配置的批次大小，我们可能可以逐渐开始减少消费者的数量，为负载高峰留出一些缓冲。
- en: Now, let’s stop the load and add some errors. Send a malformed message with
    `http://localhost:5051/send?malformed=true`. We should see that queue size remains
    small, but consumer lag grows over time.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们停止负载并添加一些错误。发送带有 `http://localhost:5051/send?malformed=true` 的格式不正确的消息。我们应该看到队列大小保持较小，但随着时间的推移，消费者延迟增长。
- en: We can also see that despite no messages being sent, we’re receiving messages,
    processing them, and failing repeatedly.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看到，尽管没有发送消息，我们却在接收消息，处理它们，并且反复失败。
- en: 'For example, we can visualize it with the following query:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以使用以下查询来可视化它：
- en: '[PRE19]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'It shows the rate of process-and-receive iterations grouped by queue name and
    status. This is shown in *Figure 11**.8*:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 它显示了按队列名称和状态分组的处理和接收迭代速率。这显示在 *图 11**.8* 中：
- en: '![Figure 11.8 – Processing rate grouped by status](img/B19423_11_08.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.8 – 按状态分组的处理速率](img/B19423_11_08.jpg)'
- en: Figure 11.8 – Processing rate grouped by status
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8 – 按状态分组的处理速率
- en: We can see here that from around 20:57, we attempt to receive messages about
    four times per second. Three of these calls don’t return any messages, and in
    the other case, processing fails. There are no successful iterations.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，大约在 20:57 左右，我们每秒尝试接收四次消息。其中三次调用没有返回任何消息，在另一种情况下，处理失败。没有成功的迭代。
- en: We sent a few malformed messages, and it seems they are being processed forever
    – this is a bug. If there were more than a few such messages, they would keep
    consumers busy and not let them process any valid messages.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发送了几条格式不正确的消息，它们似乎被永久处理了——这是一个错误。如果有更多这样的消息，它们会占用消费者，使他们无法处理任何有效消息。
- en: 'To confirm this suggestion, let’s look at the traces. Let’s open Jaeger at
    `http://localhost:16686` and filter traces with errors that come from consumers.
    One such trace is shown in *Figure 11**.9*:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认这个建议，让我们看看痕迹。让我们打开 Jaeger 在 `http://localhost:16686` 并过滤来自消费者的错误痕迹。其中一个这样的痕迹显示在
    *图 11**.9* 中：
- en: '![Figure 11.9 – Failed receive-and-process iteration](img/B19423_11_09.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.9 – 失败的接收和处理迭代](img/B19423_11_09.jpg)'
- en: Figure 11.9 – Failed receive-and-process iteration
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.9 – 失败的接收和处理迭代
- en: 'Here, we see that four messages were received, and the iteration failed with
    an error. If we could add links to this operation, we would be able to navigate
    to traces for each individual message. Instead, we have message ID stamped. Let’s
    find the trace for one of these messages using the corresponding attribute. The
    result is shown in *Figure 11**.10*:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到收到了四条消息，迭代失败并出现错误。如果我们能为此操作添加链接，我们就能导航到每个单独消息的痕迹。相反，我们只有消息 ID。让我们使用相应的属性找到这些消息中的一个的痕迹。结果显示在
    *图 11**.10* 中：
- en: '![Figure 11.10 – Trace for one of the failed messages](img/B19423_11_10.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.10 – 失败消息的痕迹](img/B19423_11_10.jpg)'
- en: Figure 11.10 – Trace for one of the failed messages
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.10 – 失败消息的痕迹
- en: This does not look great – we have 3,000 spans for just one message. If we open
    the trace and check out the `messaging.azqueues.message.dequeue_count` attribute
    for the latest processing spans, we’ll see the message was received more than
    1,000 times.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来不太好——我们只有一个消息就有 3,000 个跨度。如果我们打开痕迹并检查最新处理跨度中的 `messaging.azqueues.message.dequeue_count`
    属性，我们会看到消息被接收了超过 1,000 次。
- en: To fix the issue, we should delete messages that fail validation. We also make
    sure we do so for any other terminal error and introduce a limit to the number
    of times a message is dequeued, after which the message is deleted.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们应该删除验证失败的消息。我们还确保对任何其他终端错误也这样做，并引入对消息出队次数的限制，超过这个次数后，消息将被删除。
- en: We just saw a couple of problems that frequently arise in messaging scenarios
    (but usually in less obvious ways) and used instrumentation to detect and debug
    them. As observability vendors improve user experience for links, it will become
    even easier to do such investigations. But we already have all the means to record
    telemetry and correlate it in messaging flows.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到了在消息场景中经常出现的一些问题（但通常以不那么明显的方式），并使用配置来检测和调试它们。随着可观察性供应商改进链接的用户体验，这样的调查将变得更加容易。但我们已经拥有了记录遥测数据并在消息流中关联它们的全部手段。
- en: Summary
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored messaging instrumentation. We started with messaging
    specifics and the new challenges they bring to observability. We briefly looked
    into OpenTelemetry messaging semantic conventions and then dived into producer
    instrumentation. The producer is responsible for injecting trace context into
    messages and instrumenting publish operations so that it’s possible to trace each
    independent flow on consumers.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了消息配置。我们从消息的具体内容和它们给可观察性带来的新挑战开始。我们简要地了解了OpenTelemetry消息语义约定，然后深入到生产者配置。生产者负责将跟踪上下文注入消息并配置发布操作，以便能够跟踪消费者上的每个独立流程。
- en: Then, we instrumented the consumer with metrics and traces. We learned how to
    measure consumer health using queue size and lag and explored the instrumentation
    options for batching scenarios. Finally, we saw how we can use instrumentation
    to detect and investigate common messaging issues.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用指标和跟踪对消费者进行了配置。我们学习了如何使用队列大小和延迟来衡量消费者健康，并探讨了批处理场景的配置选项。最后，我们看到了如何使用配置来检测和调查常见的消息问题。
- en: With this, you’re prepared to instrument common messaging patterns and can start
    designing and tuning instrumentation for advanced streaming scenarios.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，你已经准备好配置常见的消息模式，并可以开始设计和调整高级流场景的配置。
- en: In the next chapter, we’re going to design a comprehensive observability store
    for databases and caching.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将为数据库和缓存设计一个全面的可观察性存储。
- en: Questions
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: How would you measure end-to-end latency for an asynchronous operation? For
    example, in a scenario when a user uploads a meme and it takes some time to process
    and index it before it appears in search results.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你会如何衡量异步操作的全端延迟？例如，在一个用户上传表情包并需要一些时间处理和索引它，然后才出现在搜索结果中的场景。
- en: How would you report batch size as a metric? How it can be used?
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你会如何报告批大小作为一个指标？它如何被使用？
- en: How would you approach baggage propagation in messaging scenarios
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你会如何处理消息场景中的行李传播
