<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-175"><a id="_idTextAnchor174"/>11</h1>
<h1 id="_idParaDest-176"><a id="_idTextAnchor175"/>Instrumenting Messaging Scenarios</h1>
<p>Messaging and asynchronous processing improve distributed system scalability and reliability by reducing coupling between services. However, they also increase complexity and introduce a new failure mode, which makes observability even more important.</p>
<p>In this chapter, we’ll work on instrumenting a messaging producer and consumer with traces and metrics and cover individual and batch message processing.</p>
<p>In this chapter, you’ll learn how to do the following:</p>
<ul>
<li>Trace individual messages as they are created and published</li>
<li>Instrument receiving and processing operations</li>
<li>Instrument batches</li>
<li>Use instrumentation to diagnose common messaging problems</li>
</ul>
<p>By the end of this chapter, you should be able to instrument your messaging application from scratch or tune the existing messaging instrumentation to your needs.</p>
<h1 id="_idParaDest-177"><a id="_idTextAnchor176"/>Technical requirements</h1>
<p>The code for this chapter is available in the book’s repository on GitHub at <a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter11">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter11</a>.</p>
<p>To run the samples and perform analysis, we’ll need the following tools:</p>
<ul>
<li>.NET SDK 7.0 or later.</li>
<li>Docker and <code>docker-compose</code>.</li>
<li>Any HTTP benchmarking tool, for example, <code>bombardier</code>. You can install it with <code>$ go get -u github.com/codesenberg/bombardier</code> if you have Go tools, or download bits directly from its GitHub repository at <a href="https://github.com/codesenberg/bombardier/releases">https://github.com/codesenberg/bombardier/releases</a>.</li>
</ul>
<p>We will also be using the Azure Storage emulator in Docker. No setup or Azure subscription is necessary.</p>
<h1 id="_idParaDest-178"><a id="_idTextAnchor177"/>Observability in messaging scenarios</h1>
<p>In <a href="B19423_10.xhtml#_idTextAnchor161"><em class="italic">Chapter 10</em></a>, <em class="italic">Tracing Network Calls</em>, we just started scratching the surface of tracing support for asynchronous processing. There, we saw how the client and server can send a <a id="_idIndexMarker586"/>stream of potentially independent messages to each other.</p>
<p>In the case of messaging, things get even more complicated: in addition to asynchronous <a id="_idIndexMarker587"/>communication, the producer and consumer interact through an intermediary – a messaging <strong class="bold">broker</strong>.</p>
<p>Operation on the producer completes once the message is published to the broker without waiting for the consumer to process this message. Depending on the scenario and application health, the consumer may process it right away, in a few seconds, or in several days.</p>
<p>In some cases, producers get a notification that the message was processed, but this usually happens through another messaging queue or a different communication channel.</p>
<p>Essentially, the producer does not know whether the consumer exists – failures or delays in the processing pipeline are not visible on the producer side. This changes how we should look at latency, throughput, or error rate from an observability standpoint – now we need to think about end-to-end flows that consist of multiple independent operations.</p>
<p>For example, when using HTTP calls only, the latency of the original request covers almost everything that happened with the request. Once we introduce messaging, we need means to measure the end-to-end latency and identify failures between different components. An example of an application that uses messaging is shown in <em class="italic">Figure 11</em><em class="italic">.1</em>:</p>
<div><div><img alt="Figure 11.1 – Application using messaging to run tasks in the background " src="img/B19423_11_01.jpg"/>
</div>
</div>
<p class="IMG---Figure">Figure 11.1 – Application using messaging to run tasks in the background</p>
<p>In such an application, when the user sends a request to the frontend, they receive a response once <a id="_idIndexMarker588"/>the backend finishes processing and publishes a message to a topic. The indexer, replicator, archiver, and any other services that post-process the data run at their own speed. The indexer usually processes the latest messages, while the archiver would only look at the messages published days ago.</p>
<p>Some of these components can fail without affecting user scenarios directly, while others impact how soon the data the user published shows up in other parts of the system and therefore can be critical.</p>
<p>Let’s explore how we can instrument such applications.</p>
<p>Before writing our own instrumentation from scratch, we should always check whether there are existing instrumentation libraries we can start with, and if there are none available, we should consult with OpenTelemetry semantic conventions.</p>
<p>We’re going to instrument Azure Queue Storage as an example. The existing instrumentation does not cover the messaging aspects of queues because of the reasons we’ll see in the next couple of sections. So, we’ll have to write our own; we’ll do it according to OpenTelemetry conventions.</p>
<h2 id="_idParaDest-179"><a id="_idTextAnchor178"/>Messaging semantic conventions</h2>
<p>The messaging <a id="_idIndexMarker589"/>conventions for tracing are available at <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/trace/semantic_conventions/messaging.md">https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/trace/semantic_conventions/messaging.md</a>.</p>
<p>They currently have experimental status and are very likely to change. There are no general metrics conventions available yet, but you can find ones specific to Kafka.</p>
<p>Conventions <a id="_idIndexMarker590"/>provide some considerations on context propagation (we’ll discuss this in the <em class="italic">Trace context propagation</em> section) and define generic attributes to describe messaging operations. Here are a few essential ones we’re going to use:</p>
<ul>
<li><code>messaging.system</code>: Indicates that the span follows messaging semantics and describes the specific messaging system used, such as <code>kafka</code> or <code>rabbitmq</code>. In our sample, we’ll use <code>azqueues</code>.</li>
<li><code>messaging.operation</code>: Identifies one of the standard operations: <code>publish</code>, <code>receive</code>, or <code>process</code>.</li>
<li><code>messaging.destination.name</code> and <code>messaging.source.name</code>: Describe a queue or topic name within a broker. The term <code>destination</code> is used on the producer and <code>source</code> is used on the consumer.</li>
<li><code>net.peer.name</code>: Identifies the broker domain name.</li>
</ul>
<p>Let’s see how we can use the conventions to add observability signals that can help us document the application behavior or detect and resolve a new class of issues happening in messaging scenarios.</p>
<h1 id="_idParaDest-180"><a id="_idTextAnchor179"/>Instrumenting the producer</h1>
<p>The producer is the component responsible for publishing messages to a broker. The publishing <a id="_idIndexMarker591"/>process itself is usually synchronous: we send a request to the broker and get a response from it indicating whether the message was published successfully.</p>
<p>Depending on the messaging system and producer needs, one publish request may carry one or more messages. We’ll discuss batching in the <em class="italic">Instrumenting batching scenarios</em> section. For now, let’s focus on a single message case.</p>
<p>To trace it, we need to make sure we create an activity when we publish a message, so we can track the call duration and status and debug individual requests. We’d also be interested in metrics for duration, throughput, and failure rate – it’s important to budget cloud messaging solutions or scale self-hosted brokers.</p>
<p>Another essential part of producer instrumentation is context propagation. Let’s stop here for a second and discuss it.</p>
<h2 id="_idParaDest-181"><a id="_idTextAnchor180"/>Trace context propagation</h2>
<p>When we instrument HTTP calls, context is propagated via HTTP request headers, which are part of <a id="_idIndexMarker592"/>the request. In messaging, the context is carried via a transport call to the broker and is not propagated to a consumer. Transport call <a id="_idIndexMarker593"/>trace context identifies the request, but not the message(s) it carries.</p>
<p>So, we need to propagate context inside the message to make sure it goes all the way to the consumer. But which <a id="_idIndexMarker594"/>context should we inject? We have several options:</p>
<ul>
<li><strong class="bold">Use context from the current activity</strong>: For instance, when we publish messages in the scope of an incoming HTTP request, we may use the context of the activity representing this HTTP server call. This works only if we send one message per incoming request. If we send more than one (each in an individual publish call), we’d not be able to tell which message the consumer call processed or identify whether we sent messages to the right queues.</li>
<li><strong class="bold">Create an activity per message and inject its context</strong>: Unique context allows us to trace messages individually and works in batching scenarios as well where we send multiple messages in one publish call. It also adds the overhead of creating an additional activity per message.</li>
<li><strong class="bold">Reuse the publish activity</strong>: When we publish one message in one call to the broker, we can uniquely identify a message and publish call with one activity.</li>
</ul>
<p>The first option goes against OpenTelemetry messaging semantic conventions, which allow us to pick a suitable option from the last two. In our example, we’re using Azure Queue Storage, which does not support batching when publishing messages. So, we’re going to use the last option and create one activity to trace a publish call and inject its context into the message.</p>
<p class="callout-heading">Note</p>
<p class="callout">When forking or routing messages from one queue to another, the message might have pre-existing trace context injected in the upstream service. The default behavior in such a case should be to keep the message context intact. To correlate all operations that happen with the message, we can always add a link to an existing trace context in the message when publishing or receiving it.</p>
<p>Another interesting <a id="_idIndexMarker595"/>aspect of Azure Queue Storage is that <a id="_idIndexMarker596"/>it doesn’t support message metadata – the message is an opaque payload without any prescribed structure or format that the service carries over. So, similarly to gRPC streaming, which we covered in <a href="B19423_10.xhtml#_idTextAnchor161"><em class="italic">Chapter 10</em></a>, <em class="italic">Tracing Network Calls</em>, we’ll need to define our own message structure or use one of the well-known event formats <a id="_idIndexMarker597"/>available out there, such as <strong class="bold">CloudEvents</strong>.</p>
<p class="callout-heading">Note</p>
<p class="callout">CloudEvents (<a href="https://cloudevents.io">https://cloudevents.io</a>) is an <a id="_idIndexMarker598"/>open standard that defines event structure in a vendor- and technology-agnostic way. It’s commonly used by cloud providers to notify applications about infrastructure changes or when implementing data change feeds. CloudEvents have distributed tracing extensions to carry W3C Trace Context as well as general-purpose metadata that can be used for other formats. OpenTelemetry also provides semantic conventions for CloudEvents.</p>
<p>For demo purposes, we’ll keep things simple and define our own tiny message model in the following way:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">producer/Message.cs</p>
<pre class="source-code">
public class Message
{
  ...
  <strong class="bold">public Dictionary&lt;string, string&gt; Headers { get; set; }</strong> =
    new ();
  public string? Text { get; set; }
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Message.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Message.cs</a></p>
<p>We’ll use the <code>Headers</code> property to propagate the trace context and will keep the payload in the <code>Text</code> property.</p>
<p>Similarly to <a id="_idIndexMarker599"/>the gRPC streaming examples we saw in <a href="B19423_10.xhtml#_idTextAnchor161"><em class="italic">Chapter 10</em></a>, <em class="italic">Tracing Network Calls</em>, we can inject context into this message using the <a id="_idIndexMarker600"/>OpenTelemetry propagator with the following code snippet:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">producer/Controllers/SendController.cs</p>
<pre class="source-code">
private void InjectContext(Message message, Activity? act)
{
  if (act != null)
  {
<strong class="bold">    _propagator.Inject(new (act.Context, Baggage.Current),</strong>
<strong class="bold">      message,</strong>
<strong class="bold">    static (m, k, v) =&gt; m.Headers[k] = v);</strong>
  }
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs</a></p>
<p>Now we have all we need to instrument a publish call – let’s do it.</p>
<h2 id="_idParaDest-182"><a id="_idTextAnchor181"/>Tracing a publish call</h2>
<p>We’ll need to create a new activity and put common messaging attributes on it to identify the broker, queue operation, and add other information. In the case of Azure Queue <a id="_idIndexMarker601"/>Storage, we can use the account name as the broker identifier (as they are unique within a public cloud).</p>
<p>Then, we’ll inject <a id="_idIndexMarker602"/>context into the message and proceed with publishing. After the message is published successfully, we can also record the information returned by the broker, such as the message ID and other details we might consider useful.</p>
<p>Here’s the corresponding code:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">producer/Controllers/SendController.cs</p>
<pre class="source-code">
Stopwatch? duration = PublishDuration.Enabled ?
  Stopwatch.StartNew() : null;
using var act = StartPublishActivity();
InjectContext(message, Activity.Current);
try
{
  var receipt = await _queue.SendMessageAsync(
    BinaryData.FromObjectAsJson(message));
  act?.SetTag("messaging.message.id",
    receipt.Value.MessageId);
  RecordPublishMetrics(duration, "ok");
  ...
}
catch (Exception ex)
{
  act?.SetStatus(ActivityStatusCode.Error, ex.Message);
  RecordPublishMetrics(duration, "fail")
  ...
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs</a></p>
<p>Here, we injected the context of <code>Activity.Current</code> with the <code>Inject</code> method we implemented before. This may be useful if you want to turn off per-message activities. In such a case, per-message tracing will be limited, but consumer and producer calls will still be <a id="_idIndexMarker603"/>correlated. We also record metrics here – stay tuned for <a id="_idIndexMarker604"/>the details; we’re going to cover them in the next section.</p>
<p>Here’s the <code>StartPublishActivity</code> method implementation:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">producer/Controllers/SendController.cs</p>
<pre class="source-code">
var act = Source.StartActivity($"{_queue.Name} publish",
  ActivityKind.Producer);
if (act?.IsAllDataRequested == true)
  act.SetTag("messaging.system", "azqueues")
    .SetTag("messaging.operation", "publish")
    .SetTag("messaging.destination.name", _queue.Name)
    .SetTag("net.peer.name", _queue.AccountName)
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs</a></p>
<p>The activity here has a <code>producer</code> kind, which indicates the start of an async flow. The name follows <a id="_idIndexMarker605"/>OpenTelemetry semantic conventions, which <a id="_idIndexMarker606"/>recommend using the <code>{queue_name} {operation}</code> pattern. We can also cache it to avoid unnecessary string formatting.</p>
<p>This is it; we’ve covered producer tracing – let’s look at metrics now.</p>
<h2 id="_idParaDest-183"><a id="_idTextAnchor182"/>Producer metrics</h2>
<p>Messaging-specific metrics <a id="_idIndexMarker607"/>come as an addition to resource utilization, .NET runtime, HTTP, and other metrics you might want to expose.</p>
<p>To some extent, we can use HTTP metrics to monitor calls to Azure Queue Storage since they work on top of HTTP. This would allow us to monitor duration, success rate, and throughput for <a id="_idIndexMarker608"/>individual HTTP calls to storage, but won’t allow us to distinguish queues within one storage account.</p>
<p>So, if we rely on metrics, we should record some messaging-specific ones that cover common indicators such as publish call duration, throughput, and latency for each queue we use.</p>
<p>We can report all of them using a duration histogram, as we saw in <a href="B19423_07.xhtml#_idTextAnchor115"><em class="italic">Chapter 7</em></a>, <em class="italic">Adding Custom Metrics</em>. First, let’s initialize the duration histogram, as shown in the following code snippet:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">producer/Controllers/SendController.cs</p>
<pre class="source-code">
private static readonly Meter Meter = new("Queue.Publish");
private static readonly Histogram&lt;double&gt; PublishDuration =
  Meter.CreateHistogram&lt;double&gt;(
    "messaging.azqueues.publish.duration", ...);</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs</a></p>
<p><code>Meter</code> and <code>Histogram</code> are static since we defined them in the controller. The controller lifetime is scoped to a request, so we keep them static to stay efficient.</p>
<p>As we saw in the tracing example, every time we publish a message, we’re also going to record a publish duration. Here’s how it’s implemented:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">producer/Controllers/SendController.cs</p>
<pre class="source-code">
public void RecordPublishMetrics(Stopwatch? dur,
  string status)
{
  ...
  TagList tags = new() {
    { "net.peer.name", _queue.AccountName },
    { "messaging.destination.name", _queue.Name },
    { "messaging.azqueue.status", status }};
  PublishDuration.Record(dur.Elapsed. TotalMilliseconds,
    tags);
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs</a></p>
<p>Here, we used <a id="_idIndexMarker609"/>the same attributes to describe the queue and added a custom status <a id="_idIndexMarker610"/>attribute. Keep in mind that we need it to have low cardinality, so we only use <code>ok</code> and <code>fail</code> statuses when we call this method.</p>
<p>We’re done with the producer. Having basic tracing and metrics should give us a good starting point to diagnose and debug most of the issues and monitor overall producer health, as we’ll see in the <em class="italic">Performance analysis in messaging scenarios</em> section later. Let’s now explore instrumentation on consumers.</p>
<h1 id="_idParaDest-184"><a id="_idTextAnchor183"/>Instrumenting the consumer</h1>
<p>While you might be able to get away without custom instrumentation on the producer, consumer instrumentation is unavoidable.</p>
<p>Some brokers <a id="_idIndexMarker611"/>push messages to consumers using synchronous HTTP or RPC calls, and the existing framework instrumentation can provide the bare minimum of observability data. In all other cases, messaging traces and metrics are all we have to detect consumer health and debug issues.</p>
<p>Let’s start by tracing individual messages – recording when they arrive in the consumer and how they are processed. This allows us to debug issues by answering questions such as “Where is this message now?” or “Why did it take so long to process the data?”</p>
<h2 id="_idParaDest-185"><a id="_idTextAnchor184"/>Tracing consumer operations</h2>
<p>When using Azure Queue Storage, applications request one or more messages from the queue. Received messages stay in the queue but become invisible to other consumers for configurable <a id="_idIndexMarker612"/>visibility timeout. The application processes messages and, when done, deletes them from the queue. If processing fails with a transient issue, applications don’t delete messages. The same flow is commonly used when working with AWS SQS.</p>
<p>RabbitMQ- and AMQP-based messaging flows look similar, except messages can be pushed to the consumer so that the application reacts to the client library callback instead of polling the queue.</p>
<p>Callback-based delivery allows us to implement instrumentation in client libraries or provide a shared instrumentation library, and with a poll-based model, we essentially are forced to write at least some custom instrumentation for processing. Let’s do it.</p>
<p>First, let’s instrument message processing in isolation from receiving. We’ll need to create an activity to track processing that will capture everything that happens there, including message deletion:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">consumer/SingleReceiver.cs</p>
<pre class="source-code">
using var act = StartProcessActivity(msg);
...
try
{
  await ProcessMessage(msg, token);
  await _queue.DeleteMessageAsync(msg.MessageId,
    msg.PopReceipt, token);
}
catch (Exception ex)
{
  await _queue.UpdateMessageAsync(msg.MessageId,
    msg.PopReceipt, visibilityTimeout: BackoffTimeout,
    cancellationToken: token);
  ...
  act?.SetStatus(ActivityStatusCode.Error, ex.Message);
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs</a></p>
<p>Here, all the <a id="_idIndexMarker613"/>processing logic happens in the <code>ProcessMessage</code> method. When it completes successfully, we delete the message from the queue. Otherwise, we update its visibility to reappear in the queue after the backoff timeout.</p>
<p>Here’s the <code>StartProcessActivity</code> implementation:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">consumer/SingleReceiver.cs</p>
<pre class="source-code">
PropagationContext ctx = ExtractContext(msg);
var current = new ActivityLink(Activity.Current?.Context ??
  default);
var act = _messageSource.StartActivity(
  $"{_queue.Name} process",
  ActivityKind.Consumer,
  ctx.ActivityContext,
  links: new[] { current });
if (act?.IsAllDataRequested == true)
  act.SetTag("net.peer.name",_queue.AccountName)
     .SetTag("messaging.system", "azqueues")
     .SetTag("messaging.operation", "process")
     .SetTag("messaging.source.name", _queue.Name)
     .SetTag("messaging.message.id", msg.MessageId);
  ...</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs ">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs</a></p>
<p>Here, we extracted <a id="_idIndexMarker614"/>the context from the message and used it as a parent of the processing activity. It has the <code>consumer</code> kind, which indicates the continuation of the asynchronous flow. We also kept <code>Activity.Current</code> as a link to preserve correlation. We also added messaging attributes.</p>
<p>Message deletion and updates are traces by HTTP or Azure Queue SDK instrumentations. They don’t have messaging semantics, but should give us reasonable observability. Corresponding activities become children of the processing one, as shown in <em class="italic">Figure 11</em><em class="italic">.2</em>:</p>
<div><div><img alt="Figure 11.2 – Message trace from producer to consumer" src="img/B19423_11_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – Message trace from producer to consumer</p>
<p>The message was published and then we see two attempts to process it on the consumer: the first attempt failed. The second try was successful, and the message was deleted.</p>
<p>What’s missing in the preceding screenshot? We don’t see how and when the message was received. This might not be important on this trace, but look at another one in <em class="italic">Figure 11</em><em class="italic">.3</em>:</p>
<div><div><img alt="Figure 11.3 – Message trace with a nine-minute gap between producer and consumer" src="img/B19423_11_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3 – Message trace with a nine-minute gap between producer and consumer</p>
<p>Here, nothing has <a id="_idIndexMarker615"/>happened for almost nine minutes. Was the message received by a consumer during that time? Were the consumers alive? What were they doing? Were there any problems in the Azure Queue service that prevented messages from being received?</p>
<p>We’ll see how to answer these questions later. Now, let’s focus on tracing the receive operation.</p>
<p>The challenge with the receive operation is that the message trace context is available after the message is received and the corresponding operation is about to end. We could add links to message trace contexts then, but it’s currently only possible to add them at activity start time.</p>
<p>This is likely to change, but for now, we’ll work around it by tracing the receive-and-process iteration and adding an attribute with the received message ID so we can find all spans that touched this message:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">consumer/SingleReceiver.cs</p>
<pre class="source-code">
using var act = _receiverSource
  .StartActivity("ReceiveAndProcess");
try
{
  var response = await _queue.ReceiveMessagesAsync(1,
    ProcessingTimeout, token);
  QueueMessage[] messages = response.Value;
  if (messages.Length == 0)
  {
    ...; continue;
  }
  act?.SetTag("messaging.message.id",
    messages[0].MessageId);
  await ProcessAndSettle(messages[0], token);
  ...
}
catch (Exception ex)
{
  act?.SetStatus(ActivityStatusCode.Error, ex.Message);
  ...
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs</a></p>
<p>Here, we receive <a id="_idIndexMarker616"/>at most one message from the queue. If a message was received, we process it.</p>
<p>One iteration is tracked with the <code>ReceiveAndProcess</code> activity, which becomes a parent to the receiving operation. The message processing activity is created in the <code>ProcessAndSettle</code> method and links to the <code>ReceiveAndProcess</code> activity, as shown in <em class="italic">Figure 11</em><em class="italic">.4</em>:</p>
<div><div><img alt="Figure 11.4 – Link from processing to outer loop activity" src="img/B19423_11_04.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4 – Link from processing to outer loop activity</p>
<p>If we follow <a id="_idIndexMarker617"/>the link, we’ll see an outer loop trace like the one shown in <em class="italic">Figure 11</em><em class="italic">.5</em>:</p>
<div><div><img alt="Figure 11.5 – Trace representing the receive and process iteration" src="img/B19423_11_05.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5 – Trace representing the receive and process iteration</p>
<p>Since more and more observability backends are providing better support for links, it can be more convenient to use them in your backend.</p>
<p>With iteration instrumented, we can now correlate receiving and processing or see how long a full loop cycle takes. This can help us understand whether consumers are alive and trying to receive and process something.</p>
<p>We’re stamping the <code>messaging.message.id</code> attribute on all spans to simplify finding all operations related to any given message.</p>
<p>Now, back to the nine-minute gap we saw in <em class="italic">Figure 11</em><em class="italic">.3</em>. What happened there is that we got too many messages in the queue – they were produced faster than we consumed them. By looking at gaps in individual traces, we can suspect that message spent time in the queue, but can’t tell for sure. What we need is to see the rate at which messages are published, processed, and deleted. We should also understand how long messages spend in the queue and how big the queue is. Let’s see how we can record and use such metrics.</p>
<h2 id="_idParaDest-186"><a id="_idTextAnchor185"/>Consumer metrics</h2>
<p>Similar to producers, we should enable common runtime and process metrics so we know the <a id="_idIndexMarker618"/>resource utilization for consumer processes. We should also record the processing loop duration, which will give us the error rate and throughput.</p>
<p>From a messaging perspective, we’d also want to cover the following things:</p>
<ul>
<li>The amount of time messages spend in the queue is a great indicator of consumer health and scale. When there are not enough consumers, the amount of time spent in the queue will grow and can be used to scale consumers up. When it decreases consistently, it could serve as a signal to scale consumers down.</li>
<li>The number of messages in the queue provides similar data, but in real time. It includes messages that have not yet been processed. Queue size metric can also be recorded on the producer side without ever depending on the consumer.</li>
</ul>
<p>These metrics, or similar ones you can come up with, and their trends over time provide a great indication of consumer health.</p>
<p>These metrics increase if consumer performance degrades or the error rate increases. They won’t be helpful if consumers fail to process messages but immediately delete them from the queue, but this will manifest in high error rate. So, let’s go ahead and instrument our application with these metrics.</p>
<h3>Duration, throughput, and failure rate</h3>
<p>We’re going to measure the processing loop duration, which includes trying to receive a message <a id="_idIndexMarker619"/>and its processing. Measuring the receiving and processing duration independently would be even more precise and is something to consider in your production applications.</p>
<p>At the beginning of the loop, we’ll start a stopwatch to measure operation duration, and once processing completes, we’ll report it as a histogram along with queue information and the status. Let’s first create the histogram instrument:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">consumer/SingleReceiver.cs</p>
<pre class="source-code">
private readonly Meter _meter = new ("Queue.Receive");
private readonly Histogram&lt;double&gt; _loopDuration;
...
_loopDuration = _meter.CreateHistogram&lt;double&gt;(
  "messaging.azqueues.process.loop.duration", "ms",
  "Receive and processing duration.");</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs</a></p>
<p>We create <a id="_idIndexMarker620"/>meter and duration instruments here as instance variables, which we dispose of along with the <code>SingleReceiver</code> instance. The receiver extends the <code>BackgroundService</code> interface and is registered in the dependency injection container as a singleton, so they are all disposed of once the application shuts down.</p>
<p>The processing loop instrumentation can be done in the following way:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">consumer/SingleReceiver.cs</p>
<pre class="source-code">
Stopwatch? duration = Stopwatch.StartNew();
try
{
  var response = await _queue.ReceiveMessagesAsync(1,
    ProcessingTimeout, token);
  QueueMessage[] messages = response.Value;
  RecordLag(messages);
  if (messages.Length == 0)
  {
    ...
    RecordLoopDuration(duration, "empty");
    continue;
  }
  ...
  await ProcessAndSettle(messages[0], token);
  RecordLoopDuration(duration, "ok");
}
catch (Exception ex)
{
  RecordLoopDuration(duration, "fail"); ...
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs</a></p>
<p>Here, we <a id="_idIndexMarker621"/>record the duration of each iteration along with the queue information and status. The status can have the following values: <code>ok</code>, <code>fail</code>, or <code>empty</code> (if no messages were received). In real applications, you probably want to be more precise and add a few more statuses to indicate the failure reason. For example, it would be important to record why the receive operation failed, whether there was a serialization or validation error, processing timed out, or it failed with a terminal or transient error.</p>
<p>The <code>RecordLoopDuration</code> method implementation is shown in this snippet:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">consumer/SingleReceiver.cs</p>
<pre class="source-code">
TagList tags = new () {
  { "net.peer.name", _queue.AccountName },
  { "messaging.source.name", _queue.Name },
  { "messaging.azqueue.status", status }};
_loopDuration.Record(duration.ElapsedMilliseconds, tags);</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs</a></p>
<p>We’ll see how we can use this metric later in this chapter. Let’s first implement consumer lag and queue size.</p>
<h3>Consumer lag</h3>
<p>In the <a id="_idIndexMarker622"/>code sample showing metrics in the processing loop, we called into the <code>RecordLag</code> method as soon as we received a message. Consumer lag records the approximate time a message spent in the queue – the delta between the receive and enqueue time.</p>
<p>The enqueue time is recorded by the Azure Queue service and is exposed as a property on the <code>QueueMessage</code> instance. We can record the metric with the following code:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">consumer/SingleReceiver.cs</p>
<pre class="source-code">
_consumerLag = _meter.CreateHistogram&lt;double&gt;(
  "messaging.azqueues.consumer.lag", "s", ...);
...
long receivedAt = DateTimeOffset.UtcNow
  .ToUnixTimeMilliseconds();
TagList tags = new () {
  { "net.peer.name", _queue.AccountName },
  { "messaging.source.name", _queue.Name }};
foreach (var msg in messages
    .Where(m =&gt; m.InsertedOn.HasValue))
{
  long insertedOn = msg.InsertedOn!
    .Value.ToUnixTimeMilliseconds());
  long lag = Math.Max(1, receivedAt - insertedOn);
  _consumerLag.Record(lag/1000d, tags);
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs</a></p>
<p>Here, we create a histogram that represents the lag (in seconds) and record it for every received message as the difference between the current time and the time at which the message was received by the broker.</p>
<p>Note that <a id="_idIndexMarker623"/>these timestamps usually come from two different computers – the difference can be negative and is not precise due to clock skew. The margin of error can reach seconds but may, to some extent, be corrected within your system.</p>
<p>Clock skew should be expected, but sometimes things can go really wrong. I once was involved in investigating an incident that took our service down in one of the data centers. It happened because of the wrong time server configuration, which moved the clock on one of the services back a few hours. It broke authentication – the authentication tokens had timestamps from hours ago and were considered expired.</p>
<p>Despite being imprecise, consumer lag should give us an idea of how long messages spend in a queue. We record it every time a message is received, so it also reflects redeliveries. Also, we record it before we know whether processing was successful, so it does not have any status.</p>
<p>Before we record lag on the consumer, we first need to receive a message. When we see a huge lag, it’s a good signal that something is not right, but it does not tell us how many messages have not yet been received.</p>
<p>For example, when the load is low and the queue is empty, there might be a few invalid messages that are stuck there. It’s a bug, but it can be fixed during business hours. To detect how big the issue is, we also need to know how many messages are in the queue. Let’s see how to implement it.</p>
<h3>Queue size</h3>
<p>Azure Queue Storage as well as Amazon SQS allow us to retrieve an approximate count of messages. We can register another <code>BackgroundService</code> implementation to retrieve the <a id="_idIndexMarker624"/>count periodically. This can be done on the consumer or producer.</p>
<p>We’ll use a gauge instrument to report it, as shown in this code snippet:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">consumer/QueueSizeReporter.cs</p>
<pre class="source-code">
TagList tags = new () {
  { "net.peer.name", queue.AccountName},
  { "messaging.source.name", queue.Name}};
_queueSize = _meter.CreateObservableGauge(
  "messaging.azqueues.queue.size",
  () =&gt; new Measurement&lt;long&gt;(_currentQueueSize, tags), ...);</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/QueueSizeReporter.cs</p>
<p>We passed a callback that returns a <code>_currentQueueSize</code> instance variable. We’re going to update it every several seconds as we retrieve the size from the queue:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">consumer/QueueSizeReporter.cs</p>
<pre class="source-code">
var res = await _queue.GetPropertiesAsync(token);
_currentQueueSize = res.Value.ApproximateMessagesCount;</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/QueueSizeReporter.cs</p>
<p>That’s it – now we measure the queue size. This number alone does not tell the entire story, but if it’s significantly different from the baseline or grows fast, this is a great indication of a problem.</p>
<p>Once the load grows, the queue size will also go up and we may try to add more consumers or optimize them. One of the typical optimizations is batching – it helps reduce the <a id="_idIndexMarker625"/>number of network calls and would utilize consumer instances better. Let’s see how we can instrument it.</p>
<h1 id="_idParaDest-187"><a id="_idTextAnchor186"/>Instrumenting batching scenarios</h1>
<p>Instrumentation <a id="_idIndexMarker626"/>for batching scenarios can be different depending on the use case – transport-level batching needs a slightly different approach compared to batch processing.</p>
<h2 id="_idParaDest-188"><a id="_idTextAnchor187"/>Batching on a transport level</h2>
<p>Messages can be batched together to minimize the number of network calls. It can be used by <a id="_idIndexMarker627"/>producers or consumers, and systems such as Kafka, Amazon SQS, or Azure Service Bus support batching on both sides.</p>
<p>On the consumer, when multiple messages are received together but processed independently, everything we had for single message processing still applies.</p>
<p>From a <a id="_idIndexMarker628"/>tracing perspective, the only thing we’d want to change is to add attributes that record all received message identifiers and batch size on the outer iteration activity.</p>
<p>From the metrics side, we’d also want to measure individual message processing duration, error rate, and throughput. We can track them all by adding a message processing duration histogram.</p>
<p>When we send multiple messages in a batch, we still need to trace these messages independently . To do so, we’d have to create an activity per message and inject unique trace context into each message. The publish activity then should be linked to all the messages being published.</p>
<p>The main question here is when to create a per-message activity and inject context into the message. Essentially, the message trace context should continue the operation that created the message. So, if we buffer messages from different unrelated operations and then send them in a background thread, we should create message activities when the message is created. Then, a batch publish operation will link to independent, unrelated trace contexts.</p>
<p>The duration <a id="_idIndexMarker629"/>metric for the publish operation remains the same as for the single-message case we implemented before, but we should consider adding another metric to describe the batch size and the exact number of sent messages – we won’t be able to figure it out from the publish duration.</p>
<h2 id="_idParaDest-189"><a id="_idTextAnchor188"/>Processing batches</h2>
<p>In some cases, we process messages in batches, for example, when aggregating data for analytic purposes, replicating or archiving received data. In such cases, it’s just not possible to <a id="_idIndexMarker630"/>separate individual messages. Things get even more complicated in scenarios such as routing or sharding, when a received batch is split into several new batches and sent to the next destination.</p>
<p>We can record relationships using links – this will allow us to tell whether (when and how many times) a message was received, and which processing operation it contributed to.</p>
<p>Essentially, we create a batch-processing activity with links to all messages being processed. Links have attributes and there we can put important message metadata, such as the delivery count, message ID, or insertion time.</p>
<p>From a metrics perspective, consumer lag (measured per message), queue size, and processing duration (throughput and failure rate) still apply. We might also want to report the batch size as a histogram.</p>
<p class="callout-heading">Note</p>
<p class="callout">Message and batch processing are frequently done outside of messaging client library control, by application code or integration frameworks. It’s rarely possible for auto-instrumentation to trace or measure processing calls. These scenarios vary a lot from application to application, requiring custom instrumentations tuned to specific use cases and messaging systems.</p>
<p>Now that we have an idea of how to instrument messaging scenarios, let’s see how we can use it in practice.</p>
<h1 id="_idParaDest-190"><a id="_idTextAnchor189"/>Performance analysis in messaging scenarios</h1>
<p>We’re going to use our demo application to simulate a few common problems and use signals we <a id="_idIndexMarker631"/>have to detect and debug issues. Let’s start the application with the following command:</p>
<pre class="console">
$ docker-compose up --build --scale consumer=3</pre>
<p>It will run one producer and three consumers along with the observability stack.</p>
<p>You can now send a request to the producer at <code>http://localhost:5051/send</code>, which sends one message to the queue and returns receipt information as a response.</p>
<p>Now you need to add some load with the tool of your choice. If you use <code>bombardier</code>, you can do it with the following command:</p>
<pre class="console">
$ bombardier -c 1 -d 30m http://localhost:5051/send</pre>
<p>It sends requests to the producer in one connection. You can play with a different number of connections and consumers in the <code>docker-compose</code> command to see how the metrics change.</p>
<p>You might also want to install Grafana and import the dashboard from the book’s repository (https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/grafana-dashboard.json) to look at all metrics at once.</p>
<p>How do we check whether the consumers are working properly? We can start with consumer lag and queue size metrics. <em class="italic">Figure 11</em><em class="italic">.6</em> shows the 95th percentile for consumer lag obtained with the following query:</p>
<pre class="console">
histogram_quantile(0.95,
  sum(rate(messaging_azqueues_consumer_lag_seconds_bucket[1m]))
  by (le, messaging_source_name, net_peer_name)
)</pre>
<div><div><img alt="Figure 11.6 – Consumer lag grows over time" src="img/B19423_11_06.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.6 – Consumer lag grows over time</p>
<p>Consumer lag <a id="_idIndexMarker632"/>grows almost to 600 seconds, and if we look at the queue size, as shown in <em class="italic">Figure 11</em><em class="italic">.7</em>, we’ll see there were up to about 11,000 messages in the queue:</p>
<div><div><img alt="Figure 11.7 – Queue size grows and then slowly goes down" src="img/B19423_11_07.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.7 – Queue size grows and then slowly goes down</p>
<p>Here’s the query for the queue size:</p>
<pre class="console">
max by (net_peer_name, messaging_source_name)
(messaging_azqueues_queue_size_messages)</pre>
<p>Consumer lag stays high for a long time until all messages are processed  at around 19:32, but we can judge by the queue size that things started to improve at 19:27.</p>
<p>The trend changed and the queue quickly shrunk because I stopped the application and restarted it with 15 consumers.</p>
<p>But now we <a id="_idIndexMarker633"/>have too many consumers and are wasting resources. We can check the average batch size we retrieve – if it’s consistently and noticeably lower than the configured batch size, we may slowly start decreasing the number of consumers, leaving some buffer for bursts of load.</p>
<p>Now, let’s stop the load and add some errors. Send a malformed message with <code>http://localhost:5051/send?malformed=true</code>. We should see that queue size remains small, but consumer lag grows over time.</p>
<p>We can also see that despite no messages being sent, we’re receiving messages, processing them, and failing repeatedly.</p>
<p>For example, we can visualize it with the following query:</p>
<pre class="console">
sum(
 rate(messaging_azqueues_process_loop_duration_milliseconds_
    count[1m]))
by (messaging_source_name, messaging_azqueue_status)</pre>
<p>It shows the rate of process-and-receive iterations grouped by queue name and status. This is shown in <em class="italic">Figure 11</em><em class="italic">.8</em>:</p>
<div><div><img alt="Figure 11.8 – Processing rate grouped by status" src="img/B19423_11_08.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.8 – Processing rate grouped by status</p>
<p>We can see <a id="_idIndexMarker634"/>here that from around 20:57, we attempt to receive messages about four times per second. Three of these calls don’t return any messages, and in the other case, processing fails. There are no successful iterations.</p>
<p>We sent a few malformed messages, and it seems they are being processed forever – this is a bug. If there were more than a few such messages, they would keep consumers busy and not let them process any valid messages.</p>
<p>To confirm this suggestion, let’s look at the traces. Let’s open Jaeger at <code>http://localhost:16686</code> and filter traces with errors that come from consumers. One such trace is shown in <em class="italic">Figure 11</em><em class="italic">.9</em>:</p>
<div><div><img alt="Figure 11.9 – Failed receive-and-process iteration" src="img/B19423_11_09.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.9 – Failed receive-and-process iteration</p>
<p>Here, we see that four messages were received, and the iteration failed with an error. If we could <a id="_idIndexMarker635"/>add links to this operation, we would be able to navigate to traces for each individual message. Instead, we have message ID stamped. Let’s find the trace for one of these messages using the corresponding attribute. The result is shown in <em class="italic">Figure 11</em><em class="italic">.10</em>:</p>
<div><div><img alt="Figure 11.10 – Trace for one of the failed messages" src="img/B19423_11_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.10 – Trace for one of the failed messages</p>
<p>This does not look great – we have 3,000 spans for just one message. If we open the trace and check out the <code>messaging.azqueues.message.dequeue_count</code> attribute for the latest processing spans, we’ll see the message was received more than 1,000 times.</p>
<p>To fix the issue, we should delete messages that fail validation. We also make sure we do so for <a id="_idIndexMarker636"/>any other terminal error and introduce a limit to the number of times a message is dequeued, after which the message is deleted.</p>
<p>We just saw a couple of problems that frequently arise in messaging scenarios (but usually in less obvious ways) and used instrumentation to detect and debug them. As observability vendors improve user experience for links, it will become even easier to do such investigations. But we already have all the means to record telemetry and correlate it in messaging flows.</p>
<h1 id="_idParaDest-191"><a id="_idTextAnchor190"/>Summary</h1>
<p>In this chapter, we explored messaging instrumentation. We started with messaging specifics and the new challenges they bring to observability. We briefly looked into OpenTelemetry messaging semantic conventions and then dived into producer instrumentation. The producer is responsible for injecting trace context into messages and instrumenting publish operations so that it’s possible to trace each independent flow on consumers.</p>
<p>Then, we instrumented the consumer with metrics and traces. We learned how to measure consumer health using queue size and lag and explored the instrumentation options for batching scenarios. Finally, we saw how we can use instrumentation to detect and investigate common messaging issues.</p>
<p>With this, you’re prepared to instrument common messaging patterns and can start designing and tuning instrumentation for advanced streaming scenarios.</p>
<p>In the next chapter, we’re going to design a comprehensive observability store for databases and caching.</p>
<h1 id="_idParaDest-192"><a id="_idTextAnchor191"/>Questions</h1>
<ol>
<li>How would you measure end-to-end latency for an asynchronous operation? For example, in a scenario when a user uploads a meme and it takes some time to process and index it before it appears in search results.</li>
<li>How would you report batch size as a metric? How it can be used?</li>
<li>How would you approach baggage propagation in messaging scenarios</li>
</ol>
</div>
</body></html>