<html><head></head><body>
        

                            
                    <h1 class="header-title">Caching Strategies for Distributed Systems</h1>
                
            
            
                
<p class="Normal1">In the previous chapter, we learned all about common patterns for applying security to a network-hosted application. In this chapter, we will look at various ways to improve the performance of our network software by establishing intermediate caches. We'll see how using a cache to persist frequently accessed highly available data can grant us those performance improvements. We'll look at what a cache is and some of the various ways it can be used. Then, we'll undertake a thorough examination of one of the most common and complex architectural patterns for network caches. Finally, we'll demonstrate how to use caching at various levels of the application architecture to achieve our goals with a reasonable balance of developer effort and latency reduction.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>The potential performance improvements to be gained by caching the results of common requests</li>
<li>Basic patterns for caching session data to enable reliable interactions with parallel deployments of an application</li>
<li>Understanding out to leverage caches within our .NET Core applications</li>
<li>The strengths and weaknesses of various cache providers, including distributed network-hosted caches, memory caches, and database caches</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p>This chapter will have sample code to demonstrate each of the caching strategies we discuss. To work with that code, you'll need your trusty IDE (Visual Studio) or code editor (Visual Studio Code). You can download the sample code to work directly with it from this book's GitHub repository: <a href="https://github.com/PacktPublishing/Hands-On-Network-Programming-with-C-and-.NET-Core/tree/master/Chapter%2015">https://github.com/PacktPublishing/Hands-On-Network-Programming-with-C-and-.NET-Core/tree/master/Chapter 15</a>.</p>
<p>Check out the following video to see the code in action: <a href="http://bit.ly/2HY67CM">http://bit.ly/2HY67CM</a></p>
<p>We'll also be using the Windows Subsystem for Linux to host the Linux-based Redis cache server on our local machine. Of course, if you're already running on a *nix system such as OS X or a Linux distribution, you don't have to worry about this. Alternatively, when you're running the application locally, if you don't have admin privileges or don't have any interest in learning the Redis cache server, you can modify the sample code slightly to use a different cache provider, which you'll learn about as we move through this chapter. However, I would recommend familiarizing yourself with the Redis cache since it is widely used and is an excellent choice for most circumstances. If you choose to do so, you can find instructions for installing the Linux Subsystem here: <a href="https://docs.microsoft.com/en-us/windows/wsl/install-win10">https://docs.microsoft.com/en-us/windows/wsl/install-win10.</a></p>
<p>Once that's done, you can find the instructions for installing and running Redis here: <a href="https://redislabs.com/blog/redis-on-windows-10/">https://redislabs.com/blog/redis-on-windows-10/.</a></p>


            

            
        
    

        

                            
                    <h1 class="header-title">Why cache at all?</h1>
                
            
            
                
<p>While it introduces additional complexity for the developers who are responsible for implementing it, a well-designed caching strategy can improve application performance significantly. If your software relies heavily on network resources, maximizing your use of caches can save your users time in the form of faster performance, and your company money in the form of lower network overhead. However, knowing when to cache data, and when it would be inappropriate to do so, is not always intuitive for developers. So, when should you be leveraging a caching strategy, and why?</p>


            

            
        
    

        

                            
                    <h1 class="header-title">An ideal caching scenario</h1>
                
            
            
                
<p>Suppose you have an application that builds reports based on quarterly sales numbers. Imagine it has to pull hundreds of thousands of records from a handful of different databases, each with variable response times. Once it's acquired all of that data, it has to run extensive aggregation calculations on the records returned to produce the statistics that are displayed in the report output. Moreover, these reports are generated by dozens or even hundreds of different business analysts in a given day. Each report aggregates, for the most part, the same information, but some reports are structured to highlight different aspects of the data for different business concerns of the analysts. A naive approach to this problem would simply be to access the requested data on demand and return the results, reliably, with terrible response times. But does that necessarily have to be the case?</p>
<p>What I've just described is actually an ideal scenario for designing and implementing a caching strategy. What I've described is a system reliant on data owned by an external resource or process, which means that round-trip latency can be eliminated. I also noted that it is quarterly sales data, which means that it is presumably only ever updated, at most, once every three months. Finally, I mentioned that there are users generating reports with this remotely accessed data dozens to hundreds of times a day. For a distributed system, there is an almost no more obvious circumstance for pre-caching the remote data for faster and more reliable access in your on-demand application operations.</p>
<p>The contexts that motivate cache usage won't always be so cut and dried, but in general these three criteria will be a strong guide for when you should consider it. Just always ask yourself if any of them are met:</p>
<ul>
<li>Accessing resources external to your application's hosted context</li>
<li>Accessing resources that are infrequently updated</li>
<li>Accessing resources that are frequently used by your application</li>
</ul>
<p>If any of those circumstances are met, you should start to think about what benefits you might reap by caching. If all of them are met, you would need to make a strong case for why you wouldn't implement a caching strategy. Of course, to understand why these criteria make caching necessary, you must first understand exactly what caching is and, just as importantly, what it isn't.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The principles of a data cache</h1>
                
            
            
                
<p>Put simply, a cache is nothing more than an intermediary data store that can serve its data faster than the source from which the cached data originated. There are a number of reasons a cache could provide these speed improvements, and each of them requires their own consideration, so let's look at a few.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Caching long-running queries</h1>
                
            
            
                
<p>If you have any formal database design experience, you likely know that relational databases tend toward highly normalized designs to eliminate duplicate data storage, and increase stability and data integrity. This normalization breaks out data records into various tables with highly atomic field definitions, and cross-reference tables for aggregating hierarchical data structures. And if you're not familiar with those principles of database design, then it's sufficient to say that it typically improves the usage of space at the cost of access time for a record lookup.</p>
<p>If you have a well-normalized relational database storing information that you would like to access as de-normalized flat records representing their application models, the queries that are used to flatten those records are often time-consuming and redundant. In this case, you might have a cache that stores flattened records, whose design is optimized for use by your application. This means that whenever a record needs to be updated, the process of de-normalizing the data can happen exactly once, sending the flattened structure to your cache. Thus, your application's interaction with the underlying data store can be reduced from a potentially beefy aggregation query against multiple tables to a simple lookup against a single application-specific table.</p>
<p>In this scenario, simply adding an intermediary cache for the long-running data-access operations can guarantee a performance improvement, even if the actual data storage system for the cache is the same as the origin. The primary performance bottleneck being mitigated by the cache is the query operation itself, so even if there is no reduction in network latency, you could still expect to reap some meaningful benefits.</p>
<p>It should be mentioned that this strategy can be applied to any long-running operations in your application flow. I used the example of slow database queries simply because those are the most commonly encountered bottlenecks with larger enterprise systems. However, in your work, you may find it beneficial to cache the results of computationally intensive operations executed within your application's host process. In this case, you'd likely be using an in-memory cache or a cache hosted on your own system, so there would be no possibility of improving your latency. But imagine deploying your application to a cloud hosting provider that's charging you by your application's uptime. In that scenario, cutting out a multi-second calculation from your application's most frequently used workflow could save thousands in compute costs over time. When you're caching the results of a method call or calculation local to your system, this is called <strong>memoization.</strong></p>


            

            
        
    

        

                            
                    <h1 class="header-title">Caching high-latency network requests</h1>
                
            
            
                
<p>Another common motivating factor for caching is high-latency network requests. In this scenario, your software would be dependent on a network resource, but some aspect of the network infrastructure makes accessing that resource unacceptably slow. It could be that your application is hosted behind a firewall, and the request validation protocols for an incoming or outgoing request introduce high latency. Or, it might just be a matter of locality, with your application server hosted in a separate physical region from your nearest data center.</p>
<p>Whatever the reason, a common solution to this problem is to minimize the impact of the network latency by caching the results in a more local data store. Let's suppose, for instance, the issue is a gateway or firewall introducing unacceptable latency to your data access requests. In that case, you could stand up a cache behind the firewall to eliminate the latency it introduces. With this sort of caching strategy, your objective is to store your cached data on some host that introduces less latency than the source. Even if the time to look up a record in your cache is no faster than the time to look up the same record at the source, the minimized latency is the objective.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Caching to preserve state</h1>
                
            
            
                
<p>The last strategy for caching data is to facilitate state management. In cloud-deployed application architectures, you might have a user interacting with multiple instances of your application, running in parallel containers on different servers. However, if their interaction with your application depends on persisting any sort of session state, you'll need to share that state across all instances of your application that might service an individual request over the course of that session. When this is the case, you'll likely use a shared cache that all of the instances of your application can access and read from to determine if a request from a user relies on the state that was determined by another instance.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">When to write to a cache</h1>
                
            
            
                
<p class="mce-root">As I've described it so far, a cache might just sound like a duplicated data store that is optimized for your application. In some sense, that's true, but it's not technically correct, since a cache should never mirror its source system perfectly. After all, if you can store a complete copy of the underlying data store in a higher performance cache, what value is there in the underlying data store in the first place?</p>
<p class="mce-root">Instead, a cache will typically only contain a very small subset of the underlying data store. In most cases, the small size of a cache is necessary for its performance benefits, since even a simple query will scale linearly over the size of the set being queried. But if our data cache is not a perfect mirror of the source system, then we must determine which data makes it into our cache and when.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Pre-caching data</h1>
                
            
            
                
<p>One simple but effective strategy for writing data to a cache is called <strong>pre-caching</strong>. In a system that pre-caches it's data, the developers will determine what is likely to be the lowest-performing or most-frequently requested data access operations, whose results are least<em> </em>likely to change over the lifetime of the application. Once that determination is made, those operations are performed once, usually in the initialization of the application, and loaded into the cache before any requests are received or processed by the application.</p>
<p>The example I mentioned earlier, involving frequently requested reports of quarterly sales data, is an ideal scenario for pre-cached data. In this case, we could request the sales data and run all of the statistical operations necessary for the output of the reports at the startup of our application. Then, we could cache the finished view models for each kind of report the application serviced. Upon receiving a request for a report, our application could reliably query the cache for the view model, and then populate the report template accordingly, saving time and compute costs over the course of the lifetime of the application.</p>
<p>The downside to this approach, though, is that it requires synchronization of any updates to the underlying data with refreshes of the application's cache. In cases where the application and the underlying data store are owned and managed by the same team of engineers, this synchronization is trivial. However, if the data store is owned by a different team from the engineers responsible for the application, the coordination introduces a risk. One failure to synchronize updates could result in stale data being served to customers. To mitigate this risk, you should establish a clear and resilient strategy for refreshing your cache, and automate the task as much as is possible.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">On-demand cache writing</h1>
                
            
            
                
<p>Most implementations of a caching system will be designed to write data to the cache on demand. In an on-demand system, the application will have a need for some piece of data that it can be certain is stored in the underlying database. However, prior to making the slower data access request all the way back to the underlying database, the application will first check for the requested data in the cache. If the data is found, it's called a <strong>cache hit</strong>. With a cache hit, the cache entry is used, and no additional call is made back to the dataset, thus improving the performance of the application.</p>
<p>In the alternative situation, where the requested data hasn't been written to the cache, the application has what's called a <strong>cache miss</strong>. With a miss, the application must make the slower call to the underlying data store. At this point, though, the cost of accessing the requested data from the lower-performing system has been paid. So now, the application can use whatever heuristics have been set for it to determine if the retrieved data should then be written to the cache, thus saving time on subsequent requests for the same piece of data.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Cache replacement strategies</h1>
                
            
            
                
<p>If your cache is of a fixed limited size, you may find yourself needing to define a cache replacement policy. A cache replacement policy is how you determine when to replace older records with newer, potentially more relevant, ones. This will generally happen when your application experiences a cache miss. Once the data is retrieved, the application will determine whether or not to write it to the cache. If it does end up writing a record to the cache, it will need to determine which record to remove. The trouble, though, is that it is very difficult to determine a consistent heuristic for identifying which records won't be needed again soon.</p>
<p>You might have come up with a seemingly obvious answer in your own head just thinking about it; I certainly did when I first learned about this problem. But most of the obvious solutions don't hold up to scrutiny. For example, a fairly popular replacement policy involves eliminating the record that was least recently used. This just means replacing the entry that hasn't generated a cache hit for the longest series of cache queries. However, it may be the case that the longer it's been since a record has been used, the more likely it is that it will be the next record used, with records looked up in a cyclical order. In that case, eliminating the least recently used record would increase the chances of another cache miss in subsequent requests.</p>
<p>Alternatively, you could try the least frequently used replacement policy. This would drop the record with the fewest cache hits out of all records on the system, regardless of how recently those hits occurred. Of course, the drawback for this approach is that, without accounting for recency, you ignore the possibility that a recently used record might have been queried recently because it will become relevant for a series of subsequent operations the user intends to perform with it. By eliminating it due to its low hit rate, and ignoring the recency of the hit, you increase the chances of a cache miss in the future.</p>
<p>Each cache replacement policy has its own drawbacks and should be considered within the context of your application. However, there is a key metric by which you might determine the relative success of your replacement policy. Once your initial heuristic is designed and deployed, you can track your cache's hit ratio. A cache's hit ratio is, quite simply, the number of cache hits divided by the number of cache misses. The closer that number is to 1.0, the better your cache replacement policy is.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Cache invalidation</h1>
                
            
            
                
<p>As you consider your cache replacement strategies, you may find that a piece of information stored in your cache will only be relevant to your application for a short period of time. When that's the case, instead of waiting for a new cache miss to overwrite the irrelevant data, you may want to expire the entry after a certain timeout. This is what's known as <strong>cache invalidation</strong>. Put simply, cache invalidation is the process of determining that a record in your cache should no longer be used to service subsequent requests.</p>
<p>In cases, as I've described, where you have a known time-to-live for any given record written to your cache, invalidating those records is as simple as setting and enforcing an expiration on the record as it's being written. However, there are other cases where it might not be so obvious that a cached record should be invalidated. Consider a web browser caching a response from the server. Without a predetermined expiration date, the browser can't know for sure that the cached response still represents the current state of the server without first checking the server, thus eliminating the performance benefit of the cache.</p>
<p>Since you should never be serving stale or invalid data to your users, you should always design some mechanism for invalidating your cached records. I've just discussed the two most common, and you'd be hard-pressed to find a reason not to implement at least one of them. So, if you have control over the cache itself, as in the case of a cache contained within your application architecture, you should always be diligent to invalidate cache records whenever the underlying data store is updated. And for the cases when you're not in control of your responses being cached, make sure you're always setting a reasonable cache expiration on your responses.</p>
<p>At this point, we've learned about what a cache is, why you might implement one in your software architecture, and what sort of strategies are at your disposal for optimizing its performance. So, now it's time to look at one of the most common caching strategies in modern cloud-deployed network architectures.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Distributed caching systems</h1>
                
            
            
                
<p>In the previous section, I discussed using a cache for the purpose of preserving application state between parallel deployments of the same application. This is one of the most common use cases for caching in modern cloud-based architectures. However, useful as it may be, this sort of distributed session cache can introduce a whole host of challenges to the application design. </p>


            

            
        
    

        

                            
                    <h1 class="header-title">A cache-friendly architecture</h1>
                
            
            
                
<p>Caches have historically been used to improve performance by reducing latency or operation times. With a session cache for distributed architecture, however, the cache itself is not intended to provide any specific performance improvement on a given operation. Instead, it's designed to facilitate a necessary interaction between multiple instances of an application. Its design is to facilitate state management that would otherwise involve complicated orchestration of multiple hosts. To understand how this is done, let's consider an example.</p>
<p>Suppose you have a cloud-hosted API that is responsible for verifying a user's identity and age. To do so, it requests various pieces of information that, taken together, could serve as verification of the user. In an effort to design your user experience to be as non-intrusive as possible, you start by asking only a few questions about their birth date and current address, which are most likely to successfully verify their age and identity. Once they've submitted their answers, your application would attempt to verify them. If it succeeds, your user can proceed, but if the initial set of questions is unsuccessful, your application follows up with a handful more questions that, when combined with the answers from the first set, are highly likely to verify the user's identity. The user submits their answers, and you continue again with failure, resulting in one final question requesting the last four digits of the user's social security number. Upon submission, the user is either successfully verified or rendered permanently unable to access your system.</p>
<p>That process is relatively straightforward from a business-logic perspective, but how would you implement it on a network process that is meant to remain entirely stateless between network requests? And as a cloud-hosted application, how would you maintain the current state of the user's position in that workflow across multiple possible instances of your app server processing a given request?</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The case for a distributed cache</h1>
                
            
            
                
<p>There are a handful of techniques available in this particular case, each with their own advantages and drawbacks. You could use sticky sessions to force subsequent requests from a particular host in a given session to be serviced by the same app server that processed the initial request. This would allow for some minor local state management over the course of a session. The downside to this is that it eliminates the performance benefits of horizontal scaling in a cloud-hosted system. If a user is always forced to interact with a single server over the course of a session, regardless of the traffic to that server or the availability of other servers, they may as well be interacting with a single-instance monolithic application architecture. Moreover, you would no longer be staying true to the architectural ideal of a "stateless" service, as you would be implementing some mechanism for preserving a user's position in the workflow over the course of interaction on your active service.</p>
<p>Alternatively, you could use the same principle we saw in the <em>Self-encoded tokens</em> section of <a href="bf84cf6c-16d3-4225-b590-b3657aaa3832.xhtml">Chapter 14</a>, <em>Authentication and Authorization on Networks</em>. In this case, you'd be self-encoding the user's current state in the response from your server, and your user would be responsible for returning that self-encoded state back to the server in subsequent requests. This allows each request body to serve as a breadcrumb trail leading back to the first interaction with the client, from which the server could rebuild the state that was created in previous interactions with each subsequent interaction.</p>
<p>This approach will increase the complexity of your request/response models, though. It will also introduce the added risk of unenforceable limits on verification attempts. Suppose, for security purposes, your business rules dictate that a user should only be allowed to attempt each round of questions once. If you self-encode the state of the user session in your request/response models, you're relying on your users to return an accurate representation of each previous attempt with each of their requests. It would be easy for a dedicated malicious actor to make as many attempts as they please by simply scrubbing the workflow state from their subsequent requests.</p>
<p>In this scenario, I would argue that the most reliable solution for maintaining state across requests is a distributed cache shared by each app server in your cloud environment. This prevents you from maintaining state across your app servers, thus preserving the stateless principle of cloud-deployed service architecture, while still allowing your services to maintain full control over a user's progression through your verification workflow.</p>
<p>To implement this, you would host the cache provider on its own server, independently of any instances of your application servers in your cloud. Any server that successfully processes a given step in the workflow would refuse to fully resolve the interaction and provide a response to the client unless and until the result of that step was successfully written back to your data cache. In this way, the current app server instance could do the following:</p>
<ul>
<li>Verify that no other instances have successfully processed the same request already by confirming that no record of the workflow step exists for the user in the cache</li>
<li> Notify other instances of the application that the step had been processed so that they would cease to duplicate the transaction</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">The benefits of a distributed cache</h1>
                
            
            
                
<p>The system I've described has the benefit of ensuring the consistency of the user state throughout their interaction across all<em> </em>deployed instances of your application, with only the minor orchestration of reading from and writing to a cache. This data consistency, is key even when your distributed cache is not used for state management. It can prevent multiple instances of an application from trying to perform two incompatible modifications to the same piece of data, or allow synchronization of transactions across multiple app servers prior to committing them to the underlying database. And most importantly, from the developer's perspective, it can eliminate difficult to reproduce and difficult to track down bugs caused by race conditions between services.</p>
<p>Hosting the cache server independently of all other applications also provides it with a measure of resilience against app restarts or crashes. By isolating your data store, you can isolate costs associated with higher availability and resiliency guarantees from your cloud provider. It can also help to minimize the memory footprint of your application containers living on your app servers. If you pay for RAM usage, this can save you thousands as your cache scales out. So, how exactly do we reap these benefits in our code?</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Working with caches in code</h1>
                
            
            
                
<p>To see how we can benefit from the various caching mechanisms supported by .NET Core, we'll be setting up a somewhat complicated demo application structure. The first thing we'll do is create a remote data store that has long-running operations to return results from queries. Once that's done, we'll set up an application dependent on that data, and provide it a caching strategy to mitigate the impact of our artificially slowed down remote data storage. </p>


            

            
        
    

        

                            
                    <h1 class="header-title">Writing our backing data system</h1>
                
            
            
                
<p>We'll be creating our backing data system as a simple Web API project. The goal is to expose a couple of endpoints on a single controller that expose data of different types to demonstrate how we can write the values to our cache regardless of the type discrepancy between the records. First, let's create our project with the .NET Core CLI. Let's look at the following command:</p>
<pre><strong>dotnet new webapi -n DataSimulation</strong></pre>
<p>Next, since we'll be hosting this project at the same time as our cache-ready application, we'll want to configure it to use its own port, instead of the default settings. Within your <kbd>Program.cs</kbd> file, modify your <kbd>CreateWebHostBuilder(string[] args)</kbd> method to use whatever custom URLs you want this application to listen on:</p>
<pre>public static IWebHostBuilder CreateWebHostBuilder(string[] args) =&gt;<br/>  WebHost.CreateDefaultBuilder(args)<br/>    .UseUrls("https://[::]:33333")<br/>    .UseStartup&lt;Startup&gt;();</pre>
<p>Then, we'll modify the <kbd>ValuesController.cs</kbd> class to serve up our data. First, we'll change the name of the class to <kbd>DataController</kbd> so that our routing is a bit more intuitive. We'll be getting rid of all of the preconfigured endpoints and replacing them with three new endpoints, each returning a unique data type. First, though, let's create a new data type for us to return. It will be a simple model with an ID and two arbitrary properties; one will be of the <kbd>string</kbd> type, and the other will be a <kbd>List&lt;string&gt;</kbd>:</p>
<pre>public class OutputRecord {<br/>  public int Id { get; set; }<br/>  public string SimpleString { get; set; }<br/>  public List&lt;string&gt; StringList { get; set; } = new List&lt;string&gt;();<br/>}</pre>
<p>With this model set up, we can define the endpoints we'll be exposing. For this demonstration, we'll return a simple <kbd>List&lt;string&gt;</kbd> string, a single <kbd>OutputRecord</kbd> instance, and a <kbd>List&lt;OutputRecord&gt;</kbd> method. So, by the time we've defined a lookup endpoint for each data type, we'll have methods returning simple strings, lists of strings, complex records, and lists of complex records. Let's look at the following code:</p>
<pre>public class DataController : ControllerBase {<br/><br/>  [HttpGet("value/{id}")]<br/>  public ActionResult&lt;string&gt; GetString(int id) {<br/>    return $"{id}: some data";<br/>  }<br/><br/>  [HttpGet("values/{id}")]<br/>  public ActionResult&lt;IEnumerable&lt;string&gt;&gt; GetStrings(int id) {<br/>    return new string[] { $"{id}: value1", $"{id + 1}: value2" };<br/>  }</pre>
<p>These define our simple string responses, and will be relatively straightforward to test with our cache. For our <kbd>OutputRecord</kbd> endpoints, though, we'll want to apply unique data to each property so that we can confirm that the full object is properly cached. So, the endpoint returning a single <kbd>OutputRecord</kbd> instance will look like this:</p>
<pre>[HttpGet("record/{id}")]<br/>public ActionResult&lt;OutputRecord&gt; GetRecord(int id) {<br/>  return new OutputRecord() {<br/>    Id = id,<br/>    SimpleString = $"{id}: value 1",<br/>    StringList = new List&lt;string&gt; {<br/>      $"{id}:value 2",<br/>      $"{id}:value 3"<br/>    }<br/>  };<br/>}</pre>
<p class="mce-root">This gives us an object with distinct property values, tied together by the same ID, which will make it easy for us to validate the behavior of our cache. Finally, we'll define an endpoint to return a list of the <kbd>OutputRecord</kbd> instances:</p>
<pre>[HttpGet("records/{id}")]<br/>public ActionResult&lt;IEnumerable&lt;OutputRecord&gt;&gt; GetRecords(int id) {<br/>  return new List&lt;OutputRecord&gt;(){<br/>    new OutputRecord() {<br/>      Id = id,<br/>      SimpleString = $"{id}: value 1",<br/>      StringList = new List&lt;string&gt; {<br/>        $"{id}:value 2",<br/>        $"{id}:value 3"<br/>      }<br/>    }, new OutputRecord() {<br/>      Id = id + 1,<br/>      SimpleString = $"{id + 1}: value 4",<br/>      StringList = new List&lt;string&gt; {<br/>        $"{id + 1}:value 5",<br/>        $"{id + 1}:value 6"<br/>      }<br/>    }<br/>  };<br/>}</pre>
<p>Each of these endpoints returns some trivial object or string with the provided ID that's used in the response objects, but this will just be a way of distinguishing one response from the next. The important aspect of our responses will be the perceivable delay we'll be applying. For that, we'll a five second delay to each method prior to returning their result. This will give us an obvious way to identify when the backing data store has been hit versus when our user-facing application has a successful cache hit.</p>
<p>To simulate this delay, we'll sleep the current thread for five seconds, and then return some arbitrary string that incorporates the given ID:</p>
<pre>[HttpGet("value/{id}")]<br/>public ActionResult&lt;string&gt; GetString(int id) {<br/>    Thread.Sleep(5000);<br/>    return $"{id}: some data";<br/>}</pre>
<p>Each additional method will do the same thing, applying the delay and then initializing its expected return type with its arbitrary values. Now, if you run the application and ping your <kbd>/data/value/1234</kbd> endpoint, you should see the result come back after five seconds:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-937 image-border" src="img/e26b5043-1f91-4888-a09d-441220d4f5ab.png" style="width:79.67em;height:32.58em;"/></p>
<p>Note the response time of 5269ms. This delay will be our indication of a cache miss, going forward. And with our data store ready, we can build our application and define its caching strategy.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Leveraging a cache</h1>
                
            
            
                
<p>To start working with our cache, we'll first install and run a local instance of a Redis server. Redis is an open source, in-memory data store. It's frequently used in enterprise deployments as a simple key-value data store or cache. It's also supported out of the box by Azure cloud hosting environments, making it very popular for .NET based microservices and cloud-based applications.</p>
<p>To install it, follow the instructions in the <em>Technical requirements</em> section of this chapter. Once you've done so, you'll have your local instance running. If you've already installed the server, make sure it's up and running by opening your Windows Subsystem for Linux interface, and enter the following commands to verify its listening port:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-938 image-border" src="img/f5d37ee1-04b1-41e7-b710-bcf809ee71b5.png" style="width:44.50em;height:9.17em;"/></p>
<p>Once you've got your Redis instance running, you'll be ready to implement your sample microservice. Since we'll be loading cache misses from our backend API, we'll want to configure <kbd>HttpClient</kbd> for that particular application in our <kbd>Startup.cs</kbd> file. For this, I've created a static <kbd>Constants</kbd> class just to avoid magic strings used in my code, and used a <kbd>DATA_CLIENT</kbd> property to register a named instance of <kbd>HttpClient</kbd> inside my <kbd>ConfigureServices(IServiceCollection services)</kbd> method:</p>
<pre>services.AddHttpClient(Constants.DATA_CLIENT, options =&gt; {<br/>  options.BaseAddress = new Uri("https://localhost:33333");<br/>  options.DefaultRequestHeaders.Add("Accept", "application/json");<br/>});</pre>
<p>Next, we'll create a service client to abstract the details of the HTTP requests we'll be making behind a clean data-access interface, using the same patterns we established in Chapter 9,<em> HTTP in .NET</em>. Our interface definition will provide the following simple methods:</p>
<pre>public interface IDataService{<br/>  Task&lt;string&gt; GetStringValueById(string id);<br/>  Task&lt;IEnumerable&lt;string&gt;&gt; GetStringListById(string id);<br/>  Task&lt;DataRecord&gt; GetRecordById(string id);<br/>  Task&lt;IEnumerable&lt;DataRecord&gt;&gt; GetRecordListById(string id);<br/>}</pre>
<p class="mce-root">Within the implementing class for this interface, we'll have a private instance of  <kbd>IHttpClientFactory</kbd>, and we'll be using our named <kbd>HttpClient</kbd> instance to access our backend data store. This common task is isolated to a private method for the actual HTTP interaction:</p>
<pre>private async Task&lt;string&gt; GetResponseString(string path) {<br/>    var client = _httpFactory.CreateClient(Constants.DATA_CLIENT);<br/>    var request = new HttpRequestMessage(HttpMethod.Get, path);<br/>    var response = await client.SendAsync(request);<br/>    return await response.Content.ReadAsStringAsync();<br/>}</pre>
<p>Then, each of the public interface methods implements an endpoint-specific variation of the general pattern established here:</p>
<pre>public async Task&lt;DataRecord&gt; GetRecordById(string id) {<br/>    var respStr = await GetResponseString($"api/data/record/{id}");<br/>    return JsonConvert.DeserializeObject&lt;DataRecord&gt;(respStr);<br/>}</pre>
<p>Extending this logic for all four of our access methods, we'll complete our backend data client. At this point, we should modify our controller to expose each of the backend API endpoints, and use them to test our data-access service. We'll expose the same service contract we had in our backend API, with four endpoints for each type of record we could look up. Instead of renaming our file, we'll just redefine our controller's route, and define a public constructor to allow the dependency injection framework to provide our <kbd>DataService</kbd> instance (just don't forget to register the concrete implementation in your <kbd>Startup.cs</kbd>). Lets, look at the following code:</p>
<pre>[Route("api/cache-client")]<br/>[ApiController]<br/>public class ValuesController : ControllerBase {<br/><br/>    private IDataService _dataService;<br/><br/>    public ValuesController(IDataService data) {<br/>        _dataService = data;<br/>    }<br/>    ...</pre>
<p>Once we have our data service, we can use our API endpoints to call into each requested object from our backend system:</p>
<pre>[HttpGet("value/{id}")]<br/>public async Task&lt;ActionResult&lt;string&gt;&gt; GetValue(string id) {<br/>    return await _dataService.GetStringValueById(id);<br/>}<br/><br/>[HttpGet("values/{id}")]<br/>public async Task&lt;IEnumerable&lt;string&gt;&gt; GetValues(string id) {<br/>    return await _dataService.GetStringListById(id);<br/>}<br/><br/>[HttpGet("record/{id}")]<br/>public async Task&lt;ActionResult&lt;DataRecord&gt;&gt; GetRecord(string id) {<br/>    return await _dataService.GetRecordById(id);<br/>}<br/><br/>[HttpGet("records/{id}")]<br/>public async Task&lt;IEnumerable&lt;DataRecord&gt;&gt; Get(string id) {<br/>    return await _dataService.GetRecordListById(id);<br/>}</pre>
<p>At this point, by running both your backend API and your cache service API, you should be able to request the same values from your cache service, with the same five second delay. So, now that our application is fully wired up to request data from our backend service, let's improve its performance with caching.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The distributed cache client in .NET</h1>
                
            
            
                
<p>One of the major benefits of using Redis for our distributed caching solution is that it's supported by .NET Core out of the box. There's even an extension method on the <kbd>IServicesCollection</kbd> class specifically for registering a Redis cache for use within your application. Simply install the <kbd>Microsoft.Extensions.Caching.Redis</kbd> NuGet package for your current project, and then add the following code:</p>
<pre>services.AddDistributedRedisCache(options =&gt; {<br/>    options.Configuration = "localhost";<br/>    options.InstanceName = "local";<br/>});</pre>
<p>This will automatically register an instance of the <kbd>RedisCache</kbd> class as the concrete implementation for any instances of <kbd>IDistributedCache</kbd> you inject into any of your services. The localhost configuration setting will use the default configurations for a local deployment of the Redis client, so there's no need to specify an IP address and port unless you explicitly change it on your local deployment. Meanwhile, the <kbd>InstanceName</kbd> field will give the entries stored in the cache that were set by this application an application-specific prefix. So, in this example, if I set a record with the  <kbd>1234</kbd> key with my setting of local, that key will be stored in the cache as <kbd>local1234</kbd>. The <kbd>RedisCache</kbd> instance that is registered by the <kbd>AddDistributedRedisCache()</kbd> method will automatically look for keys with the <kbd>InstanceName</kbd> prefix that we've specified in our options. We'll see this later when we inspect our cache instance. </p>
<p>With our Redis cache running, and our <kbd>IDistributedCache</kbd> instance configured and registered with our dependency injection container, we can write a <kbd>CacheService</kbd> class. This will follow a similar pattern to our <kbd>DataService</kbd> class, where it exposes only a small number of logical operations as public methods, hiding the details of the cache interactions. Our interface for this <kbd>CacheService</kbd> class is as follow:</p>
<pre>public interface ICacheService {<br/>    Task&lt;bool&gt; HasCacheRecord(string id);<br/>    Task&lt;string&gt; FetchString(string id);<br/>    Task&lt;T&gt; FetchRecord&lt;T&gt;(string id);<br/>    Task WriteString(string id, string value);<br/>    Task WriteRecord&lt;T&gt;(string id, T record);<br/>}</pre>
<p>Here, we're making the distinction between writing a single string and writing a more complex record to distinguish between the need to serialize and deserialize our entries in each method implementation.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Getting and setting cache records</h1>
                
            
            
                
<p>The <kbd>IDistributedCache</kbd> class provides a simple mechanism for interacting with our cached data. It operates on a dumb get/set pattern whereby attempts to get a record will either return the cached byte array or string based on the given ID, or return null if no record exists. There's no error handling or state checking. The speed of the cache is dependent on this simple interaction mechanism and fail state.</p>
<p>Likewise, setting a record is equally easy. Simply define your ID for the record, and then provide some serialized representation of the record for storage. This serialized format can be either a string with the <kbd>SetString(string id, string value)</kbd> method, or a byte array using the <kbd>Set(string id, byte[] value)</kbd> method.</p>
<p>Additionally, when you write a value to the cache, you can set additional options for your cache record to specify expiration time spans. The kinds of expiration settings you can apply are as follows:</p>
<ul>
<li><strong>AbsoluteExpiration</strong>: This sets the expiration to a specific moment in time at which point the record will be invalidated, no matter how recently it has been used.</li>
<li><strong>AbosluteExpirationRelativeToNow</strong>: This sets a fixed moment at which the record will be invalidated no matter how recently it has been used. The only difference with this and AbsoluteExpiration is that the expiration time is expressed in terms of some length of time from the moment the record is set in the cache.</li>
<li><strong>SlidingExpiration</strong>: This sets an expiration time relative to the last time the record was accessed. So, if the sliding expiration is set for 60 minutes, and the record isn't accessed again for 62 minutes, it will have expired. However, if the record is accessed again in 58 minutes, the expiration is reset for 60 minutes from that second access.</li>
</ul>
<p>So, let's look at how we'll implement this cache. First, we've got to inject the <kbd>IDistributedCache</kbd> instance that was registered in our <kbd>Startup.cs</kbd> class:</p>
<pre>public class CacheService : ICacheService {<br/>    IDistributedCache _cache;<br/><br/>    public CacheService(IDistributedCache cache) {<br/>        _cache = cache;<br/>    }</pre>
<p>Then, we'll implement the methods of our interface. The first method is fairly straightforward and only notifies our consumers if there has been a cache hit:</p>
<pre>public async Task&lt;bool&gt; HasCacheRecord(string id) {<br/>    var record = await _cache.GetStringAsync(id);<br/>    return record != null;<br/>}</pre>
<p>Next, we'll implement our record retrieval methods. The only difference with each of these is that retrieval of complex data types (records and lists of strings) will require an extra step of deserialization. Other than that, though, our <kbd>Fetch...()</kbd> methods should look fairly straightforward:</p>
<pre>public async Task&lt;string&gt; FetchString(string id) {<br/>    return await _cache.GetStringAsync(id);<br/>}<br/><br/>public async Task&lt;T&gt; FetchRecord&lt;T&gt;(string id) {<br/>    var record = await _cache.GetStringAsync(id);<br/>    T result = JsonConvert.DeserializeObject&lt;T&gt;(record);<br/>    return result;<br/>}</pre>
<p>Finally, we'll need to implement the write methods. For the sake of demonstration, we'll write all of our records with a 60-minute sliding expiration time using the <kbd>DistributedCacheEntryOptions</kbd> class. After that, we can simply pass in our key to the cache, along with a serialized value (we'll be using JSON here, to take advantage of the <kbd>Newtonsoft.Json</kbd> libraries) and our expiration options:</p>
<pre>public async Task WriteString(string id, string value) {<br/>    DistributedCacheEntryOptions opts = new DistributedCacheEntryOptions() {<br/>        SlidingExpiration = TimeSpan.FromMinutes(60)<br/>    };<br/>    await _cache.SetStringAsync(id, value, opts);<br/>}<br/><br/>public async Task WriteRecord&lt;T&gt;(string id, T record) {<br/>    var value = JsonConvert.SerializeObject(record);<br/>    DistributedCacheEntryOptions opts = new DistributedCacheEntryOptions() {<br/>        SlidingExpiration = TimeSpan.FromMinutes(60)<br/>    };<br/><br/>    await _cache.SetStringAsync(id, value, opts);<br/>}</pre>
<p>And with that, our cache should be ready to use. Now, it's time to pull it all together in our controller endpoints. For this, the interaction pattern will be the same across each method, with the only difference being the type of read/write operation we perform on our cache. So Let's look at how we'll implement our cache strategy:</p>
<pre>[HttpGet("record/{id}")]<br/>public async Task&lt;ActionResult&lt;DataRecord&gt;&gt; GetRecord(string id) {<br/>    var key = $"{id}record";<br/>    if (await _cache.HasCacheRecord(key)) {<br/>        return await _cache.FetchRecord&lt;DataRecord&gt;(key);<br/>    }<br/>    var value = await _dataService.GetRecordById(id);<br/>    await _cache.WriteRecord(key, value);<br/>    return value;<br/>}</pre>
<p>The first thing you'll notice is that I apply a suffix to our given ID that matches my route. This is to allow duplicate IDs in my cache for each distinct data type. Next, we check our <kbd>HasCacheRecord</kbd> (key) method to determine whether we have a cache hit. If we do, we simply fetch the cache record and return the result. When we have a miss, though, we have to fetch the data from our underlying data store. Once we have it, we write it to our cache for faster retrieval in any subsequent requests, and then return the value.</p>
<p>After applying this pattern with the appropriate modifications to each of our endpoints, we're ready to test. To confirm the behavior of our cache, first run the same query against any endpoint with a new ID, twice in a row. If everything's working properly, you should have a five second delay on your first request, and almost zero delays on your subsequent request.</p>
<p>Once you have at least a record or two stored in your cache, you can observe the values with your redis-cli in your Windows Subsystem for Linux console. The <kbd>RedisCache</kbd> class will store the entries as hash types in the underlying cache, so you'll need to look for the key values using those commands. The operations I performed to look up the records I wrote while testing the app are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-939 image-border" src="img/9584f3f5-0bfa-480a-b9dd-01b711217714.png" style="width:42.58em;height:12.75em;"/></p>
<p>The first command, <kbd>keys *</kbd>, simply searches all active keys that match the given pattern (* is the wildcard, so <kbd>keys *</kbd> matches all keys). Then, I used the <kbd>hgetall [key]</kbd> command to get each property in my entry's hash. In that output, you can clearly see the JSON written to the cache from my application, demonstrating the successful and expected interactions between my app and my cache.</p>
<p>I'd also like to point out the key structure. As I mentioned before, the keys I set (in this case, 2345 records) are prefixed with  <kbd>InstanceName</kbd> of  <kbd>RedisCacheOptions</kbd>, with which I configured  <kbd>RedisCache</kbd> in the <kbd>Startup.cs</kbd> file. And with that output, you've seen the full interaction pattern established by Microsoft for working with a Redis cache instance.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Cache providers</h1>
                
            
            
                
<p>While we demonstrated the use of a data cache with an instance of the <kbd>IDistributedCache</kbd> class in our sample code, that is hardly the only cache provider we have access to with .NET Core. Before we close out the subject of caches, I just want to briefly discuss the other two most common providers in the framework.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The SqlServerCache provider</h1>
                
            
            
                
<p>Redis is certainly popular among engineers as being a high-performance cache implementation. However, it's hardly the only distributed provider out there. In fact, Microsoft's own SQL Server can serve as a cache when the situation calls for it, and they've defined a similar implementation for the <kbd>IDistributedCache</kbd> class to expose it.</p>
<p>One of the biggest differences with the <kbd>SqlServerCache</kbd> provider and the <kbd>RedisCache</kbd> instance is in the configuration it requires. Where Redis is a simple key-value store, <kbd>SqlServer</kbd> remains a full-featured relational database. Thus, to provide the lightweight interactions necessary for a high performing cache, you'll have to specify the precise schema, table, and database connection you intend to leverage when you set it as your <kbd>IDistributedCache</kbd> provider. And since SQL Server doesn't support the hash tables that Redis does, the table to which your application connects for caching should implement the expected structure of an <kbd>IDistributedCache</kbd> record. Thankfully, the .NET Core CLI provides a utility command for establishing just such a table: the <kbd>sql-cache create</kbd> command. And notably, since your application should only ever be interacting with injected instances of <kbd>IDistributedCache</kbd>, you won't even notice the difference, except perhaps in performance. However, for the sake of performance, I would recommend using Redis wherever possible. It is quickly becoming the industry standard and its speed is truly unmatched by SQL Server.</p>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">The MemoryCache provider</h1>
                
            
            
                
<p>Finally, if your application either doesn't have the need, or doesn't have the means, to support a standalone cache instance, you can always leverage an in-memory caching strategy. The <kbd>MemoryCache</kbd> class of the <kbd>System.Runtime.Caching</kbd> namespace will provide exactly that. Configuring it is as simple as invoking the <kbd>services.AddMemoryCache()</kbd> method in <kbd>Startup.cs</kbd>, and it provides a similar interface to the <kbd>IDistributedCache</kbd> class we've already looked at.</p>
<p>It does bring some major caveats with it, however. Since you're hosting the cache within your application's own process, memory becomes a much more valuable resource. Disciplined use of a cache replacement policy, and an aggressive expiration time, becomes much more important with an in-memory caching solution. Additionally, since any state that must persist over the lifetime of a session will only be persisted within a single instance of your application, you'll need to implement sticky sessions. This will ensure that users will always interact with the app server that has their data cached in its memory.</p>
<p>Ultimately, your business needs and environmental constraints will play a large role in determining what sort of caching policies and strategies you should be taking advantage of in your application. However, with the information in this chapter, you should be well-suited to making the best possible decisions for your circumstances. Meanwhile, we'll be continuing our consideration of performance optimization in the next chapter as we consider performance monitoring and data tracing in a network-hosted application.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we took an extensive tour of the motivations for, and use cases of, data caches in a distributed network application. We started by exploring some common business and design problems that would likely reap the benefits of a caching strategy. In doing so, we identified some of the basic considerations you can make when determining if the complexity of introducing a cache management system is the right decision for your application. Then, we looked at exactly which benefits could be gained from caching, and precisely how caching can provide them.</p>
<p>Once we learned why we might use a cache, we looked at some of the common problems that must be solved for when implementing a cache. First, we tackled the tactics of pre-caching data and caching results on demand. Then, we looked at how to determine which data or resources should be cache. We learned about establishing a cache replacement policy that is well-suited to your application's most common data interactions, and how to invalidate records in your cache to make sure you're never returning stale results.</p>
<p>Finally, we saw how we could use a cache in our applications. We learned how to run a distributed cache, and we saw how to write to and read from that cache within the code. We saw that a cache record could be an arbitrary data structure with an arbitrary key, and how to detect hits within our cache instance. Finally, we looked at alternative caching mechanisms available with C# and .NET Core.</p>
<p>In the next chapter, we'll continue with our focus on optimizing our application performance for a network, and look at the tools available to us for monitoring our application's performance and identifying any bottlenecks in our network.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Questions</h1>
                
            
            
                
<ol>
<li>What are three criteria that should motivate a caching strategy?</li>
<li>What are three common pain points caches can help resolve?</li>
<li>What is a cache hit? What is a cache miss?</li>
<li>What is a cache replacement policy? What are some common cache replacement policies?</li>
<li>What is a hit ratio, and how does it relate to a replacement strategy?</li>
<li>What is cache invalidation?</li>
<li>What are some of the benefits of using a distributed cache?</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Further reading</h1>
                
            
            
                
<p>For even more hands-on guidance for building caches in a modern .NET context, I recommend the book <em>The Modern C# Challenge</em>, by <em>Rod Stephens</em>. It takes a deep dive into the same sorts of patterns and practices we discussed in this chapter with an incredibly approachable presentation. It can be found through Packt publishing, here: <a href="https://www.packtpub.com/application-development/modern-c-challenge-0">https://www.packtpub.com/application-development/modern-c-challenge-0.</a><br/>
<br/>
Alternatively, if you want to consider other challenges inherent to distributed, horizontally scaled application architectures, you should check out <em>Microservice Patterns and Best Practices</em> by <em>Vinicius Feitosa Pacheco</em>. It's also available from Packt, and you can get it here: <a href="https://www.packtpub.com/application-development/microservice-patterns-and-best-practices">https://www.packtpub.com/application-development/microservice-patterns-and-best-practices.</a></p>


            

            
        
    </body></html>