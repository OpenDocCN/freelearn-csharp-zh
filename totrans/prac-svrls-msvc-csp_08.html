<html><head></head><body>
<div><h1 class="chapterNumber"><a id="_idTextAnchor205"/>8</h1>
<h1 class="chapterTitle" id="_idParaDest-148"><a id="_idTextAnchor206"/>Practical Microservices Organization with Kubernetes</h1>
<p class="normal">This chapter is dedicated to a fundamental building block of microservice applications: orchestrators! The focus <a id="_idIndexMarker511"/>is on <strong class="keyWord">Kubernetes</strong>, but the concepts learned here are fundamental for understanding other orchestration options. In particular, <strong class="keyWord">Azure Container Apps</strong> is a serverless <a id="_idIndexMarker512"/>alternative to Kubernetes, implemented with Kubernetes itself, and uses simplified configuration options, but the objects to configure and concepts involved are exactly the same. Azure Container Apps is described in <a href="Chapter_9.xhtml#_idTextAnchor261"><em class="italic">Chapter 9</em></a><em class="italic">, Simplifying Containers and Kubernetes: Azure Container Apps</em><em class="italic"> and other Tools</em>.</p>
<p class="normal">All concepts will be exemplified with small examples and with the car-sharing book case study applicati<a id="_idTextAnchor207"/>on. After a general description of orchestrators’ role and functionalities, we will describe how to configure and interact in practice with a Kubernetes cluster. We will use <strong class="keyWord">Minikube</strong>, which is a local <a id="_idIndexMarker513"/>simulator of a Kubernetes cluster, throughout the chapter. However, we will also explain how to create and use a Kubernetes Azure cluster.</p>
<p class="normal">We will also describe how to test and debug the interaction of some microservices during development with <strong class="keyWord">Docker </strong>first, and then <a id="_idIndexMarker514"/>the complete application running in a Kubernetes <a id="_idIndexMarker515"/>cluster. A .NET-specific alternative for testing a microservices application in the development stage is <strong class="keyWord">.NET Aspire</strong>, which will be described in <a href="Chapter_12.xhtml#_idTextAnchor345"><em class="italic">Chapter 12</em></a><em class="italic">, Simplifying Microservices with .NET Aspire</em>.</p>
<p class="normal">More specifically, this chapter cov<a id="_idTextAnchor208"/>ers:</p>
<ul>
<li class="bulletList">Introduc<a id="_idTextAnchor209"/>tion to orchestrators and their configuration</li>
<li class="bulletList">Kubernetes basics</li>
<li class="bulletList">Interacting with Kubernetes: Kubectl and Minikube</li>
<li class="bulletList">Configuring your application in Kubern<a id="_idTextAnchor210"/>etes</li>
<li class="bulletList">Running your microservices on Kubernetes</li>
<li class="bulletList">Advanced Kubernetes configuration</li>
</ul>
<h1 class="heading-1" id="_idParaDest-149"><a id="_idTextAnchor211"/>Technical requirements</h1>
<p class="normal">This chapter requires:</p>
<ol>
<li class="numberedList" value="1">At least the Visual Studio 2022 free <em class="italic">community edition</em>.</li>
<li class="numberedList">An SQL instance accepting TCP/IP requests and user/password authentication, and <strong class="keyWord">Docker Desktop</strong> for Windows, the installation for which was explained in the <em class="italic">Technical requirements</em> section of <a href="Chapter_7.xhtml#_idTextAnchor151"><em class="italic">Chapter 7</em></a><em class="italic">, Microservices in Practice</em>.</li>
<li class="numberedList"><strong class="keyWord">If you would like to interact with a Kubernetes cluster on Azure, you need Azure CLI. The page at </strong><a href="https://learn.microsoft.com/bs-latn-ba/cli/azure/install-azure-cli-windows?tabs=azure-cli">https://learn.microsoft.com/bs-latn-ba/cli/azure/install-azure-cli-windows?tabs=azure-cli</a><strong class="keyWord"> contains the links to both the 32-bit and 64-bit Windows installers.</strong></li>
<li class="numberedList"><strong class="keyWord">Minikube</strong>: The easiest way to install Minikube is by using the Windows installer you can find on the official installation page: <a href="https://minikube.sigs.k8s.io/docs/start/">https://minikube.sigs.k8s.io/docs/start/</a>. During the installation, you will be prompted on the kind of virtualization tool to use – please specify Docker. The previous link also gives a PowerShell command for adding <code class="inlineCode">minicube.exe</code> to the Windows path.</li>
<li class="numberedList"><strong class="keyWord">Kubectl</strong>: First of all, verify if it is already installed by opening a Windows console and issuing this command: <code class="inlineCode">Kubectl -h</code>. If the response is the list of all Kubectl commands, it is already installed. Otherwise, the simplest way to install it is through the <strong class="keyWord">Chocolatey</strong> package installer:
        <pre class="programlisting con-one"><code class="hljs-con">choco install kubernetes-cli
</code></pre>
</li>
<li class="numberedList">If Chocolatey is not already installed, you can install it by launching <strong class="keyWord">PowerShell</strong> in administrative mode and then issuing the PowerShell command suggested on the official Chocolatey page: <a href="https://chocolatey.org/install#individual">https://chocolatey.org/install#individual</a>. You can launch PowerShell<strong class="keyWord"> </strong>in administrative mode as follows:<ol class="alphabeticList level-2" style="list-style-type: lower-alpha;">
<li class="alphabeticList level-2" value="1">Search <strong class="keyWord">PowerShell</strong> in the Windows search box.</li>
<li class="alphabeticList level-2">Right-click on the <strong class="keyWord">PowerShell</strong> link and select to execute it as an administrator.</li>
</ol>
</li>
</ol>
<p class="normal">You can find the sample code for this chapter at <a href="https://github.com/PacktPublishing/Practical-Serverless-and-Microservices-with-Csharp">https://github.com/PacktPublishing/Practical-Serverless-and-Microservices-with-Csharp</a>.</p>
<h1 class="heading-1" id="_idParaDest-150"><a id="_idTextAnchor212"/>Introduction to orchestrators and their configuration</h1>
<p class="normal">Orchestrators were mainly conceived for balancing microservices’ load. Therefore, one might ask if they are necessary for all applications. I can’t say they are necessary, but, for sure, renouncing them <a id="_idIndexMarker516"/>doesn’t mean just manually configuring where to place each replica of each microservice. We should also find efficacious solutions for dynamically reconfiguring the number of replicas and their locations, for balancing the load among several replicas allocated on different servers, and for balancing the traffic among the various replicas of each microservice.</p>
<p class="normal">The above <a id="_idIndexMarker517"/>simple considerations show that an efficacious orchestrator should offer at least the following services:</p>
<ol>
<li class="numberedList" value="1">Accepting high-level specifications and translating them into actual allocations of microservice replicas on different servers of a given cluster.</li>
<li class="numberedList">Providing a unique virtual address for all replicas of the same microservices and automatically splitting the traffic among them. This way, the code of each microservice can reference just this unique virtual address without caring where each replica is.</li>
<li class="numberedList">Recognizing faulty replicas, killing them, and replacing them with newly created replicas.</li>
<li class="numberedList">Downloading microservices container images from container registries.</li>
</ol>
<p class="normal">Moreover, since microservice replicas are ephemeral and can be destroyed and moved from one server to another, they can’t use the disk storage of the servers that host them. Instead, they must use network storage. Orchestrators must also provide simple ways to allocate disk storage and mount it inside the containers where the microservices run. In general, they must provide easy ways of projecting everything that can be projected inside a container, namely:</p>
<ol>
<li class="numberedList" value="1">Disk storage</li>
<li class="numberedList">Environment variables</li>
<li class="numberedList">Communication ports</li>
</ol>
<p class="normal">As a matter of fact, each orchestrator also offers other services, but the seven services listed above are the starting point for learning and assessing any orchestrator.</p>
<p class="normal">The behavior of an orchestrator is controlled with tree-like settings coming from various sources: configuration files, command arguments, and so on. Behind the curtain, all sources are packaged by a <a id="_idIndexMarker518"/>client that communicates with an orchestrator web API.</p>
<p class="normal">All possible orchestrator settings are organized like .NET configuration settings in a tree data structure. Therefore, analogously to .NET settings, they can be provided in JSON format or other equivalent formats. As a matter of fact, all orchestrators accept settings either in JSON or in another equivalent format called <code class="inlineCode">.yaml</code>. Some orchestrators accept both formats; others might accept just one of them. The <code class="inlineCode">.yaml</code> format is described in the next subsection.</p>
<h2 class="heading-2" id="_idParaDest-151"><a id="_idTextAnchor213"/>.yaml files</h2>
<p class="normal"><code class="inlineCode">.yaml</code> files, like JSON files, can be used to describe nested objects and collections in a human-readable way, but they do it with a different syntax. You have objects and lists, but object properties are <a id="_idIndexMarker519"/>not surrounded by <code class="inlineCode">{}</code>, and lists are not surrounded by <code class="inlineCode">[]</code>. Instead, nested objects are declared by simply indenting their content with spaces. The number <a id="_idIndexMarker520"/>of spaces can be freely chosen, but once they’ve been chosen, they must be used consistently.</p>
<p class="normal">List items can be distinguished from object properties by preceding them with a hyphen (-). Below, there is an example involving nested objects and collections:</p>
<pre class="programlisting code"><code class="hljs-code">Name: John
Surname: Smith
Spouse:
Name: Mary
Surname: Smith
Addresses:
- Type: home
Country: England # I am a comment
Town: London
Street: My home street
- Type: office
Country: England
Town: London
Street: My home street
</code></pre>
<p class="normal">In each line, all characters following a <code class="inlineCode">#</code> character are considered comments.</p>
<p class="normal">The previous <code class="inlineCode">Person</code> object has a <code class="inlineCode">Spouse</code> nested object and a nested collection of addresses. The same example in JSON would be:</p>
<pre class="programlisting code"><code class="hljs-code">{
Name: John
Surname: Smith
Spouse:
{
  Name: Mary
  Surname: Smith
}
Addresses:
[
{
  Type: home
  Country: England
  Town: London
  Street: My home street
 },
{
  Type: office
  Country: England
  Town: London
  Street: My home street
 }
]
}
</code></pre>
<p class="normal">As you can see, the <code class="inlineCode">.yaml</code> syntax is more readable, since it avoids the overhead of parentheses.</p>
<p class="normal"><code class="inlineCode">.yaml</code> files can contain <a id="_idIndexMarker521"/>several sections, each defining a different object, that are <a id="_idIndexMarker522"/>separated by a line containing the <code class="inlineCode">---</code> string. Comments are preceded by a # symbol, which must be repeated on each comment line.</p>
<div><p class="normal">Since spaces/tabs contribute to object semantics, YAML is space/tabs sensitive, so attention must be paid to add the right number of spaces. </p>
</div>
<div><p class="normal">Small collections or small objects can also be specified in-line with the usual <code class="inlineCode">[]</code> and <code class="inlineCode">{}</code> syntax, that is, after the colon in the same line of the property they are the value of.</p>
</div>
<p class="normal">With the basics of orchestrators and <code class="inlineCode">.yaml</code> files, we are ready to learn about the most widespread orchestrator: <strong class="keyWord">Kubernetes</strong>. At the moment, it is also the most complete. So, once you’ve learned about it, learning about other orchestrators should be very easy.</p>
<h1 class="heading-1" id="_idParaDest-152"><a id="_idTextAnchor214"/>Kubernetes basics</h1>
<p class="normal">The Kubernetes <a id="_idIndexMarker523"/>orchestrator is distributed software that must be installed on all virtual servers of a network. Most of the Kubernetes software is installed on just some machines that are called <strong class="keyWord">master nodes</strong>, while all <a id="_idIndexMarker524"/>other machines run just interface software <a id="_idIndexMarker525"/>called <strong class="keyWord">Kubelet</strong> that connects with the software running on the master nodes and locally executes tasks decided on by the master nodes. All machines in a Kubernetes <a id="_idIndexMarker526"/>cluster are called <strong class="keyWord">nodes</strong>.</p>
<p class="normal">Actually, all nodes must also run a container runtime in order to be able to run containers. As we will see later on, all nodes also run software that handles virtual addressing.</p>
<p class="normal">Kubernetes configuration units are abstract objects with properties, subparts, and references to other objects. They are <a id="_idIndexMarker527"/>referred to as <strong class="keyWord">Kubernetes resources</strong>. We have resources that describe a single microservice replica and other resources that describe a set of replicas. Resources describe communication settings, disk storage, users, roles, and various kinds of security constraints.</p>
<p class="normal">Cluster nodes and <a id="_idIndexMarker528"/>all resources they host are managed by master nodes that communicate with human cluster administrators through an API server, as shown in the following diagram:</p>
<figure class="mediaobject"><img alt="Figure 8.1: Kubernetes cluster" src="img/B31916_08_1.png"/></figure>
<p class="packt_figref">Figure 8.1: Kubernetes cluster</p>
<p class="normal">Kubectl is the client typically used to send commands and configuration data to the API server. The scheduler allocates resources to nodes according to the administrator constraints, while the controller manager groups several daemons that monitor the cluster’s actual state and try to move it toward the desired state declared through the API server. There are controllers for several Kubernetes resources, from Microservices replicas to communication facilities. In fact, each resource has some target objectives to be maintained while the application runs, and the controller verifies these objectives are actually achieved, possibly triggering corrective actions, such as moving <a id="_idIndexMarker529"/>some pods running too slowly onto less crowded nodes.</p>
<p class="normal">The deployment unit, that is, the unit that can be deployed on a server, started, killed, and/or moved to another server, is not a single <a id="_idIndexMarker530"/>container, but a set of containers called a <strong class="keyWord">Pod</strong></p>
<div><p class="normal">A Pod is a set of containers that are constrained to run all together on the same server..</p>
</div>
<p class="normal">The concept of the Pod is fundamental since it enables very useful, strong cooperation patterns. For instance, we may attach another container to our main container whose unique purpose is to read the log files created by the main container and send them to a centralized log service.</p>
<div><p class="normal">The <strong class="keyWord">Sidecar</strong> pattern consists <a id="_idIndexMarker531"/>of enhancing a main container with a secondary container deployed on the same Pod and whose only purpose is to offer some services to the main container.</p>
</div>
<p class="normal">In general, we put several containers together inside the same Pod when we need them to communicate through their node file system, or when we need each container replica to be somehow associated with a specific replica of other containers.</p>
<p class="normal">In Kubernetes, communication between <strong class="keyWord">Pods</strong> is handled by <a id="_idIndexMarker532"/>resources called <strong class="keyWord">Services </strong>that are assigned virtual addresses by the Kubernetes infrastructure and that forward their communications to sets of pods that satisfy some constraints. In short, Services are Kubernetes’ way to assign constant virtual addresses to sets of <strong class="keyWord">Pods</strong>.</p>
<p class="normal">All Kubernetes resources <a id="_idIndexMarker533"/>may be assigned name-value pairs called <strong class="keyWord">labels</strong> that are used to reference them through a pattern-matching mechanism. Thus, for instance, all <strong class="keyWord">Pods</strong> that receive traffic from the same <strong class="keyWord">Service </strong>are selected by specifying labels that they must have in the <strong class="keyWord">Service</strong> definition.</p>
<p class="normal">Kubernetes clusters <a id="_idIndexMarker534"/>can be on-premises, that is, Kubernetes may be installed on any private network. But, more often, they are offered as cloud services. For instance, Azure offers <strong class="keyWord">Azure Kubernetes Service (AKS)</strong>.</p>
<p class="normal">In the remainder of the book, we will <a id="_idIndexMarker535"/>use the <strong class="keyWord">Minikube</strong> Kubernetes simulator running on your development machine, since an actual AKS service might quickly exhaust all your Azure free credits. However, all operations in our examples can be replicated on an actual cluster, and whenever there are differences, we will also describe how to perform operations on AKS.</p>
<p class="normal">Let’s start by interacting with a Kubernetes cluster.</p>
<h1 class="heading-1" id="_idParaDest-153"><a id="_idTextAnchor215"/>Interacting with Kubernetes: Kubectl, Minikube, and AKS</h1>
<p class="normal">Before interacting with a Kubernetes cluster with the Kubectl client, we must configure Kubectl and furnish <a id="_idIndexMarker536"/>it with both the cluster URL and the necessary credentials.</p>
<p class="normal">Once installed, Kubectl creates <a id="_idIndexMarker537"/>a different JSON configuration file for each computer user, which will contain configuration info for all Kubernetes clusters and their users. Kubectl has commands for inserting new Kubernetes cluster configurations and for making a cluster configuration the current one.</p>
<p class="normal">Each pair made of a <a id="_idIndexMarker538"/>Kubernetes cluster API URL plus a user credential is called a <strong class="keyWord">context</strong>. Contexts, credentials, and cluster connections can be defined with various <code class="inlineCode">kubectl config</code> subcommands. Below are the most useful ones:</p>
<ol>
<li class="alphabeticList" value="1">Viewing the overall configuration file:
        <pre class="programlisting con-one"><code class="hljs-con">kubectl config view
</code></pre>
</li>
<li class="alphabeticList">Adding a new Kubernetes cluster:
        <pre class="programlisting con-one"><code class="hljs-con">kubectl config set-cluster my-cluster --server=https://&lt;your cluster API server URL&gt;
</code></pre>
</li>
<li class="alphabeticList">User credentials are based on client certificates. A valid certificate can be obtained by creating a certificate request and submitting it to the Kubernetes cluster, which will create an approved certificate. The detailed procedure will be shown in <a href="Chapter_10.xhtml#_idTextAnchor297"><em class="italic">Chapter 10</em></a><em class="italic">, Security and Observability for Serverless and Microservices Applications</em>. Once you get an approved certificate, the user can be created with:
        <pre class="programlisting con-one"><code class="hljs-con">Kube<a id="_idTextAnchor216"/>ctl config set-credentials newusername --client-key= newusername.key --client-certificate=poweruser.crt --embed-certs=true
</code></pre>
</li>
</ol>
<p class="normal-one">Where <code class="inlineCode">newusername.key</code> is the complete path to the private key you used to create the certificate request, and <code class="inlineCode">newusername.crt</code> is the complete path of the approved certificate file.</p>
<ol>
<li class="alphabeticList" value="4">Once you have both a server and a user, you can create a context for the connection of that user to that server, with:
        <pre class="programlisting con-one"><code class="hljs-con">kubectl config set-context newcontext --cluster= my-cluster --user= newusername
</code></pre>
</li>
<li class="alphabeticList">Once all the contexts you need have been properly defined, you can switch to a given context with:
        <pre class="programlisting con-one"><code class="hljs-con">kubectl config use-context newcontext
</code></pre>
</li>
<li class="alphabeticList">After having set a new current context, all Kubectl commands will use both the cluster and the user defined in that context.</li>
</ol>
<p class="normal">If you are the cluster <a id="_idIndexMarker539"/>administrator, your user already exists in the system, so you don’t need to create it. However, you need to get the administrator user credentials and add them to your configuration file. Each cloud service has a login procedure that does this job. For instance, in the case of AKS, the procedure is:</p>
<ol>
<li class="numberedList" value="1">Log in to Azure with Azure CLI:
        <pre class="programlisting con-one"><code class="hljs-con">az login
</code></pre>
</li>
<li class="numberedList">The default browser should open, and you should be prompted for your Azure credentials.</li>
<li class="numberedList">If not already installed, install the package for interacting with AKS:
        <pre class="programlisting con-one"><code class="hljs-con">az aks install-cli
</code></pre>
</li>
<li class="numberedList">Ask to add your AKS credentials to your Kubectl configuration file:
        <pre class="programlisting con-one"><code class="hljs-con">az aks get-credentials --resource-group &lt;your AKS resource group name&gt; --name &lt;your AKS name&gt;
</code></pre>
</li>
<li class="numberedList">If the command is successful, a new cluster, new user, and new context will be added to your Kubectl configuration, and the new context will be made the current one. Please run <code class="inlineCode">kubectl config view</code> to see all configuration file modifications.</li>
</ol>
<p class="normal">Minikube comes with a default user, a default cluster name, and a default context, which are all called <code class="inlineCode">minikube</code>. When you start your Minikube cluster with <code class="inlineCode">minikube start</code>, if not already defined, all the above entities will be added to your Kubectl configuration file. Moreover, the <code class="inlineCode">minikube</code> context will be automatically made the current one, so no extra actions are needed after you start your cluster. Of course, you may define other users and other contexts.</p>
<p class="normal">Minikube can be stopped with <code class="inlineCode">minikube stop</code>, and paused with <code class="inlineCode">minikube pause</code>. Both stopping and pausing do not delete the cluster data and configuration. Other useful commands will be shown later on while using Minikube in our examples.</p>
<p class="normal">Let’s try some Kubectl commands on Minikube (ensure Minikube has been started):</p>
<pre class="programlisting con"><code class="hljs-con">kubectl get nodes
</code></pre>
<p class="normal">It should show <a id="_idIndexMarker540"/>all virtual network Kubernetes nodes. As the default, Minikube creates a cluster with a single node called <code class="inlineCode">minikube</code>, so you should see something like:</p>
<pre class="programlisting con"><code class="hljs-con">NAME       STATUS            ROLES               AGE   VERSION
minikube   Ready    control-plane,master   35m   v1.22.3
</code></pre>
<p class="normal">Since we specified Docker as the virtualization tool, the whole cluster will be embedded in a Docker container, as you can verify by listing all running containers with <code class="inlineCode">docker ps</code> (remember that all Docker commands must be issued in a Linux shell).</p>
<p class="normal">As the default, this unique node contains 2 CPUs and 4 gigabytes of RAM, but we can modify all these parameters, and we can also create clusters with several nodes by passing some options to <code class="inlineCode">minikube start</code>:</p>
<ul>
<li class="bulletList"><code class="inlineCode">--nodes &lt;n&gt;</code>: Specifies the number of nodes in the cluster. Please consider that nodes are virtual machines that will run simultaneously, so a large number of nodes can be set only on a powerful workstation with several cores and say 32-64 gigabytes of RAM. The default is 1.</li>
<li class="bulletList"><code class="inlineCode">--cpus &lt;n or no-limits&gt;</code>: The number of CPUs allocated to Kubernetes, or <code class="inlineCode">no-limits</code>, to let Minikube allocate as many CPUs as needed. The default is 2.</li>
<li class="bulletList"><code class="inlineCode">--memory &lt;string&gt;</code>: The amount of RAM to be allocated to Kubernetes (format: &lt;number&gt;[&lt;unit&gt;], where unit = b, k, m, or g). Use “max” to use the maximum amount of memory. Use “no-limit” to not specify a limit.</li>
<li class="bulletList"><code class="inlineCode">--profile &lt;string&gt;</code>: The name of the Minikube virtual machine (defaults to <code class="inlineCode">minikube</code>). Useful for having more than one Minikube virtual machine – for instance, one with one node and another with two nodes.</li>
<li class="bulletList"><code class="inlineCode">--disk-size &lt;string&gt;</code>: The disk size allocated to the Minikube VM (format: &lt;number&gt;[&lt;unit&gt;], where unit = b, k, m, or g). The default is “20000mb”.</li>
</ul>
<div><p class="normal">If you want to change one of the above settings after having created the Minikube container with your first <code class="inlineCode">minikube start</code>, you need either to delete the previous container with <code class="inlineCode">minikube delete</code> or create a new Minikube container with a custom name with the <code class="inlineCode">--profile</code> option.</p>
</div>
<p class="normal">After this short parenthesis, let’s re<a id="_idTextAnchor217"/>turn to Kubectl! Let’s type:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl get all
</code></pre>
<p class="normal">It lists all Kubernetes resources. If you have not created any resources, the cluster should contain just a <a id="_idIndexMarker541"/>single resource of type ClusterIP, as shown below:</p>
<pre class="programlisting con"><code class="hljs-con">NAME                        TYPE     CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   87m
</code></pre>
<p class="normal">It is part of the Kubernetes infrastructure.</p>
<p class="normal">In general, <code class="inlineCode">kubectl get &lt;resource type&gt;</code> lists all resources of a gi<a id="_idTextAnchor218"/>ven type. Thus, for instance, <code class="inlineCode">kubectl get pods</code> lists all Pods, and <code class="inlineCode">kubectl get services</code> lists all services.</p>
<p class="normal"> If, instead, we need more detailed information on a given object, we may use <code class="inlineCode">kubectl describe &lt;object type&gt; &lt;object name&gt;</code>. Thus, for instance, if we need more information on the Minikube single node called <code class="inlineCode">minikube</code>, we may issue the command below:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl describe node minikube
</code></pre>
<p class="normal">Please try it!</p>
<p class="normal">You will see other Kubectl commands when learning how to define Pods, Services, and other Kubernetes resources in other sections of this chapter. The next subsection explains how to create an Azure Kubernetes cluster, so if at the moment you don’t plan to use Azure Kubernetes, feel free to skip it. You can return to it<a id="_idTextAnchor219"/> when you need to create one.</p>
<h2 class="heading-2" id="_idParaDest-154"><a id="_idTextAnchor220"/>Creating an Azure Kubernetes cluster</h2>
<p class="normal">To create <a id="_idIndexMarker542"/>an AKS cluster, do the following:</p>
<ol>
<li class="numberedList" value="1">Type <code class="inlineCode">AKS</code> into the Azure search box.</li>
<li class="numberedList">Select <strong class="screenText">Kubernetes services</strong>.</li>
<li class="numberedList">Then click the <strong class="screenText">Create</strong> button.</li>
<li class="numberedList">Select <strong class="screenText">Kubernetes Cluster</strong>.</li>
</ol>
<p class="normal">After that, the following form will appear:</p>
<figure class="mediaobject"><img alt="Figure 8.2: AKS creation first form" src="img/B31916_08_2.png"/></figure>
<p class="packt_figref">Figure 8.2: AKS creation first form</p>
<p class="normal">Here, as usual, you can <a id="_idIndexMarker543"/>select one of your Azure subscriptions, an existing resource group, or you can create a new one. Let’s move on to the AKS-specific configuration:</p>
<ol>
<li class="numberedList" value="1"><strong class="screenText">Cluster preset configuration</strong>: Here, you can choose among various preconfigured settings that are a good starting point for various situations. In the preceding screenshot, I have chosen <strong class="screenText">Dev/Test</strong>, which is specific for development and learning, so it proposes the cheapest options. However, you can also select a standard production or an economic production initial setting.</li>
<li class="numberedList"><strong class="screenText">Kubernetes cluster name</strong>: Here, you must select a unique name for your cluster. </li>
<li class="numberedList">For all other settings, you can choose the proposed defaults. In particular, the <strong class="screenText">Region</strong> field should propose the most adequate region for you. <strong class="screenText">AKS pricing tier</strong> should be set to <strong class="screenText">Free</strong>, meaning you will pay just for the virtual machines that make up the cluster. However, you can also select paying options that include support and super-big clusters with up to 5,000 nodes. The <strong class="screenText">Availability zones</strong> field enables geographic redundancy in up to 3 different geographic zones.</li>
</ol>
<p class="normal">If you selected <strong class="screenText">Dev/Test</strong>, the cluster will include from 2 to 5 nodes with automatic scaling. That is, the number <a id="_idIndexMarker544"/>of starting nodes is 2, but it can automatically increase up to 5 if the workload increases. Let’s go to the <strong class="screenText">node pools</strong> tab to customize both the node number and type:</p>
<figure class="mediaobject"><img alt="Figure 8.3: AKS node pool configuration" src="img/B31916_08_3.png"/></figure>
<p class="packt_figref">Figure 8.3: AKS node pool configuration</p>
<p class="normal">If you selected <strong class="screenText">Dev/Test</strong>, there should be a unique node pool that will be used for both Kubernetes master nodes and standard nodes. Pay attention that the <strong class="screenText">Dev/Test</strong> server type (D4ds-v5) has a high monthly price, so please use the price calculator (<a href="https://azure.microsoft.com/en-us/pricing/details/virtual-machines/linux/#pricing">https://azure.microsoft.com/en-us/pricing/details/virtual-machines/linux/#pricing</a>) to verify the cost of a machine before choosing it.</p>
<p class="normal">The standard production selection, instead, would create two node pools – one for master nodes and the other for standard nodes.</p>
<p class="normal">Anyway, you can change the node pools and edit each of them. In the case of the preceding screenshot, let’s click on <strong class="screenText">agentpool</strong>. A new form will open. Here, you can change both the machine type and the maximum number of nodes. A good option for experimenting without wasting too much credit is choosing 3 nodes and an <code class="inlineCode">A</code> family machine. When you have done either, click on update or on cancel to return to the previous form.</p>
<p class="normal">Finally, you can associate Azure Container Registry with the cluster by going to the <strong class="screenText">Integrations</strong> tab:</p>
<figure class="mediaobject"><img alt="Figure 8.4: Connect your cluster to ACR" src="img/B31916_08_4.png"/></figure>
<p class="packt_figref">Figure 8.4: Connect your cluster to ACR</p>
<p class="normal">If you already <a id="_idIndexMarker545"/>defined an Azure Container Registry for experimenting in the <em class="italic">A few more Docker commands and options</em> subsection of <a href="Chapter_3.xhtml#_idTextAnchor067"><em class="italic">Chapter 3</em></a><em class="italic">, Setup and Theory: Docker and Onion Architecture</em>, select that registry; otherwise, you can create a new one in a new browser window and select it, or you can associate a registry to your cluster at a later time.</p>
<div><p class="normal">When you associate a registry to your cluster, you enable the cluster to access and download all its Docker images.</p>
</div>
<p class="normal">When you’ve finished, select <strong class="screenText">Review + Create</strong>.</p>
<p class="normal">Once you’ve created your cluster, you can connect to it with the login procedure we explained earlier in this section.</p>
<p class="normal">Now that you have learned how to connect with both Minikube and AKS, let’s move on to experimenting with Kubernetes resources.</p>
<h1 class="heading-1" id="_idParaDest-155"><a id="_idTextAnchor221"/>Configuring your application in Kubernetes</h1>
<p class="normal">As already mentioned, the simplest Kubernetes resource is the Pod. We will never create a single Pod <a id="_idIndexMarker546"/>since we will always create several replicas of each microservice, but being able to configure a Pod is also fundamental for creating more complex resources, so let’s start creating a single Pod.</p>
<p class="normal">A Pod can be defined through a <code class="inlineCode">.yaml</code> file with the content below:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: v1
kind: Pod
metadata:
name: my-podname
namespace: mypodnamespace
labels:
labenname1: labelvalue1
labelname2: labelvalue2
spec:
restartPolicy: Always #Optional. Possible values: Always (default), OnFailure. Never.
containers:
…
initContainers:
…
</code></pre>
<p class="normal">All Kubernetes configuration files start with the name of the API where the resources being configured are defined, and its version. In the case of Pods, we have just the version since they are defined in the <strong class="keyWord">core API</strong>. Then, <code class="inlineCode">kind</code> defines the type of resource to be configured – in our case, a Pod.</p>
<p class="normal">Like types in C#, Kubernetes resources are also organized in namespaces. Therefore, together with any resource name, we must also specify a namespace. If no namespace is specified, a namespace called <code class="inlineCode">default</code> is assumed.</p>
<div><p class="normal"> Pay attention! While the intent of Kubernetes and C# namespaces is the same, there are substantial differences between them. Namely, C# namespaces are hierarchical, while Kubernetes namespaces are not. Moreover, namespaces are not applicable to all Kubernetes resources since there are cluster-wide resources that belong to no specific namespace.</p>
</div>
<p class="normal">If the namespace <a id="_idIndexMarker547"/>used in a resource definition doesn’t exist yet, it must be defined with the snippet below:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: v1
kind: Namespace
metadata:
name: my-namespace
</code>
---</code> row.</pre>
<p class="normal">Name and namespace are specified as sub-properties of <code class="inlineCode">metadata</code>, together with optional l<code class="inlineCode">abels</code>. Labels are free name-value pairs we can use to classify the object. Typically, they specify information such as the role of the resource in the application and the tier or module it belongs to.</p>
<p class="normal">As already mentioned in the previous section, other resources can use labels to select a set of resources.</p>
<p class="normal">The <code class="inlineCode">spec</code> property specifies the actual content of the Pod, that is, its containers and its restart policy (<code class="inlineCode">restartPolicy</code>). The restart policy specifies when to restart a Pod:</p>
<ul>
<li class="bulletList"><code class="inlineCode">restartPolicy: Always</code>: This is the default. The Pod is restarted whenever all containers terminate or a container terminates with a failure.</li>
<li class="bulletList"><code class="inlineCode">restartPolicy: OnFailure</code>: The Pod is restarted when at least one container exits with a failure</li>
<li class="bulletList"><code class="inlineCode">restartPolicy: Never</code>: The Pod is never restarted.</li>
</ul>
<p class="normal">Containers are <a id="_idIndexMarker548"/>spl<a id="_idTextAnchor222"/>it into two lists: <code class="inlineCode">containers</code> and <code class="inlineCode">initContainers</code>. The containers in the <code class="inlineCode">containers</code> list are started only after all containers in <code class="inlineCode">initContainers</code> are <strong class="keyWord">successful</strong>, and each container in the <code class="inlineCode">initContainers</code> list is started only after the previous container is <strong class="keyWord">successful</strong>. In turn, a container in the <code class="inlineCode">initContainers</code> list is considered <strong class="keyWord">successful</strong> in the two circumstances:</p>
<ol>
<li class="numberedList" value="1">If a container configuration has the <code class="inlineCode">restartPolicy</code> property set to <code class="inlineCode">Always</code>, then the container is considered successful if it has been successfully started. This option is useful for implementing <strong class="keyWord">sidecar</strong> containers. This way, we ensure that <strong class="keyWord">sidecars</strong> are ready before the containers they enhance are started. Please refer to the Pod definition at the beginning of the <em class="italic">Kubernetes basics</em> section for an explanation of what a <strong class="keyWord">sidecar</strong> is.</li>
<li class="numberedList">If a container configuration doesn’t have the <code class="inlineCode">restartPolicy</code> property set to <code class="inlineCode">Always</code>, then the container is considered successful if it is successfully terminated. This option is useful for performing some startup initialization – for instance, for waiting for a database or a message broker to be ready. In a similar situation, the container code is a loop that continuously tries a connection with the database/message broker, and terminates as soon as it succeeds.</li>
</ol>
<div><p class="normal">A failed <code class="inlineCode">initContainers</code> doesn’t cause a whole Pod restart. Instead, it is retried with an exponential retry several times before causing a whole Pod failure. For this reason, they should be designed as idempotent since their actions might be executed more than once.</p>
</div>
<p class="normal">Each container in any of the two above lists is something like:</p>
<pre class="programlisting code"><code class="hljs-code"> - name: &lt;container name&gt;
image: &lt;container image URL&gt;
command: […] # square bracket contains all strings that compose the OS command
resources:
requests:
cpu: 100m
memory: 128Mi
limits:
cpu: 250m
memory: 256Mi
ports:
- containerPort: 80
- containerPort: …
…
env:
-name: env-name1
value: env-value1
…
volumeMounts:
- name: data
mountPath: /mypath/mysubpath….
subPath: /vsubpath #optional. If provided the path of data mounted in mountPath
…
</code></pre>
<p class="normal">We specify both a name for the container and the URL of its image in a container reg<a id="_idTextAnchor223"/>istry, which accounts for point 4 of the minimal services any orchestrator should offer (see the beginning of the <em class="italic">Introduction to orchestrators and their configuration</em> section). These two properties are obligatory, while all other properties are optional. The <code class="inlineCode">command</code> property, when provided, overwrites the <code class="inlineCode">CMD</code> instruction of the image Docker file.</p>
<p class="normal">Then, we also <a id="_idIndexMarker549"/>account for points 5, 6, and 7 of the minimal services any orchestrator should offer, that is, disk storage, environment variables, and communication ports. More specifically, we have:</p>
<ul>
<li class="bulletList"><code class="inlineCode">volumeMount</code> specifies how a virtual storage volume specified by <code class="inlineCode">name</code> is mapped to the path specified by <code class="inlineCode">mountPath</code> in the container file system. If the optional <code class="inlineCode">subPath</code> is provided, just that <code class="inlineCode">subpath</code> of the volume specified by <code class="inlineCode">name</code> is mounted. Virtual storage volumes are described later on in this chapter (in the <em class="italic">Dynamic provisioning of permanent disk space</em> subsection), together with other <code class="inlineCode">volumeMounts</code> properties.</li>
<li class="bulletList"><code class="inlineCode">env</code> specifies all container’s environment variables as a list of <code class="inlineCode">name-value</code> pairs.</li>
<li class="bulletList"><code class="inlineCode">ports</code> specifies the list of all ports exposed by the container we would like to use in our application. These ports may be mapped to other ports in the actual communication between Pods. However, the port mapping is specified in other resources called <code class="inlineCode">services</code> that provide virtual Pod addresses and other communication-related options.</li>
</ul>
<p class="normal">Finally, the <code class="inlineCode">resource</code>s section specifies both the minimal computational resources needed for starting the container (<code class="inlineCode">requests</code>) and the maximum computational resources it can waste (<code class="inlineCode">limits</code>).</p>
<p class="normal">The constraints in the <code class="inlineCode">requests</code> property are used to choose the virtual machine to place a Pod. <code class="inlineCode">limits</code>, instead, are enforced by the operating system kernel as follows:</p>
<ul>
<li class="bulletList">CPU limits are enforced with throttling. That is, containers exceeding the CPU limit are delayed, putting them in sleeping mode for enough time.</li>
<li class="bulletList">Memory limits are enforced by throwing an exception when they are exceeded. In turn, the exception causes the application of the Pod restart policy, which usually causes a Pod restart.</li>
</ul>
<p class="normal">With regard to units <a id="_idIndexMarker550"/>of measure, typical memory units of measure are <code class="inlineCode">Ti</code> (terabytes), <code class="inlineCode">Gi</code> (gigabytes), <code class="inlineCode">Mi</code> (megabytes), and <code class="inlineCode">Ki</code> (kilobytes). CPU time, instead, can be measured either in millicores (<code class="inlineCode">mi</code>) or as a fractional number of cores (no unit of measure after the value).</p>
<p class="normal">Let’s try a Pod with a sidecar container, which shows both the practical usage of the described syntax and how a sidecar can help in building application-level monitoring. The main container will be a fake microservice based on the Alpine Linux distribution Docker image, which just puts log messages in a file located in a directory shared with the sidecar. In an actual application, the log would be organized in several files (for instance, one for each day), and old files would be periodically deleted. Moreover, the sidecar would read these files and send their content to a log API. Our didactical sidecar, instead, will just periodically read the last 10 rows of the file and will display them in its console.</p>
<p class="normal">The code is quite simple. First of all, we define a namespace that encloses our example:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: v1
kind: Namespace
metadata:
name: basic-examples
</code></pre>
<p class="normal">Then, after a <code class="inlineCode">---</code> row, we place the actual Pod definition:</p>
<pre class="programlisting code"><code class="hljs-code">---
apiVersion: v1
kind: Pod
metadata:
name: pod-demo
namespace: basic-examples
labels:
app: myapp
spec:
containers:
- name: myapp
image: alpine:latest
command: ['sh', '-c', 'while true; do echo $(date) &gt;&gt; /opt/logs.txt; sleep 1; done']
    volumeMounts:
- name: data
mountPath: /opt
initContainers:
- name: logshipper
image: alpine:latest
restartPolicy: Always
command: ['sh', '-c', 'tail -F /opt/logs.txt']
    volumeMounts:
- name: data
mountPath: /opt
volumes:
- name: data
emptyDir: {}
</code></pre>
<p class="normal">Both containers use a simple Alpine Linux distribution Docker image and confine the application-specific code in the <code class="inlineCode">command</code>, which is a Linux script. This technique is used for adapting preexisting images or for very simple tasks such as the ones often performed by a sidecar. We also used <a id="_idIndexMarker551"/>the same technique for the main container because the main container does nothing and has a purely didactical purpose.</p>
<p class="normal">Accordingly, with the previously exposed syntax, the sidecar is defined in the <code class="inlineCode">initContaines</code> list with <code class="inlineCode">restartPolicy: Always</code>.</p>
<p class="normal">The main container command executes an endless loop where it just writes the current date and time in the <code class="inlineCode">/opt/logs.txt</code> file and then sleeps for one second.</p>
<p class="normal">The sidecar container command uses <code class="inlineCode">sh -c</code> to execute a single shell command, the <code class="inlineCode">tail</code> command with the <code class="inlineCode">-f</code> option on the <code class="inlineCode">/opt/logs.txt</code> file. This command shows the last 10 rows of the file in the container console and updates them whenever new rows are added, so that the console always contains the current last 10 rows of the file.</p>
<p class="normal">The file processed by both containers is the same because both containers mount the same data volume in the same <code class="inlineCode">/opt</code> directory on their filesystems with:</p>
<pre class="programlisting code"><code class="hljs-code">volumeMounts:
- name: data
mountPath: /opt
</code></pre>
<p class="normal">The data volume is defined in a <code class="inlineCode">volumes</code> list that is a direct descendant of the <code class="inlineCode">spec</code> property, as:</p>
<pre class="programlisting code"><code class="hljs-code">- name: data
emptyDir: {}
</code></pre>
<p class="normal"><code class="inlineCode">emptyDir</code> defines and allocates <a id="_idIndexMarker552"/>a volume that is specific to the Pod where it is defined. This means that it can’t be accessed by any other Pod. The volume is implemented with the disk memory of the node that hosts the Pod. This means that if the Pod is deleted or moved to a different node, the volume is destroyed and its content is lost. <code class="inlineCode">EmptyDir</code> is the preferred way to provide temporary disk storage that’s used somehow in the Pod computations. It has <a id="_idIndexMarker553"/>an optional <code class="inlineCode">sizeLimit</code> property that specifies a maximum disk space the Pod can use. For instance, we can set <code class="inlineCode">sizeLimit: 500Mi</code> to specify 500 mega of maximum disk space.</p>
<p class="normal">Since we have not specified any size limit, the <code class="inlineCode">emptyDir</code> object has no properties, so we are forced to add the empty object value <code class="inlineCode">{}</code> to get a correct <code class="inlineCode">.yaml</code> syntax (we can’t have a colon followed by nothing).</p>
<p class="normal">Let’s create a folder for experimenting with <code class="inlineCode">.yaml</code> files in Minikube, and let’s place the whole example code in a file called <code class="inlineCode">SimplePOD.yaml</code> inside that folder. This file is also available in the <code class="inlineCode">ch08</code> folder of the book’s GitHub repository.</p>
<p class="normal">Now, right-click on the newly created folder and open a Windows console in that directory. After having verified that Minikube is started by issuing a <code class="inlineCode">kubectl get all</code> command, we can apply all our definitions with the <code class="inlineCode">kubectl apply</code> command:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl apply <a id="_idTextAnchor224"/>-f SimplePOD.yaml
</code></pre>
<p class="normal">Now, if we issue the <code class="inlineCode">kubectl get pods</code> command, we don’t see a new Pod! This is right because that command just lists resources defined in the <code class="inlineCode">default</code> namespace, while our Pod has been defined in a new namespace called <code class="inlineCode">basic-examples</code>, so if we would like to operate on a resource in this namespace, we must add the <code class="inlineCode">-n basic-examples</code> option to our commands:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl get pods -n basic-examples
</code></pre>
<p class="normal">In order to access our sidecar console, we can use the Kubectl <code class="inlineCode">logs</code> command. In fact, all console output of all container Pods is automatically collected by Kubernetes and can be inspected with this command. The command needs the Pod name and its namespace if different from <code class="inlineCode">default</code>. Moreover, if the Pod contains several containers, it also needs the name of the container we would like to inspect, which can be provided with the <code class="inlineCode">-c</code> option. Summing up, our command is:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl logs -n basic-examples  pod-demo -c logshipper
</code></pre>
<p class="normal">The command above will show just the current console content and then it will exit. If we would like the content to update automatically as the console content changes, we must add the <code class="inlineCode">-f</code> option:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl logs -f -n basic-examples  pod-demo -c logshipper
</code></pre>
<p class="normal">This way, our window <a id="_idIndexMarker554"/>freezes on the command and automatically updates. The command can be exited with <code class="inlineCode">ctrl-c</code>.</p>
<p class="normal">We can also have a console into the <code class="inlineCode">logshipper</code> container with the Kubectl <code class="inlineCode">exec</code> command. It needs namespace, Pod, and container names, and after the <code class="inlineCode">–</code> characters, the Linux command to execute in the container file system. If you need a console, the Linux command is <code class="inlineCode">sh</code>, and if we would like to interact with that console, we need to also specify the <code class="inlineCode">-it</code> options that stand for “interactive tty.” Summing up, we have:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl exec -it -n basic-examples pod-demo -c logshipper -- sh
</code></pre>
<p class="normal">Once in the container, we can move into the <code class="inlineCode">/opt</code> directory with <code class="inlineCode">cd /opt</code>, and verify if the <code class="inlineCode">logs.txt</code> file is there, with <code class="inlineCode">ls</code>.</p>
<p class="normal">Once finished, you can exit the container console by issuing the <code class="inlineCode">exit</code> command.</p>
<div><p class="normal"> The <code class="inlineCode">kubectl exec</code> command is <a id="_idIndexMarker555"/>very useful for debugging applications, especially when they are already in production or staging.</p>
</div>
<p class="normal">When you have finished with all resources created by a <code class="inlineCode">.yaml</code> file, you can delete all of them with <code class="inlineCode">kubectl deleted &lt;file name&gt;.yaml</code>. Thus, in our case, we can destroy all our example entities with:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl delete -f SimplePOD.yaml
</code></pre>
<div><p class="normal"> <code class="inlineCode">kubectl apply</code> can also be used for modifying previously created resources. It is enough to edit the <code class="inlineCode">.yaml</code> file used to create the resources and then repeat the <code class="inlineCode">apply</code> command on it.</p>
</div>
<p class="normal">We have seen how to create temporary disk space with <code class="inlineCode">emptyDir</code>. Now let’s see the typical way of allocating permanent network disk space and sharing it between various Pods.</p>
<h2 class="heading-2" id="_idParaDest-156"><a id="_idTextAnchor225"/>Dynamic provisioning of permanent disk space</h2>
<p class="normal">Volume definitions similar to <code class="inlineCode">emptyDir</code> are called in-tree definitions because the instruction that <a id="_idIndexMarker556"/>creates the volume is inserted directly into the Pod definition. There is no way to share an in-tree definition with other Pod definitions, so it is not easy to share in-tree volumes between different Pods.</p>
<p class="normal">Actually, disk space sharing can also be achieved with in-tree definitions by adequately configuring the device that provides the disk space. For instance, suppose we are using an NFS server connected to our Kubernetes cluster to furnish network disk space. We can connect a Pod with it with the instruction below:</p>
<pre class="programlisting code"><code class="hljs-code">volumes
- nfs:
server: my-nfs-server.example.com
path: /my-nfs-volume
readOnly: true # optional. If provided the volume is accessible as read-only
</code></pre>
<p class="normal">Where <code class="inlineCode">server</code> is a server name or an IP address, and path is the directory to share. In order to share the same disk space between PodS, it is enough that they specify the same server and path.</p>
<p class="normal">However, this technique has two cons:</p>
<ul>
<li class="bulletList">The share is not explicitly declared, but it is indirect, thus it undermines code maintainability and readability.</li>
<li class="bulletList">Kubernetes is not informed about the Pods that are using a share, so it can’t be instructed to release the share when it is not needed anymore.</li>
</ul>
<p class="normal">Therefore, in-tree <a id="_idIndexMarker557"/>definitions are more adequate for temporary disk space that is not shared among Pods. Luckily, the problem is not the NFS protocol itself, but just the in-tree syntax. For this reason, Kubernetes also offers an out-of-tree syntax <a id="_idIndexMarker558"/>based on two separate objects: <strong class="keyWord">Persistent Volume Claims</strong> (<strong class="keyWord">PVCs</strong>), which represent disk space needs, and <strong class="keyWord">Persistent Volumes</strong> (<strong class="keyWord">PVs</strong>), which represent actual disk space.</p>
<p class="normal">The whole technique works this way:</p>
<ol>
<li class="numberedList" value="1">We define the disk space specification in a PVC.</li>
<li class="numberedList">All Pods that need to share the same disk space reference the same PVC.</li>
<li class="numberedList">Kubernetes, somehow, tries to satisfy each PVC with a compatible PV that is then mounted on all Pods sharing that PVC.</li>
</ol>
<p class="normal">When all Pods that share the same PV are destroyed, we can instruct Kubernetes to keep the allocated disk space or delete it.</p>
<p class="normal">The way a PVC catches the needed disk and returns a PV depends on the driver used to serve the PVC. Drivers must be installed in the Kubernetes cluster, but all cloud providers furnish predefined drivers.</p>
<p class="normal">Driver names <a id="_idIndexMarker559"/>and related settings are organized in resources called <strong class="keyWord">Storage Classes</strong> (<code class="inlineCode">kind: StorageClass</code>). Together with <a id="_idIndexMarker560"/>predefined drivers, all cloud providers also offer predefined storage classes based on those drivers. However, you can define new storage classes based on the same driver but with different settings.</p>
<p class="normal">You can also install drivers and storage classes based on those drivers on on-premises Kubernetes clusters (there are a lot of open-source drivers). Minikube has addons that install various storage drivers and related storage classes, too.</p>
<p class="normal">Drivers that simply match PVCs with PVs that are manually predefined by the user are called static. While drivers that dynamically create PV resources, taking the needed disk space from a common pool of available disk space, are called dynamic.</p>
<p class="normal">In this section, we will focus just on dynamic storage allocation since it is the most relevant in actual microservices applications. You may find more details on storage classes and how to define them in the official Kubernetes documentation: <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">https://kubernetes.io/docs/concepts/storage/storage-classes/</a>.</p>
<p class="normal">The first step in creating a PVC is the verification of the available storage classes:</p>
<pre class="programlisting con"><code class="hljs-con">Kubectl get storageclasses
</code></pre>
<p class="normal">Then the details of a specific class can be obtained with <code class="inlineCode">kubectl describe</code>. In Minikube, we obtain:</p>
<pre class="programlisting con"><code class="hljs-con">NAME               PROVISIONER            RECLAIMPOLICY   VOLUMEBINDINGMODE ...  
standard (default) k8s.io/minikube-hostpath   Delete          Immediate      ...   
</code></pre>
<p class="normal">The “default” after the class name informs us that the <code class="inlineCode">standard</code> class is the default storage class, that is, the one used when no storage class is specified.</p>
<p class="normal">When using dynamic provisioning, a PVC needs to specify just:</p>
<ul>
<li class="bulletList">The storage needed</li>
<li class="bulletList">The storage class</li>
<li class="bulletList">The access modality: <code class="inlineCode">ReadWriteOnce</code> (only a si<a id="_idTextAnchor226"/>ngle node can read and write on the storage), <code class="inlineCode">ReadOnlyMany</code> (several nodes can read), <code class="inlineCode">ReadWriteMany</code> (several nodes can both read and write), <code class="inlineCode">ReadWriteOncePod</code> (only a single Pod can read and write on the storage)</li>
</ul>
<p class="normal">In fact, all the information needed to get a PV is contained in the storage class. Since a PVC describes a Pod need and not a specific PV, the provisioned storage will provide at least the required access mode, but it can support more accesses, too.</p>
<p class="normal">If the <a id="_idIndexMarker561"/>driver used by the storage class doesn’t support the required modality, the operation fails. Therefore, before using a storage class, you must verify the operations supported by its driver. <code class="inlineCode">ReadOnlyMany</code> doesn’t make sense with dynamic provisioning, since allocated storage always comes clean, so there is nothing to read.</p>
<p class="normal">In practice, drivers th<a id="_idTextAnchor227"/>at support dynamic provisioning always support <code class="inlineCode">ReadWriteOnce</code>, and some of them also support <code class="inlineCode">ReadWriteMany</code>. Therefore, if you need a volume that is shared among several Pods, you must verify that the chosen driver supports <code class="inlineCode">ReadWriteMany</code>; otherwise, all Pods that share the volume will be allocated on the same node to ensure that all of them can access the claimed <code class="inlineCode">ReadWriteOnce</code> storage.</p>
<p class="normal">A PVC is defined as shown below:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
name: myclaim
namespace: a-namespace
spec:
accessModes:
- ReadWriteOnce # ReadWriteOnce, ReadOnlyMany, ReadWriteMany, ReadWriteOncePod
resources:
requests:
storage: 8Gi
storageClassName: &lt;my storage classname&gt;
</code></pre>
<p class="normal">The needed storage is specified with the same syntax as the RAM required by a container. If the storage class is not provided, Kubernetes uses a storage class that has been marked as the default storage class, if any.</p>
<p class="normal">Once you’ve defined a PVC, the volume property of the Pod needs to reference it:</p>
<pre class="programlisting code"><code class="hljs-code">volumes:
- name: myvolume
   persistentVolumeClaim:
     claimName: myclaim
</code></pre>
<p class="normal">However, the PVC and Pod must belong to the same namespace; otherwise, the operation fails.</p>
<p class="normal">Now that <a id="_idIndexMarker562"/>we have all the building blocks, we can move on to more complex resources built on top of these blocks. Single Pods are not useful since we always need several replicas of each microservice, but luckily, Kubernetes already has built-in resources for handling both undistinguishable replicas and indexed replicas useful for implementing sharding strategies.</p>
<h2 class="heading-2" id="_idParaDest-157"><a id="_idTextAnchor228"/>ReplicaSets, Deployments, and their services</h2>
<p class="normal"><strong class="keyWord">ReplicaSets</strong> are resources <a id="_idIndexMarker563"/>that automatically create N replicas of a Pod. However, they are rarely used because it is <a id="_idIndexMarker564"/>more convenient to use <strong class="keyWord">Deployments</strong>, which are <a id="_idIndexMarker565"/>built on top of ReplicaSets <a id="_idIndexMarker566"/>and automatically handle a smooth transition when the number of replicas or other parameters are modified.</p>
<p class="normal">The definition of a Deployment is:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: apps/v1
kind: Deployment
metadata:
name: my-deployment-name
namespace: my-namespace
labels:
app: my-app
spec:
replicas: 3
selector:
matchLabels:
my-pod-label-name: my-pod-label-value
...
template:
</code></pre>
<p class="normal">Deployments are not contained in the API core, so their API name (<code class="inlineCode">apps</code>) must be specified. The metadata section is identical to that of a Pod. The <code class="inlineCode">spec</code> section contains the desired number of replicas (<code class="inlineCode">replicas</code>) and a selector that specifies a condition for a Pod to belong to the deployment: it must have all labels with the specified values.</p>
<p class="normal"><code class="inlineCode">template</code> specifies how to create a Pod for the Deployment. If the cluster already contains some Pods that satisfy the selector conditions, then the template is used to create just the Pods needed to reach the <code class="inlineCode">replicas</code> target number.</p>
<p class="normal">The template is a complete Pod definition whose syntax is identical to the one we use for specifying a single Pod. The only differences being:</p>
<ul>
<li class="bulletList">The Pod <a id="_idTextAnchor229"/>definition is not preceded by any API specification</li>
<li class="bulletList">The Pod metadata section doesn’t contain a Pod name, since we are providing a template for creating <code class="inlineCode">replica</code> Pods. Pod names are automatically created by the Deployment.</li>
<li class="bulletList">The Pod metadata section doesn’t contain a Pod namespace since Pods inherit the same namespace as the Deployment.</li>
</ul>
<p class="normal">Needless to say, the Pod <a id="_idIndexMarker567"/>template must specify labels that <a id="_idIndexMarker568"/>match the selector <code class="inlineCode">conditions</code>. Below is a complete example:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: apps/v1
kind: Deployment
metadata:
name: nginx
namespace: basic-examples
labels:
app: webservers
spec:
selector:
matchLabels:
app: webservers
replicas: 2
template:
metadata:
labels:
app: webservers
spec:
containers:
- image: nginx
name: nginx
ports:
- containerPort: 80
name: web
volumeMounts:
- mountPath: /usr/share/nginx/html
name: website
volumes:
- name: website
persistentVolumeClaim:
claimName: website
</code></pre>
<p class="normal">The Deployment creates <a id="_idIndexMarker569"/>two replicas of an <strong class="keyWord">nginx</strong> web server that share <a id="_idTextAnchor230"/>a common disk space. More specifically, they share the <code class="inlineCode">/usr/share/nginx/html</code> path that is mapped to a common PVC. <code class="inlineCode">/usr/share/nginx/html</code> is the folder where <strong class="keyWord">nginx</strong> looks for static web content, so if we place an <code class="inlineCode">index.html</code> file there, it should be accessible by both web servers.</p>
<p class="normal">The code above implements two load-balanced web servers that s<a id="_idTextAnchor231"/>erve the same content. Let’s place the Deployment in a <code class="inlineCode">WebServers.yaml</code> file. We will use it in a short while, after having added the <a id="_idIndexMarker570"/>missing code, that is, the PVC definition <a id="_idIndexMarker571"/>and a Service that forwards traffic from outside of the Kubernetes cluster and load-balances it among the replicas.</p>
<p class="normal">Deployments can be connected to three kinds of services:</p>
<ul>
<li class="bulletList"><strong class="keyWord">ClusterIP</strong>, which forwards <a id="_idIndexMarker572"/>traffic from inside the network to the Deployment</li>
<li class="bulletList"><strong class="keyWord">LoadBalancer</strong>, which <a id="_idIndexMarker573"/>forwards traffic from outside of the cluster to the Deployment</li>
<li class="bulletList"><strong class="keyWord">NodePort, </strong>which is not <a id="_idIndexMarker574"/>fundamental for application developers and will not be described</li>
</ul>
<p class="normal">The definition of a <strong class="keyWord">ClusterIP</strong> is:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: v1
kind: Service
metadata:
name: my-service
namespace: my-namespace
spec:
selector:
my-selector-label: my-selector-value
...
ports:
- name: http
protocol: TCP
port: 80
targetPort: 80
- name: https
protocol: TCP
port: 443
targetPort: 443
</code></pre>
<p class="normal"><code class="inlineCode">selector</code> defines the Pods that will receive the traffic from the service. The Pods must belong to the same namespace as the service. The <code class="inlineCode">ports</code> list defines the mapping from external ports (<code class="inlineCode">port</code>) to the ports inside the Pod containers (<code class="inlineCode">targetPort</code>). Each map can also specify an optional name <a id="_idIndexMarker575"/>and an optional protocol. If no protocol is specified, all protocols <a id="_idIndexMarker576"/>will be forwarded to the Pods.</p>
<p class="normal">A <strong class="keyWord">ClusterIP</strong> service is <a id="_idIndexMarker577"/>assigned the <code class="inlineCode">&lt;service name&gt;.&lt;namespace&gt;.svc.cluster.local</code> domain name, but it can also be accessed with <code class="inlineCode">&lt;service name&gt;.&lt;namespace&gt;</code> (or simply <code class="inlineCode">&lt;service name&gt;</code> if the namespace is <code class="inlineCode">default</code>).</p>
<p class="normal">Summing up, all traffic sent to either <code class="inlineCode">&lt;service name&gt;.&lt;namespace&gt;.svc.cluster.local</code> or to <code class="inlineCode">&lt;service name&gt;.&lt;namespace&gt;</code> is forwarded to the Pods selected by the <code class="inlineCode">selector</code>.</p>
<p class="normal">A <strong class="keyWord">LoadBalance</strong>r service is <a id="_idIndexMarker578"/>completely analogous, the only difference being the two sub-properties of <code class="inlineCode">spec</code> below:</p>
<pre class="programlisting code"><code class="hljs-code">spec:
type: LoadBalancer
loadBalancerIP: &lt;yourpublic ip&gt;
selector:
…
</code></pre>
<p class="normal">If you specify an IP address, that IP address must be a static IP address you bought somehow; otherwise, in the case of cloud Kubernetes clusters, you can omit the <code class="inlineCode">loadBalancerIP</code> property and a dynamic IP address is automatically assigned to the service by the infrastructure. In AKS, you must also specify the resource group where the IP address has been allocated in an annotation:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: v1
kind: Service
metadata:
annotations:
service.beta.kubernetes.io/azure-load-balancer-resource-group: &lt;IP resource group name&gt;
</code></pre>
<p class="normal">Moreover, you must give the “Network Contributor” role on the resource group where you defined the static IP address to the managed identity associated to the AKS cluster (as a default, a managed identity is automatically assigned to any newly created AKS cluster). See the detailed procedure for performing this operation here: <a href="https://learn.microsoft.com/en-us/azure/aks/static-ip">https://learn.microsoft.com/en-us/azure/aks/static-ip</a>.</p>
<p class="normal">You can also specify an annotation with a label:</p>
<pre class="programlisting con"><code class="hljs-con">service.beta.kubernetes.io/azure-dns-label-name: &lt;label &gt;
</code></pre>
<p class="normal">In which case, Azure will automatically associate the <code class="inlineCode">&lt;label&gt;.&lt;location&gt;.cloudapp.azure.com</code> domain name to the LoadBalancer.</p>
<p class="normal">If you want to publish the service on a custom domain name, you need to buy a domain name, and then <a id="_idIndexMarker579"/>you need to create an Azure DNS zone with appropriate <a id="_idIndexMarker580"/>DNS records. However, in this case, it is better to use an Ingress instead of a simple LoadBalancer (see the <em class="italic">Ingresses</em> subsection).</p>
<div><p class="normal"> The loadBalancerIP property has been declared obsolete and will be removed in future Kubernetes versions. It should be replaced by a platform-dependent annotation. In the case of AKS, the annotation is: <code class="inlineCode">service.beta.kubernetes.io/azure-pip-name: &lt;your static IP address&gt;</code></p>
</div>
<p class="normal">Let’s go back to our nginx example and let’s create a LoadBalancer Service to expose our load-balanced web servers on the internet:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: v1
kind: Service
metadata:
name: webservers-service
namespace: basic-examples
spec:
type: LoadBalancer
selector:
app: webservers
ports:
- name: http
protocol: TCP
port: 80
targetPort: 80
</code></pre>
<p class="normal">We don’t specify an IP address since we are going to test the example in Minikube, a simulator that uses a particular procedure to expose LoadBalancer Services.</p>
<p class="normal">Let’s place the Service definition in a file named <code class="inlineCode">WebServersService.yaml</code>.</p>
<p class="normal">In a <code class="inlineCode">WebServersPVC.yaml</code> file, let’s also place the missing PVC:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
name: website
namespace: basic-examples
spec:
accessModes:
- ReadWriteMany
resources:
requests:
storage: 1Gi
</code></pre>
<p class="normal">We have not specified a storage class because we will use the default one.</p>
<p class="normal">Let’s also create a <code class="inlineCode">BasicExamples.yaml</code> file for defining the <code class="inlineCode">basic-examples</code> namespace:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: v1
kind: Namespace
metadata:
name: basic-examples
</code></pre>
<p class="normal">Now let’s copy the <code class="inlineCode">index.html</code> file contained in the <code class="inlineCode">ch08</code> folder of the book’s GitHub repository, or any <a id="_idIndexMarker581"/>other self-contained HTML page with no <a id="_idIndexMarker582"/>external references to other images/content, in the same folder containing all the above <code class="inlineCode">.yaml</code> files. We will use that page as experimental content to be shown by the web servers.</p>
<p class="normal">Let’s start our experiment:</p>
<ol>
<li class="numberedList" value="1">Open a console on the folder containing all <code class="inlineCode">.yaml</code> files (right-click on the folder and select the console option).</li>
<li class="numberedList">Ensure Minikube is running, and if not, start it with <code class="inlineCode">minikube start</code>.</li>
<li class="numberedList">Deploy all files in the right sequence, that is, ensuring that all resources referenced in a file have already been created.
        <pre class="programlisting con-one"><code class="hljs-con">kubectl apply -f BasicExamples.yaml
kubectl apply -f WebServersPVC.yaml
kubectl apply -f WebServers.yaml
kubectl apply -f WebServersService.yaml
</code></pre>
</li>
<li class="numberedList">Now we need to copy the <code class="inlineCode">index.html</code> files in the <code class="inlineCode">/usr/share/nginx/html</code> folder of either of the two created Pods. It will also be seen by the other Pod, since they share the same disk storage. For this operation, we need a Pod name. Let’s get it with:
        <pre class="programlisting con-one"><code class="hljs-con">kubectl get pods -n Basic-Examples
</code></pre>
</li>
<li class="numberedList">A file can be copied in a Kubernetes Pod with the <code class="inlineCode">kubectl cp</code> command:
        <pre class="programlisting con-one"><code class="hljs-con">kubectl cp  &lt;source path&gt; &lt;namesapace&gt;/&lt;pod name&gt;:&lt;destination folder&gt;
</code></pre>
</li>
<li class="numberedList">In our case, the <code class="inlineCode">cp</code> command becomes:
        <pre class="programlisting con-one"><code class="hljs-con">kubectl cp Index.html basic-examples/&lt;pod name&gt;:/usr/share/nginx/html
</code></pre>
</li>
<li class="numberedList">In Minikube, you can <a id="_idIndexMarker583"/>access the cluster <a id="_idIndexMarker584"/>through a LoadBalancer service by creating a tunnel. Do the following:<ol class="alphabeticList level-2" style="list-style-type: lower-alpha;">
<li class="alphabeticList level-2" value="1">Open a new console window</li>
<li class="alphabeticList level-2">In this new window, issue the <code class="inlineCode">minikube tunnel</code> command</li>
<li class="alphabeticList level-2">The window will freeze on the command. As long as the window remains open, the <code class="inlineCode">LoadBalancer</code> is accessible through <code class="inlineCode">localhost</code>. Anyway, you can verify the external IP assigned to the LoadBalancer by issuing <code class="inlineCode">kubectl get services -n Basic-Examples</code> in the previous window.</li>
</ol>
</li>
<li class="numberedList">Open your favourite browser and go to <code class="inlineCode">http://localhost</code>. You should see the content of the <code class="inlineCode">index.html</code> page.</li>
</ol>
<p class="normal">Once you’ve finished experimenting, let’s destroy all resources in reverse order (the opposite order in which you created them):</p>
<pre class="programlisting con"><code class="hljs-con">kubectl delete -f WebServersService.yaml
kubectl delete -f WebServers.yaml
kubectl delete -f WebServersPVC.yaml
</code></pre>
<p class="normal">You can keep the namespace definition since we will use it in the next example.</p>
<p class="normal">All Deployment replicas are identical; they have no identity, so there is no way to refer to a specific replica from your code. If a replica goes down, for instance, because of a node crash, the system might have a small performance issue, but will continue working properly since replicas are just a way to improve performance, so no replica is indispensable.</p>
<div><p class="normal"> It is worth pointing out that as soon as Kubernetes detects a node fault, it recreates all Pods hosted on that node elsewhere. However, this operation might take time since the fault might not be detected as soon as it takes place. In the meantime, applications might have malfunctions if a Pod hosted by the faulty node is indispensable, which is why Deployments must be preferred whenever possible.</p>
</div>
<p class="normal">Unfortunately, there are <a id="_idIndexMarker585"/>situations where identical copies can’t <a id="_idIndexMarker586"/>achieve the needed parallelism, but we need non-identical sharded copies. If you don’t remember what sharding is and why it is necessary in some situations, please refer to the <em class="italic">Ensuring that messages are processed in proper order</em> section of <a href="Chapter_7.xhtml#_idTextAnchor151"><em class="italic">Chapter 7</em></a><em class="italic">, Microservices in Practice</em>. <strong class="keyWord">StatefulSets</strong> furnish the kind of replication needed for sharding.</p>
<h2 class="heading-2" id="_idParaDest-158"><a id="_idTextAnchor232"/>StatefulSets and Headless Services</h2>
<p class="normal">All replicas of a <code class="inlineCode">StatefulSet</code> are assigned indexes that go from 0 to N-1, where N is the number of replicas. Their Pod names are predictable, too, since they are built as <code class="inlineCode">&lt;StatefulSet name&gt;-&lt;replica index&gt;</code>. Their domain names also contain the Pod names, so that each Pod has its own domain name: <code class="inlineCode">&lt;POD name&gt;.&lt;service name&gt;.&lt;namespace&gt;.svc.cluster.local</code>, or simply <code class="inlineCode">&lt;POD name&gt;.&lt;service name&gt;.&lt;namespace&gt;</code>.</p>
<p class="normal">When a <a id="_idIndexMarker587"/>StatefulSet is created, all replicas are created in order <a id="_idIndexMarker588"/>of increasing index; while when it is destroyed, all replicas are destroyed in decreasing index order. The same happens when the number of replicas is changed.</p>
<p class="normal">Each <code class="inlineCode">StatefulSet</code> must have an associated Service that must be declared in the <code class="inlineCode">serviceName</code> property of the <code class="inlineCode">StatefulSet</code>. The definition of a <code class="inlineCode">StatefulSet</code> is almost identical to that of a <code class="inlineCode">Deployment</code>; the only <a id="_idIndexMarker589"/>difference being that <code class="inlineCode">kind</code> is <code class="inlineCode">StatefulSet</code> and there is <a id="_idIndexMarker590"/>the <code class="inlineCode">serviceName:”&lt;service name&gt;“</code> property immediately under the <code class="inlineCode">spec</code> section.</p>
<p class="normal">The service associated to <code class="inlineCode">StatefulSet</code> must be a so-called <code class="inlineCode">Headless</code> service, which is defined as a ClusterIP service but with a <code class="inlineCode">ClusterIP: None</code> property under <code class="inlineCode">spec</code>:</p>
<pre class="programlisting code"><code class="hljs-code">...
spec:
clusterIP: None
selector:
...
</code></pre>
<p class="normal">It is also worth pointing out that, typically, each replica has its own private storage, so, usually, StatefulSet definitions do not have a reference to a PVC, but instead use a PVC template that attaches a different PVC to each created Pod:</p>
<pre class="programlisting code"><code class="hljs-code">volumeClaimTemplates:
- metadata
...
spec:
...
</code></pre>
<p class="normal">Where both the <code class="inlineCode">metadata</code> and <code class="inlineCode">spec</code> properties are identical to those of a PVC resource.</p>
<p class="normal">Below is an <a id="_idIndexMarker591"/>example of a StatefulSet with its associated <a id="_idIndexMarker592"/>Headless Service. The Pod name is passed to <a id="_idIndexMarker593"/>each container through an environment variable, so that the <a id="_idIndexMarker594"/>code is aware of its index and its possible role in a sharding algorithm:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: v1
kind: Service
metadata:
name: podname
namespace: basic-examples
labels:
app: podname
spec:
ports:
- port: 80
clusterIP: None
selector:
app: podname
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
name: podname
namespace: basic-examples
spec:
selector:
matchLabels:
app: podname
serviceName: "podname"
replicas: 3
template:
metadata:
labels:
app: podname
spec:
containers:
- name: test
image: alpine:latest
command: ['sh', '-c', 'while true; do echo $(MY_POD_NAME); sleep 3; done']
        ports:
- containerPort: 80
env:
- name: MY_POD_NAME
valueFrom:
fieldRef:
fieldPath: metadata.name
volumeClaimTemplates:
- metadata:
name: volumetest
spec:
accessModes: [ "ReadWriteOnce" ]
      resources:
requests:
storage: 1Gi
</code></pre>
<p class="normal">Each Pod <a id="_idIndexMarker595"/>contains just the Alpine Linux distribution, and the <a id="_idIndexMarker596"/>actual code is provided in <code class="inlineCode">command</code>, which just prints <a id="_idIndexMarker597"/>the <code class="inlineCode">MY_POD_NAME</code> environment variable in <a id="_idIndexMarker598"/>an endless loop. In turn, the <code class="inlineCode">MY_POD_NAME</code> environment variable is set with:</p>
<pre class="programlisting code"><code class="hljs-code">- name: MY_POD_NAME
valueFrom:
fieldRef:
fieldPath: metadata.name
</code></pre>
<p class="normal">This code takes the value from the <code class="inlineCode">metadata.name</code> field of the Pod. In fact, if we did not specify a name in the Pod template metadata section, a name would automatically be created by the StatefulSet and added to the resource internal representation of the Pod. The Kubernetes component that makes the Pod fields available to environment variables definition is <a id="_idIndexMarker599"/>called the <strong class="keyWord">downward API</strong>.</p>
<p class="normal">The above StatefulSet does nothing useful but just shows how to pass the Pod name to your containers.</p>
<p class="normal">Put the above code in a <code class="inlineCode">StateFulSetExample.yaml</code> file and apply it!</p>
<p class="normal">If you issue the <code class="inlineCode">kubectl get pods -n basic-examples</code> command, you can verify that all 3 replicas were <a id="_idIndexMarker600"/>created with the right names based on the StatefulSet name and <a id="_idIndexMarker601"/>on your indexes. Now let’s verify that <code class="inlineCode">podname-1</code> correctly received its name, by displaying its log:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl logs podname-1 -n basic-examples
</code></pre>
<p class="normal">You should <a id="_idIndexMarker602"/>see several lines with the right Pod name. Great!</p>
<p class="normal">Now let’s <a id="_idIndexMarker603"/>verify that our code created 3 different PVCs:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl get persistentvolume -n basic-examples
</code></pre>
<p class="normal">You should see three different claims.</p>
<p class="normal">When you finish experimenting with the example, you can delete everything with <code class="inlineCode">kubectl delete -f StateFulSetExample.yaml</code>. Unluckily, deleting everything does not also delete the PVC created by templates, as you can verify at this point. The simplest way to delete them is by deleting the <code class="inlineCode">basic-examples</code> namespace with:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl delete namespace basic-exampleswhole
</code></pre>
<p class="normal">Then, if you want, you can recreate it with:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl create namespace basic-examples
</code></pre>
<p class="normal">Statefulsets are used to deploy RabbitMQ clusters and database clusters in Kubernetes. If a master node is needed, then one with a specific index (usually 0) elects itself as a master. Each replica uses its own disk storage so that both data sharding and data replication strategies can be enforced. It’s likely that you won’t need to do this yourself, since the code for deploying clusters of the most famous message-broker and database clusters is already available on the web.</p>
<p class="normal">Having learned how to create and maintain several replicas of a microservice, we have to learn how to set and update the number of replicas, that is, how to scale our microservices.</p>
<h2 class="heading-2" id="_idParaDest-159"><a id="_idTextAnchor233"/>Scaling and autoscaling</h2>
<p class="normal">Scaling is fundamental for application performance tuning. We must distinguish between scaling the <a id="_idIndexMarker604"/>number of replicas of each microservice and scaling the number of nodes of the whole Kubernetes cluster.</p>
<p class="normal">The number <a id="_idIndexMarker605"/>of nodes is usually tuned according to the average CPU busy percentage. For instance, one might start with a 50% percentage when the initial application traffic is low. Then, as the application traffic increases, we maintain the same number of nodes till we are able to keep a good response time, possibly tuning the number of microservice replicas. Suppose that performance starts to decrease when the CPU busy percentage is 80%. Then, we can target, say, a 75% CPU busy time.</p>
<p class="normal">Automatic cluster scaling is possible just with cloud clusters, and each cloud provider offers some kind of autoscaling.</p>
<p class="normal">With regard to AKS, in the <em class="italic">Creating an Azure Kubernetes cluster</em> section, we saw that we can specify both a minimum and a maximum number of nodes, and AKS tries to optimize performance for us. You can also fine-tune how AKS decides the number of nodes. More details on this customization are given in the references in the <em class="italic">Further reading</em> section. </p>
<p class="normal">There are also automatic auto-scalers that integrate with various cloud providers (<a href="https://kubernetes.io/docs/concepts/cluster-administration/cluster-autoscaling/">https://kubernetes.io/docs/concepts/cluster-administration/cluster-autoscaling/</a>). As a default, auto-scalers increase the number of nodes when Kubernetes is not able to satisfy the resources required by a Pod, which is the sum of the <code class="inlineCode">resource-&gt;request</code> fields of all Pod containers.</p>
<p class="normal">Scaling microservice replicas, instead, is a more difficult task. You may calculate it by measuring the average replica response time and then calculating:</p>
<pre class="programlisting code"><code class="hljs-code">&lt;number of replicas&gt; = &lt;target throughput (requests per second)&gt;&lt;average response time in seconds&gt;
</code></pre>
<p class="normal">Where the target throughput should be a raw estimate calculated with simple calculations. For frontend microservices, it is just the number of requests you expect your application will receive for each API call. For Worker services, it can depend on the number of requests expected on several frontend services, but there is no standard way to compute it. Instead, you need to reason about how the application works and how the requests directed to that Worker microservice are created.</p>
<p class="normal">Then, you should monitor the system performance, looking for bottlenecks, according to the following procedure:</p>
<ol>
<li class="numberedList" value="1">Look for a microservice that is a bottleneck</li>
<li class="numberedList">Increase its number of replicas till it stops being a bottleneck</li>
<li class="numberedList">Repeat point 1 till there are no evident bottlenecks</li>
<li class="numberedList">Then optimize the number of cluster nodes to achieve good performance</li>
<li class="numberedList">Store the average CPU utilization memory occupation of all Deployments and StatefulSets, and the average number of requests reaching the whole application. You may use this data for setting auto-scalers. </li>
</ol>
<p class="normal">While StatefulSets are difficult to scale automatically, Deployments can be automatically scaled without causing problems. Therefore, you may use a Kubernetes Pod auto-scaler to scale them automatically.</p>
<p class="normal">Pod auto-scaler <a id="_idIndexMarker606"/>targets are either average per Pod resource consumption or metrics somehow connected with the traffic. In the first case, the auto-scaler chooses the number of <a id="_idIndexMarker607"/>replicas that makes the resource consumption closest to a specified target. In the second case, the number of replicas is set to the actual value of the traffic metric divided by the target value of the metric, that is, the traffic target is interpreted as the target traffic sustained by each Deployment Pod.</p>
<p class="normal">If several target types are specified, the maximum number of replicas proposed by each of them is taken.</p>
<p class="normal">An auto-scaler can be defined as follows:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: myautoscalername
  namespace: mynamespace
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
name: mydeploymentname
  minReplicas: 1
maxReplicas: 10
metrics:
  - type: &lt;resource or pod or object&gt;
    …
</code></pre>
<p class="normal">We specify the type of resource to control and the API where it is defined, and its name. Both the controlled resource and the auto-scaler must be defined in the same namespace. You can set <code class="inlineCode">scaleTargetRef-&gt;kind</code> also to <code class="inlineCode">StatefulSet</code>, but you need to verify that the change in the number <a id="_idIndexMarker608"/>of replicas doesn’t break your sharding algorithm, both in the long run and during transitions between different numbers of replicas.</p>
<p class="normal">Then, we specify the <a id="_idIndexMarker609"/>maximum and minimum number of replicas. If the computed number of replicas exceeds this interval, it is cut to either <code class="inlineCode">minReplicas</code> or <code class="inlineCode">maxReplicas</code>.</p>
<p class="normal">Finally, we have the list of criteria, where each criterion may refer to three types of <strong class="keyWord">metrics</strong>: <code class="inlineCode">resource</code>, <code class="inlineCode">pod</code>, or <code class="inlineCode">object</code>. We will describe each of them in a separate subsection.</p>
<h3 class="heading-3" id="_idParaDest-160"><a id="_idTextAnchor234"/>Resource metrics</h3>
<p class="normal">Resource metrics are based on the average memory and CPU resources wasted by each Pod. The target <a id="_idIndexMarker610"/>consumption may be an absolute value such as 100Mb, or 20mi (millicores), in which <a id="_idIndexMarker611"/>case the number of replicas is computed as <code class="inlineCode">&lt;actual average consumption&gt;/&lt;target consumption&gt;</code>. Resource metrics based on absolute values are declared as:</p>
<pre class="programlisting code"><code class="hljs-code">- type: Resource
resource:
name: &lt;memory or cpu&gt;
target:
type: AverageValue
averageValue: &lt;target memory or cpu&gt;
</code></pre>
<p class="normal">The target can also be specified as a percentage of the total Pod <code class="inlineCode">resource-&gt;request</code> declared (sum of all Pod containers). In this case, Kubernetes first computes:</p>
<pre class="programlisting code"><code class="hljs-code">&lt;utilization&gt; = 100*&lt;actual average consumption&gt;/&lt;declared resource request&gt;
</code></pre>
<p class="normal">Then, the number of replicas is computed as <code class="inlineCode">&lt;utilization&gt;/&lt;target utilization&gt;</code>. For instance, if the target CPU utilization is 50 on average, each Pod must waste 50% of the CPU millicores declared in the request. Therefore, if the average CPU wasted by all Pods of a Deployment is 30Mi, while the CPU required by each Pod is 20mi, we compute the utilization as 100*30/20= 150. So, the number of replicas is 150/50 = 3.</p>
<p class="normal">In this case, the code is:</p>
<pre class="programlisting code"><code class="hljs-code">- type: Resource
resource:
name: &lt;memory or cpu&gt;
target:
type: Utilization
averageUtilization: &lt;target memory or cpu utilization&gt;
</code></pre>
<h3 class="heading-3" id="_idParaDest-161"><a id="_idTextAnchor235"/>Pod metrics</h3>
<p class="normal">Pod metrics <a id="_idIndexMarker612"/>are not standard but depend on the metrics actually computed <a id="_idIndexMarker613"/>by each specific cloud platform or on-premise installation. Pod metric constraints are declared as:</p>
<pre class="programlisting code"><code class="hljs-code">- type: Pods
pods:
metric:
name: packets-per-second
target:
type: AverageValue
averageValue: 1k
</code></pre>
<p class="normal">Where we suppose that the <code class="inlineCode">packets-per-second</code> metric exists in the platform and computes the average communication packets received per second by a Pod. The calculation of the number of replicas is done as in the case of <code class="inlineCode">averageValue</code> for resource metrics.</p>
<h3 class="heading-3" id="_idParaDest-162"><a id="_idTextAnchor236"/>Object metrics</h3>
<p class="normal">Object metrics refer to metrics computed on objects outside of the controlled Pods but inside the Kubernetes <a id="_idIndexMarker614"/>cluster. Like Pod metrics, object metrics are also not <a id="_idIndexMarker615"/>standard but depend on the metrics actually computed by each specific platform.</p>
<p class="normal">In the <em class="italic">Advanced Kubernetes configuration</em> section, we will describe Kubernetes resources called <strong class="keyWord">Ingresses</strong> that interface <a id="_idIndexMarker616"/>the Kubernetes cluster with the external world. Typically, all Kubernetes input traffic transits through a single Ingress, so we can measure the total input traffic by measuring the traffic inside that Ingress. Once a cluster has been empirically optimized, and we need to just adapt it to temporary peaks, the easiest way to do it is by connecting the number of replicas of each frontend microservice and also of some Worker microservice to the total application input traffic. This can be done with Object metric constraints that reference the unique application Ingress:</p>
<pre class="programlisting code"><code class="hljs-code">- type: Object
object:
metric:
name: requests-per-second
describedObject:
apiVersion: networking.k8s.io/v1
kind: Ingress
name: application-ingress
target:
type: Value
value: 10k
</code></pre>
<p class="normal">In this case, we have a <code class="inlineCode">value</code> since we don’t average on several objects, but the number of replicas is computed as for the Pod metrics. Moreover, in this case, we must be sure that the <code class="inlineCode">requests-per-second</code> metric is actually computed by the infrastructure on all Ingresses.</p>
<p class="normal">Personally, I always use CPU and memory metrics since they are available on all platforms, and since using <a id="_idIndexMarker617"/>the procedure sketched in this subsection, it is reasonably easy to find good target values for them.</p>
<p class="normal">Though all <a id="_idIndexMarker618"/>cloud providers offer useful Kubernetes metrics, there are open-source metric servers that can also be installed on on-premises Kubernetes clusters through <code class="inlineCode">.yaml</code> files. See the <em class="italic">Further reading</em> section for an example.</p>
<p class="normal">Minikube has a metrics-server addon that can be installed with <code class="inlineCode">minikube addons enable metrics-server</code>. You also need it to use standard resource metrics like CPU and memory.</p>
<p class="normal">In the next section, we will analyze how to test and deploy a microservice application and will put these concepts into practice by running and debugging the Worker microservice we implemented in <a href="Chapter_7.xhtml#_idTextAnchor151"><em class="italic">Chapter 7</em></a><em class="italic">, Microservices in Practice</em>, on Minikube.</p>
<h1 class="heading-1" id="_idParaDest-163"><a id="_idTextAnchor237"/>Running your microservices on Kubernetes</h1>
<p class="normal">In this section, we will test the routes-matching worker microservice in Minikube, but we will also <a id="_idIndexMarker619"/>describe how to organize the various environments your microservices application will be deployed to: development, staging, and production. Each environment has its own peculiarities, such as an easy way to test each change in development and maximizing performance in production.</p>
<h2 class="heading-2" id="_idParaDest-164"><a id="_idTextAnchor238"/>Organizing all deployment environments</h2>
<p class="normal">It is also <a id="_idIndexMarker620"/>worth pointing out that the simplest test in Minikube requires a not-negligible setup time. Therefore, most development simply uses Docker, that is, a few containerized microservices organized into a unique Visual Studio solution that starts all of them when you launch the solution.</p>
<p class="normal">At this stage, we don’t test the whole application but just a few tightly interacting microservices, possibly simulating the remainder of the application with stubs. If communication is handled through a message broker, it is enough to launch all microservices and the message broker to test everything; otherwise, if we rely on direct communication between microservices, we must connect all microservices in a virtual network.</p>
<p class="normal">Docker offers the possibility to both create a virtual network and connect running containers to it. The virtual network created by Docker also includes your development machine, which gets the <strong class="keyWord">host.docker.internal</strong> hostname. Therefore, all microservices can use various services running on the development machine, such as RabbitMQ, SQL Server, and Redis.</p>
<p class="normal">You can create a test virtual network in Docker with:</p>
<pre class="programlisting con"><code class="hljs-con">docker network create myvirtualnet
</code></pre>
<p class="normal">Then, attaching <a id="_idIndexMarker621"/>all running microservices to this network is super easy. It is enough to modify their project files as follows:</p>
<pre class="programlisting code"><code class="hljs-code">&lt;PropertyGroup&gt;
&lt;TargetFramework&gt;net9.0&lt;/TargetFramework&gt;
…
&lt;DockerfileRunArguments&gt;--net myvirtualnet --name myhostname&lt;/DockerfileRunArguments&gt;
&lt;/PropertyGroup&gt;
</code></pre>
<p class="normal">Then, you can also <a id="_idIndexMarker622"/>add other <code class="inlineCode">docker run</code> arguments, such as a volume mount.</p>
<p class="normal">Testing on Minikube can be performed at the end of the working day or simply after the complete implementation of a feature.</p>
<p class="normal">In the next subsections, we will compare all deployment environments on the following axes:</p>
<ol>
<li class="numberedList" value="1">Database engine and database installation</li>
<li class="numberedList">Container registries</li>
<li class="numberedList">Message broker installation</li>
<li class="numberedList">Debugging techniques</li>
</ol>
<h3 class="heading-3" id="_idParaDest-165"><a id="_idTextAnchor239"/>Database engine and database installation</h3>
<p class="normal">Development tests with Docker or Minikube may all use a database engine running directly on the <a id="_idIndexMarker623"/>development machine. You may use <a id="_idIndexMarker624"/>either an actual installation or an engine running as a Docker container. The advantage is that the database is also accessible from Visual Studio, so you can pass all migrations while you develop them.</p>
<p class="normal">You can also use fresh Docker containers running the database engine to start databases from scratch and perform unit tests, or to test the overall migration set.</p>
<div><p class="normal"> If you installed Minikube with the Docker driver, a database running on your development machine can be reached from inside your Minikube containers by using either the <strong class="keyWord">host.minikube.internal</strong> or <strong class="keyWord">host.docker.internal</strong> hostnames. Therefore, if you use <strong class="keyWord">host.docker.internal</strong>, you will be able to reach your host machine from both Minikube and from your containerized applications directly launched by Visual Studio.</p>
</div>
<p class="normal">On both staging and production, you can use database cloud services that ensure good performance, are scalable, and offer clustering, replication, geographic redundancy, and so on. It’s also possible to deploy the database inside your Kubernetes cluster, but in this case, you must buy a license, you should dedicate ad hoc Kubernetes nodes for the database (virtual machines that <a id="_idIndexMarker625"/>ensure optimal database performance), and you should fine-tune the database configuration. Therefore, if there are no compelling <a id="_idIndexMarker626"/>reasons for a different choice, it is more convenient to opt for cloud services.</p>
<p class="normal">Moreover, both in production and staging, you can’t configure your Deployments to automatically apply migrations when they start; otherwise, all replicas will attempt to apply them. It’s better to extract a database script from your migrations and apply it with a database DBO user privilege, while leaving the microservice replicas with a less privileged database user.</p>
<p class="normal">A database script can be extracted from all migrations with the migration command below:</p>
<pre class="programlisting con"><code class="hljs-con">Script-Migration -From<a id="_idTextAnchor240"/> &lt;initial migration&gt; -To &lt;final migration&gt; -Output &lt;name
of output file&gt;
</code></pre>
<p class="normal">Let’s move on to container registries.</p>
<h3 class="heading-3" id="_idParaDest-166"><a id="_idTextAnchor241"/>Container registries</h3>
<p class="normal">As far as staging and production are concerned, they can both use the same container registry since containers <a id="_idIndexMarker627"/>are versioned. So, for instance, production can use <code class="inlineCode">v1.0</code>, while staging can use <code class="inlineCode">v2.0-beta1</code>. It is better if registries belong to the <a id="_idIndexMarker628"/>same cloud subscription of the Kubernetes cluster to simplify credential handling. For instance, in the case of AKS, it is enough to associate a registry to an AKS cluster once and for all to grant access to the cluster to the registry (see the <em class="italic">Creating an Azure Kubernetes cluster</em> subsection of this chapter).</p>
<p class="normal">As far as development is concerned, each developer can use the same registry used by the staging environment for the containers they are not working on, but each developer should have a private registry for the containers they are working on, so they can experiment with no risk of dirtying the “official image” registries. Therefore, the simplest solution is to install a local registry in your Docker Desktop. You can do this with:</p>
<pre class="programlisting con"><code class="hljs-con">docker run -d -p 5000:5000 --name registry registry:2
</code></pre>
<p class="normal">Once the container has been created with the instruction above, you can stop and restart it from the Docker Desktop graphical user interface.</p>
<p class="normal">Unluckily, as a default, both Docker and Minikube do not accept interacting with insecure registries, that is, with registries that do not support HTTPS with a certificate signed by a public authority, so we must <a id="_idIndexMarker629"/>instruct both Docker and Minikube to accept insecure interaction with the local registry.</p>
<p class="normal">Let’s open the <a id="_idIndexMarker630"/>Docker Desktop graphical user interface and click on the settings image in the top-right corner:</p>
<figure class="mediaobject"><img alt="Figure 8.5: Docker settings" src="img/B31916_08_5.png"/></figure>
<p class="packt_figref">Figure 8.5: Docker settings</p>
<p class="normal">Then, select <strong class="screenText">Docker Engine</strong> from the left menu, and edit the big text box that contains Docker configuration information, and add the entry shown below to the existing JSON content:</p>
<pre class="programlisting code"><code class="hljs-code"> …….,
"insecure-registries": [
    "host.docker.internal:5000",
    "host.minikube.internal:5000"
</code></pre>
<p class="normal">The above settings add the 5000 ports of both hostnames that point to your host computer to the allowed insecure registries. The result should be something like:</p>
<figure class="mediaobject"><img alt="Figure 8.6: Adding a local registry to Docker allowed insecure registries" src="img/B31916_08_6.png"/></figure>
<p class="packt_figref">Figure 8.6: Adding a local registry to Docker allowed insecure registries</p>
<p class="normal">As far as Minikube is concerned, you have to destroy your current Minikube VM with:</p>
<pre class="programlisting con"><code class="hljs-con">minikube delete
</code></pre>
<p class="normal">Then, you need <a id="_idIndexMarker631"/>to create a new VM image with <a id="_idIndexMarker632"/>the right insecure registry settings:</p>
<pre class="programlisting con"><code class="hljs-con">minikube start --insecure-registry="host.docker.internal:5000" --insecure-registry="host.minikube.internal:5000"
</code></pre>
<p class="normal">Please execute all the above steps because we will need a local registry for testing the route-planning microservice.</p>
<p class="normal">If Minikube also needs to access other password-protected registries, you must configure and enable the <strong class="keyWord">registry-creds</strong> addon:</p>
<pre class="programlisting con"><code class="hljs-con">minikube addons configure registry-creds
</code></pre>
<p class="normal">Once you issue the above command, you will be asked to configure Google, AWS, Azure, or Docker private registries and enter your credentials.</p>
<p class="normal">After a successful configuration, you can enable the credential usage with:</p>
<pre class="programlisting con"><code class="hljs-con">minikube addons enable registry-creds
</code></pre>
<p class="normal">Let’s move on to the message broker.</p>
<h3 class="heading-3" id="_idParaDest-167"><a id="_idTextAnchor242"/>Message broker installation</h3>
<p class="normal">RabbitMQ can be installed both locally and in the cloud, and works on all clouds, so it really is a good option. You can run a single RabbitMQ server or a server cluster. A RabbitMQ cluster can also be <a id="_idIndexMarker633"/>installed on the Kubernetes cluster itself. During development, you may install it on Minikube, but it is more convenient to run it outside of Minikube, so it can also be easily reached by applications running outside of Minikube, which, in turn, facilitates application debugging, as we will see in the next subsection.</p>
<p class="normal">In staging <a id="_idIndexMarker634"/>and production, the simplest way to install a RabbitMQ cluster is by installing the so-called <strong class="keyWord">RabbitMQ Cluster Operator</strong> with:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl apply -f https://raw.githubusercontent.com/rabbitmq/cluster-operator/main/docs/examples/hello-world/rabbitmq.yaml
</code></pre>
<p class="normal">The RabbitMQ operator <a id="_idIndexMarker635"/>defines the <strong class="keyWord">RabbitmqCluster</strong> custom resource that represents a RabbitMQ Cluster. You can create and configure <strong class="keyWord">RabbitmqCluster</strong> as you configure any other Kubernetes resource:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: rabbitmq.com/v1beta1
kind: RabbitmqCluster
metadata:
name: &lt;cluster name&gt;
namespace: &lt;cluster namespace&gt;
spec:
replicas: 3 # default is 1. Replicas should be odd.
persistence:
storageClassName: &lt;storage class name&gt; # default is the default storage class
storage: 20Gi # default 10Gi
</code></pre>
<p class="normal">The persistence section specifies the options for persisting queues on persistent storage. If you omit it, all default values will be taken. If you omit the number of replicas, a cluster with a single server will be created. More options are available in the official documentation: <a href="https://www.rabbitmq.com/kubernetes/operator/using-operator">https://www.rabbitmq.com/kubernetes/operator/using-operator</a>.</p>
<p class="normal">You can get the username and password of your RabbitMQ cluster default user by printing the <code class="inlineCode">&lt;cluster name&gt;-default-user</code> secret where they are stored, as shown below:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl get secret &lt;cluster name&gt;-default-user -n &lt;cluster namespace&gt; -o yaml
</code></pre>
<p class="normal">Both username and password are base-64 encoded. The simplest way to decode them is by copying each of them from the console output, opening a Linux console, and using the <code class="inlineCode">base64</code> command:</p>
<pre class="programlisting con"><code class="hljs-con">echo &lt;string to decode&gt; | base64 -d
</code></pre>
<p class="normal">If you want, you may also install the RabbitMQ cluster operator in Minikube, but in this case, it is better to start Minikube with at least 4 CPUs and 6-8 gigabytes of run.</p>
<p class="normal">If you need to connect to the RabbitMQ cluster from outside of the Kubernetes cluster for debugging purposes, you can use the <code class="inlineCode">kubectl port-forward</code> command:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl port-forward service/&lt;cluster name&gt; 5672:5672
</code></pre>
<p class="normal">The above instruction <a id="_idIndexMarker636"/>freezes the console and forwards port 5672 of the <code class="inlineCode">service/&lt;cluster name&gt;</code> ClusterIP service that is part of the RabbitMQ cluster to port 5672 of localhost. The port-forwarding remains active while the console window is open or <code class="inlineCode">ctrl-c</code> is issued to abort the instruction.</p>
<p class="normal">The general <code class="inlineCode">kubectl port-forward</code> syntax is:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl port-forward service/&lt;service name&gt; &lt;local host port&gt;:&lt;service port&gt;
</code></pre>
<p class="normal">In our case, the service name is equal to the cluster name.</p>
<div><p class="normal"> The service &lt;cluster name&gt; is the ClusterIP service you must use to access the RabbitMQ cluster from inside the Kubernetes cluster. Therefore, the RabbitMQ hostname to specify in the connection is <code class="inlineCode">&lt;cluster name&gt;.&lt;cluster namespace&gt;</code>.</p>
</div>
<p class="normal">You can also access the RabbitMQ management UI with your browser by forwarding the 15672 port:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl port-forward service/&lt;cluster name&gt; 15672:15672
</code></pre>
<p class="normal">Then, the UI will be available at <code class="inlineCode">localhost:15672</code>. There, you must use the credentials you previously extracted from the <code class="inlineCode">cluster name&gt;-default-user</code> secret.</p>
<p class="normal">The port forwarding is safe and doesn’t expose RabbitMQ to the outside world since the connection between localhost and the service is mediated by the Kubernetes API server. It can be safely used to connect test code running on the development machine with the RabbitMQ cluster, as we will see in more detail in the next subsection.</p>
<h3 class="heading-3" id="_idParaDest-168"><a id="_idTextAnchor243"/>Debugging techniques</h3>
<p class="normal">When you launch all containers from Visual Studio, you can debug your code without performing <a id="_idIndexMarker637"/>any further configuration. However, if you need to debug some microservices running either in Minikube, in staging, or in production, you need some supplementary configuration.</p>
<p class="normal">Instead of trying to attach the debugger inside of your Kubernetes cluster, a simpler approach is to use the so-called bridge: you select a specific microservice to debug, and instead of debugging it in Kubernetes, you redirect its traffic to a replica of your microservice running in Visual Studio, then you redirect all local microservice output traffic again inside the cluster. This way, you debug just a local copy that has been compiled in debug mode, overcoming both the need to replace the release code with debug code, and the difficulty of attaching a debugger inside of your Kubernetes cluster.</p>
<p class="normal">The image below exemplifies the bridge idea:</p>
<figure class="mediaobject"><img alt="Figure 8.7: Bridging" src="img/B31916_08_7.png"/></figure>
<p class="packt_figref">Figure 8.7: Bridging</p>
<p class="normal">If both inputs and outputs are handled by a message broker, bridging is easy: it is enough to connect the local copy to the same RabbitMQ queues of the in-cluster replicas. This way, part of the traffic will be automatically forwarded to the local copy. If the RabbitMQ cluster runs inside the Kubernetes cluster, you need to forward its ports on localhost as explained in the previous section.</p>
<p class="normal">Moreover, if the microservice is connected to a database, we must also connect the local copy to the same database. If you are in production, this might require the definition of a firewall rule to enable access of your development machine to the database.</p>
<p class="normal">If some input and output are handled by services instead of message brokers, bridging becomes more complex. More specifically, forwarding the output to a service inside the Kubernetes cluster is quite easy since it requires just port-forwarding the target service on localhost with <code class="inlineCode">kubectl port-forward</code>. However, forwarding traffic from a service to the local microservice copy requires some kind of hack on the service.</p>
<p class="normal">Services compute <a id="_idIndexMarker638"/>the Pods they must route the traffic to and then create resources called <code class="inlineCode">EndpointSlice </code>containing the IP addresses where they must route the traffic. Therefore, in order to route all service traffic to your local machine, you need to override the <code class="inlineCode">EndpointSlices</code> of that service. This can be done by removing the selector of the target service so that all <code class="inlineCode">EndpointSlices</code> will be deleted, and then manually adding an <code class="inlineCode">EndpointSlice</code> that points to your development machine.</p>
<p class="normal">You can do this as follows:</p>
<ol>
<li class="numberedList" value="1">Get the target service definition with:
        <pre class="programlisting con-one"><code class="hljs-con">kubectl get service &lt;service name&gt; -n &lt;service namespace&gt; -o yaml
</code></pre>
</li>
<li class="numberedList">Remove the selector, and apply the new definition.</li>
<li class="numberedList">If you are working on a remote cluster, add the <code class="inlineCode">EndpointSlice</code> below:
        <pre class="programlisting code-one"><code class="hljs-code">apiVersion: discovery.k8s.io/v1
kind: EndpointSlice
metadata:
name: &lt;service name&gt;-1
namespaces: &lt;service namespace&gt;
labels:
kubernetes.io/service-name: &lt;service name&gt;
addressType: IPv4
ports:
- name: http # should match with the name of the service port
appProtocol: http
protocol: TCP
port: &lt;target port&gt;
endpoints:
- addresses:
- "&lt;your development machine IP address&gt;"
</code></pre>
</li>
<li class="numberedList">If, instead, you are working on a Minikube local cluster, add the <code class="inlineCode">EndpointSlice</code> below:
        <pre class="programlisting code-one"><code class="hljs-code">apiVersion: discovery.k8s.io/v1
kind: EndpointSlice
metadata:
name: &lt;service name&gt;-1
namespaces: &lt;service namespace&gt;
labels:
kubernetes.io/service-name: &lt;service name&gt;
addressType: FQDN
ports:
- name: http # should match with the name of the service port
appProtocol: http
protocol: TCP
port: &lt;target port&gt;
endpoints:
- addresses:
- "host.minikube.local"
</code></pre>
</li>
<li class="numberedList">When you finish debugging, reapply the original service definition. Your custom <code class="inlineCode">EndpointSlice</code> will be automatically destroyed.</li>
</ol>
<p class="normal">As you can see, using message brokers simplifies a lot of the debugging. It is the advised option when implementing applications. Services are a better option when implementing tools, such as database <a id="_idIndexMarker639"/>clusters, or message brokers that run inside your cluster.</p>
<p class="normal">There are <a id="_idIndexMarker640"/>tools that automatically handle all needed service hacking, such as <strong class="keyWord">Bridge to Kubernetes</strong> (<a href="https://learn.microsoft.com/en-us/visualstudio/bridge/bridge-to-kubernetes-vs">https://learn.microsoft.com/en-us/visualstudio/bridge/bridge-to-kubernetes-vs</a>), but unluckily, Microsoft announced that it will stop supporting it. Microsoft will advise a valid alternative.</p>
<p class="normal">Now we are<a id="_idTextAnchor244"/> finally ready to test an actual Microservice on Minikube.</p>
<h2 class="heading-2" id="_idParaDest-169"><a id="_idTextAnchor245"/>Testing the route-matching worker microservice</h2>
<p class="normal">We will <a id="_idIndexMarker641"/>test the route-matching worker microservice implemented in <a href="Chapter_7.xhtml#_idTextAnchor151"><em class="italic">Chapter 7</em></a><em class="italic">, Microservices in Practice</em>, together with two stub microservices. The first one will send test input to it, while <a id="_idIndexMarker642"/>the other will collect all its output and will write it in its console, so that we may access this output with the <code class="inlineCode">kubectl logs</code> command. This is a typical way to perform a preliminary test. Then, more complex tests may also involve other application services.</p>
<p class="normal">Let’s create a copy of our route-matching worker microservice solution, then add two more <strong class="keyWord">Worker service</strong> projects, and call them respectively <code class="inlineCode">FakeSource </code>and <code class="inlineCode">FakeDestination</code>. For each of them, enable container support for Linux as shown in the following screenshot:</p>
<figure class="mediaobject"><img alt="Figure 8.8: Worker services project settings" src="img/B31916_08_8.png"/></figure>
<p class="packt_figref">Figure 8.8: Worker services project settings</p>
<p class="normal">Then, let’s <a id="_idIndexMarker643"/>also add all needed EasyNetQ packages to enable both services to interact with a RabbitMQ cluster:</p>
<ol>
<li class="numberedList" value="1"><code class="inlineCode">EasyNetQ</code></li>
<li class="numberedList"><code class="inlineCode">EasyNetQ.Serialization.NewtonsoftJson</code></li>
<li class="numberedList"><code class="inlineCode">EasyNetQ.Serialization.SystemTextJson</code></li>
</ol>
<p class="normal">Select <a id="_idIndexMarker644"/>at least version 8, also if it is still a prerelease.</p>
<p class="normal">Then you must add RabbitMQ to the services in the <code class="inlineCode">Program.cs</code> of both projects:</p>
<pre class="programlisting code"><code class="hljs-code">builder.Services.AddEasyNetQ(
    builder.Configuration?.GetConnectionString("RabbitMQConnection") ?? 
        string.Empty);
</code></pre>
<p class="normal">The RabbitMQ connection string must be added in the environment variables defined in <code class="inlineCode">Properties-&gt;launchSettings.json</code>, as shown below:</p>
<pre class="programlisting code"><code class="hljs-code">"Container (Dockerfile)": {
"commandName": "Docker",
"environmentVariables": {
"ConnectionStrings__RabbitMQConnection":
        "host=host.docker.internal:5672;username=guest;password=_myguest;
        publisherConfirms=true;timeout=10"
}
</code></pre>
<p class="normal">Finally, refer to the <code class="inlineCode">SharedMessages</code> project from both <code class="inlineCode">FakeSource </code>and <code class="inlineCode">FakeDestination</code>, so they <a id="_idIndexMarker645"/>can use all application communication messages.</p>
<p class="normal">At this <a id="_idIndexMarker646"/>point, we are ready to code our stub services. In the <code class="inlineCode">Worker.cs</code> file scaffolded by Visual Studio in the <code class="inlineCode">FakeDestination</code> project, replace the existing class with:</p>
<pre class="programlisting code"><code class="hljs-code">public class Worker: BackgroundService
{
    private readonly ILogger&lt;Worker&gt; _logger;
    private readonly IBus _bus;
    public Worker(ILogger&lt;Worker&gt; logger, IBus bus)
    {
        _logger = logger;
        _bus= bus;
    }
    protected override async Task ExecuteAsync(CancellationToken 
        stoppingToken)
    {
        var routeExtensionProposalSubscription = await _bus.PubSub.
        SubscribeAsync&lt;
            RouteExtensionProposalsMessage&gt;(
               "FakeDestination",
                m =&gt;
                {
                    var toPrint=JsonSerializer.Serialize(m);
                    _logger.LogInformation("Message received: {0}", 
                                            toPrint);
                },
            stoppingToken);
        await Task.Delay(Timeout.Infinite, stoppingToken);
        routeExtensionProposalSubscription.Dispose();
    }
}
</code></pre>
<p class="normal">The hosted service adds a subscription named <code class="inlineCode">FakeDestination</code> to the <code class="inlineCode">RouteExtensionProposalsMessage</code> event. This way, it receives all matching proposals between an existing route and some requests. Once the subscription handler receives a proposal, it just logs the message in JSON format, so we can verify that the right match proposal events are generated by exploring the <code class="inlineCode">FakeDestination</code> logs.</p>
<p class="normal">In the <code class="inlineCode">Worker.cs</code> file scaffolded by Visual Studio in the FakeSource project, we will replace the existing class with simple code that does the following:</p>
<ol>
<li class="numberedList" value="1">Creates <a id="_idIndexMarker647"/>three town messages: Phoenix, Santa Fe,<a id="_idTextAnchor246"/> and Cheyenne.</li>
<li class="numberedList">Sends a request going from Phoenix to Santa Fe.</li>
<li class="numberedList">Sends <a id="_idIndexMarker648"/>a route offer passing from Phoenix, Santa Fe, and Cheyenne. As soon as this message is received by the route planning wo<a id="_idTextAnchor247"/>rker microservice, it should create a proposal to match this offer with the previous request. This proposal should be received by <code class="inlineCode">FakeDestination</code> and logged.</li>
<li class="numberedList">Sends a request going from Santa Fe to Cheyenne. As soon as this message is received by the routes planning worker microservice, it should create a proposal to match this request with the previous offer. This proposal should be received by <code class="inlineCode">FakeDestination</code> and logged.</li>
<li class="numberedList">After 10 seconds, it simulates that both previous proposals have been accepted and creates a route extension event based on the previous offer and containing both the matched requests. As soon as this message is received by the route planning worker microservice, it should both update the offer and should add the two requests to the offer. As a result, the <code class="inlineCode">RouteId</code> field of both requests should point to the offer <code class="inlineCode">Id</code>.</li>
</ol>
<p class="normal">The code of the <code class="inlineCode">Worker.cs</code> class is:</p>
<pre class="programlisting code"><code class="hljs-code">public class Worker : BackgroundService
{
    private readonly ILogger&lt;Worker&gt; _logger;
    private readonly IBus _bus;
    public Worker(ILogger&lt;Worker&gt; logger, IBus bus)
 {
        _logger = logger;
        _bus = bus;
    }
    protected override async Task ExecuteAsync(CancellationToken 
        stoppingToken)
 {
        …
        …
        /* The code that defines all messages has been omitted */
        var delayInterval = 5000;
        await Task.Delay(delayInterval, stoppingToken);
        await _bus.PubSub.PublishAsync&lt;RouteRequestMessage&gt;(request1);
        await Task.Delay(delayInterval, stoppingToken);
        await _bus.PubSub.PublishAsync&lt;RouteOfferMessage&gt;(offerMessage);
        await Task.Delay(delayInterval, stoppingToken);
        await _bus.PubSub.PublishAsync&lt;RouteRequestMessage&gt;(request2);
        await Task.Delay(2*delayInterval, stoppingToken);
        await _bus.PubSub.PublishAsync&lt;
RouteExtendedMessage&gt;(extendedMessage);
        await Task.Delay(Timeout.Infinite, stoppingToken);
    }
</code></pre>
<p class="normal">The code <a id="_idIndexMarker649"/>that defines all messages has been omitted. You can find the full code in the <code class="inlineCode">ch08-&gt;CarSharing-&gt;FakeSource-&gt;Worker.cs</code> file of the GitHub repository associated with the book.</p>
<p class="normal">Now let’s <a id="_idIndexMarker650"/>prepare to execute all microservices in Docker by performing the following steps:</p>
<ol>
<li class="numberedList" value="1">Right-click on the solution line in Visual Studio Solution Explorer and select <strong class="screenText">Configure Startup Projects…</strong>.</li>
<li class="numberedList">Then select <strong class="screenText">Multiple startup projects</strong>, and change the name of the launch option to <strong class="screenText">AllMicroservices</strong>.</li>
<li class="numberedList">Then, select all three <code class="inlineCode">FakeDestination</code>, <code class="inlineCode">FakeSource</code>, and <code class="inlineCode">RoutesPlanning</code> projects, and for each of them, choose <strong class="screenText">Start</strong> for <strong class="screenText">Action</strong> and <strong class="screenText">Container (Docker file)</strong> for <strong class="screenText">Debug Target</strong>, as shown below:</li>
</ol>
<figure class="mediaobject"><img alt="Figure 8.9: Launch settings" src="img/B31916_08_9.png"/></figure>
<p class="packt_figref">Figure 8.9: Launch settings</p>
<p class="normal">Now you <a id="_idIndexMarker651"/>can launch all projects simultaneously by choosing <strong class="screenText">AllMicroservices</strong> in Visual Studio Debug Launcher.</p>
<p class="normal">Ensure <a id="_idIndexMarker652"/>that both the application’s SQL Server and the RabbitMQ server are running. Then, build the project and launch it.</p>
<p class="normal">In the Containers tab that appears, select <code class="inlineCode">FakeDestination</code>, so you can inspect its logs. After a few seconds, you should see the two match proposal messages, as shown below:</p>
<figure class="mediaobject"><img alt="Figure 8.10: FakeDestination logs" src="img/B31916_08_10.png"/></figure>
<p class="packt_figref">Figure 8.10: FakeDestination logs</p>
<p class="normal">Then, in the SQL Server Object Explorer pane, select the application database, if already there; otherwise, connect to it, and then show its tables:</p>
<figure class="mediaobject"><img alt="Figure 8.11: Application database" src="img/B31916_08_11.png"/></figure>
<p class="packt_figref">Figure 8.11: Application database</p>
<p class="normal">Right-click <a id="_idIndexMarker653"/>on both <strong class="screenText">dbo.RouteOffers</strong> and <strong class="screenText">dbo.RouteRequests</strong> and select <strong class="screenText">View Data</strong> to see all their data. You should see that the offer’s <code class="inlineCode">Timestamp</code> changed to 2 because the offer <a id="_idIndexMarker654"/>was updated once the two matching proposals were accepted:</p>
<figure class="mediaobject"><img alt="Figure 8.12: Updated offer" src="img/B31916_08_12.png"/></figure>
<p class="packt_figref">Figure 8.12: Updated offer</p>
<p class="normal">Moreover, you should see that the two requests have been associated with the offer:</p>
<figure class="mediaobject"><img alt="Figure 8.13: Updated requests" src="img/B31916_08_13.png"/></figure>
<p class="packt_figref">Figure 8.13: Updated requests</p>
<p class="normal">Now let’s stop debugging and delete all records in the <strong class="screenText">dbo.RouteOffers</strong> and <strong class="screenText">dbo.RouteRequests</strong> tables.</p>
<p class="normal">It’s time to deploy our Microservices in Minikube!</p>
<p class="normal">We will use the same RabbitMQ and SQL Servers running on the development machine. However, there are some preliminary steps to perform before we start deploying our <code class="inlineCode">.yaml</code> files in Minikube:</p>
<ol>
<li class="numberedList" value="1">We must create adequate Docker images, since the debug images created by Visual Studio can’t run outside of Visual Studio. They all have a <code class="inlineCode">dev</code> version. Go to the Docker files of the three <code class="inlineCode">FakeDestination</code>, <code class="inlineCode">FakeSource</code>, and <code class="inlineCode">RoutesPlanning</code> projects in Visual Studio Explorer, right-click on them, and select <strong class="screenText">Build Docker Image</strong>. These actions will create three Docker images with the latest version.</li>
<li class="numberedList">Launch the local registry container from inside the Docker UI. If you have not yet created a registry container, please refer to the <em class="italic">Container registries</em> subsection for installation instructions.</li>
<li class="numberedList">Push our <a id="_idIndexMarker655"/>newly created images in this registry so they can be downloaded <a id="_idIndexMarker656"/>by Minikube (remember that you need a Linux console to issue the commands below):
        <pre class="programlisting con-one"><code class="hljs-con">docker tag fakesource:latest localhost:5000/fakesource:latest
docker push localhost:5000/fakesource:latest
docker tag fakedestination:latest localhost:5000/fakedestination:latest
docker push localhost:5000/fakedestination:latest
docker tag routesplanning:latest localhost:5000/routesplanning:latest
docker push localhost:5000/routesplanning:latest
</code></pre>
</li>
</ol>
<p class="normal">We need to create 3 deployments, one for each of our three microservices. Let’s create a <code class="inlineCode">Kubernetes</code> folder in the <code class="inlineCode">CarSharing</code> solution folder. We will place our deployment definitions there.</p>
<p class="normal">Below <code class="inlineCode">FakeSource.yaml</code>:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: apps/v1
kind: Deployment
metadata:
name: fakesource
namespace: car-sharing
labels:
app: car-sharing
classification: stub
role: fake-source
spec:
selector:
matchLabels:
app: car-sharing
role: fake-source
replicas: 1
template:
metadata:
labels:
app: car-sharing
classification: stub
role: fake-source
spec:
containers:
- image: host.docker.internal:5000/fakesource:latest
name: fakesource
resources:
requests:
cpu: 10m
memory: 10Mi
env:
- name: ConnectionStrings__RabbitMQConnection
value:
  "host=host.docker.internal:5672;username=guest;password=_myguest;
              publisherConfirms=true;timeout=10"
</code></pre>
<p class="normal">It contains <a id="_idIndexMarker657"/>just a single environment variable for the RabbitMQ connection string – the same one we defined in <code class="inlineCode">launchSettings.json</code>. The resource request is minimal. Labels are a <a id="_idIndexMarker658"/>documentation tool, too. Therefore, they define both the application name, the role in the application, and the fact that this microservice is a stub.</p>
<p class="normal">We designed the <code class="inlineCode">car-sharing</code> namespace to host the whole application.</p>
<p class="normal"><code class="inlineCode">host.docker.internal:5000</code> is the hostname of our local registry as seen from inside Minikube.</p>
<p class="normal">Our deployments don’t need services since they communicate through RabbitMQ.</p>
<p class="normal"><code class="inlineCode">FakeDestination.yaml</code> is completely analogous:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: apps/v1
kind: Deployment
metadata:
name: fakedestination
namespace: car-sharing
labels:
app: car-sharing
classification: stub
role: fake-destination
spec:
selector:
matchLabels:
app: car-sharing
role: fake-destination
replicas: 1
template:
metadata:
labels:
app: car-sharing
classification: stub
role: fake-destination
spec:
containers:
- image: host.docker.internal:5000/fakedestination:latest
name: fakedestination
resources:
requests:
cpu: 10m
memory: 10Mi
env:
- name: ConnectionStrings__RabbitMQConnection
value: "host=host.docker.internal:5672;username=guest;password=_myguest;publisherConfirms=true;timeout=10"
</code></pre>
<p class="normal"><code class="inlineCode">RoutesPlanning.yaml</code> differs from the other just because it contains a lot more environment variables <a id="_idIndexMarker659"/>and because it exposes the <code class="inlineCode">8080</code> port, which we might exploit to check the service’s health <a id="_idIndexMarker660"/>state (see the <em class="italic">Readiness, liveness, and startup probes</em> subsection in the next section).</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: apps/v1
kind: Deployment
metadata:
name: routesplanning
namespace: car-sharing
labels:
app: car-sharing
classification: worker
role: routes-planning
spec:
selector:
matchLabels:
app: car-sharing
role: routes-planning
replicas: 1
template:
metadata:
labels:
app: car-sharing
classification: worker
role: routes-planning
spec:
containers:
- image: host.docker.internal:5000/routesplanning:latest
name: routesplanning
ports:
- containerPort: 8080
resources:
requests:
cpu: 10m
memory: 10Mi
env:
- name: ASPNETCORE_HTTP_PORTS
value: "8080"
- name: ConnectionStrings__DefaultConnection
value: "Server=host.docker.internal;Database=RoutesPlanning;User 
                  Id=sa;Password=Passw0rd_;Trust Server 
                  Certificate=True;MultipleActiveResultSets=true"
- name: ConnectionStrings__RabbitMQConnection
value: "host=host.docker.internal:5672;username=guest;password=_
                  myguest;publisherConfirms=true;timeout=10"
- name: Messages__SubscriptionIdPrefix
value: "routesPlanning"
- name: Topology__MaxDistanceKm
value: "50"
- name: Topology__MaxMatches
value: "5"
- name: Timing__HousekeepingIntervalHours
value: "48"
- name: Timing__HousekeepingDelayDays
value: "10"
- name: Timing__OutputEmptyDelayMS
value: "500"
- name: Timing__OutputBatchCount
value: "10"
- name: Timing__OutputRequeueDelayMin
value: "5"
- name: Timing__OutputCircuitBreakMin
value: "4"
</code></pre>
<p class="normal">Let’s open <a id="_idIndexMarker661"/>a Windows console on the <code class="inlineCode">Kubernetes</code> folder, and start deploying our application:</p>
<ol>
<li class="numberedList" value="1">Let’s start Minikube with <code class="inlineCode">minikube start</code>.</li>
<li class="numberedList">Let’s create the <code class="inlineCode">car-sharing</code> namespace with <code class="inlineCode">kubectl create namespace car-sharing</code>.</li>
<li class="numberedList">Let’s deploy <code class="inlineCode">FakeDestination.yaml</code> first: <code class="inlineCode">kubectl apply -f FakeDestination.yaml</code>.</li>
<li class="numberedList">Now let’s <a id="_idIndexMarker662"/>verify all Pods are okay and ready with <code class="inlineCode">kubectl get all -n car-sharing</code>. If they’re not ready, please repeat the command until they are ready.</li>
<li class="numberedList">Let’s copy the name of the created Pod. We need it to access its logs.</li>
<li class="numberedList">Then, let’s deploy <code class="inlineCode">RoutesPlanning.yaml</code>: <code class="inlineCode">kubectl apply -f RoutesPlanning.yaml</code>.</li>
<li class="numberedList">Again, let’s verify all Pods are okay and ready with <code class="inlineCode">kubectl get all -n car-sharing</code>.</li>
<li class="numberedList">Then, let’s deploy <code class="inlineCode">FakeSource.yaml</code>: <code class="inlineCode">kubectl apply -f FakeSource.yaml</code>.</li>
<li class="numberedList">Again, let’s verify all Pods are okay and ready with <code class="inlineCode">kubectl get all -n car-sharing</code>.</li>
<li class="numberedList">Now let’s check the <code class="inlineCode">FakeDestination</code> logs to verify it received the match proposals with: <code class="inlineCode">kubectl logs &lt;FakeDestination POD name&gt; -n car-sharing</code>. Where <code class="inlineCode">&lt;FakeDestination POD name&gt;</code> is the name that we got in <em class="italic">step5.</em></li>
<li class="numberedList">Also check the database table to verify that the applications work properly.</li>
<li class="numberedList">When you’ve finished experimenting, delete everything by simply deleting the <code class="inlineCode">car-sharing</code> namespace: <code class="inlineCode">kubectl delete namespace car-sharing</code>.</li>
<li class="numberedList">Also delete the records in the <strong class="screenText">dbo.RouteOffers</strong> and <strong class="screenText">dbo.RouteRequests</strong> database tables.</li>
<li class="numberedList">Stop Minikube with: <code class="inlineCode">minikube stop</code>.</li>
</ol>
<p class="normal">Now, if you would like to experiment with debugging with the bridge technique, repeat the above steps, but replace points 6 and 7, which deploy the <code class="inlineCode">RoutePlanning</code> microservice with the launch of the single <code class="inlineCode">RoutePlanning</code> project inside of Visual Studio (just replace <code class="inlineCode">AllMicroservices</code> with <code class="inlineCode">RoutePlanning</code> in the Visual Studio debug widget, and then start the debugger).</p>
<p class="normal">Since all <a id="_idIndexMarker663"/>containers are attached to the same RabbitMQ server, the container running in Visual <a id="_idIndexMarker664"/>Studio will receive all input messages created from within Minikube, and all its output messages will be routed inside of Minikube. Let’s place a breakpoint wherever you would like to analyze the cod<a id="_idTextAnchor248"/><a id="_idTextAnchor249"/>e before continuing the Kubernetes deployment. A few seconds after the deployment of the <code class="inlineCode">FakeSource.yaml</code> file, the breakpoint should be hit!</p>
<h1 class="heading-1" id="_idParaDest-170"><a id="_idTextAnchor250"/>Advanced Kubernetes configuration</h1>
<p class="normal">This section <a id="_idIndexMarker665"/>describes advanced Kubernetes resources that play a fundamental role in application design. Other advanced resources and configurations related specifically to security and observability will be described in <a href="Chapter_10.xhtml#_idTextAnchor297"><em class="italic">Chapter 10</em></a><em class="italic">, Security and Observability for Serverless and Microservices Applications</em>.</p>
<p class="normal">Let’s start with secrets.</p>
<h2 class="heading-2" id="_idParaDest-171"><a id="_idTextAnchor251"/>Secrets</h2>
<p class="normal">Kubernetes allows various kinds of Secrets. Here, we will describe just <code class="inlineCode">generic</code> and <code class="inlineCode">tls</code> secrets, which are <a id="_idIndexMarker666"/>the ones used in the practical development of applications based on microservices.</p>
<p class="normal">Each generic <a id="_idIndexMarker667"/>Secret contains a collection of entry-name/entry-value pairs. Secrets can be defined with .<code class="inlineCode">yaml</code> files, but since it is not prudent to mix sensitive information with code, they are usually defined with <code class="inlineCode">kubectl</code> commands.</p>
<p class="normal">Below is how to define a Secret, taking the entry values from file contents:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl create secret generic credentials --from-file=username.txt --from-file=password.txt
</code></pre>
<p class="normal">The file names become entry names (just the file name with its extension – the path information is removed), while file contents become the associated entry values. Each entry is defined with a different <code class="inlineCode">--from-file=…</code> option. </p>
<p class="normal">Creates two files with the above names in a directory, put some content in them, then open a console on that directory, and finally try the above command. Once created, you can see it in <code class="inlineCode">.yaml</code> format with:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl get secret credentials -o yaml
</code></pre>
<p class="normal">In the data section, you will see the two entries, but the entry values appear encrypted. Actually, they are not encrypted but just base64-encoded. Needless to say, you can prevent some Kubernetes users from accessing Secret resources. We will see how in <a href="Chapter_10.xhtml#_idTextAnchor297"><em class="italic">Chapter 10</em></a><em class="italic">, Security and Observability for Serverless and Microservices Applications</em>.</p>
<p class="normal">A Secret can be deleted with:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl delete secret credentials
</code></pre>
<p class="normal">Instead of <a id="_idIndexMarker668"/>using files, one can specify the entry values in line:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl create secret generic credentials --from-literal=username=devuser --from-literal=password='$dsd_weew1'
</code></pre>
<p class="normal">As usual, we can <a id="_idIndexMarker669"/>specify the Secret namespace with the <code class="inlineCode">-n</code> option.</p>
<p class="normal">Once defined, generic Secrets can be mounted as volumes on Pods:</p>
<pre class="programlisting code"><code class="hljs-code">volumes:
- name: credentialsvolume
  secret:
secretName: credentials
</code></pre>
<p class="normal">Each entry is seen as a file whose name is the entry name and whose content is the entry value.</p>
<div><p class="normal"> Do not forget that entry values are base64-encoded, so they must be decoded before usage.</p>
</div>
<p class="normal">Secrets can also be passed as environment variables:</p>
<p class="normal">env:</p>
<pre class="programlisting code"><code class="hljs-code">- name: USERNAME
valueFrom:
secretKeyRef:
name: credentials
key: username
- name: PASSWORD
valueFrom:
secretKeyRef:
name: credentials
key: password
</code></pre>
<p class="normal">In this case, Secret values are automatically base64-decoded before passing them as environment variables.</p>
<div><p class="normal"> <code class="inlineCode">Let’s try Secrets on the routes-matching worker microservices. Let’s create a Kubernetes Secret that contains the RabbitMQ connection string and correct FakeDestination.yaml, FakeSource.yaml, and RoutesPlanning.yaml, to use this Secret.</code></p>
</div>
<p class="normal"><code class="inlineCode">tls</code> Secrets are designed <a id="_idIndexMarker670"/>for storing web servers’ certificates. We will see how to use them in the <em class="italic">Ingresses</em> subsection. <code class="inlineCode">tls</code> secrets take as input both <a id="_idIndexMarker671"/>the private key certificate (.key) and the public key approved certificate (.crt):</p>
<pre class="programlisting con"><code class="hljs-con">kubectl create secret tls test-tls --key="tls.key" --cert="tls.crt"
</code></pre>
<p class="normal">The next important topic concerns how our container code may help Kubernetes verify both whether each container is ready to interact with the remainder of the application and if it is in good health.</p>
<h2 class="heading-2" id="_idParaDest-172"><a id="_idTextAnchor252"/>Readiness, liveness, and startup probes</h2>
<p class="normal">Liveness probes inform Kubernetes when containers are in an unrecoverable faulty state, so Kubernetes <a id="_idIndexMarker672"/>must kill and restart them. If a container has no liveness probe <a id="_idIndexMarker673"/>defined for it, Kubernetes restarts it just in case it crashes due to some unpredictable exception or because it exceeded its memory <a id="_idIndexMarker674"/>limits. Liveness probes must be carefully designed to detect actual <a id="_idIndexMarker675"/>unrecoverable error situations; otherwise, the container might end up in an endless loop of restarts.</p>
<p class="normal">Temporary failures, instead, are connected to readiness probes. When a readiness probe fails, it informs Kubernetes <a id="_idIndexMarker676"/>that the container is not able to receive traffic. Accordingly, Kubernetes <a id="_idIndexMarker677"/>removes the failed container from all the lists of matching services that could send traffic to it. This way, traffic is split only among the ready containers. The faulty container is not restarted and is reinserted in the services list as soon as the readiness probe succeeds again.</p>
<p class="normal">Finally, a startup probe informs Kubernetes that the container has completed its startup procedure. Its only purpose is avoiding Kubernetes killing and restarting the container during startup because of liveness probe failures. In fact, similar occurrences might move the container into an endless loop of restarts.</p>
<p class="normal">Put simply, Kubernetes starts liveness and readiness probes only after the startup probe succeeds. Since both liveness and readiness probes already have initial delays, startup probes are necessary only in case of very long startup procedures.</p>
<p class="normal">All probes <a id="_idIndexMarker678"/>have a <strong class="keyWord">probe operation</strong> that may either fail or succeed, with the following parameters:</p>
<ol>
<li class="numberedList" value="1"><code class="inlineCode">failureThreshold</code>: The <a id="_idIndexMarker679"/>number of consecutive times the probe operation must fail to consider the probe as failed. If not provided, it defaults to 3.</li>
<li class="numberedList"><code class="inlineCode">successThreshold</code>: Used <a id="_idIndexMarker680"/>only for readiness probes. This is the minimum number of consecutive successes for the probe to be considered successful after having failed. It defaults to 1.</li>
<li class="numberedList"><code class="inlineCode">initialDelaySeconds</code>: The time in seconds Kubernetes must wait after the container <a id="_idIndexMarker681"/>starts before trying the first probe. The default value is 0.</li>
<li class="numberedList"><code class="inlineCode">periodSeconds</code>: The time <a id="_idIndexMarker682"/>in seconds between two successive probes. The default is 10 seconds.</li>
<li class="numberedList"><code class="inlineCode">timeoutSeconds</code>: The number <a id="_idIndexMarker683"/>of seconds after which the probe times out. The default is 1 second.</li>
</ol>
<p class="normal">Often, liveness <a id="_idIndexMarker684"/>and readiness probes are <a id="_idIndexMarker685"/>implemented with the same probe operation, but the liveness probe has a greater failure threshold.</p>
<p class="normal">Probes are <a id="_idIndexMarker686"/>container-level properties, that is, they <a id="_idIndexMarker687"/>are on the same level as container ports, and <code class="inlineCode">name</code>.</p>
<p class="normal">Probe <a id="_idIndexMarker688"/>operations may be based on shell commands, HTTP requests, or TCP/IP <a id="_idIndexMarker689"/>connection attempts.</p>
<p class="normal">Probes based on shell commands are defined as:</p>
<pre class="programlisting code"><code class="hljs-code">livenessProbe/readinessProbe/startupProbe:
exec:
command:
- cat
- /tmp/healthy
initialDelaySeconds: 10
periodSeconds: 5
...
</code></pre>
<p class="normal">The <code class="inlineCode">command</code> list contains the command and all its arguments. The operation succeeds if it is completed with a <code class="inlineCode">0</code> status code, that is, if the command completes with no errors. In the example above, the command succeeds if the <code class="inlineCode">/tmp/healthy</code> file exists.</p>
<p class="normal">Probes based on TCP/IP connections are defined as:</p>
<pre class="programlisting code"><code class="hljs-code">livenessProbe/readinessProbe/startupProbe:
tcpSocket:
port: 8080
initialDelaySeconds: 10
periodSeconds: 5
</code></pre>
<p class="normal">The operation <a id="_idIndexMarker690"/>succeeds if a TCP/connection <a id="_idIndexMarker691"/>is successfully established.</p>
<p class="normal">Finally, probes <a id="_idIndexMarker692"/>based on HTTP requests <a id="_idIndexMarker693"/>are defined as:</p>
<pre class="programlisting code"><code class="hljs-code">livenessProbe/readinessProbe/startupProbe:
httpGet:
path: /healthz
port: 8080
httpHeaders:
-name: Custom-Health-Header
value: Kubernetes-probe
initialDelaySeconds: 10
periodSeconds: 5
</code></pre>
<p class="normal"><code class="inlineCode">path</code> and <code class="inlineCode">port</code> specify <a id="_idIndexMarker694"/>the endpoint <a id="_idIndexMarker695"/>path and port. The optional <code class="inlineCode">httpHeaders</code> section lists all HTTP headers that Kubernetes must provide in its request. The operation succeeds if the response returns a status code satisfying: <code class="inlineCode">200&lt;=status&lt;400</code>.</p>
<p class="normal">Let’s add a liveness probe to the <code class="inlineCode">RoutesPlanning.yaml</code> deployment of the <em class="italic">Testing the route-matching worker microservice</em> section. We don’t need a readiness probe, since readiness probes only affect services, and we don’t use services since all communications are handled by RabbitMQ.</p>
<p class="normal">First of all, let’s define the following API in the <code class="inlineCode">Program.cs</code> file of the <code class="inlineCode">RoutesPlanning</code> project:</p>
<pre class="programlisting code"><code class="hljs-code">app.MapGet("/liveness", () =&gt;
{
    if (MainService.ErrorsCount &lt; 6) return Results.Ok();
    else return Results.InternalServerError();
})
.WithName("GetLiveness");
</code></pre>
<p class="normal">The code <a id="_idIndexMarker696"/>returns an error status if there were at least 6 consecutive failed <a id="_idIndexMarker697"/>attempts to communicate with RabbitMQ.</p>
<p class="normal">In the <code class="inlineCode">RoutesPlanning.yaml</code> deployment, we must add the code below:</p>
<pre class="programlisting code"><code class="hljs-code">livenessProbe:
httpGet:
path: /liveness
port: 8080
initialDelaySeconds: 10
periodSeconds: 5
</code></pre>
<p class="normal">After <a id="_idIndexMarker698"/>this change, if you want, you can retry <a id="_idIndexMarker699"/>the whole Minikube test from the <em class="italic">Testing the route-matching worker microservice</em> section.</p>
<p class="normal">The next <a id="_idIndexMarker700"/>section describes a structured, modular, and efficient <a id="_idIndexMarker701"/>way to handle the interaction between our cluster and the external world.</p>
<h2 class="heading-2" id="_idParaDest-173"><a id="_idTextAnchor253"/>Ingresses</h2>
<p class="normal">Most microservices applications have several frontend microservices, so exposing them with LoadBalancer services <a id="_idIndexMarker702"/>would require a different IP address for each of them. Moreover, inside of our Kubernetes cluster, we don’t need the burden of HTTPS and certificates for <a id="_idIndexMarker703"/>each microservice, so the best solution is a unique entry point for the whole cluster with a unique IP address that takes care of HTTPS communication with the external world while forwarding HTTP communication to the services inside of the cluster. Both functionalities are typical of web servers.</p>
<p class="normal">Typically, each IP address has several domain names attached, and a web server splits the traffic between several applications according to both the domain name and the request path inside each domain. This web <a id="_idIndexMarker704"/>server functionality is called <strong class="keyWord">virtual hosting</strong>.</p>
<p class="normal">The translation <a id="_idIndexMarker705"/>between HTTPS and HTTP is a peculiarity of web servers, too. It is called <strong class="keyWord">HTTPS termination</strong>.</p>
<p class="normal">Finally, web servers furnish further services, such as request filtering to prevent various kinds of attacks. More generally, they understand the HTTP protocol and offer HTTP-related services such as access to static files, and various kinds of protocol and content negotiations with the client.</p>
<p class="normal">On the other hand, LoadBalancer services just handle the lower-level TCP/IP protocol and perform some load balancing. Therefore, it would be great to use an actual web server to interface our Kubernetes cluster with the external world instead of several LoadBalancer services.</p>
<p class="normal">Kubernetes offers the possibility to run actual web servers inside of resources called <strong class="keyWord">Ingresses</strong>. Ingresses act <a id="_idIndexMarker706"/>as interfaces between an actual web server and the Kubernetes API, and enable us to configure most web server services with a common interface that doesn’t <a id="_idIndexMarker707"/>depend <a id="_idTextAnchor254"/>on the specific web server that is behind the Ingress.</p>
<p class="normal">The following diagram exemplifies how an Ingress splits traffic among all frontend microservices inside a Kubernetes cluster:</p>
<figure class="mediaobject"><img alt="Figure 8.14: Ingress" src="img/B31916_08_14.png"/></figure>
<p class="packt_figref">Figure 8.14: Ingress</p>
<p class="normal">Ingresses can be <a id="_idIndexMarker708"/>created in a cluster only after an <strong class="keyWord">Ingress controller</strong> has been installed in the cluster. Each Ingress controller installation supplies both a specific web server, such as NGINX, and the code that interfaces it with the Kubernetes API.</p>
<p class="normal">The information about the Ingress controller and its settings is provided in a resource called <code class="inlineCode">IngressClass</code>, which is referenced in the actual Ingress definition. However, often, Ingress controller installations already define a default <code class="inlineCode">IngressClass</code> class, so there is no need to specify its name inside the ingress definition.</p>
<p class="normal">Below is how to define an IngressClass:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
labels:
app.kubernetes.io/component: controller
name: nginx-example
annotations:
ingressclass.kubernetes.io/is-default-class: "true"
spec:
controller: k8s.io/ingress-nginx
parameters: # optional parameters that depend on the installed controller
</code></pre>
<p class="normal">Each class <a id="_idIndexMarker709"/>specifies just the controller’s name (<code class="inlineCode">controller</code>), if it is the default class (<code class="inlineCode">…/is-default-class</code> annotation), and some optional parameters <a id="_idIndexMarker710"/>that depend on the specific controller.</p>
<p class="normal">Below is how to define an Ingress:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
name: my-example-ingress
namespace: my-namespace
# annotations used to configure the ingress
spec:
ingressClassName: &lt;IngressClass name&gt; # Sometimes it is not needed
tls: # HTTPS termination data
...
rules: # virtual hosting rules
...
</code></pre>
<p class="normal">Some controllers, such as the NGINX-based controller, use annotations placed in the metadata section to configure the web server.</p>
<p class="normal">HTTPS termination rules (<code class="inlineCode">tls</code>) are pairs made of a collection of domain names and an HTTPS certificate associated to them, where each certificate must be packaged as a <code class="inlineCode">tls</code> secret (see the <em class="italic">Secrets</em> subsection):</p>
<pre class="programlisting code"><code class="hljs-code">tls:
- hosts:
- www.mydomain.com
secretName: my-certificate1
- hosts:
- my-subdomain.anotherdomain.com
secretName: my-certificate2
</code></pre>
<p class="normal">In the example above, each certificate applies just to a single domain, but if that domain has subdomains that are secured by the same certificate, we may add them to the same certificate list.</p>
<p class="normal">There is a virtual hosting rule for each domain, and each of these rules has subrules for various paths:</p>
<pre class="programlisting code"><code class="hljs-code">rules:
- host: *.mydomain.com # leave this field empty to catch all domains
http:
paths:
- path: /
pathType: Prefix # or Exact
backend:
service:
name: my-service-name
port:
number: 80
- host: my-subdomain.anotherdomain.com
...
</code></pre>
<p class="normal">Domain segments <a id="_idIndexMarker711"/>may be replaced by wildcards (<code class="inlineCode">*</code>). Each <code class="inlineCode">path</code> subrule specifies a <a id="_idIndexMarker712"/>service name, and all traffic matching that rule will be sent to that service, at the port specified in the rule. The service, in turn, forwards the traffic to all matching Pods.</p>
<p class="normal">If <code class="inlineCode">pathType</code> is prefix, it will match all request paths that have the specified path as a subsegment. Otherwise, a perfect match is required. In the example above, the first rule matches all paths since all paths have the empty segment<code class="inlineCode">/</code>as subsegment.</p>
<div><p class="normal"> If an input request matches more paths, the more specific one (the one containing more segments) is preferred.</p>
</div>
<p class="normal">In the next subsection, we will put into practice what we have learned about Ingresses with a very simple example in Minikube.</p>
<h3 class="heading-3" id="_idParaDest-174"><a id="_idTextAnchor255"/>Testing Ingresses with Minikube</h3>
<p class="normal">The easiest <a id="_idIndexMarker713"/>way to install an NGINX-based Ingress controller <a id="_idIndexMarker714"/>in Minikube is to enable the <code class="inlineCode">ingress</code> addon. Therefore, after having started Minikube, let’s enable this addon:</p>
<pre class="programlisting con"><code class="hljs-con">minikube addons enable ingress
</code></pre>
<p class="normal">As a result, some Pods are created in the <code class="inlineCode">ingress-nginx</code> namespace. Let’s check it with <code class="inlineCode">kubectl get pods -n ingress-nginx</code>!</p>
<p class="normal">The addon installs the same NGINX-based ingress controller used by most Kubernetes environments (<a href="https://github.com/kubernetes/ingress-nginx?tab=readme-ov-file">https://github.com/kubernetes/ingress-nginx?tab=readme-ov-file</a>). The installation also automatically creates an <code class="inlineCode">IngressClass</code> called <code class="inlineCode">nginx</code>. The annotations supported by this controller are listed here: <a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/">https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/</a>.</p>
<p class="normal">The <code class="inlineCode">ch08</code> folder of the GitHub book repository contains <code class="inlineCode">IngressExampleDeployment.yaml</code> and the <code class="inlineCode">IngressExampleDeployment2.yaml</code> files. They define two Deployments with their associated ClusterIP services. They deploy two different versions of a very simple web application that creates a simple HTML page. </p>
<p class="normal">As usual, let’s copy <a id="_idIndexMarker715"/>the two <code class="inlineCode">.yaml</code> files in a folder and open a console <a id="_idIndexMarker716"/>on that folder. As the first step, let’s apply these files:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl apply -f IngressExampleDeployment.yaml
kubectl apply -f IngressExampleDeployment2.yaml
</code></pre>
<p class="normal">Now we will create an ingress that connects the first version of the application to <code class="inlineCode">/</code> and the second version of the application to <code class="inlineCode">/v2</code>. The names of the ClusterIP services of the two deployments are <code class="inlineCode">helloworldingress-service</code> and <code class="inlineCode">helloworldingress2-service</code>, and both receive on the <code class="inlineCode">8080</code> port. Therefore, we need to bind the <code class="inlineCode">helloworldingress-service</code> <code class="inlineCode">8080</code> port to <code class="inlineCode">/</code> and the <code class="inlineCode">helloworldingress2-service</code> <code class="inlineCode">8080</code> port to <code class="inlineCode">/v2</code>:</p>
<pre class="programlisting code"><code class="hljs-code">apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
name: example-ingress
namespace: basic-examples
spec:
ingressClassName: nginx
rules:
- host:
http:
paths:
- path: /
pathType: Prefix
backend:
service:
name: helloworldingress-service
port:
number: 8080
- path: /v2
pathType: Prefix
backend:
service:
name: helloworldingress2-service
port:
number: 8080
</code></pre>
<p class="normal">It is worth pointing out that the host property is empty, so the Ingress doesn’t perform any selection based on the domain name, but the microservice selection is based just on the path. This was a <a id="_idIndexMarker717"/>forced choice since we are experimenting on an isolated <a id="_idIndexMarker718"/>development machine without the support of a DNS, so we can’t associate domain names to IP addresses.</p>
<p class="normal">Let’s put the above code in a file named <code class="inlineCode">IngressConfiguration.yaml</code> and let’s apply it:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl apply -f IngressConfiguration.yaml
</code></pre>
<p class="normal">In order to connect with the Ingress, we need to open a tunnel with the Minikube virtual machine. As usual, open another console and issue the <code class="inlineCode">minikube tunnel</code> command in it. Remember that the tunnel works as long as this window remains open.</p>
<p class="normal">Now open the browser and go to <code class="inlineCode">http://localhost</code>. You should see something like:</p>
<p class="normal"><strong class="screenText">Hello, world!</strong></p>
<p class="normal"><strong class="screenText">Version: 1.0.0</strong></p>
<p class="normal"><strong class="screenText">Hostname: ……</strong></p>
<p class="normal">Then go to <code class="inlineCode">http://localhost/v2</code>. You should see something like:</p>
<p class="normal"><strong class="screenText">Hello, world!</strong></p>
<p class="normal"><strong class="screenText">Version: 2.0.0</strong></p>
<p class="normal"><strong class="screenText">Hostname: ……</strong></p>
<p class="normal">We were able to split the traffic between the two applications according to the request path!</p>
<p class="normal">When you have finished experimenting, let’s clean up the environment with:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl delete -f IngressConfiguration.yaml
kubectl delete -f IngressExampleDeployment2.yaml
kubectl delete -f IngressExampleDeployment.yaml
</code></pre>
<p class="normal">Finally, let’s stop Minikube with: <code class="inlineCode">minikube stop</code>.</p>
<p class="normal">The next subsection explains how to install the same Ingress controller on AKS.</p>
<h3 class="heading-3" id="_idParaDest-175"><a id="_idTextAnchor256"/>Using an NGNIX-based Ingress in AKS</h3>
<p class="normal">You can manually install the NGNIX-based Ingress on AKS either with a .<code class="inlineCode">yaml</code> file or with a package <a id="_idIndexMarker719"/>manager called Helm. However, then, you should <a id="_idIndexMarker720"/>handle complex permissions-related <a id="_idIndexMarker721"/>configurations to associate a static IP and an Azure DNS zone to your AKS cluster. The interested reader can find the complete procedure here: <a href="https://medium.com/@anilbidary/domain-name-based-routing-on-aks-azure-kubernetes-service-using-ingress-cert-manager-and-9b4028d762ed">https://medium.com/@anilbidary/domain-name-based-routing-on-aks-azure-kubernetes-service-using-ingress-cert-manager-and-9b4028d762ed</a>.</p>
<p class="normal">Luckily, you can let Azure do all of this job for you, because Azure has an AKS application routing addon that automatically installs the Ingress for you and facilitates all permission configuration. This addon can be enabled on an existing cluster with:</p>
<pre class="programlisting con"><code class="hljs-con">az aks approuting enable --resource-group &lt;ResourceGroupName&gt; --name &lt;ClusterName&gt;
</code></pre>
<p class="normal">The addon creates <code class="inlineCode">webapprouting.kubernetes.azure.com</code> <code class="inlineCode">IngressClass</code>, which you must reference in all your Ingresses.</p>
<p class="normal">An IP address is created whenever you create a new Ingress and remains allocated for the lifetime of the Ingress. Moreover, if you create an Azure DNS zone and associate it to the addon, the addon will automatically add all needed records for all domains defined in the rules of your Ingresses.</p>
<p class="normal">You just need to create an Azure DNS zone with:</p>
<pre class="programlisting con"><code class="hljs-con">az network dns zone create --resource-group &lt;ResourceGroupName&gt; --name &lt;ZoneName&gt;
</code></pre>
<p class="normal">In order to associate this zone to the addon, you need the zone’s unique ID, which you can get with:</p>
<pre class="programlisting con"><code class="hljs-con">ZONEID=$(az network dns zone show --resource-group &lt;ResourceGroupName&gt; --name &lt;ZoneName&gt; --query "id" --output tsv)
</code></pre>
<p class="normal">Now you can attach the zone with:</p>
<pre class="programlisting con"><code class="hljs-con">az aks approuting zone add --resource-group &lt;ResourceGroupName&gt; --name &lt;ClusterName&gt; --ids=${ZONEID} --attach-zones
</code></pre>
<p class="normal">After this command, all domain names used in your Ingress’s rules will be automatically added to the zone with adequate records. Obviously, you must update your domain data in the provider where you bought your domain names. More specifically, you must force them to point to the names of the Azure DNS servers that handle your zone. You can easily get these DNS server <a id="_idIndexMarker722"/>names by going to the newly created <a id="_idIndexMarker723"/>DNS zone in the Azure portal.</p>
<p class="normal">We have <a id="_idIndexMarker724"/>finished our amazing Kubernetes trip. We will return to most of the concepts learned about here in most of the remaining chapters, and in particular in <a href="Chapter_11.xhtml#_idTextAnchor332"><em class="italic">Chapter 11</em></a><em class="italic">, </em><em class="italic">The Car Sharing App</em>.</p>
<p class="normal">The next chapter shows how to start a new microservices application smoothly and with low costs with the help of Azure Container Apps.</p>
<h1 class="heading-1" id="_idParaDest-176"><a id="_idTextAnchor257"/>Summary</h1>
<p class="normal">In this chapter, you learned about the basics of orchestrators and then learned how to install and configure a Kubernetes cluster. More specifically, you learned how to interact with a Kubernetes cluster through Kubectl and Kubectl’s main commands. Then you learned how to deploy and maintain a microservices application, and how to test it locally with the help of Docker and Minikube.</p>
<p class="normal">You also learned how to interface your Kubernetes cluster with a LoadBalancer and with an Ingress, and how to fine-tune it to optimize performance.</p>
<p class="normal">All concepts were put into practice with both simple examples and with a more complete example taken from the car-sharing case study.</p>
<h1 class="heading-1" id="_idParaDest-177"><a id="_idTextAnchor258"/>Questions </h1>
<ol>
<li class="numberedList" value="1">Why do Kubernetes applications need network disk storage?</li>
</ol>
<p class="normal-one">Because PODs can’t rely on the disk storage of the nodes where they run, since they might be moved to different nodes.</p>
<ol>
<li class="numberedList" value="2">Is it true that if a node containing a Pod of a Deployment with 10 replicas crashes, your application will continue running properly?</li>
</ol>
<p class="normal-one">Yes.</p>
<ol>
<li class="numberedList" value="3">Is it true that if a node containing a Pod of a StatefulSet with 10 replicas crashes, your application will continue running properly?</li>
</ol>
<p class="normal-one">Not necessarily.</p>
<ol>
<li class="numberedList" value="4">Is it true that if a Pod crashes, it is always automatically restarted?</li>
</ol>
<p class="normal-one">Yes.</p>
<ol>
<li class="numberedList" value="5">Why do StatefulSets need persistent volume claim templates instead of persistent volume claims?</li>
</ol>
<p class="normal-one">Because each POD of the StatefulSet needs a different volume.</p>
<ol>
<li class="numberedList" value="6">What is the utility of persistent volume claims?</li>
</ol>
<p class="normal-one">They enable Kubernetes users to request and manage storage resources dynamically, decoupling storage provisioning from application deployment.</p>
<ol>
<li class="numberedList" value="7">What is more adequate for interfacing an application with three different frontend services, a LoadBalancer or an ingress?</li>
</ol>
<p class="normal-one">An Ingress. LoadBalancers are adequate just when there is an unique Frontend service.</p>
<ol>
<li class="numberedList" value="8">What is the most adequate way of passing a connection string to a container running in a Pod of a Kubernetes cluster?</li>
</ol>
<p class="normal-one">By using a Kubernetes Secret since it contains sensitive information.</p>
<ol>
<li class="numberedList" value="9">How are HTTPS certificates installed in Ingresses?</li>
</ol>
<p class="normal-one">Through a specific type of secret.</p>
<ol>
<li class="numberedList" value="10">Does standard Kubernetes syntax allow the installation of an HTTPS certificate on a LoadBalancer service?</li>
</ol>
<p class="normal-one">No.</p>
<h1 class="heading-1" id="_idParaDest-178"><a id="_idTextAnchor259"/>Further reading</h1>
<ul>
<li class="bulletList">Kubernetes official documentation: <a href="https://kubernetes.io/docs/home/">https://kubernetes.io/docs/home/</a>.</li>
<li class="bulletList">AKS official documentation: <a href="https://learn.microsoft.com/en-us/azure/aks/">https://learn.microsoft.com/en-us/azure/aks/</a>.</li>
<li class="bulletList">Minikube official documentation: <a href="https://minikube.sigs.k8s.io/docs/">https://minikube.sigs.k8s.io/docs/</a>.</li>
<li class="bulletList">AKS autoscaling: <a href="https://learn.microsoft.com/en-us/azure/aks/cluster-autoscaler?tabs=azure-cli ">https://learn.microsoft.com/en-us/azure/aks/cluster-autoscaler?tabs=azure-cli</a></li>
<li class="bulletList">Cloud-independent cluster auto-scalers: <a href="https://kubernetes.io/docs/concepts/cluster-administration/cluster-autoscaling/">https://kubernetes.io/docs/concepts/cluster-administration/cluster-autoscaling/</a></li>
<li class="bulletList">Storage classes: <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">https://kubernetes.io/docs/concepts/storage/storage-classes/</a>.</li>
<li class="bulletList">Assigning a static Azure IP address to a LoadBalancer: <a href="https://learn.microsoft.com/en-us/azure/aks/static-ip">https://learn.microsoft.com/en-us/azure/aks/static-ip</a></li>
<li class="bulletList">Example metrics server: <a href="https://github.com/kubernetes-sigs/metrics-server">https://github.com/kubernetes-sigs/metrics-server</a>.</li>
<li class="bulletList">NGINX-based Ingress controller: <a href="https://github.com/kubernetes/ingress-nginx?tab=readme-ov-file">https://github.com/kubernetes/ingress-nginx?tab=readme-ov-file</a> .</li>
<li class="bulletList">Manual installation of NGINX-based Ingress of AKS: <a href="https://www.medium.com/@anilbidary/domain-name-based-routing-on-aks-azure-kubernetes-service-using-ingresscert-manager-and-9b4028d762ed">https://medium.com/@anilbidary/</a></li>
<li class="bulletList"><a href="https://www.medium.com/@anilbidary/domain-name-based-routing-on-aks-azure-kubernetes-service-using-ingresscert-manager-and-9b4028d762ed">domain-name-based-routing-on-aks-azure-kubernetes-service-using-ingresscert-manager-and-9b4028d762ed</a></li>
<li class="bulletList">Using RabbitMQ Cluster operator: <a href="https://www.rabbitmq.com/kubernetes/operator/using-operator">https://www.rabbitmq.com/kubernetes/operator/using-operator</a></li>
<li class="bulletList">Installing a RabbitMQ Cluster on Kubernetes: <a href="https://www.rabbitmq.com/kubernetes/operator/install-operator">https://www.rabbitmq.com/kubernetes/operator/install-operator</a>.</li>
</ul>
<h1 class="heading-1" id="_idParaDest-179"><a id="_idTextAnchor260"/>Join our community on Discord</h1>
<p class="normal">Join our community’s Discord space for discussions with the author and other readers:</p>
<p class="normal"><a href="https://packt.link/PSMCSharp">https://packt.link/PSMCSharp</a></p>
<p class="normal"><img alt="A qr code with black squares  AI-generated content may be incorrect." src="img/B31916_Discord-QR-Code.png"/></p>
</div>
</body></html>