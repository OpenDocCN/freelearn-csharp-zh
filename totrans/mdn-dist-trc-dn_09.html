<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-149"><a id="_idTextAnchor148"/>9</h1>
<h1 id="_idParaDest-150"><a id="_idTextAnchor149"/>Best Practices</h1>
<p>In the previous chapters, we focused on how to collect, enrich, correlate, and use individual telemetry signals. In this chapter, we’re going to discuss what information to collect and how to represent it efficiently using all the available signals. We’ll start by providing recommendations on how to pick a suitable telemetry signal and suggest cross-signal cost optimization strategies. Finally, we’ll explore about OpenTelemetry semantic conventions and use them to create consistent telemetry supported by most observability vendors.</p>
<p>You’ll learn how to do the following:</p>
<ul>
<li>Find telemetry signals that work for your scenarios</li>
<li>Control telemetry costs with aggregation, sampling, and verbosity</li>
<li>Follow common practices when reporting telemetry with OpenTelemetry semantic conventions</li>
</ul>
<p>By the end of this chapter, you will be able to use existing semantics for common technologies or create your own cross-signal and cross-service conventions.</p>
<h1 id="_idParaDest-151"><a id="_idTextAnchor150"/>Technical requirements</h1>
<p>There are no specific requirements for this chapter, and there are no associated code files either.</p>
<h1 id="_idParaDest-152"><a id="_idTextAnchor151"/>Choosing the right signal</h1>
<p>When we discussed<a id="_idIndexMarker518"/> individual telemetry signals in <em class="italic">Chapters 6</em> to <em class="italic">8</em>, we provided suggestions on when to use each of them. Let’s do a quick recap:</p>
<ul>
<li><strong class="bold">Distributed traces</strong> describe individual network calls and other interesting operations in detail. Spans have causal relationships, allowing us to understand the request flow in distributed systems.</li>
</ul>
<p>Traces document the request flow through the system and are essential for investigating errors or outliers in the long tail of latency distribution. Traces provide means to correlate other telemetry signals.</p>
<ul>
<li><strong class="bold">Metrics</strong> collect aggregated data with low-cardinality attributes and provide a low-resolution view of the overall system state. They help optimize telemetry collection and reduce storage costs and query time.</li>
<li><strong class="bold">Events</strong> provide highly structured information about individual occurrences of important things. The key difference between spans and events is that spans have unique contexts and describe something that lasts.</li>
</ul>
<p>Events have high-cardinality attributes and can help answer ad hoc questions about system behavior and usage.</p>
<ul>
<li><strong class="bold">Logs</strong> provide details about operations in a human-readable and less structured format.</li>
</ul>
<p>They are useful for<a id="_idIndexMarker519"/> debugging things when other signals don’t provide enough information. Also, logs are the only signal that supports verbosity.</p>
<ul>
<li><strong class="bold">Profiles</strong> are low-level performance data describing individual operations within a single process that helps optimize performance and identify resource bottlenecks.</li>
</ul>
<p>When instrumenting some specific scenario, we usually need a combination of signals.</p>
<p>For example, to get observability into network calls, we need traces to ensure we can track the request flow across services and correlate other signals. Logs are necessary to record exceptions and warnings, describe local operations, and provide debug-level data for complicated investigations. Finally, we may need metrics to record non-sampled measurements, optimize collection, and reduce query time.</p>
<p class="callout-heading">Note</p>
<p class="callout">Before thinking about signals, we should have an idea of what information we want to be available, how we’re going to use it, how fast and frequently we need it, how many details we want to capture, for how long we need it, how much we can afford, and the downtime cost.</p>
<p>The answers to these questions should shape our decisions around observability.</p>
<p>Essentially, we have multiple trade-offs between having enough data to investigate issues fast and the cost of the observability solution. For example, collecting too many traces would give us all the details we need to investigate all sorts of issues. It would have a noticeable performance impact and significantly increase observability backend costs. As a result, traces might become so deep and detailed that it would be hard to understand where<a id="_idIndexMarker520"/> the problems are.</p>
<p>The conversation about a good set of telemetry signals is not possible without talking about costs. Let’s see how we can control them.</p>
<h1 id="_idParaDest-153"><a id="_idTextAnchor152"/>Getting more with less</h1>
<p>Since we usually need to<a id="_idIndexMarker521"/> collect multiple signals about the same component, we need to be able to tune them individually, depending on our needs.</p>
<p>The key is to reduce the volume of expensive, but not essential, data, potentially replacing it with cheaper options while keeping the system observable. We saw how we can do this by combining hot and cold storage or changing the retention period in <a href="B19423_08.xhtml#_idTextAnchor131"><em class="italic">Chapter 8</em></a>, <em class="italic">Writing Structured and Correlated Logs</em>. Here, let’s focus on the collection side.</p>
<p>While observability vendors have different pricing models, it’s common for them to bill for traces, logs, and events depending on the volume, and for metrics depending on the number of time series. Queries (or API calls) can also be charged for and may have concurrency limits.</p>
<p>Of course, we can always add or remove instrumentations or stop writing logs and events, but there are a few more factors affecting how much telemetry is collected:</p>
<ul>
<li>We can control tracing volume with the sampling rate and by adding or removing new attributes</li>
<li>To control the number of metric time series, we can add or remove resource attributes or drop dimensions or instruments</li>
<li>We can tune logging verbosity for individual categories or do so globally and add or remove attributes</li>
</ul>
<p>Applications’ needs may vary, depending on their maturity, the number of changes, the downtime they<a id="_idIndexMarker522"/> can afford, and other factors – let’s go through several examples to demonstrate possible compromises they can apply.</p>
<h2 id="_idParaDest-154"><a id="_idTextAnchor153"/>Building a new application</h2>
<p>When writing<a id="_idIndexMarker523"/> the first version of an application, telemetry<a id="_idIndexMarker524"/> can play a critical role in helping teams investigate issues and move faster. The interesting part here is that we don’t know which type of telemetry we need and how we’re going to use it.</p>
<p>We can leverage existing instrumentations that allow us to focus our efforts on building the application and having all means to debug it as it evolves, while also finding answers to questions about telemetry we outlined before.</p>
<p>The initial stages are a great time to design the observability story and it makes sense to start with the most flexible signals – traces, events, and logs. Initially, telemetry volume is likely to be low, so recording traces with a high sampling rate or just rate-limiting should be affordable. Also, we probably don’t have strict SLAs yet and don’t use dashboards and alerts much.</p>
<p>Until we get some real users, metrics or events might be unnecessary, but this is a good time to experiment and get familiar with them.</p>
<p>Even if the telemetry volume is quite low and we can capture verbose data, we should avoid adding excessive amounts of telemetry and should remove unused signals.</p>
<h2 id="_idParaDest-155"><a id="_idTextAnchor154"/>Evolving applications</h2>
<p>As our<a id="_idIndexMarker525"/> application<a id="_idIndexMarker526"/> starts getting some real users, getting performance data quickly becomes critical. By this time, we have more clarity on what’s important to measure in the application and how to debug issues (hopefully not relying on verbose logging).</p>
<p>This is the time to optimize and tune telemetry collection. As the load grows, we usually want to lower the sampling rate for traces and reduce log verbosity.</p>
<p>Also, we would probably need to create alerts and build dashboards that are much more efficient when done over metrics, as we discussed in <a href="B19423_07.xhtml#_idTextAnchor115"><em class="italic">Chapter 7</em></a>, <em class="italic">Adding Custom Metrics</em>. While instrumentation libraries should cover the basics, we might need to add custom metrics where we previously relied on queries over traces. As we scale up, the number of time series only increases with the number of service instances.</p>
<p>At this stage, we might also decide to collect precise and unsampled usage data with events and metrics.</p>
<p>The application is still changing a lot and we frequently need to investigate functional issues for specific requests and optimize requests from the long tail of latency. So, tracing still plays a key role in day-to-day work. We might need to instrument more layers in the application to capture logical operations or add applications-specific context. At the same time, we may find some auto-instrumentations too verbose and can tune them to remove unnecessary attributes or suppress some spans.</p>
<p>Sometimes, we need to capture profiles or use diagnostic tools to investigate lower-level issues, so having a continuous profiler or adding <code>dotnet-monitor</code> in a sidecar could make such investigations much easier.</p>
<p>If the application (or some parts of it) becomes more stable due to having fewer and fewer issues, it makes sense to remove non-essential traces and reduce the sampling rate for stable services or endpoints. Tail-based sampling could help capture more traces for failures or long requests.</p>
<p>When the application<a id="_idIndexMarker527"/> is not changing anymore except for <a id="_idIndexMarker528"/>basic maintenance, but more importantly, if it does not have many issues and investigations, it could be reasonable to reduce tracing to just incoming and outgoing requests, potentially forwarding logs to colder storage.</p>
<h2 id="_idParaDest-156"><a id="_idTextAnchor155"/>Performance-sensitive scenarios</h2>
<p>Instrumentation <a id="_idIndexMarker529"/>introduces performance overhead. Between traces, metrics, and logs, traces are the most expensive. When instrumenting an HTTP request, this overhead is usually negligible compared to the call itself.</p>
<p>But in some cases, instrumentation costs can be too high. For example, when returning cached responses or rate-limiting requests across all service instances, logging or tracing all such calls can significantly impact performance. Moreover, if we recorded a trace for every request, a DDOS attack or buggy client might kill our observability pipeline, if not the whole service.</p>
<p>Tracing overhead, to some extent, can be reduced with sampling, which protects the observability pipeline and reduces the number of allocations when populating attributes, but a new <code>Activity</code> is created and a new <code>SpanId</code> is generated, regardless of the sampling decision.</p>
<p>Adding tracing for a hot path should be done with caution. Keep the number of traces to a minimum: trace incoming requests only if the corresponding request is going to be processed by your application and avoid tracing outgoing network calls to the leaf services if they’re extremely fast or reliable. For example, it makes sense to report an event instead of a span when talking to Redis.</p>
<p>Metrics are the most performant telemetry signal and should be preferred for a hot path when possible. For example, reporting the Redis call duration as a metric with a cache hit/miss dimension would likely be cheaper than an event. And for tracing purposes, we can put a hit/miss flag as an attribute on an existing current span (for example, one representing an incoming request).</p>
<p>Recording exceptions and errors is usually fine from a performance perspective since exceptions create a huge overhead anyway. But in the case of a failure storm, we get too many of them, so it’s a good idea to throttle exception reporting.</p>
<p>Implementing<a id="_idIndexMarker530"/> efficient, useful, but minimalistic instrumentation usually requires several iterations. Luckily, OpenTelemetry provides a set of semantic conventions for common scenarios that can help with it. Let’s see how.</p>
<h1 id="_idParaDest-157"><a id="_idTextAnchor156"/>Staying consistent with semantic conventions</h1>
<p>One of the most<a id="_idIndexMarker531"/> important questions we’re yet to discuss is what information to add to telemetry signals to make them useful – this is where OpenTelemetry semantic conventions come into play.</p>
<p>Semantic conventions describe what information to collect for specific technologies, such as HTTP or gRPC calls, database operations, messaging scenarios, serverless environments, runtime metrics, resource attributes, and so on.</p>
<p>Semantic conventions are part of the OpenTelemetry specification and have been published in the specification repository at <a href="https://github.com/open-telemetry/opentelemetry-specification">https://github.com/open-telemetry/opentelemetry-specification</a>. They apply to all instrumentations authored by the OpenTelemetry project.</p>
<p class="callout-heading">Note</p>
<p class="callout">At the time of writing, semantic conventions are in an experimental status. The community is actively working on stabilization and the attributes I use in this book will likely be renamed or changed in other ways.</p>
<p>The goal of semantic conventions is to unify telemetry collection for specific scenarios or technology across languages, runtimes, and libraries. For example, traces and metrics for all HTTP clients look very similar, making it possible to visualize or query HTTP telemetry or diagnose problems in the same way for any application. Let’s look at HTTP semantic<a id="_idIndexMarker532"/> conventions to understand how they work and give you an idea of what other conventions look like.</p>
<h2 id="_idParaDest-158"><a id="_idTextAnchor157"/>Semantic conventions for HTTP requests</h2>
<p>The conventions <a id="_idIndexMarker533"/>cover tracing and metrics for<a id="_idIndexMarker534"/> incoming and outgoing HTTP requests. Spans with <code>client</code> kind describe outgoing requests, whereas <code>server</code> spans describe incoming requests. Instrumentations create a new span for each attempt.</p>
<p><code>client</code> HTTP spans contain attributes that describe the request, response, and remote destination. According to the current version, a minimal HTTP client instrumentation must report the following attributes: <code>http.method</code>, <code>http.url</code>, <code>net.peer.name</code>, <code>net.peer.port</code>, and <code>http.status_code</code>.</p>
<p>If a response is not received, the <code>http.status_code</code> attribute is not populated; instead, the span status would indicate an error and provide a status description that explained what happened. The port (<code>net.peer.port</code>) attribute may be skipped if it’s 80 or 443. Other attributes are required, so all instrumentations that follow conventions must populate them in all scenarios. These attributes, combined with the span start timestamp, duration, and status, provide a minimal necessary description of the HTTP request.</p>
<p>All the attributes except <code>http.status_code</code> should be provided at the span start time – this allows us to make sampling decisions based on these attributes.</p>
<p>You probably noticed that the host and port information is available inside the URL and via separate attributes. The URL is a high-cardinality attribute, but the host and port are very likely to be of low cardinality, so reporting all of them allows us to unify instrumentation code and report traces and metrics in one place. It also makes it possible to calculate metrics from traces and simplify queries.</p>
<p>Minimal HTTP server instrumentation reports the <code>http.method</code>, <code>http.status_code</code>, <code>http.scheme</code>, <code>http.target</code>, <code>net.host.name</code>, <code>net.host.port</code>, and <code>http.route</code> attributes.</p>
<p>Since HTTP servers don’t have full URLs readily available, instrumentations don’t construct them and report individual URL components instead. Route information is provided by an HTTP framework such as ASP.NET Core and even there, you may handle requests in middleware without using routing. Reporting route is quite important for metrics, as we’ve seen in <a href="B19423_07.xhtml#_idTextAnchor115"><em class="italic">Chapter 7</em></a>, <em class="italic">Adding Custom Metrics</em>, so if you don’t have the route available out of the box, you might want to provide one manually to distinguish different classes of API calls. HTTP client and server instrumentations usually also report recommended attributes, such as the <code>User-Agent</code> header, request and response content length, HTTP <a id="_idIndexMarker535"/>protocol <a id="_idIndexMarker536"/>version, and remote IP address.</p>
<p>Conventions also standardize attribute value types – for example, <code>http.status_code</code> has an integer type, simplifying comparison at query time.</p>
<p>You can find the full HTTP tracing conventions at <a href="https://opentelemetry.io/docs/reference/specification/trace/semantic_conventions/http">https://opentelemetry.io/docs/reference/specification/trace/semantic_conventions/http</a>.</p>
<p>Metrics are based on the same tracing attributes and cover request duration, content size, and the number of active requests on servers. The metrics conventions are available at <a href="https://opentelemetry.io/docs/reference/specification/metrics/semantic_conventions/http-metrics">https://opentelemetry.io/docs/reference/specification/metrics/semantic_conventions/http-metrics</a>.</p>
<p>HTTP semantic conventions provide a good set of default things to collect. You can move between teams, companies, and web frameworks, or start using a different programming language, but OpenTelemetry instrumentations would provide a common baseline everywhere.</p>
<p>Having a<a id="_idIndexMarker537"/> reliable<a id="_idIndexMarker538"/> set of required attributes helps the backend visualize traces and service maps, build dashboards, and automate analysis and issue detection.</p>
<h2 id="_idParaDest-159"><a id="_idTextAnchor158"/>General considerations</h2>
<p>When you need to<a id="_idIndexMarker539"/> instrument some specific technology or scenario and no suitable instrumentation library is available, make sure to also check whether there is an applicable semantic convention. By following it, you will be able to leverage any experiences built on top of it by your observability backend, prevent inconsistent signals coming from different parts of your system, and also save some time designing and polishing your signals.</p>
<p>But what if you want to instrument something very specific to your application, such as adding spans for logical operations or adding usage metrics? Let’s see.</p>
<h3>Tracing</h3>
<p>As we’ve<a id="_idIndexMarker540"/> seen in <a href="B19423_06.xhtml#_idTextAnchor098"><em class="italic">Chapter 6</em></a>, <em class="italic">Tracing Your Code</em>, we can create a new <code>Activity</code> instance without specifying any parameters. By default, it’s named after the caller method and has an <code>internal</code> kind.</p>
<p>OpenTelemetry recommends using low-cardinality span names. HTTP client span names follow the <code>HTTP &lt;method&gt;</code> pattern (for example, <code>HTTP GET</code>), while the HTTP server span name looks like <code>&lt;method&gt; &lt;route&gt;</code> (for example, <code>GET /users/{userId}</code>). The span name describes a class of operations and is frequently used to group common spans.</p>
<p>Another important property is the span kind: it helps backends visualize and query traces them. <code>client</code> spans represent outgoing requests – their context is propagated over the wire, and they become remote parents of <code>server</code> spans. When instrumenting a remote call, we would typically want to create a new span for each attempt so that we know how long an attempt took, how many there were, and what the backoff interval was.</p>
<p>The <code>server</code> spans are those that track incoming requests; they either have no parents or have a remote parent.</p>
<p>OpenTelemetry also defines <code>consumer</code> and <code>producer</code> kinds – they are used in asynchronous scenarios where a request-response pattern is not applicable. A <code>producer</code> span could be a parent of a <code>consumer</code> span (or be linked to it), but it usually ends before the corresponding <code>consumer</code> span.</p>
<p>All other spans are <code>internal</code>. For example, to represent an I/O operation or a local long-running call, we should use the <code>internal</code> kind. When instrumenting client library calls or logical operations that can do multiple HTTP requests underneath, it makes sense to describe them as <code>internal</code> spans.</p>
<p>If an operation ends with an error, we should reflect it with a span status, but this can be tricky. For example, HTTP semantic conventions recommend setting the status to an error on the client side if a response was not received, there were too many redirects, or when the status code was in the 4xx or 5xx ranges. But for HTTP servers, a 4xx response does not indicate an error and should be left unset. Even for client requests, status codes such as 404 (<code>Not Found</code>) do not necessarily indicate an error and can be used to check whether some resource exists.</p>
<p>When recording errors, the status description can be used to record some predictable and short information about it, such as its exception type and/or message. Exceptions follow their own<a id="_idIndexMarker541"/> semantic conventions – we discussed this in <a href="B19423_06.xhtml#_idTextAnchor098"><em class="italic">Chapter 6</em></a>, <em class="italic">Tracing Your Code</em>. They can be huge (because of stack traces), so we should avoid recording handled exceptions.</p>
<h4>Attributes</h4>
<p>Application-specific <a id="_idIndexMarker542"/>context or details about an operation can be recorded in attributes. Before inventing a new attribute name, make sure you check existing semantic conventions to see whether something similar is defined there already. For example, you can use general network attributes to describe remote destinations or host and RPC calls.</p>
<p>If you must create a new attribute, use a short name that consists of basic Latin characters. OpenTelemetry recommends using namespaces to avoid naming collisions (they are delimited with the dot (<code>.</code>) character) and using <code>snake_case</code> to separate words. For example, in <code>http.status_code</code>, <code>http</code> is a namespace. So, if you’re defining a new attribute specific to your company, it makes sense to use the company name in the namespace.</p>
<p>The number of attributes per span is limited to 128 by default, but this limit can be increased.</p>
<p>Keeping consistent names and value types across your system can be challenging, so it’s a good idea to come up with some registry to keep them consistent.</p>
<p>So, which information would you add to attributes? Anything that describes your operation, except sensitive information or secrets. Be cautious with long values and avoid adding something that needs to be serialized or calculated – use verbose logging for it.</p>
<p>It’s also a good idea<a id="_idIndexMarker543"/> to avoid duplication and record a reasonable set of information, moving static attributes to resources instead of spans.</p>
<h3>Metrics</h3>
<p>When creating<a id="_idIndexMarker544"/> instruments, we can provide a name, unit, and description.</p>
<p>Instrument names are case-insensitive and consist of alphanumeric characters, underscores, dots, and dashes. Instrument names must be short – up to 63 characters.</p>
<p>Instrument names are formatted similarly to attribute names and support namespaces – for example, the <code>http.server.active_requests</code> counter or the <code>http.server.duration</code> histogram, which represent the number of active HTTP requests and server-side duration of requests, respectively.</p>
<p>Units usually follow UCUM<a id="_idIndexMarker545"/> standards (<a href="https://ucum.org/">https://ucum.org/</a>) and it’s important to keep them consistent for the same instrument across the whole system.</p>
<p>Attribute naming conventions are common between different signals and usually, metrics rely on a subset of tracing attributes. The most important characteristic of metric attributes is low cardinality, which we described in <a href="B19423_07.xhtml#_idTextAnchor115"><em class="italic">Chapter 7</em></a>, <em class="italic">Adding </em><em class="italic">Custom Metrics</em>.</p>
<p>Before adding custom metrics, make sure to check whether there is an existing instrumentation library or an OpenTelemetry semantic convention. For example, there is a generic one for RPC requests, process <a id="_idIndexMarker546"/>and system resource utilization metrics, databases, and other technology-specific ones, such as Kafka.</p>
<h1 id="_idParaDest-160"><a id="_idTextAnchor159"/>Summary</h1>
<p>In this chapter, we discussed suggestions and recommendations for telemetry collection. To describe some scenario or operation, we usually need multiple signals: tracing enables correlation and causation, logs provide additional information not covered by traces, events collect usage information, and metrics optimize instrumentations, queries, and alerts.</p>
<p>Depending on your application’s needs and stability, you can control costs by tuning the sampling rate on tracing and using metrics for performance data and events for usage reports.</p>
<p>OpenTelemetry semantic conventions provide instrumentation recipes for common technologies and concepts. By following them, you can create high-quality instrumentations with good defaults that you can tune to your needs. Observability backends can provide their best experiences to help you visualize, detect anomalies, and perform other semi-automated analyses. For proprietary technologies or application-specific instrumentation, where there are no existing conventions, it’s important to follow general the OpenTelemetry specification and naming patterns and report telemetry consistently across your system.</p>
<p>With this, you should be ready to instrument advanced scenarios with multiple signals and provide a rich context while following the available practices. In the next chapter, we’re going to apply these skills to instrument gRPC streaming calls that are not covered by any existing conventions. Stay tuned.</p>
<h1 id="_idParaDest-161"><a id="_idTextAnchor160"/>Questions</h1>
<ol>
<li>Can you instrument a tiny stateless RESTful microservice with tracing only?</li>
<li>When working on an application that processes thousands of requests per second on each instance, which sampling rate would you choose?</li>
<li>Your application communicates with client devices over web sockets. How would you approach instrumenting this communication?</li>
</ol>
</div>
</body></html>