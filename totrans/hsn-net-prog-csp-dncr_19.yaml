- en: Caching Strategies for Distributed Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned all about common patterns for applying security
    to a network-hosted application. In this chapter, we will look at various ways
    to improve the performance of our network software by establishing intermediate
    caches. We'll see how using a cache to persist frequently accessed highly available
    data can grant us those performance improvements. We'll look at what a cache is
    and some of the various ways it can be used. Then, we'll undertake a thorough
    examination of one of the most common and complex architectural patterns for network
    caches. Finally, we'll demonstrate how to use caching at various levels of the
    application architecture to achieve our goals with a reasonable balance of developer
    effort and latency reduction.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The potential performance improvements to be gained by caching the results of
    common requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic patterns for caching session data to enable reliable interactions with
    parallel deployments of an application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding out to leverage caches within our .NET Core applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The strengths and weaknesses of various cache providers, including distributed
    network-hosted caches, memory caches, and database caches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will have sample code to demonstrate each of the caching strategies
    we discuss. To work with that code, you'll need your trusty IDE (Visual Studio)
    or code editor (Visual Studio Code). You can download the sample code to work
    directly with it from this book's GitHub repository: [https://github.com/PacktPublishing/Hands-On-Network-Programming-with-C-and-.NET-Core/tree/master/Chapter
    15](https://github.com/PacktPublishing/Hands-On-Network-Programming-with-C-and-.NET-Core/tree/master/Chapter%2015).
  prefs: []
  type: TYPE_NORMAL
- en: Check out the following video to see the code in action: [http://bit.ly/2HY67CM](http://bit.ly/2HY67CM)
  prefs: []
  type: TYPE_NORMAL
- en: We'll also be using the Windows Subsystem for Linux to host the Linux-based
    Redis cache server on our local machine. Of course, if you're already running
    on a *nix system such as OS X or a Linux distribution, you don't have to worry
    about this. Alternatively, when you're running the application locally, if you
    don't have admin privileges or don't have any interest in learning the Redis cache
    server, you can modify the sample code slightly to use a different cache provider,
    which you'll learn about as we move through this chapter. However, I would recommend
    familiarizing yourself with the Redis cache since it is widely used and is an
    excellent choice for most circumstances. If you choose to do so, you can find
    instructions for installing the Linux Subsystem here: [https://docs.microsoft.com/en-us/windows/wsl/install-win10.](https://docs.microsoft.com/en-us/windows/wsl/install-win10)
  prefs: []
  type: TYPE_NORMAL
- en: Once that's done, you can find the instructions for installing and running Redis
    here: [https://redislabs.com/blog/redis-on-windows-10/.](https://redislabs.com/blog/redis-on-windows-10/)
  prefs: []
  type: TYPE_NORMAL
- en: Why cache at all?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While it introduces additional complexity for the developers who are responsible
    for implementing it, a well-designed caching strategy can improve application
    performance significantly. If your software relies heavily on network resources,
    maximizing your use of caches can save your users time in the form of faster performance,
    and your company money in the form of lower network overhead. However, knowing
    when to cache data, and when it would be inappropriate to do so, is not always
    intuitive for developers. So, when should you be leveraging a caching strategy,
    and why?
  prefs: []
  type: TYPE_NORMAL
- en: An ideal caching scenario
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Suppose you have an application that builds reports based on quarterly sales
    numbers. Imagine it has to pull hundreds of thousands of records from a handful
    of different databases, each with variable response times. Once it's acquired
    all of that data, it has to run extensive aggregation calculations on the records
    returned to produce the statistics that are displayed in the report output. Moreover,
    these reports are generated by dozens or even hundreds of different business analysts
    in a given day. Each report aggregates, for the most part, the same information,
    but some reports are structured to highlight different aspects of the data for
    different business concerns of the analysts. A naive approach to this problem
    would simply be to access the requested data on demand and return the results,
    reliably, with terrible response times. But does that necessarily have to be the
    case?
  prefs: []
  type: TYPE_NORMAL
- en: What I've just described is actually an ideal scenario for designing and implementing
    a caching strategy. What I've described is a system reliant on data owned by an
    external resource or process, which means that round-trip latency can be eliminated.
    I also noted that it is quarterly sales data, which means that it is presumably
    only ever updated, at most, once every three months. Finally, I mentioned that
    there are users generating reports with this remotely accessed data dozens to
    hundreds of times a day. For a distributed system, there is an almost no more
    obvious circumstance for pre-caching the remote data for faster and more reliable
    access in your on-demand application operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The contexts that motivate cache usage won''t always be so cut and dried, but
    in general these three criteria will be a strong guide for when you should consider
    it. Just always ask yourself if any of them are met:'
  prefs: []
  type: TYPE_NORMAL
- en: Accessing resources external to your application's hosted context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing resources that are infrequently updated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing resources that are frequently used by your application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If any of those circumstances are met, you should start to think about what
    benefits you might reap by caching. If all of them are met, you would need to
    make a strong case for why you wouldn't implement a caching strategy. Of course,
    to understand why these criteria make caching necessary, you must first understand
    exactly what caching is and, just as importantly, what it isn't.
  prefs: []
  type: TYPE_NORMAL
- en: The principles of a data cache
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Put simply, a cache is nothing more than an intermediary data store that can
    serve its data faster than the source from which the cached data originated. There
    are a number of reasons a cache could provide these speed improvements, and each
    of them requires their own consideration, so let's look at a few.
  prefs: []
  type: TYPE_NORMAL
- en: Caching long-running queries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have any formal database design experience, you likely know that relational
    databases tend toward highly normalized designs to eliminate duplicate data storage,
    and increase stability and data integrity. This normalization breaks out data
    records into various tables with highly atomic field definitions, and cross-reference
    tables for aggregating hierarchical data structures. And if you're not familiar
    with those principles of database design, then it's sufficient to say that it
    typically improves the usage of space at the cost of access time for a record
    lookup.
  prefs: []
  type: TYPE_NORMAL
- en: If you have a well-normalized relational database storing information that you
    would like to access as de-normalized flat records representing their application
    models, the queries that are used to flatten those records are often time-consuming
    and redundant. In this case, you might have a cache that stores flattened records,
    whose design is optimized for use by your application. This means that whenever
    a record needs to be updated, the process of de-normalizing the data can happen
    exactly once, sending the flattened structure to your cache. Thus, your application's
    interaction with the underlying data store can be reduced from a potentially beefy
    aggregation query against multiple tables to a simple lookup against a single
    application-specific table.
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, simply adding an intermediary cache for the long-running data-access
    operations can guarantee a performance improvement, even if the actual data storage
    system for the cache is the same as the origin. The primary performance bottleneck
    being mitigated by the cache is the query operation itself, so even if there is
    no reduction in network latency, you could still expect to reap some meaningful
    benefits.
  prefs: []
  type: TYPE_NORMAL
- en: It should be mentioned that this strategy can be applied to any long-running
    operations in your application flow. I used the example of slow database queries
    simply because those are the most commonly encountered bottlenecks with larger
    enterprise systems. However, in your work, you may find it beneficial to cache
    the results of computationally intensive operations executed within your application's
    host process. In this case, you'd likely be using an in-memory cache or a cache
    hosted on your own system, so there would be no possibility of improving your
    latency. But imagine deploying your application to a cloud hosting provider that's
    charging you by your application's uptime. In that scenario, cutting out a multi-second
    calculation from your application's most frequently used workflow could save thousands
    in compute costs over time. When you're caching the results of a method call or
    calculation local to your system, this is called **memoization.**
  prefs: []
  type: TYPE_NORMAL
- en: Caching high-latency network requests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another common motivating factor for caching is high-latency network requests.
    In this scenario, your software would be dependent on a network resource, but
    some aspect of the network infrastructure makes accessing that resource unacceptably
    slow. It could be that your application is hosted behind a firewall, and the request
    validation protocols for an incoming or outgoing request introduce high latency.
    Or, it might just be a matter of locality, with your application server hosted
    in a separate physical region from your nearest data center.
  prefs: []
  type: TYPE_NORMAL
- en: Whatever the reason, a common solution to this problem is to minimize the impact
    of the network latency by caching the results in a more local data store. Let's
    suppose, for instance, the issue is a gateway or firewall introducing unacceptable
    latency to your data access requests. In that case, you could stand up a cache
    behind the firewall to eliminate the latency it introduces. With this sort of
    caching strategy, your objective is to store your cached data on some host that
    introduces less latency than the source. Even if the time to look up a record
    in your cache is no faster than the time to look up the same record at the source,
    the minimized latency is the objective.
  prefs: []
  type: TYPE_NORMAL
- en: Caching to preserve state
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last strategy for caching data is to facilitate state management. In cloud-deployed
    application architectures, you might have a user interacting with multiple instances
    of your application, running in parallel containers on different servers. However,
    if their interaction with your application depends on persisting any sort of session
    state, you'll need to share that state across all instances of your application
    that might service an individual request over the course of that session. When
    this is the case, you'll likely use a shared cache that all of the instances of
    your application can access and read from to determine if a request from a user
    relies on the state that was determined by another instance.
  prefs: []
  type: TYPE_NORMAL
- en: When to write to a cache
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As I've described it so far, a cache might just sound like a duplicated data
    store that is optimized for your application. In some sense, that's true, but
    it's not technically correct, since a cache should never mirror its source system
    perfectly. After all, if you can store a complete copy of the underlying data
    store in a higher performance cache, what value is there in the underlying data
    store in the first place?
  prefs: []
  type: TYPE_NORMAL
- en: Instead, a cache will typically only contain a very small subset of the underlying
    data store. In most cases, the small size of a cache is necessary for its performance
    benefits, since even a simple query will scale linearly over the size of the set
    being queried. But if our data cache is not a perfect mirror of the source system,
    then we must determine which data makes it into our cache and when.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-caching data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One simple but effective strategy for writing data to a cache is called **pre-caching**.
    In a system that pre-caches it's data, the developers will determine what is likely
    to be the lowest-performing or most-frequently requested data access operations,
    whose results are leastlikely to change over the lifetime of the application.
    Once that determination is made, those operations are performed once, usually
    in the initialization of the application, and loaded into the cache before any
    requests are received or processed by the application.
  prefs: []
  type: TYPE_NORMAL
- en: The example I mentioned earlier, involving frequently requested reports of quarterly
    sales data, is an ideal scenario for pre-cached data. In this case, we could request
    the sales data and run all of the statistical operations necessary for the output
    of the reports at the startup of our application. Then, we could cache the finished
    view models for each kind of report the application serviced. Upon receiving a
    request for a report, our application could reliably query the cache for the view
    model, and then populate the report template accordingly, saving time and compute
    costs over the course of the lifetime of the application.
  prefs: []
  type: TYPE_NORMAL
- en: The downside to this approach, though, is that it requires synchronization of
    any updates to the underlying data with refreshes of the application's cache.
    In cases where the application and the underlying data store are owned and managed
    by the same team of engineers, this synchronization is trivial. However, if the
    data store is owned by a different team from the engineers responsible for the
    application, the coordination introduces a risk. One failure to synchronize updates
    could result in stale data being served to customers. To mitigate this risk, you
    should establish a clear and resilient strategy for refreshing your cache, and
    automate the task as much as is possible.
  prefs: []
  type: TYPE_NORMAL
- en: On-demand cache writing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most implementations of a caching system will be designed to write data to the
    cache on demand. In an on-demand system, the application will have a need for
    some piece of data that it can be certain is stored in the underlying database.
    However, prior to making the slower data access request all the way back to the
    underlying database, the application will first check for the requested data in
    the cache. If the data is found, it's called a **cache hit**. With a cache hit,
    the cache entry is used, and no additional call is made back to the dataset, thus
    improving the performance of the application.
  prefs: []
  type: TYPE_NORMAL
- en: In the alternative situation, where the requested data hasn't been written to
    the cache, the application has what's called a **cache miss**. With a miss, the
    application must make the slower call to the underlying data store. At this point,
    though, the cost of accessing the requested data from the lower-performing system
    has been paid. So now, the application can use whatever heuristics have been set
    for it to determine if the retrieved data should then be written to the cache,
    thus saving time on subsequent requests for the same piece of data.
  prefs: []
  type: TYPE_NORMAL
- en: Cache replacement strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If your cache is of a fixed limited size, you may find yourself needing to define
    a cache replacement policy. A cache replacement policy is how you determine when
    to replace older records with newer, potentially more relevant, ones. This will
    generally happen when your application experiences a cache miss. Once the data
    is retrieved, the application will determine whether or not to write it to the
    cache. If it does end up writing a record to the cache, it will need to determine
    which record to remove. The trouble, though, is that it is very difficult to determine
    a consistent heuristic for identifying which records won't be needed again soon.
  prefs: []
  type: TYPE_NORMAL
- en: You might have come up with a seemingly obvious answer in your own head just
    thinking about it; I certainly did when I first learned about this problem. But
    most of the obvious solutions don't hold up to scrutiny. For example, a fairly
    popular replacement policy involves eliminating the record that was least recently
    used. This just means replacing the entry that hasn't generated a cache hit for
    the longest series of cache queries. However, it may be the case that the longer
    it's been since a record has been used, the more likely it is that it will be
    the next record used, with records looked up in a cyclical order. In that case,
    eliminating the least recently used record would increase the chances of another
    cache miss in subsequent requests.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you could try the least frequently used replacement policy. This
    would drop the record with the fewest cache hits out of all records on the system,
    regardless of how recently those hits occurred. Of course, the drawback for this
    approach is that, without accounting for recency, you ignore the possibility that
    a recently used record might have been queried recently because it will become
    relevant for a series of subsequent operations the user intends to perform with
    it. By eliminating it due to its low hit rate, and ignoring the recency of the
    hit, you increase the chances of a cache miss in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Each cache replacement policy has its own drawbacks and should be considered
    within the context of your application. However, there is a key metric by which
    you might determine the relative success of your replacement policy. Once your
    initial heuristic is designed and deployed, you can track your cache's hit ratio.
    A cache's hit ratio is, quite simply, the number of cache hits divided by the
    number of cache misses. The closer that number is to 1.0, the better your cache
    replacement policy is.
  prefs: []
  type: TYPE_NORMAL
- en: Cache invalidation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you consider your cache replacement strategies, you may find that a piece
    of information stored in your cache will only be relevant to your application
    for a short period of time. When that's the case, instead of waiting for a new
    cache miss to overwrite the irrelevant data, you may want to expire the entry
    after a certain timeout. This is what's known as **cache invalidation**. Put simply,
    cache invalidation is the process of determining that a record in your cache should
    no longer be used to service subsequent requests.
  prefs: []
  type: TYPE_NORMAL
- en: In cases, as I've described, where you have a known time-to-live for any given
    record written to your cache, invalidating those records is as simple as setting
    and enforcing an expiration on the record as it's being written. However, there
    are other cases where it might not be so obvious that a cached record should be
    invalidated. Consider a web browser caching a response from the server. Without
    a predetermined expiration date, the browser can't know for sure that the cached
    response still represents the current state of the server without first checking
    the server, thus eliminating the performance benefit of the cache.
  prefs: []
  type: TYPE_NORMAL
- en: Since you should never be serving stale or invalid data to your users, you should
    always design some mechanism for invalidating your cached records. I've just discussed
    the two most common, and you'd be hard-pressed to find a reason not to implement
    at least one of them. So, if you have control over the cache itself, as in the
    case of a cache contained within your application architecture, you should always
    be diligent to invalidate cache records whenever the underlying data store is
    updated. And for the cases when you're not in control of your responses being
    cached, make sure you're always setting a reasonable cache expiration on your
    responses.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we've learned about what a cache is, why you might implement
    one in your software architecture, and what sort of strategies are at your disposal
    for optimizing its performance. So, now it's time to look at one of the most common
    caching strategies in modern cloud-deployed network architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed caching systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, I discussed using a cache for the purpose of preserving
    application state between parallel deployments of the same application. This is
    one of the most common use cases for caching in modern cloud-based architectures.
    However, useful as it may be, this sort of distributed session cache can introduce
    a whole host of challenges to the application design.
  prefs: []
  type: TYPE_NORMAL
- en: A cache-friendly architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Caches have historically been used to improve performance by reducing latency
    or operation times. With a session cache for distributed architecture, however,
    the cache itself is not intended to provide any specific performance improvement
    on a given operation. Instead, it's designed to facilitate a necessary interaction
    between multiple instances of an application. Its design is to facilitate state
    management that would otherwise involve complicated orchestration of multiple
    hosts. To understand how this is done, let's consider an example.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you have a cloud-hosted API that is responsible for verifying a user's
    identity and age. To do so, it requests various pieces of information that, taken
    together, could serve as verification of the user. In an effort to design your
    user experience to be as non-intrusive as possible, you start by asking only a
    few questions about their birth date and current address, which are most likely
    to successfully verify their age and identity. Once they've submitted their answers,
    your application would attempt to verify them. If it succeeds, your user can proceed,
    but if the initial set of questions is unsuccessful, your application follows
    up with a handful more questions that, when combined with the answers from the
    first set, are highly likely to verify the user's identity. The user submits their
    answers, and you continue again with failure, resulting in one final question
    requesting the last four digits of the user's social security number. Upon submission,
    the user is either successfully verified or rendered permanently unable to access
    your system.
  prefs: []
  type: TYPE_NORMAL
- en: That process is relatively straightforward from a business-logic perspective,
    but how would you implement it on a network process that is meant to remain entirely
    stateless between network requests? And as a cloud-hosted application, how would
    you maintain the current state of the user's position in that workflow across
    multiple possible instances of your app server processing a given request?
  prefs: []
  type: TYPE_NORMAL
- en: The case for a distributed cache
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a handful of techniques available in this particular case, each with
    their own advantages and drawbacks. You could use sticky sessions to force subsequent
    requests from a particular host in a given session to be serviced by the same
    app server that processed the initial request. This would allow for some minor
    local state management over the course of a session. The downside to this is that
    it eliminates the performance benefits of horizontal scaling in a cloud-hosted
    system. If a user is always forced to interact with a single server over the course
    of a session, regardless of the traffic to that server or the availability of
    other servers, they may as well be interacting with a single-instance monolithic
    application architecture. Moreover, you would no longer be staying true to the
    architectural ideal of a "stateless" service, as you would be implementing some
    mechanism for preserving a user's position in the workflow over the course of
    interaction on your active service.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you could use the same principle we saw in the *Self-encoded
    tokens* section of [Chapter 14](bf84cf6c-16d3-4225-b590-b3657aaa3832.xhtml), *Authentication
    and Authorization on Networks*. In this case, you'd be self-encoding the user's
    current state in the response from your server, and your user would be responsible
    for returning that self-encoded state back to the server in subsequent requests.
    This allows each request body to serve as a breadcrumb trail leading back to the
    first interaction with the client, from which the server could rebuild the state
    that was created in previous interactions with each subsequent interaction.
  prefs: []
  type: TYPE_NORMAL
- en: This approach will increase the complexity of your request/response models,
    though. It will also introduce the added risk of unenforceable limits on verification
    attempts. Suppose, for security purposes, your business rules dictate that a user
    should only be allowed to attempt each round of questions once. If you self-encode
    the state of the user session in your request/response models, you're relying
    on your users to return an accurate representation of each previous attempt with
    each of their requests. It would be easy for a dedicated malicious actor to make
    as many attempts as they please by simply scrubbing the workflow state from their
    subsequent requests.
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, I would argue that the most reliable solution for maintaining
    state across requests is a distributed cache shared by each app server in your
    cloud environment. This prevents you from maintaining state across your app servers,
    thus preserving the stateless principle of cloud-deployed service architecture,
    while still allowing your services to maintain full control over a user's progression
    through your verification workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement this, you would host the cache provider on its own server, independently
    of any instances of your application servers in your cloud. Any server that successfully
    processes a given step in the workflow would refuse to fully resolve the interaction
    and provide a response to the client unless and until the result of that step
    was successfully written back to your data cache. In this way, the current app
    server instance could do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Verify that no other instances have successfully processed the same request
    already by confirming that no record of the workflow step exists for the user
    in the cache
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notify other instances of the application that the step had been processed so
    that they would cease to duplicate the transaction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The benefits of a distributed cache
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The system I've described has the benefit of ensuring the consistency of the
    user state throughout their interaction across alldeployed instances of your application,
    with only the minor orchestration of reading from and writing to a cache. This
    data consistency, is key even when your distributed cache is not used for state
    management. It can prevent multiple instances of an application from trying to
    perform two incompatible modifications to the same piece of data, or allow synchronization
    of transactions across multiple app servers prior to committing them to the underlying
    database. And most importantly, from the developer's perspective, it can eliminate
    difficult to reproduce and difficult to track down bugs caused by race conditions
    between services.
  prefs: []
  type: TYPE_NORMAL
- en: Hosting the cache server independently of all other applications also provides
    it with a measure of resilience against app restarts or crashes. By isolating
    your data store, you can isolate costs associated with higher availability and
    resiliency guarantees from your cloud provider. It can also help to minimize the
    memory footprint of your application containers living on your app servers. If
    you pay for RAM usage, this can save you thousands as your cache scales out. So,
    how exactly do we reap these benefits in our code?
  prefs: []
  type: TYPE_NORMAL
- en: Working with caches in code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To see how we can benefit from the various caching mechanisms supported by .NET
    Core, we'll be setting up a somewhat complicated demo application structure. The
    first thing we'll do is create a remote data store that has long-running operations
    to return results from queries. Once that's done, we'll set up an application
    dependent on that data, and provide it a caching strategy to mitigate the impact
    of our artificially slowed down remote data storage.
  prefs: []
  type: TYPE_NORMAL
- en: Writing our backing data system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll be creating our backing data system as a simple Web API project. The
    goal is to expose a couple of endpoints on a single controller that expose data
    of different types to demonstrate how we can write the values to our cache regardless
    of the type discrepancy between the records. First, let''s create our project
    with the .NET Core CLI. Let''s look at the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, since we''ll be hosting this project at the same time as our cache-ready
    application, we''ll want to configure it to use its own port, instead of the default
    settings. Within your `Program.cs` file, modify your `CreateWebHostBuilder(string[]
    args)` method to use whatever custom URLs you want this application to listen
    on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we''ll modify the `ValuesController.cs` class to serve up our data. First,
    we''ll change the name of the class to `DataController` so that our routing is
    a bit more intuitive. We''ll be getting rid of all of the preconfigured endpoints
    and replacing them with three new endpoints, each returning a unique data type.
    First, though, let''s create a new data type for us to return. It will be a simple
    model with an ID and two arbitrary properties; one will be of the `string` type,
    and the other will be a `List<string>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'With this model set up, we can define the endpoints we''ll be exposing. For
    this demonstration, we''ll return a simple `List<string>` string, a single `OutputRecord`
    instance, and a `List<OutputRecord>` method. So, by the time we''ve defined a
    lookup endpoint for each data type, we''ll have methods returning simple strings,
    lists of strings, complex records, and lists of complex records. Let''s look at
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'These define our simple string responses, and will be relatively straightforward
    to test with our cache. For our `OutputRecord` endpoints, though, we''ll want
    to apply unique data to each property so that we can confirm that the full object
    is properly cached. So, the endpoint returning a single `OutputRecord` instance
    will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us an object with distinct property values, tied together by the
    same ID, which will make it easy for us to validate the behavior of our cache.
    Finally, we''ll define an endpoint to return a list of the `OutputRecord` instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Each of these endpoints returns some trivial object or string with the provided
    ID that's used in the response objects, but this will just be a way of distinguishing
    one response from the next. The important aspect of our responses will be the
    perceivable delay we'll be applying. For that, we'll a five second delay to each
    method prior to returning their result. This will give us an obvious way to identify
    when the backing data store has been hit versus when our user-facing application
    has a successful cache hit.
  prefs: []
  type: TYPE_NORMAL
- en: 'To simulate this delay, we''ll sleep the current thread for five seconds, and
    then return some arbitrary string that incorporates the given ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Each additional method will do the same thing, applying the delay and then
    initializing its expected return type with its arbitrary values. Now, if you run
    the application and ping your `/data/value/1234` endpoint, you should see the
    result come back after five seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e26b5043-1f91-4888-a09d-441220d4f5ab.png)'
  prefs: []
  type: TYPE_IMG
- en: Note the response time of 5269ms. This delay will be our indication of a cache
    miss, going forward. And with our data store ready, we can build our application
    and define its caching strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging a cache
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To start working with our cache, we'll first install and run a local instance
    of a Redis server. Redis is an open source, in-memory data store. It's frequently
    used in enterprise deployments as a simple key-value data store or cache. It's
    also supported out of the box by Azure cloud hosting environments, making it very
    popular for .NET based microservices and cloud-based applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install it, follow the instructions in the *Technical requirements* section
    of this chapter. Once you''ve done so, you''ll have your local instance running.
    If you''ve already installed the server, make sure it''s up and running by opening
    your Windows Subsystem for Linux interface, and enter the following commands to
    verify its listening port:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5d37ee1-04b1-41e7-b710-bcf809ee71b5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once you''ve got your Redis instance running, you''ll be ready to implement
    your sample microservice. Since we''ll be loading cache misses from our backend
    API, we''ll want to configure `HttpClient` for that particular application in
    our `Startup.cs` file. For this, I''ve created a static `Constants` class just
    to avoid magic strings used in my code, and used a `DATA_CLIENT` property to register
    a named instance of `HttpClient` inside my `ConfigureServices(IServiceCollection
    services)` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll create a service client to abstract the details of the HTTP requests
    we''ll be making behind a clean data-access interface, using the same patterns
    we established in Chapter 9,* HTTP in .NET*. Our interface definition will provide
    the following simple methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Within the implementing class for this interface, we''ll have a private instance
    of  `IHttpClientFactory`, and we''ll be using our named `HttpClient` instance
    to access our backend data store. This common task is isolated to a private method
    for the actual HTTP interaction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, each of the public interface methods implements an endpoint-specific
    variation of the general pattern established here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Extending this logic for all four of our access methods, we''ll complete our
    backend data client. At this point, we should modify our controller to expose
    each of the backend API endpoints, and use them to test our data-access service.
    We''ll expose the same service contract we had in our backend API, with four endpoints
    for each type of record we could look up. Instead of renaming our file, we''ll
    just redefine our controller''s route, and define a public constructor to allow
    the dependency injection framework to provide our `DataService` instance (just
    don''t forget to register the concrete implementation in your `Startup.cs`). Lets,
    look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have our data service, we can use our API endpoints to call into each
    requested object from our backend system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: At this point, by running both your backend API and your cache service API,
    you should be able to request the same values from your cache service, with the
    same five second delay. So, now that our application is fully wired up to request
    data from our backend service, let's improve its performance with caching.
  prefs: []
  type: TYPE_NORMAL
- en: The distributed cache client in .NET
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the major benefits of using Redis for our distributed caching solution
    is that it''s supported by .NET Core out of the box. There''s even an extension
    method on the `IServicesCollection` class specifically for registering a Redis
    cache for use within your application. Simply install the `Microsoft.Extensions.Caching.Redis`
    NuGet package for your current project, and then add the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This will automatically register an instance of the `RedisCache` class as the
    concrete implementation for any instances of `IDistributedCache` you inject into
    any of your services. The localhost configuration setting will use the default
    configurations for a local deployment of the Redis client, so there's no need
    to specify an IP address and port unless you explicitly change it on your local
    deployment. Meanwhile, the `InstanceName` field will give the entries stored in
    the cache that were set by this application an application-specific prefix. So,
    in this example, if I set a record with the  `1234` key with my setting of local,
    that key will be stored in the cache as `local1234`. The `RedisCache` instance
    that is registered by the `AddDistributedRedisCache()` method will automatically
    look for keys with the `InstanceName` prefix that we've specified in our options.
    We'll see this later when we inspect our cache instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'With our Redis cache running, and our `IDistributedCache` instance configured
    and registered with our dependency injection container, we can write a `CacheService`
    class. This will follow a similar pattern to our `DataService` class, where it
    exposes only a small number of logical operations as public methods, hiding the
    details of the cache interactions. Our interface for this `CacheService` class
    is as follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Here, we're making the distinction between writing a single string and writing
    a more complex record to distinguish between the need to serialize and deserialize
    our entries in each method implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Getting and setting cache records
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `IDistributedCache` class provides a simple mechanism for interacting with
    our cached data. It operates on a dumb get/set pattern whereby attempts to get
    a record will either return the cached byte array or string based on the given
    ID, or return null if no record exists. There's no error handling or state checking.
    The speed of the cache is dependent on this simple interaction mechanism and fail
    state.
  prefs: []
  type: TYPE_NORMAL
- en: Likewise, setting a record is equally easy. Simply define your ID for the record,
    and then provide some serialized representation of the record for storage. This
    serialized format can be either a string with the `SetString(string id, string
    value)` method, or a byte array using the `Set(string id, byte[] value)` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, when you write a value to the cache, you can set additional options
    for your cache record to specify expiration time spans. The kinds of expiration
    settings you can apply are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AbsoluteExpiration**: This sets the expiration to a specific moment in time
    at which point the record will be invalidated, no matter how recently it has been
    used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AbosluteExpirationRelativeToNow**: This sets a fixed moment at which the
    record will be invalidated no matter how recently it has been used. The only difference
    with this and AbsoluteExpiration is that the expiration time is expressed in terms
    of some length of time from the moment the record is set in the cache.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SlidingExpiration**: This sets an expiration time relative to the last time
    the record was accessed. So, if the sliding expiration is set for 60 minutes,
    and the record isn''t accessed again for 62 minutes, it will have expired. However,
    if the record is accessed again in 58 minutes, the expiration is reset for 60
    minutes from that second access.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, let''s look at how we''ll implement this cache. First, we''ve got to inject
    the `IDistributedCache` instance that was registered in our `Startup.cs` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we''ll implement the methods of our interface. The first method is fairly
    straightforward and only notifies our consumers if there has been a cache hit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll implement our record retrieval methods. The only difference with
    each of these is that retrieval of complex data types (records and lists of strings)
    will require an extra step of deserialization. Other than that, though, our `Fetch...()`
    methods should look fairly straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we''ll need to implement the write methods. For the sake of demonstration,
    we''ll write all of our records with a 60-minute sliding expiration time using
    the `DistributedCacheEntryOptions` class. After that, we can simply pass in our
    key to the cache, along with a serialized value (we''ll be using JSON here, to
    take advantage of the `Newtonsoft.Json` libraries) and our expiration options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'And with that, our cache should be ready to use. Now, it''s time to pull it
    all together in our controller endpoints. For this, the interaction pattern will
    be the same across each method, with the only difference being the type of read/write
    operation we perform on our cache. So Let''s look at how we''ll implement our
    cache strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The first thing you'll notice is that I apply a suffix to our given ID that
    matches my route. This is to allow duplicate IDs in my cache for each distinct
    data type. Next, we check our `HasCacheRecord` (key) method to determine whether
    we have a cache hit. If we do, we simply fetch the cache record and return the
    result. When we have a miss, though, we have to fetch the data from our underlying
    data store. Once we have it, we write it to our cache for faster retrieval in
    any subsequent requests, and then return the value.
  prefs: []
  type: TYPE_NORMAL
- en: After applying this pattern with the appropriate modifications to each of our
    endpoints, we're ready to test. To confirm the behavior of our cache, first run
    the same query against any endpoint with a new ID, twice in a row. If everything's
    working properly, you should have a five second delay on your first request, and
    almost zero delays on your subsequent request.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have at least a record or two stored in your cache, you can observe
    the values with your redis-cli in your Windows Subsystem for Linux console. The
    `RedisCache` class will store the entries as hash types in the underlying cache,
    so you''ll need to look for the key values using those commands. The operations
    I performed to look up the records I wrote while testing the app are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9584f3f5-0bfa-480a-b9dd-01b711217714.png)'
  prefs: []
  type: TYPE_IMG
- en: The first command, `keys *`, simply searches all active keys that match the
    given pattern (* is the wildcard, so `keys *` matches all keys). Then, I used
    the `hgetall [key]` command to get each property in my entry's hash. In that output,
    you can clearly see the JSON written to the cache from my application, demonstrating
    the successful and expected interactions between my app and my cache.
  prefs: []
  type: TYPE_NORMAL
- en: I'd also like to point out the key structure. As I mentioned before, the keys
    I set (in this case, 2345 records) are prefixed with  `InstanceName` of  `RedisCacheOptions`,
    with which I configured  `RedisCache` in the `Startup.cs` file. And with that
    output, you've seen the full interaction pattern established by Microsoft for
    working with a Redis cache instance.
  prefs: []
  type: TYPE_NORMAL
- en: Cache providers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While we demonstrated the use of a data cache with an instance of the `IDistributedCache`
    class in our sample code, that is hardly the only cache provider we have access
    to with .NET Core. Before we close out the subject of caches, I just want to briefly
    discuss the other two most common providers in the framework.
  prefs: []
  type: TYPE_NORMAL
- en: The SqlServerCache provider
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Redis is certainly popular among engineers as being a high-performance cache
    implementation. However, it's hardly the only distributed provider out there.
    In fact, Microsoft's own SQL Server can serve as a cache when the situation calls
    for it, and they've defined a similar implementation for the `IDistributedCache`
    class to expose it.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the biggest differences with the `SqlServerCache` provider and the `RedisCache`
    instance is in the configuration it requires. Where Redis is a simple key-value
    store, `SqlServer` remains a full-featured relational database. Thus, to provide
    the lightweight interactions necessary for a high performing cache, you''ll have
    to specify the precise schema, table, and database connection you intend to leverage
    when you set it as your `IDistributedCache` provider. And since SQL Server doesn''t
    support the hash tables that Redis does, the table to which your application connects
    for caching should implement the expected structure of an `IDistributedCache`
    record. Thankfully, the .NET Core CLI provides a utility command for establishing
    just such a table: the `sql-cache create` command. And notably, since your application
    should only ever be interacting with injected instances of `IDistributedCache`,
    you won''t even notice the difference, except perhaps in performance. However,
    for the sake of performance, I would recommend using Redis wherever possible.
    It is quickly becoming the industry standard and its speed is truly unmatched
    by SQL Server.'
  prefs: []
  type: TYPE_NORMAL
- en: The MemoryCache provider
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, if your application either doesn't have the need, or doesn't have the
    means, to support a standalone cache instance, you can always leverage an in-memory
    caching strategy. The `MemoryCache` class of the `System.Runtime.Caching` namespace
    will provide exactly that. Configuring it is as simple as invoking the `services.AddMemoryCache()`
    method in `Startup.cs`, and it provides a similar interface to the `IDistributedCache`
    class we've already looked at.
  prefs: []
  type: TYPE_NORMAL
- en: It does bring some major caveats with it, however. Since you're hosting the
    cache within your application's own process, memory becomes a much more valuable
    resource. Disciplined use of a cache replacement policy, and an aggressive expiration
    time, becomes much more important with an in-memory caching solution. Additionally,
    since any state that must persist over the lifetime of a session will only be
    persisted within a single instance of your application, you'll need to implement
    sticky sessions. This will ensure that users will always interact with the app
    server that has their data cached in its memory.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, your business needs and environmental constraints will play a large
    role in determining what sort of caching policies and strategies you should be
    taking advantage of in your application. However, with the information in this
    chapter, you should be well-suited to making the best possible decisions for your
    circumstances. Meanwhile, we'll be continuing our consideration of performance
    optimization in the next chapter as we consider performance monitoring and data
    tracing in a network-hosted application.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we took an extensive tour of the motivations for, and use cases
    of, data caches in a distributed network application. We started by exploring
    some common business and design problems that would likely reap the benefits of
    a caching strategy. In doing so, we identified some of the basic considerations
    you can make when determining if the complexity of introducing a cache management
    system is the right decision for your application. Then, we looked at exactly
    which benefits could be gained from caching, and precisely how caching can provide
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Once we learned why we might use a cache, we looked at some of the common problems
    that must be solved for when implementing a cache. First, we tackled the tactics
    of pre-caching data and caching results on demand. Then, we looked at how to determine
    which data or resources should be cache. We learned about establishing a cache
    replacement policy that is well-suited to your application's most common data
    interactions, and how to invalidate records in your cache to make sure you're
    never returning stale results.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we saw how we could use a cache in our applications. We learned how
    to run a distributed cache, and we saw how to write to and read from that cache
    within the code. We saw that a cache record could be an arbitrary data structure
    with an arbitrary key, and how to detect hits within our cache instance. Finally,
    we looked at alternative caching mechanisms available with C# and .NET Core.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll continue with our focus on optimizing our application
    performance for a network, and look at the tools available to us for monitoring
    our application's performance and identifying any bottlenecks in our network.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are three criteria that should motivate a caching strategy?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are three common pain points caches can help resolve?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a cache hit? What is a cache miss?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a cache replacement policy? What are some common cache replacement policies?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a hit ratio, and how does it relate to a replacement strategy?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is cache invalidation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are some of the benefits of using a distributed cache?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For even more hands-on guidance for building caches in a modern .NET context,
    I recommend the book *The Modern C# Challenge*, by *Rod Stephens*. It takes a
    deep dive into the same sorts of patterns and practices we discussed in this chapter
    with an incredibly approachable presentation. It can be found through Packt publishing,
    here: [https://www.packtpub.com/application-development/modern-c-challenge-0.](https://www.packtpub.com/application-development/modern-c-challenge-0)
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, if you want to consider other challenges inherent to distributed,
    horizontally scaled application architectures, you should check out *Microservice
    Patterns and Best Practices* by *Vinicius Feitosa Pacheco*. It's also available
    from Packt, and you can get it here: [https://www.packtpub.com/application-development/microservice-patterns-and-best-practices.](https://www.packtpub.com/application-development/microservice-patterns-and-best-practices)
  prefs: []
  type: TYPE_NORMAL
