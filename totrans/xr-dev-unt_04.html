<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-75"><a id="_idTextAnchor011"/>4</h1>
<h1 id="_idParaDest-76">AR Development in Unity</h1>
<p>In this chapter, we will immerse ourselves in the fascinating realm of AR development, from creating our first AR project in Unity to launching our first AR scene on a device or simulator. We will present to you numerous AR toolkits and plugins that Unity offers, and guide you in understanding their unique functionalities.</p>
<p>In a step-by-step manner, we will walk through the process of establishing an AR project in Unity, ensuring it is primed for smooth deployment onto any AR-supportive device.</p>
<p>This chapter will cover the following topics:</p>
<ul>
<li>Understanding the AR landscape</li>
<li>Setting up an AR project in Unity using AR Foundation</li>
<li>Testing AR experiences directly in Unity</li>
<li>Deploying AR experiences onto mobile devices</li>
</ul>
<h1 id="_idParaDest-77">Technical requirements</h1>
<p>Before we dive into the practicality of the Unity Editor, it’s important to ensure your computer system is up to the task. <em class="italic">Unity 2021.3 LTS</em>, or a more recent version, is required to walk through the exercises that we’ll explore in this book. Check your hardware compatibility by comparing it with the system requirements provided on the Unity website at <a href="https://docs.unity3d.com/Manual/system-requirements.html">https://docs.unity3d.com/Manual/system-requirements.html</a>.</p>
<p>As we’ll be exploring AR development in this chapter, we will need either an Android or iOS device capable of supporting ARKit or ARCore. Review whether your device meets these requirements at <a href="https://developers.google.com/ar/devices">https://developers.google.com/ar/devices</a>.</p>
<h1 id="_idParaDest-78">Understanding the AR landscape</h1>
<p>As we start our exploration of AR, it’s <a id="_idIndexMarker272"/>crucial to first understand the foundational elements that enable this technology. How is it that our everyday devices can so effortlessly intertwine our physical reality with the digital? What mechanisms allow your device to sense, interpret, and interact with the world around it? And, perhaps most intriguingly, how can a simple screen transform into a doorway to an enhanced reality?</p>
<p>In this section, we aim to unpack the complex principles and mechanisms of AR, distilling them into a comprehensible format. </p>
<p>If terms such as AR, MR, and VR still seem opaque or interchangeable to you, consider revisiting <a href="B20869_01.xhtml#_idTextAnchor000"><em class="italic">Chapter 1</em></a> for clarification. For now, our focus remains on AR, which transforms our world by superimposing it with elements of the digital domain. Let’s look at the different types of experiences that AR offers.</p>
<h2 id="_idParaDest-79">What types of AR experiences exist?</h2>
<p>The AR landscape is<a id="_idIndexMarker273"/> diverse, with experiences typically presenting themselves through one of several mediums: handheld mobile AR, AR glasses, or other types of AR such as projection-based AR or spatial AR. Each form has its unique characteristics, and their utilization depends on the context and the level of immersion desired. Let’s learn more about these:</p>
<ul>
<li><strong class="bold">Handheld mobile AR</strong>: Handheld <a id="_idIndexMarker274"/>mobile AR is perhaps the most widespread form of <a id="_idIndexMarker275"/>AR due to the ubiquity of smartphones. Imagine this type of AR as a window into an enriched reality. Through the screen of their smartphone or tablet, a user witnesses a mingling of the digital and the real. This overlay of digital content on a live camera feed breathes life into an otherwise static physical world. A prime example of this is the popular game <em class="italic">Pokémon Go</em>, where the user hunts for digital creatures that seem to inhabit our own world.</li>
<li><strong class="bold">AR glasses</strong>: AR glasses, on<a id="_idIndexMarker276"/> the other hand, provide a <a id="_idIndexMarker277"/>more immersive and hands-free AR experience. When the user dons these glasses, they step directly into an augmented world without the need for a separate device. Thanks to transparent displays, sensors, and cameras integrated into the glasses, digital information is seamlessly woven into the user’s field of view. The <a id="_idIndexMarker278"/>implications of this technology are far-reaching, with potential applications in industries ranging from <a id="_idIndexMarker279"/>manufacturing and healthcare to entertainment. An example of a game designed for AR glasses is a new iteration of the Pokémon Go game, aptly named <em class="italic">Pokémon Go AR+</em>. Pokémon Go AR+ revolutionizes the original concept of the game, taking full advantage of AR glasses’ capabilities. When wearing AR glasses, players are immersed in the Pokémon world more deeply. They can see Pokémon in their real-world surroundings as if they were actually there. Pokéstops and Gyms are visible in real-world locations, and players can interact with them directly. For example, if a Pokémon appears, the player can reach out to touch it and initiate a catch sequence. The game also allows for real-time battle simulations with other players using the AR environment.</li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">Though both handheld mobile AR and AR glasses act as pathways to AR, they differ significantly in their form factor, user experience, and level of immersion. Mobile AR serves as a portable gateway to an augmented world, accessible through a user’s smartphone or tablet. On the other hand, AR glasses offer a fully immersive experience where the augmented world is in direct view. Given the prevalence of smartphones, this book will primarily focus on AR development for Android and iOS handheld devices.</p>
<p class="callout">This does not mean, however, that AR glasses lack potential or usage. The focus on handheld devices mainly reflects their current wider use and accessibility. Despite this, AR glasses present significant opportunities and a level of immersion that handheld devices can’t match. Even though the book focuses on handheld AR, the fundamental principles of AR development it covers, such as understanding the 3D space, user interaction, and user experience design, are largely applicable to AR glasses. Readers interested in AR glasses development can still gain valuable insights, though they might need to supplement their learning with additional resources specifically focused on AR glasses technologies.</p>
<ul>
<li><strong class="bold">Projection-based AR</strong>: Projection-based AR casts digital imagery onto real-world surfaces. A notable<a id="_idIndexMarker280"/> application of <a id="_idIndexMarker281"/>projection-based AR is found in the automotive industry, where AR has started to make a significant impact. Information such as navigation, speed, and other essential data can be projected onto the vehicle’s windshield, providing real-time visual cues to the driver without the need to take their eyes off the road.</li>
<li><strong class="bold">Spatial AR</strong>: Spatial AR<a id="_idIndexMarker282"/> uses holographic displays to <a id="_idIndexMarker283"/>create an illusion of virtual objects cohabitating in our physical environment. Holographic displays could be likened to digital mirages, using a combination of light projection and optics to create three-dimensional virtual objects that appear to float in space. This form of AR doesn’t necessitate additional devices or wearables, enabling the user to interact with the holograms as if they were physically present.</li>
</ul>
<p>Having explored the types of AR experiences, let’s now delve into the techniques and concepts that enable the overlay of virtual objects in the real world.</p>
<h2 id="_idParaDest-80">What are marker-based and mar<a id="_idTextAnchor012"/>kerless AR?</h2>
<p>AR is underpinned by a set of foundational principles that allow virtual objects to be accurately positioned and realistically interacted with in our physical world. These principles involve various<a id="_idIndexMarker284"/> technologies and techniques, such as <strong class="bold">marker-based AR</strong> and <strong class="bold">markerless AR</strong>.</p>
<p>Let’s now learn more about each of these technologies.</p>
<h3>Marker-based AR</h3>
<p>Marker-based AR <a id="_idIndexMarker285"/>depends on specific markers or targets to initiate the <a id="_idIndexMarker286"/>display of AR content. These markers can take numerous forms, such as physical objects with identifiable patterns, QR codes, or images. When the AR system detects the marker, it overlays digital content onto it.</p>
<p>When developing AR applications, there are several types of markers that can be used to trigger the display<a id="_idIndexMarker287"/> of AR content. Here’s an overview of some commonly utilized markers in AR:</p>
<ul>
<li><strong class="bold">Image markers</strong>: These <a id="_idIndexMarker288"/>are distinct visual patterns or images that serve as the trigger for AR content. When an AR system detects these markers through the camera, it overlays the corresponding digital content onto them.</li>
<li><strong class="bold">QR codes</strong>: Quick response (QR) codes, which are two-dimensional barcodes containing<a id="_idIndexMarker289"/> specific information, can also serve as AR markers. When the camera or a QR code scanning library identifies these codes, it can trigger the display of certain AR content or interactions.</li>
<li><strong class="bold">3D object markers</strong>: These<a id="_idIndexMarker290"/> markers involve using specific physical objects as triggers for AR content. The AR system identifies the object’s shape and features and uses this data to overlay the digital content.</li>
<li><strong class="bold">Location-based markers</strong>: These <a id="_idIndexMarker291"/>markers use the user’s geolocation data to trigger AR content. By leveraging GPS or other location-tracking technologies, the AR system can overlay digital content that is relevant to the user’s current location.</li>
<li><strong class="bold">Code-based markers</strong>: These<a id="_idIndexMarker292"/> are custom-designed patterns or symbols that can be recognized by the AR system and used as triggers. These markers are created and decoded using specific algorithms, providing a high degree of flexibility and customization for AR experiences.</li>
</ul>
<p>The type of marker to use depends on the specific requirements of your AR application. Factors such as the desired user experience, tracking accuracy, and ease of marker recognition will guide marker selection.</p>
<p>While many AR experiences use markers, not all do, as you will learn in the next subsection.</p>
<h3>Markerless AR</h3>
<p>Markerless AR, also<a id="_idIndexMarker293"/> known as <strong class="bold">position-based AR</strong> or <strong class="bold">simultaneous localization and mapping</strong>  (<strong class="bold">SLAM</strong>)-based AR, doesn’t rely on predetermined <a id="_idIndexMarker294"/>markers <a id="_idIndexMarker295"/>or visual cues. It employs onboard sensors and complex algorithms to overlay digital information onto the physical world. Sensors <a id="_idIndexMarker296"/>can include <strong class="bold">global positioning systems</strong> (<strong class="bold">GPS</strong>), accelerometers, and cameras to ascertain the user’s location and orientation in the physical environment. With an understanding of the user’s location, these <a id="_idIndexMarker297"/>AR systems overlay virtual content onto the physical surroundings based on geographical coordinates.</p>
<p>Let’s delve into various options for implementing markerless AR and explore their real-world applications:</p>
<ul>
<li><strong class="bold">Geolocation-based AR</strong>: This <a id="_idIndexMarker298"/>approach primarily uses GPS and is suitable for placing AR objects on a larger scale in outdoor environments.<p class="list-inset">An example of this is Niantic’s game Pokémon Go. Using the device’s GPS, the game places virtual Pokémon creatures in real-world locations, allowing players to find and catch them.</p></li>
<li><strong class="bold">Wi-Fi positioning system</strong> (<strong class="bold">WPS</strong>): This method determines the device’s location<a id="_idIndexMarker299"/> based on the strength and origin of Wi-Fi signals. It’s especially relevant for indoor AR experiences where GPS may be less effective. For instance, <em class="italic">Indoor Atlas</em> offers a platform for indoor navigation that combines magnetic information and Wi-Fi signals. This has been used to enhance AR experiences in shopping malls, guiding the user to specific stores or attractions with digital markers.</li>
<li><strong class="bold">Bluetooth</strong> and <strong class="bold">ultra-wideband</strong> (<strong class="bold">UWB</strong>): These <a id="_idIndexMarker300"/>positioning techniques are geared<a id="_idIndexMarker301"/> for micro-location experiences, providing high precision in smaller spaces such as rooms or exhibits. A practical application can be seen in museums and galleries, where Bluetooth beacons are used in AR apps. These apps then serve multimedia content to visitors based on their proximity to specific artworks or exhibits.</li>
<li><strong class="bold">SLAM</strong>: SLAM is a <a id="_idIndexMarker302"/>more advanced technique that creates a digital map of the environment while tracking the user’s location. This technique involves complex algorithms and uses the device’s camera and other sensors. Imagine you’re in a dark room and you light up a flashlight. As you move the flashlight around, you start to see and remember where different things are, such as chairs or tables. Over time, you build a map in your head of the whole room. SLAM does something similar. It’s most suitable for applications that require accurate object placement and interaction in smaller spaces.<p class="list-inset">An example<a id="_idIndexMarker303"/> of SLAM is IKEA’s app, <em class="italic">IKEA Place</em>. On the app, the user can select a piece of furniture from IKEA’s catalog and the app overlays a 3D model of it onto the camera view, allowing the user to see how the item would look in their home.</p></li>
<li><strong class="bold">Depth sensing</strong>: This<a id="_idIndexMarker304"/> involves the use of advanced sensors such <a id="_idIndexMarker305"/>as <strong class="bold">time-of-flight</strong> (<strong class="bold">ToF</strong>) or <strong class="bold">light detection and ranging</strong> (<strong class="bold">LIDAR</strong>) sensors to capture the depth information of the<a id="_idIndexMarker306"/> surrounding environment. This method allows for more accurate placement and occlusion of virtual objects, where digital objects can correctly appear behind real-world objects based on depth information.<p class="list-inset">An example of this is Apple’s <em class="italic">ARKit 4.0</em> platform, which incorporates the <strong class="bold">Depth API</strong> that<a id="_idIndexMarker307"/> leverages the LIDAR scanner available on some iPad and iPhone models. Depth API enables more realistic AR experiences. The <em class="italic">Complete Anatomy</em> app uses ARKit’s depth sensing to place a detailed 3D human anatomy model into the real world, allowing the user to explore and interact with it as if it were physically present. Because of depth sensing, this model won’t accidentally appear halfway inside your sofa but will stand correctly beside it.</p></li>
<li><strong class="bold">Machine learning and AI</strong>: Recent<a id="_idIndexMarker308"/> advances in AI and machine learning have opened new possibilities for markerless AR. Machine learning models can be trained to recognize different types of environments and objects, providing context for more intelligent and interactive AR experiences.<p class="list-inset">An example of this is Google’s <em class="italic">ARCore</em> platform, which uses machine learning to recognize and augment specific objects or types of objects. Google’s <em class="italic">AR Animals</em> feature uses ARCore to let the user search for an animal on Google and then view a 3D model of the animal in their space via AR.</p></li>
</ul>
<p>Markerless AR offers immense possibilities for creating immersive and interactive AR experiences. Whether used in gaming, interior design, education, or a myriad of other applications, it has the potential to revolutionize how we interact with the digital world.</p>
<h2 id="_idParaDest-81">Understanding AR input types for interaction</h2>
<p>Stepping into the<a id="_idIndexMarker309"/> realm of AR, one swiftly realizes that it’s not just about the mesmerizing blend of physical and virtual realities that one can see. It’s equally about how one can interact with these layered digital augmentations, a dimension defined by AR inputs. These inputs — modes by which the user interacts with the AR content — serve as a linchpin that shapes the overall AR experience.</p>
<p>As we venture further into this discussion, let’s shine a spotlight on various AR input types and how they breathe life into real-world applications:</p>
<ul>
<li><strong class="bold">Touch input</strong>: Touch input<a id="_idIndexMarker310"/> is a fundamental AR interaction. Simply put, the user can engage with the digital overlay through touch gestures on their AR device screen, be it a smartphone or a tablet. Touch input in AR includes not just tapping, but also other gestures such as swiping, pinching, and dragging. The specific gestures that can be used will depend on how the AR application is programmed. For example, a pinch gesture might be used to zoom in or out on an AR object, a swipe might rotate the object, and a drag could move the object around in the AR scene. The goal is to make the interaction with the AR elements as intuitive and natural as possible. <em class="italic">Snapchat lenses</em> provide a classic example of touch inputs at work. The user can animate the AR filters or induce changes by merely tapping different screen areas.</li>
<li><strong class="bold">Device motion</strong>: Device motion<a id="_idIndexMarker311"/> is another pivotal AR input. By harnessing the data from onboard accelerometers, gyroscopes, and magnetometers, AR applications can interpret the orientation and movement of the device as an input. This input type proves particularly useful for AR experiences that involve maneuvering through an environment or controlling virtual elements. A case in point is the game Pokémon Go, where the player can simulate the act of <em class="italic">throwing</em> Pokéballs by swinging their device.</li>
<li><strong class="bold">Voice commands</strong>: Voice <a id="_idIndexMarker312"/>commands infuse AR applications with hands-free and accessibility-friendly interaction. <em class="italic">Google Glass</em>, an AR eyewear device, employs voice commands as one of its core input methods. The user can simply say <em class="italic">Okay, Glass</em> followed by a command such as <em class="italic">get directions</em> or <em class="italic">take a picture</em> to interact with the device.</li>
<li><strong class="bold">Eye tracking</strong>: Eye tracking<a id="_idIndexMarker313"/> is typically employed in advanced AR glasses. By tracking the user’s eye movements, these systems allow the user to interact with AR content just by looking at it. <em class="italic">North Focals</em> AR glasses are a good example of this technology in action. The user can steer a small, virtual cursor just by moving their eyes.</li>
<li><strong class="bold">Hand tracking</strong> and <strong class="bold">gesture recognition</strong>: Hand tracking and gesture recognition <a id="_idIndexMarker314"/>can be used in highly immersive AR systems to interpret and track hand movements, enabling the user to touch and interact with virtual objects directly. Microsoft’s <em class="italic">HoloLens 2</em> is an example of this and allows the user to manipulate holograms with their hands, pinch to resize them, or tap them for interaction.</li>
<li><strong class="bold">Physical controllers</strong>: Physical controllers can range from handheld devices to wearable <a id="_idIndexMarker315"/>tech such as gloves, which provide tactile feedback and precise control in specific AR applications. For instance, the <em class="italic">Magic Leap One</em> AR headset is accompanied by a handheld controller, immersing the user into the AR experience by allowing them to interact with <a id="_idIndexMarker316"/>virtual content in a more nuanced manner.</li>
</ul>
<p>As we have seen, AR inputs can significantly impact the immersive quotient of AR experiences. The choice of input methods hinges on the specific nature of the AR application. Therefore, understanding and implementing the most suitable input method can enhance an AR system’s realism and usability manifold.</p>
<p class="callout-heading">Note</p>
<p class="callout">Since this book primarily focuses on handheld mobile AR devices, to ensure that readers can easily reproduce all the projects presented within, we have chosen to restrict our scope to touch inputs.</p>
<p>In the next subsection, we will explore the AR toolkits available in Unity that can be utilized to implement the techniques we have just discussed.</p>
<h2 id="_idParaDest-82">Popular AR toolkits for Unity</h2>
<p>In this section, we <a id="_idIndexMarker317"/>delve into Unity’s suite of AR toolkits, providing an overview for both budding and experienced developers seeking to navigate through their AR development journey. Each toolkit offers unique capabilities that aid developers in crafting compelling AR experiences. Let’s look at some of these now:</p>
<ul>
<li><strong class="bold">Vuforia</strong>: Vuforia <a id="_idIndexMarker318"/>is a widely adopted AR platform, providing <a id="_idIndexMarker319"/>a blend of computer vision capabilities that accommodate both marker-based and markerless AR experiences. Its extensive feature set includes image tracking, object recognition, and target recognition, with wide-ranging platform support. An additional notable feature is Vuforia’s cloud recognition, allowing developers to house a multitude of target images remotely, further expanding the AR experience’s potential.</li>
<li><strong class="bold">ARKit</strong>: Within <a id="_idIndexMarker320"/>Apple’s playground, ARKit is the <a id="_idIndexMarker321"/>optimal toolkit for experiences targeting iOS devices. It is crafted specifically to complement the iOS ecosystem, offering developers a suite of advanced features such as world tracking, face tracking, and scene understanding. These elements collectively serve to enrich the user’s AR experience. ARKit <a id="_idIndexMarker322"/>predominantly <a id="_idIndexMarker323"/>uses Swift and Objective-C, Apple’s proprietary programming languages. However, when integrated with Unity through the AR Foundation package, developers can leverage C#, enabling a more accessible and familiar programming environment.</li>
<li><strong class="bold">ARCore</strong>: Google’s <a id="_idIndexMarker324"/>ARCore is<a id="_idIndexMarker325"/> the Android equivalent of ARKit, tailored for the world’s most popular mobile operating system. ARCore equips developers with features such as environmental understanding, motion tracking, and light estimation, all of which are essential elements for crafting realistic AR experiences. Primarily, ARCore uses Java for native development. But, similar to ARKit’s integration, ARCore can be incorporated into Unity projects via the AR Foundation package, allowing developers to use C#.</li>
<li><strong class="bold">AR Foundation</strong>: AR Foundation<a id="_idIndexMarker326"/> stands as Unity’s <a id="_idIndexMarker327"/>high-level API package for constructing AR applications, unifying ARKit and ARCore’s capabilities. This ingenious package enables a single, streamlined workflow for creating cross-platform AR experiences, eradicating the need to write separate code bases for iOS and Android. It’s akin to owning a universal cookbook rather than individual recipe books for each cuisine. With AR Foundation, developers can leverage C#, a widely used, versatile programming language, making the process of crafting AR applications more efficient and intuitive.<p class="list-inset">If you were to use ARCore and ARKit separately, you would need to write separate sets of code for each platform. You would need to use ARCore to build your AR app on Android, and then rewrite the code using ARKit to make it work on iOS.</p><p class="list-inset">However, by using AR Foundation to develop AR apps, you only need to write a single set of code that works on both iOS and Android. AR Foundation provides a unified API that is compatible with both ARCore and ARKit, meaning you don’t need to write different code for each platform. It simplifies the development process and saves a lot of time and effort.</p></li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">In this chapter, we’ll focus on creating AR applications using AR Foundation.</p>
<p>However, it’s important to remember that the use of ARCore or ARKit could offer unique advantages in certain scenarios, thanks to platform-specific features exclusive to each. For example, ARKit <a id="_idIndexMarker328"/>brings to the table LIDAR support from <em class="italic">version 3.5</em> onward. This feature utilizes the LIDAR scanner integrated into select iPhone and iPad models, offering refined scene understanding and precise depth estimation. Another ARKit exclusive is the <em class="italic">Motion Capture</em> feature from <em class="italic">ARKit 3</em> onward, enabling developers to record human movement and apply it to a 3D character model, effectively transforming the device into a motion capture studio.</p>
<p>On the other hand, ARCore <a id="_idIndexMarker329"/>also has a unique set of features for Android devices. One such feature is the Depth API, which generates depth maps using a single RGB camera. While ARKit also has depth sensing capabilities, ARCore’s Depth API can function on a broader range of devices, even those without a dedicated depth sensor. Another distinctive feature of ARCore is <em class="italic">Augmented Images</em>, which allows the application to track and augment images at fixed locations, offering the potential for interaction with posters, murals, and similar items.</p>
<p class="callout-heading">Important note</p>
<p class="callout">The examples mentioned earlier in this section may not be applicable when you are reading this book. Since ARKit, ARCore, and AR Foundation are constantly evolving, it is important to consult the latest documentation (<a href="https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@5.0/manual/index.html">https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@5.0/manual/index.html</a>) for the most up-to-date and accurate information.</p>
<p>Now, it is finally time to begin our first Unity AR project.</p>
<h1 id="_idParaDest-83">Setting up an AR project in Unity using AR Foundation</h1>
<p>In this section, you <a id="_idIndexMarker330"/>will learn how to set up a simple AR project in Unity using AR Foundation. You will learn how you can place simple objects such as a cube, add plane detection functionalities, and implement touch inputs and anchors into your AR scenes.</p>
<p>Before using AR Foundation in our first AR application, however, we must first understand the architecture of this package.</p>
<h2 id="_idParaDest-84">Understanding AR Foundation’s architecture</h2>
<p>In this<a id="_idIndexMarker331"/> section, we’ll delve into the exciting world of Unity’s AR Foundation, a package that empowers you to create AR experiences across various platforms. Whether you aim to create applications for Android, Apple, or HoloLens, AR Foundation simplifies the process remarkably. Its extensive capabilities range from plane detection, image and object tracking, and face and body tracking, to point clouds and more. <em class="italic">Figure 4</em><em class="italic">.1</em> breaks down the architecture of AR Foundation into a hierarchy of components that seamlessly work together to offer a consistent AR experience across different devices.</p>
<div><div><img alt="Figure 4.1 – AR Foundation’s architecture" src="img/B20869_04_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – AR Foundation’s architecture</p>
<p>The <strong class="bold">AR App</strong> represents<a id="_idIndexMarker332"/> the application where developers craft their AR experiences. It is directly linked with the <strong class="bold">Managers</strong>, which serve as the primary interfaces for specific AR functionalities.</p>
<p>The <strong class="bold">AR PlaneManager</strong> is<a id="_idIndexMarker333"/> crucial for detecting and tracking real-world planes, such as floors or walls. It provides a consistent means to engage with the physical environment around a user, feeding the app with data about surfaces in the user’s surroundings.</p>
<p>Similarly, the <strong class="bold">ARRaycast Manager</strong> plays an<a id="_idIndexMarker334"/> essential role in understanding user interactions within this AR space. It casts rays into the AR scene, determining where these rays intersect with real-world surfaces. This is particularly vital when users intend to place virtual objects on these surfaces or wish to interact with virtual elements in relation to the real world.</p>
<p>Delving<a id="_idIndexMarker335"/> deeper, these managers interact with <strong class="bold">Subsystems</strong>, abstract layers that communicate with the actual platform-specific modules. The <strong class="bold">XRPlane Subsystem</strong> standardizes<a id="_idIndexMarker336"/> data related to plane detection, ensuring that plane-related events and data are uniform, irrespective of whether it’s ARKit or ARCore doing the underlying work. The <strong class="bold">XRRaycast Subsystem</strong> offers <a id="_idIndexMarker337"/>a consistent interface for raycasting, abstracting the nuances of each platform’s approach.</p>
<p>Lastly, the <strong class="bold">Providers</strong> represent <a id="_idIndexMarker338"/>the platform-specific SDKs that power AR on each device type. ARKit SDK is Apple’s contribution for its iOS devices, with specialized subsystems for plane detection and raycasting. On the other side, ARCore SDK is Google’s solution for Android, mirroring the functionalities offered by ARKit but tailored for the Android ecosystem.</p>
<p>As you can see, AR Foundation offers XR developers a unified framework by allowing them to focus on their application’s AR experience without worrying about platform-specific intricacies. Through its layered architecture, it ensures that applications remain consistent and high-quality, whether deployed on an iPhone or an Android device.</p>
<p>With this understanding, it is finally time to create our first AR project in Unity.</p>
<h2 id="_idParaDest-85">Creating an AR project with Unity’s AR template</h2>
<p>Creating <a id="_idIndexMarker339"/>an AR project in Unity is quite straightforward. Follow these steps to get started:</p>
<ol>
<li>Depending on your target AR platform — Android or iOS — you’ll need to navigate first to the <code>Installs</code> folder in the Unity Hub, click on the <strong class="bold">Settings</strong> icon adjacent to your version, select <strong class="bold">Add Modules</strong>, and install the appropriate build support.<p class="list-inset">To show your project in the AR Unity template, you’d follow the subsequent steps.</p></li>
<li>In the <a id="_idIndexMarker340"/>Unity Hub’s project window, click on the <strong class="bold">New project</strong> button in the top-right corner.</li>
<li>Choose the <strong class="bold">AR</strong> template and give your project a unique name of your choice, as shown in <em class="italic">Figure 4</em><em class="italic">.2</em>.</li>
</ol>
<div><div><img alt="Figure 4.2 – ﻿The AR template available in the Unity Hub" src="img/B20869_04_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 – The AR template available in the Unity Hub</p>
<p class="callout-heading">Why choose the AR template?</p>
<p class="callout">In Unity’s world, the AR template is like a bountiful garden, already seeded with pre-installed packages such as AR foundation, ARKit Face Tracking, ARKit XR-, ARCore XR-, Magic Leap XR-, and the OpenXR plugins, not to mention a sample scene. However, you might observe that this lush garden contains plants that aren’t necessary for your target landscape: Android or iOS. To maintain a cleaner garden, you can selectively pick the seeds you need – the OpenXR plugin, AR Foundation, Input System and the ARCore XR plugin (for Android) or ARKit XR plugin (for iOS) – from Unity’s own nursery, the package manager, and plant them in a regular 3D scene.</p>
<ol>
<li value="4">Finally, click on <strong class="bold">Create Project</strong>.</li>
</ol>
<p>Once the scene<a id="_idIndexMarker341"/> loads in the Unity Editor, it should look like what is shown in <em class="italic">Figure 4</em><em class="italic">.3</em>.</p>
<div><div><img alt="Figure 4.3 – ﻿Unity’s AR SampleScene" src="img/B20869_04_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3 – Unity’s AR SampleScene</p>
<p class="callout-heading">Important note</p>
<p class="callout">If you don’t see the scene shown in <em class="italic">Figure 4</em><em class="italic">.3</em> in the Unity Editor, you can open it manually by selecting <strong class="bold">Assets</strong> | <strong class="bold">ExampleAssets</strong> | <strong class="bold">SampleScene</strong>.</p>
<p>The newly opend scene contains <strong class="bold">Directional Light</strong>, <strong class="bold">AR Session Origin</strong>, and <strong class="bold">AR Session</strong> GameObjects. If you wish to bring these objects into a new scene, you can conjure an <strong class="bold">AR Session</strong> GameObject by right-clicking in the hierarchy and choosing <strong class="bold">XR</strong> | <strong class="bold">AR Session</strong>. The <strong class="bold">AR Session Origin</strong> GameObject can be summoned in a similar fashion by selecting <strong class="bold">XR</strong> | <strong class="bold">AR Session Origin</strong>. That’s the initial magic performed.</p>
<p class="callout-heading">Important note</p>
<p class="callout">At the time of this book’s publication, <strong class="bold">AR Session Origin</strong> and <strong class="bold">AR Session</strong> represent the default GameObjects integrated into the <em class="italic">AR Foundation</em>’s template. Yet, given the dynamic nature of <em class="italic">AR Foundation</em>, continuous updates and revisions are anticipated. There are indications that <strong class="bold">AR Session Origin</strong> is slated to evolve into <em class="italic">XR Origin </em>(<em class="italic">Mobile AR</em>) in upcoming versions.</p>
<p class="callout">Should you encounter such changes, don’t fret. The essence of this chapter remains intact and can be easily navigated. We recommend accessing our <em class="italic">GitHub</em> repository to clone the specific project versions we’ve worked with, ensuring compatibility with Unity. Often, transitioning from outdated GameObjects to their newer counterparts is seamless, requiring minimal, if any, adjustments to the existing logic.</p>
<p class="callout">Stay adaptable and remember: the core concepts and foundations presented here remain your guide, even as the tools evolve.</p>
<p>Before we start<a id="_idIndexMarker342"/> inspecting these GameObjects, we first need to adjust some project settings of our AR scene.</p>
<h2 id="_idParaDest-86">Changing the project settings</h2>
<p>To define which<a id="_idIndexMarker343"/> providers we want to target with our AR scene, go to <strong class="bold">XR Plug-in Management</strong> under <strong class="bold">Edit</strong> | <strong class="bold">Project Settings</strong>. Depending on the platform we’re targeting, be it an Android or an iOS device, we activate the corresponding ARCore or ARKit checkbox. If you seek to embrace both platforms, enable both checkboxes. This will install the provider packages into your project.</p>
<p class="callout-heading">Important note</p>
<p class="callout">If you don’t see Android, iOS, or both tabs, it’s likely that the <em class="italic">Android</em> or <em class="italic">iOS Build Support</em> module hasn’t been integrated into your Editor. Let’s see how we can fix this problem.</p>
<p class="callout"><em class="italic">Figure 4</em><em class="italic">.4</em> shows the installed Unity Editor with the Build Support modules marked in the Unity Hub.</p>
<div><div><img alt="Figure 4.4 – The installed Unity Editor with the Build Support modules marked" src="img/B20869_04_04.jpg"/>
</div>
</div>
<p class="callout">Figure 4.4 – The installed Unity Editor with the Build Support modules marked</p>
<p class="callout">The displayed image indicates an absence of the <em class="italic">iOS Build Support</em> module in the Unity Editor. To adjust this, go to the <strong class="bold">Installs</strong> tab in the Unity Hub. Click the <strong class="bold">Settings</strong> icon in the top-right corner of your Unity Editor installation. Then, select <strong class="bold">Add Modules</strong>, enable the checkboxes for <strong class="bold">iOS Build Support</strong> or <strong class="bold">Android Build Support</strong> – or both, if you will – and install them. Once the installation is finished, go back to your project’s <strong class="bold">XR Plug-in </strong><strong class="bold">Management</strong> window.</p>
<p class="callout">Now, you should find the <strong class="bold">ARCore</strong> and <strong class="bold">ARKit</strong> tabs under <strong class="bold">XR </strong><strong class="bold">Plug-in Management</strong>.</p>
<p>Now, select<a id="_idIndexMarker344"/> the <strong class="bold">ARCore</strong> tab to explore fields related to the configuration and settings of the ARCore packages in your Unity project. <em class="italic">Figure 4</em><em class="italic">.5</em> shows the <strong class="bold">ARCore</strong> tab with its default settings.</p>
<div><div><img alt="Figure 4.5 – The ARCore tab with its default settings" src="img/B20869_04_05.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.5 – The ARCore tab with its default settings</p>
<p>Let’s go through the function of each field:</p>
<ul>
<li><strong class="bold">Requirement</strong>: This parameter dictates the necessity of ARCore for your application. If set to <strong class="bold">Required</strong>, it establishes ARCore as an integral component for your target device. Conversely, when marked as <strong class="bold">Optional</strong>, ARCore is positioned as an additional feature, supplementing the application’s capabilities when present but not essential for the application’s core functionality.</li>
<li><strong class="bold">Depth</strong>: This is<a id="_idIndexMarker345"/> the tool that gives your device the power of depth perception within its environment. Useful for occlusion and other advanced AR effects, it lets you toggle depth estimation in ARCore. If you set <strong class="bold">Depth</strong> to <strong class="bold">Required</strong>, your AR application will need to have depth estimation capabilities to function. On the other hand, selecting <strong class="bold">Optional</strong> means your AR application is capable of utilizing depth estimation features if the device supports it, but it doesn’t hamper the basic functioning if such features are absent.</li>
<li><strong class="bold">Ignore Gradle Version</strong> checkbox: This is your control over the Gradle build system, an essential tool for building Android apps. When unchecked, Unity adheres to the specified Gradle version for your project. However, if checked, Unity gains autonomy, ignoring the specified version and choosing to operate on its default version.</li>
</ul>
<p>While the default settings – having both the <strong class="bold">Requirement</strong> and <strong class="bold">Depth</strong> fields set to <strong class="bold">Required</strong> and the <strong class="bold">Ignore Gradle Version</strong> checkbox unchecked – suit most projects, there are exceptions. For instance, if you’re creating an AR application designed for a wide variety of devices with varying capabilities, or if your application’s main functionalities do not heavily rely on ARCore’s depth perception, you might want to set the <strong class="bold">Depth</strong> field to <strong class="bold">Optional</strong>. Furthermore, if you’re working on a project that requires a specific Gradle version or has conflicts with newer versions, checking the <strong class="bold">Ignore Gradle Version</strong> box would be necessary. However, for this chapter, we can keep the default settings.</p>
<p>Next, we turn our attention to the <strong class="bold">ARKit</strong> tab. Just like its ARCore counterpart, these fields are connected to the configuration and settings of the ARKit packages in your Unity project. <em class="italic">Figure 4</em><em class="italic">.6</em> shows the <strong class="bold">ARKit</strong> tab with its default settings.</p>
<div><div><img alt="Figure 4.6 – The ARKit tab with its default settings" src="img/B20869_04_06.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.6 – The ARKit tab with its default settings</p>
<p>Let’s decode the roles of all the fields shown in <em class="italic">Figure 4</em><em class="italic">.6</em>:</p>
<ul>
<li><strong class="bold">Requirement</strong>: Acting in a manner similar to ARCore’s <strong class="bold">Requirement</strong> field, this indicator announces whether ARKit is deemed vital for your project.</li>
<li><strong class="bold">Face Tracking</strong>: Acting as a switch, this checkbox enables or disables the face tracking functionality in ARKit. When activated, it allows tracking and recognizing a range of facial features and expressions using the device’s front-facing camera.</li>
</ul>
<p>For the moment, we’ll <a id="_idIndexMarker346"/>abide by the default settings – the <strong class="bold">Requirement</strong> field standing firm at <strong class="bold">Required</strong> and the <strong class="bold">Face Tracking</strong> checkbox remaining in the off position, as it’s not currently needed.</p>
<p>In the forthcoming sections, we’re going to delve into the GameObjects that accompany the <strong class="bold">AR</strong> template. We’ll dive into their roles and demystify why they hold such significance. Let’s begin with the AR Session GameObject.</p>
<h2 id="_idParaDest-87">Understanding the AR Session GameObject</h2>
<p>The <strong class="bold">AR Session</strong> GameObject <a id="_idIndexMarker347"/>manages the lifecycle of your AR application. To understand it, click on it in the scene hierarchy and navigate to the inspector window. As shown in <em class="italic">Figure 4</em><em class="italic">.7</em>, the <strong class="bold">AR Session</strong> GameObject contains <a id="_idIndexMarker348"/>the <strong class="bold">AR Session</strong> and the <strong class="bold">AR Input </strong><strong class="bold">Manager</strong> components.</p>
<div><div><img alt="Figure 4.7 – The Inspector window when the AR Session GameObject is selected" src="img/B20869_04_07.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.7 – The Inspector window when the AR Session GameObject is selected</p>
<p>Let’s understand<a id="_idIndexMarker349"/> each of the fields shown in <em class="italic">Figure 4</em><em class="italic">.7</em>:</p>
<ul>
<li><strong class="bold">Attempt Update</strong> checkbox: This checkbox, when marked, is your instruction to the AR session to be vigilant about keeping the device’s pose and tracking state up-to-date with every frame and every tick of the clock. Checking the box commands the AR session to fetch fresh tracking data diligently, ensuring that even when the device’s tracking wavers temporarily, the AR elements maintain their alignment with the real world. Imagine an AR hoop shooting game. In this game, you see a virtual hoop overlaid onto your real-world environment through your device’s screen. The objective is to shoot virtual balls into this hoop from various angles and distances. For this to function well, it’s vital that the tracking data of your device is continuously updated. That’s where the <strong class="bold">Attempt Update</strong> checkbox comes in.<p class="list-inset">Activating <strong class="bold">Attempt Update</strong> commands the game to consistently refresh the tracking state, securing the harmony of the virtual objects with the real-world environment, notwithstanding transient tracking losses.</p></li>
<li><strong class="bold">Match Frame Rate</strong> checkbox: This, when marked, means that you’re asking the AR session <a id="_idIndexMarker350"/>to walk in step with the device’s camera frame rate. The AR session adjusts its rhythm to mirror the camera’s, culminating in a harmonious visual experience. Consider an AR app that lets the user envision virtual furniture within their actual space.<p class="list-inset">A ticked <strong class="bold">Match Frame Rate</strong> checkbox ensures the AR session’s frame rate keeps time with the camera’s, curbing visual discord between the actual space and the virtual furniture, thereby enabling a precise assessment of aesthetics.</p></li>
<li><strong class="bold">Tracking Mode</strong> drop-down menu: This is the key to defining the tracking quality of your AR session. The selected mode determines the AR system’s awareness of, and response to, the device’s motion and position in the real world. Picture an AR navigation app that superimposes virtual arrows over the physical world to guide the user.<p class="list-inset">In this navigation app, opting for <strong class="bold">Rotation and Position Tracking</strong> imparts full tracking capabilities, ensuring the virtual arrows trace the contours of the real world accurately, guiding the user along their path with precision.</p></li>
</ul>
<p>For our current exploration, we’ll stick with the default settings – both the <strong class="bold">Attempt Update</strong> and <strong class="bold">Match Frame Rate</strong> checkboxes are ticked and <strong class="bold">Tracking Mode</strong> is set to <strong class="bold">Position And Rotation</strong>. In the future, your choice of these settings should reflect your project’s goals and the intended user experience. To discover the best blend for your needs, we encourage you to experiment with different settings and configurations.</p>
<p>The second component under the AR Session umbrella is the <strong class="bold">AR Input </strong><strong class="bold">Manager</strong> component.</p>
<p>The AR Input Manager <a id="_idIndexMarker351"/>component translates the user’s interactions with the AR scene into meaningful input. It perceives and processes a variety of user inputs such as taps, touches, and gestures. It’s the invisible hand enabling interactivity in your AR application, allowing placement of objects or interactions with virtual elements. For instance, imagine an AR game where the user neutralizes virtual targets with a tap. The <strong class="bold">AR Input Manager script</strong> reads<a id="_idIndexMarker352"/> the tap on the screen and furnishes the requisite information to unleash the shooting action in the game.</p>
<p>Now, our focus shifts to the AR Session Origin GameObject.</p>
<h2 id="_idParaDest-88">Exploring the AR Session Origin GameObject</h2>
<p>The <strong class="bold">AR Session Origin</strong> GameObject<a id="_idIndexMarker353"/> is the primary element controlling the synchronization between the physical world and virtual objects projected in AR. It comprises various components, such as AR Session Origin, AR Plane Manager, AR Anchor Manager, AR Raycast Manager, and Anchor Creator, as we can see in <em class="italic">Figure 4</em><em class="italic">.8</em>.</p>
<div><div><img alt="Figure 4.8 – The Inspector when AR Session Origin is selected" src="img/B20869_04_08.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.8 – The Inspector when AR Session Origin is selected</p>
<p>For the initiation phase of our AR development, we only require the AR Session Origin component. Hence, you may disable all other components for the time being.</p>
<p>Think of AR Session Origin as the pivot point of your AR experience. It provides a basis for anchoring virtual objects within the AR environment. Let’s visualize an AR app allowing the user to place virtual furniture within their home. Here, the AR Session Origin component arranges and alignes the virtual furniture correctly within the user’s physical space.</p>
<p>AR Session Origin <a id="_idIndexMarker354"/>ensures the alignment of the user’s environment with the position and orientation of the displayed AR scene. For instance, when a user places a virtual chair, this script maintains the chair’s positioning and orientation, no matter the user’s movement within the room. This is why this component is essential in creating an authentic AR experience.</p>
<p>For blending the virtual objects with the real-world footage, the AR session necessitates the presence of a camera. In the case of an existing main camera in your scene, you can safely remove it. AR Session Origin incorporates its own child <strong class="bold">AR Camera</strong>. This camera, devoid of any <a id="_idIndexMarker355"/>skybox for a monochromatic background, comes pre-equipped with necessary components, such as <strong class="bold">Camera</strong>, <strong class="bold">Tracked Pose Driver</strong>, <strong class="bold">AR Camera Manager</strong>, and <strong class="bold">AR Camera Background</strong>, as shown in <em class="italic">Figure 4</em><em class="italic">.9</em>.</p>
<div><div><img alt="Figure 4.9 – The Inspector when AR Session Origin’s child AR Camera is selected" src="img/B20869_04_09.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.9 – The Inspector when AR Session Origin’s child AR Camera is selected</p>
<p> Let’s now learn more about each of these:</p>
<ul>
<li>Like a real-world camera, the <strong class="bold">Camera</strong> component captures and displays a portion of the game scene to the player – essentially, what you see on your device’s screen when playing an AR game. It can be positioned and rotated to capture different views within the game scene.</li>
<li>The <strong class="bold">AR Camera Manager</strong> component <a id="_idIndexMarker356"/>controls the phone camera settings, such as <strong class="bold">Facing Direction</strong>, <strong class="bold">Light Estimation</strong>, and <strong class="bold">Auto Focus</strong>. <strong class="bold">Auto Focus</strong> makes the camera automatically adjust its focus to keep AR objects sharp, just like autofocus in a regular camera. <strong class="bold">Light Estimation</strong> allows you to choose <a id="_idIndexMarker357"/>how the AR system estimates real-world lighting conditions to make virtual objects look more realistic. Options range from not estimating lighting at all (<strong class="bold">None</strong>) to estimating various aspects of lighting, such as <strong class="bold">Ambient Light Intensity</strong> and <strong class="bold">Color</strong>, and the main light source’s <strong class="bold">Direction</strong> and <strong class="bold">Intensity</strong>. <strong class="bold">Facing Direction</strong> decides the direction in which the AR Camera is facing. The <strong class="bold">World</strong> setting is typically for the rear-facing camera (seeing the environment), and <strong class="bold">User</strong> is for the front-facing camera (selfie mode). <strong class="bold">None</strong> disables this setting. For now, we can leave these settings as they are.</li>
<li>The <strong class="bold">Tracked Pose Driver</strong> component <a id="_idIndexMarker358"/>takes information about the device’s physical position and orientation, collectively referred to as its pose, and uses that to set the position, rotation, and scale of the camera within the Unity game scene. This is what’s referred to as the camera’s <strong class="bold">Transform</strong>. This ensures that the AR scene remains in harmony with the phone’s movements. To facilitate this, the Tracked Pose Driver provides several configurable options. <strong class="bold">Device</strong> lets you select the type of device being tracked. <strong class="bold">Pose Source</strong> allows you to choose the part of the device that provides the tracking data. The <strong class="bold">Tracking Type</strong> drop-down menu indicates what type of movement (rotation, position, or both) is tracked.<p class="list-inset"><strong class="bold">Update Type</strong> dictates when the tracking information updates, whether during the regular update cycle, before rendering the next frame, or both. If the <strong class="bold">Use Relative Transform</strong> checkbox is ticked, the tracking data is based on the device’s position and rotation relative to its initial state.</p><p class="list-inset">Finally, the <strong class="bold">Use Pose Provider</strong> field can be enabled to utilize an external source for tracking data, which is potentially useful for specialized tracking systems. Together, these settings give <strong class="bold">Tracked Pose Driver</strong> the flexibility to handle a wide range of AR scenarios.</p></li>
<li>The <strong class="bold">AR Camera Background</strong> component <a id="_idIndexMarker359"/>controls how the real-world view captured by your device’s camera is displayed in your AR scene. The <strong class="bold">Use Custom Material</strong> checkbox allows you to apply a custom material, for example, to add special visual effects to the real-world view. If left unchecked, the real-world view is displayed as is, without any additional effects.</li>
</ul>
<p>The AR Session Origin GameObject<a id="_idIndexMarker360"/> performs the critical task of converting AR session space into Unity’s world space. Given the unique coordinate system employed in AR (referred to as AR session space), this conversion is vital for accurately positioning the GameObjects relative to the AR Camera.</p>
<p>With this information about the most important GameObjects of our AR scene in mind, it is time to place a simple object in our AR scene.</p>
<h2 id="_idParaDest-89">Placing a simple cube into the AR scene</h2>
<p>To begin testing our <a id="_idIndexMarker361"/>AR functionality, we need a 3D object in our scene to visualize the AR effect. For this purpose, you can create a simple cube object. Right-click in your project hierarchy and choose <code>0</code>,<code>0</code>,<code>3</code>) and apply a rotation of (<code>30</code>,<code>0</code>,<code>0</code>), as shown in <em class="italic">Figure 4</em><em class="italic">.10</em>.</p>
<div><div><img alt="Figure 4.10 – The Transform values of the cube" src="img/B20869_04_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.10 – The Transform values of the cube</p>
<p>These specific coordinates and rotations are only examples and can be altered according to your requirements.</p>
<p>The key objective is to position the object somewhere in the direction of the AR Camera and at a distinct distance. This placement will help verify whether the AR system is correctly overlaying virtual content onto the real world, as it provides a benchmark for alignment with your physical surroundings. It also allows you to gauge the depth perception capabilities of your AR system.</p>
<p>To validate whether the cube will be correctly displayed in the AR view, you can either select the AR Camera in the hierarchy – this will cause a camera view to appear in the bottom-right corner <a id="_idTextAnchor013"/>of the <strong class="bold">Scene</strong> window – or switch to the <strong class="bold">Game</strong> window, as shown in <em class="italic">Figure 4</em><em class="italic">.11</em>.</p>
<div><div><img alt="Figure 4.11 – The Unity project showing how to validate whether the cube will be correctly displayed in the AR view" src="img/B20869_04_11.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.11 – The Unity project showing how to validate whether the cube will be correctly displayed in the AR view</p>
<p>In an AR application, AR Session Origin<a id="_idIndexMarker362"/> typically corresponds to the starting location of your device’s camera when the AR experience begins. However, positioning a virtual object directly at <strong class="bold">Session Origin</strong> can indeed cause some issues. Here are a few potential problems that could arise:</p>
<ul>
<li><strong class="bold">Inaccurate scaling</strong>: If you were to place a virtual cube directly at <strong class="bold">Session Origin</strong>, it could appear disproportionately large or small compared to real-world objects. It’s like holding a real-life object very close to your eyes – even a small object can appear large.</li>
<li><strong class="bold">Interference with tracking</strong>: <strong class="bold">Session Origin</strong> serves as a reference point for tracking the device’s movements in the real world. If you place a virtual object at this exact point, the AR system might struggle to track both the object and the device’s movements accurately.</li>
<li><strong class="bold">Unpleasant user experience</strong>: If a virtual object is positioned directly at <strong class="bold">Session Origin</strong>, it might appear too close to the user or even obstruct the user’s view of the rest of the AR scene, creating a less enjoyable experience.</li>
</ul>
<p class="callout-heading">Tip</p>
<p class="callout">To avoid these issues, it’s generally recommended to position virtual objects at a reasonable distance from <strong class="bold">Session Origin</strong>. The precise location would depend on the specific requirements of your AR experience. For instance, if you’re creating an AR furniture placement app, you might position virtual furniture based on the detected real-world surfaces, such as floors and tables. By doing so, you ensure that the virtual furniture appears at an appropriate scale, doesn’t interfere with tracking, and provides a pleasant user experience by appearing in the correct place in the real world.</p>
<p>After placing <a id="_idIndexMarker363"/>your cube, you can proceed with testing the scene by following the instructions outlined in the <em class="italic">Deploying AR experiences onto mobile devices</em> section of this chapter. As it stands, your AR application is simple, featuring a static object positioned in front of the user. However, we’re aiming for a more interactive experience, so we’ll delve into the anchor and plane detection functionalities in the upcoming section.</p>
<h2 id="_idParaDest-90">Implementing plane detection in AR Foundation</h2>
<p>In <a id="_idIndexMarker364"/>this section, we will make use of AR Foundation’s <strong class="bold">plane detection</strong> capabilities. AR Foundation’s plane detection forms the crux of numerous practical applications by providing a cornerstone understanding of real-world environments. Its ability to identify and comprehend flat surfaces enables digital objects to interact meaningfully and realistically with physical spaces, enhancing the overall augmented reality experience.</p>
<p>In the retail and e-commerce sectors, plane detection is fundamental to accurate product visualization. It ensures a virtual couch is placed right in front of your living room wall and not in a random position in your room.</p>
<p>This technology also underpins the immersive experiences in AR gaming. By identifying real-world surfaces, games such as Pokémon Go can accurately place virtual elements, enhancing the thrill of the chase. Other games use this technology to create worlds on your coffee table, maintaining the illusion by ensuring that virtual objects interact convincingly with the physical planes.</p>
<p>To make use of plane detection, enable the <strong class="bold">AR Plane Manager</strong> component you disabled in the <em class="italic">Exploring the AR Session Origin GameObject</em> section. Then, you can also delete the cube as we don’t need it anymore. Now, select <strong class="bold">AR Session Origin</strong> and inspect the <strong class="bold">AR Plane Manager</strong> component in the hierarchy.</p>
<p>The <strong class="bold">AR Plane Manager</strong> component in Unity’s AR Foundation is responsible for detecting and<a id="_idIndexMarker365"/> tracking real-world surfaces, or planes, via your device’s camera. It utilizes AR technology to understand the physical environment, creating digital representations that can interact with virtual objects.</p>
<p>Here’s a breakdown of what <strong class="bold">AR Plane </strong><strong class="bold">Manager</strong> does:</p>
<ul>
<li><strong class="bold">Plane Prefab</strong>: This is essentially the digital template or blueprint used to represent detected planes. When <strong class="bold">AR Plane Manager</strong> identifies a flat surface (a plane) in the real world, it creates a <strong class="bold">Plane Prefab</strong> instance in the AR space. Each prefab instance corresponds to a specific real-world plane, providing a surface on which you can place or interact with virtual objects.<p class="list-inset">Let’s say, for example, you’re creating an AR game where virtual cats roam around. The <strong class="bold">Plane Prefab</strong> instance could be a flat surface that the cats can walk on. If your living room floor is detected as a plane, a <strong class="bold">Plane Prefab</strong> instance will be created, providing a surface for your cats to frolic on.</p></li>
<li><strong class="bold">Detection Mode</strong>: This setting determines the orientation of planes that <strong class="bold">AR Plane Manager</strong> should detect. Here are some components within <strong class="bold">Detection Mode</strong> that would be useful for you:<ul><li><strong class="bold">Everything</strong>: This setting combines the detection of horizontal and vertical planes by <strong class="bold">AR Plane Manager</strong>. If you’re creating an AR furniture placement app, you might use this setting. The app could then detect both the floor (a horizontal plane, for placing a chair) and the walls (vertical planes, for hanging pictures).</li><li><strong class="bold">Horizontal</strong>: <strong class="bold">AR Plane Manager</strong> will detect only flat surfaces oriented horizontally, such as floors and tabletops. For instance, if you’re creating an AR game where characters run around on the floor, you’d likely use this mode.</li><li><strong class="bold">Vertical</strong>: Conversely, this setting will detect only vertically oriented surfaces, such as walls and doors. This might be used in an AR interior design app that allows the user to place virtual paintings or wallpapers on their walls.</li></ul></li>
</ul>
<p>As we <a id="_idIndexMarker366"/>want to detect horizontal and vertical planes, the drop-down selection can remain at <strong class="bold">Everything</strong>.</p>
<p>Next, you will notice that the AR template already has an <strong class="bold">ARPlane</strong> prefab assigned. This prefab is what you will see on top of the detected surfaces. As this prefab is already configured, you can use the <strong class="bold">ARPlane </strong>prefab. However, you can also create your custom-made plane prefab. This can be done by right-clicking in the hierarchy window and selecting <strong class="bold">XR</strong> | <strong class="bold">AR </strong><strong class="bold">Default Plane.</strong></p>
<p><strong class="bold">AR Default Plane GameObject</strong> in Unity’s AR Foundation is composed of various components, each of which performs a specific function within the overall system. Here’s a breakdown of what each of these components does:</p>
<ul>
<li><strong class="bold">AR Plane</strong>: This script controls the essential behavior of a plane in the AR environment. Two notable fields in this script are as follows:<ul><li><strong class="bold">Destroy on Removal</strong> checkbox: If checked, when a detected plane is removed or no longer needed, the corresponding <strong class="bold">Plane</strong> GameObject is destroyed or removed from the Unity scene to free up resources. Imagine this to be like cleaning up toys when you’re done playing with them to make space for other activities.</li><li><strong class="bold">Vertex Changed Threshold</strong>: This field controls how sensitive the plane is to changes in its detected shape. If the real-world surface changes, the plane’s mesh vertices will need to be updated. This threshold determines how much change is needed before an update occurs. It’s like deciding when to adjust a puzzle because the pieces have moved – too often can be unnecessary, yet too seldom and the picture may not make sense.</li></ul></li>
<li><strong class="bold">AR Plane Mesh Visualizer</strong>: This script is responsible for rendering the visual representation of a detected plane. Imagine you’re using an AR app on your phone to preview a piece of furniture in your room before buying it. When you point your phone’s camera at the floor, the app detects a flat surface – a plane. <strong class="bold">AR Plane Mesh Visualizer</strong> then creates a visible grid or pattern overlay on your phone screen that represents this detected plane.</li>
<li><strong class="bold">Tracking State Visibility </strong>dropdown (<strong class="bold">None</strong>, <strong class="bold">Limited</strong>, <strong class="bold">Tracking</strong>): This setting <a id="_idIndexMarker367"/>determines when the plane should be visible based on the tracking quality. For instance, you might only want to see the plane when it’s fully tracked (good quality), or also when tracking is limited (lower quality). It’s like deciding when to display a picture – if it’s blurry, you might prefer to wait for it to load until it’s clear.</li>
<li><strong class="bold">Hide Subsumed</strong> checkbox: If checked, when one detected plane is subsumed by another (i.e., entirely overlapped by a larger plane), the smaller plane is hidden. This can make the AR scene less cluttered and more efficient. Think of this to be like removing smaller rugs when you put down a larger one that covers them completely.</li>
<li><strong class="bold">Mesh Collider</strong>: This component allows virtual objects to physically interact with the plane as if it were a solid surface. For example, if you drop a virtual ball onto the plane, <strong class="bold">Mesh Collider</strong> ensures that the ball bounces back rather than falling through the surface.</li>
<li><strong class="bold">Mesh Filter</strong> and <strong class="bold">Mesh Renderer</strong>: These work together to create and display the 3D mesh for the plane. <strong class="bold">Mesh Filter</strong> generates the shape of the plane (the mesh), while <strong class="bold">Mesh Renderer</strong> applies materials and textures and renders it on the screen. They’re like a sculptor and a painter respectively, working together to create a lifelike statue.</li>
<li><strong class="bold">Line Renderer</strong>: This component is often used to draw the border around the detected plane, helping the user to see the extent of the plane. It’s like drawing a chalk outline around an area to indicate its boundaries.</li>
</ul>
<p>All these <a id="_idIndexMarker368"/>components work in harmony to detect, represent, and interact with real-world surfaces in your AR applications, ensuring a seamless blend of virtual objects with the physical environment. You can keep the default settings for our scene.</p>
<p>After creating <code>Prefabs</code>. Next, drag <code>Prefabs</code> folder. Doing so will automatically create a prefab out of it. With <strong class="bold">AR Session Origin</strong> now selected, drag the <strong class="bold">AR Default Plane</strong> prefab to the appropriate <strong class="bold">Plane Prefab</strong> field of the <strong class="bold">AR Plane Manager</strong> component within the Inspector, as illustrated in <em class="italic">Figure 4</em><em class="italic">.12</em>.</p>
<div><div><img alt="Figure 4.12 – The AR Default Plane prefab assigned to the appropriate Plane Prefab field of the AR Plane Manager component" src="img/B20869_04_12.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.12 – The AR Default Plane prefab assigned to the appropriate Plane Prefab field of the AR Plane Manager component</p>
<p>Now, your application would be able to detect a plane in your environment and place the plane prefab of your choice on top of it.</p>
<p><em class="italic">Figure 4</em><em class="italic">.13</em> shows the default plane prefab of Unity’s AR template.</p>
<div><div><img alt="Figure 4.13 – The plane prefab of Unity’s AR template detecting the wooden floor of an apartment" src="img/B20869_04_13.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.13 – The plane prefab of Unity’s AR template detecting the wooden floor of an apartment</p>
<p>As we can see, with<a id="_idIndexMarker369"/> AR Foundation, it is straightforward to incorporate plane detection into a project and to project patterns onto detected surfaces. In the upcoming section, we will merge these concepts with the use of anchors and touch inputs. This combination will enable us to place an object onto a detected surface simply by tapping on the screen.</p>
<h2 id="_idParaDest-91">Implementing touch inputs and anchors</h2>
<p>To get <a id="_idIndexMarker370"/>started on working with touch inputs and anchors, enable the <strong class="bold">AR Anchor Manager</strong>, <strong class="bold">AR Raycast Manager</strong>, and <strong class="bold">Anchor Creator</strong> components in the Inspector when <strong class="bold">AR Session Origin</strong> is selected. These components are all we need to get the touch input and anchors to work. Let’s break each of them down:</p>
<ul>
<li><strong class="bold">AR Anchor Manager</strong> script: <strong class="bold">AR Anchor Manager</strong> operates as the custodian <a id="_idIndexMarker371"/>of object stability in your AR space. When you designate a position for a virtual object within your AR scene, <strong class="bold">Anchor Manager</strong> ensures the object remains tethered to that specific real-world location, irrespective of changes in device perspective. It is akin to an overseer, maintaining each virtual object in its correct placement relative to the real-world coordinates.</li>
<li><strong class="bold">AR Raycast Manager</strong> script: Acting<a id="_idIndexMarker372"/> as the tactile sense of your AR application, <strong class="bold">AR Raycast Manager</strong> emits invisible rays from your device into the scene. When these rays encounter a surface, they return data about its position and orientation. This process is key to interactions such as placing a virtual object on a real-world surface, as it enables your application to perceive and understand the topography of the environment.</li>
<li><strong class="bold">Anchor Creator</strong> script: This<a id="_idIndexMarker373"/> script functions as an efficient tool, facilitating the creation and placement of new anchors within your scene. Given that anchors are pivotal to securing virtual objects in a consistent real-world location, <strong class="bold">Anchor Creator</strong> simplifies the process of generating these anchor points. This can range from situating a new virtual object to immobilizing an object in transit.</li>
</ul>
<p>Even though all components have a <strong class="bold">Prefab</strong> field, we just need to assign a prefab to the <strong class="bold">AR Anchor Manager</strong> component. This prefab will be spawned when we tap on the screen.</p>
<p>When you tap<a id="_idIndexMarker374"/> on the screen, <strong class="bold">AR Raycast Manager</strong> determines where in the real world you’ve indicated by casting a ray from your screen touch point into the AR scene. If this ray hits a detected surface, <strong class="bold">AR Anchor Manager</strong> creates an anchor at this real-world location and ties the prefab to it. This process ensures the stable positioning of your virtual object in the AR scene.</p>
<p>The prefab attached to <strong class="bold">AR Anchor Manager</strong> essentially acts as a template for the object you want to instantiate (or create) in the AR environment whenever the screen is tapped. This is why it’s essential to assign a prefab to <strong class="bold">AR </strong><strong class="bold">Anchor Manager</strong>.</p>
<p>The other components do not require a prefab because they are not directly responsible for creating objects in your AR scene. <strong class="bold">AR Raycast Manager</strong> deals with detecting the real-world surfaces you are interacting with, and <strong class="bold">Anchor Creator</strong> facilitates the creation and placement of the anchors themselves. Neither of these tasks necessitates the creation of a new object from a prefab.</p>
<p>For demonstration purposes, we use a simple capsule primitive. Simply right-click in the hierarchy and select <code>0.2</code>,<code>0.2</code>,<code>0.2</code>). Now, create a new folder in the <code>Project</code> folder called <code>Prefabs</code> (right-click + <strong class="bold">Create</strong> | <strong class="bold">Folder</strong>) and drag the capsule into this folder. This will automatically create a prefab of the capsule that you can drag into the <strong class="bold">Anchor Prefab</strong> field of the <strong class="bold">AR Anchor </strong><strong class="bold">Manager</strong> component.</p>
<p>Now, your app should spawn the capsule on a touch inp<a id="_idTextAnchor014"/>ut. <em class="italic">Figure 4</em><em class="italic">.14</em> shows the deployed application.</p>
<div><div><img alt="Figure 4.14 – The deployed application" src="img/B20869_04_14.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.14 – The deployed application</p>
<p>The <a id="_idIndexMarker375"/>preceding image illustrates the instantiated capsule prefabs following screen taps. These capsules are positioned atop the detected surface, confirming the effective operation of the AR systems. Specifically, <strong class="bold">AR Raycast Manager</strong> accurately casts rays from the screen touch point onto the detected surface, and in response, <strong class="bold">AR Anchor Manager</strong> successfully creates anchors, anchoring the prefabs at the indicated locations. Everything should now be working perfectly, so it’s time to build the scene onto your device. Depending on whether you’re using an Android or iOS device, you can navigate to the relevant subsection in the following section.</p>
<h1 id="_idParaDest-92">Testing AR experiences directly in Unity</h1>
<p>As of <em class="italic">AR Foundation 5.0</em>, developers<a id="_idIndexMarker376"/> can conveniently test AR scenes right in the Unity Editor using the <strong class="bold">XR Simulation</strong> feature, without the constant need to deploy on mobile devices. By the time you read this book, this feature might already be pre-installed. Let’s quickly check that by following these steps:</p>
<ol>
<li>Navigate to <strong class="bold">Edit</strong> | <strong class="bold">Project Settings</strong>.</li>
<li>Choose <strong class="bold">XR </strong><strong class="bold">Plug-in Management</strong>.</li>
<li>Look for the <strong class="bold">XR Simulation</strong> option under <strong class="bold">Plug-in Providers</strong> and enable it.</li>
</ol>
<p>If you don’t see the <strong class="bold">XR Simulation</strong> option, you’ll need to manually install <em class="italic">AR Foundation 5.0</em> or a newer version. The next section explains how to do this.</p>
<h2 id="_idParaDest-93">Installing AR Foundation 5.0 or later versions and related packages</h2>
<p>When <a id="_idIndexMarker377"/>you edit your project manifest, you control which package versions Unity loads into your project. There are two ways to edit your project manifest: add a package by name in <strong class="bold">Package Manager</strong>, or manually edit the project manifest file. Let’s do it in <strong class="bold">Package Manager</strong>:</p>
<ol>
<li>Select <strong class="bold">Window</strong> | <strong class="bold">Package Manager</strong> to open the <strong class="bold">Package </strong><strong class="bold">Manager</strong> window.</li>
<li>Click on <a id="_idIndexMarker378"/>the small <code>com.unity.xr.arfoundation</code>. This will automatically add the most recent version available. At the time of writing, this is version <strong class="bold">5.1.0</strong>.<p class="list-inset">If you want to use a specific version, you can type in your desired version in the <strong class="bold">Version (</strong><strong class="bold">optional)</strong> field.</p></li>
<li>Continue by updating the <code>com.unity.xr.openxr</code>. This action will import the latest version of the <strong class="bold">OpenXR</strong> plugin.</li>
<li>If you’re upgrading from <em class="italic">AR Foundation 4</em> to a newer version, uninstall both the <code>com.unity.xr.arkit-face-tracking</code> and <code>com.unity.xr.arsubsystems</code>. If they appear in the search results, proceed with uninstallation; otherwise, they are not present in your project.</li>
<li>Next, depending on your targeted mobile device platform, it’s important to update either the <em class="italic">ARCore</em> or <em class="italic">ARKit</em> packages. Navigate to the <code>com.unity.xr.arcore</code> and ensure its version aligns with that of <em class="italic">AR Foundation</em>. For iOS platforms, input <code>com.unity.xr.arkit</code>, making sure its version <a id="_idIndexMarker379"/>matches <em class="italic">AR Foundation</em>’s.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">Always remember to refer to the <em class="italic">AR Foundation</em> documentation for the most recent version details and Editor compatibility: <a href="https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@5.0/manual/project-setup/install-arfoundation.html">https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@5.0/manual/project-setup/install-arfoundation.html</a>.</p>
<p>Now, if you revisit <strong class="bold">XR Plug-in Management</strong> via the aforementioned quick check, the <strong class="bold">XR Simulation</strong> option should be visible in the <strong class="bold">Windows, Mac, Linux settings</strong> tab, as shown in <em class="italic">Figure 4</em><em class="italic">.15</em>.</p>
<div><div><img alt="Figure 4.15 – The Windows, Mac, Linux settings tab of the XR Plug-in Management window with the XR Simulation checkbox enabled" src="img/B20869_04_15.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.15 – The Windows, Mac, Linux settings tab of the XR Plug-in Management window with the XR Simulation checkbox enabled</p>
<p>With this feature activated, you’re primed to choose an environment and test the AR scene within Unity.</p>
<h2 id="_idParaDest-94">Choosing an environment and testing the scene</h2>
<p>To test an <a id="_idIndexMarker380"/>AR scene within Unity, you’ll need an environment that emulates the real-world setting. To find the right simulation setting, go to <strong class="bold">Window</strong> | <strong class="bold">XR</strong> | <strong class="bold">AR</strong> <strong class="bold">Foundation</strong> | <strong class="bold">XR Environment</strong>. In the middle of the <strong class="bold">XR Environment</strong> interface, there’s an <strong class="bold">Environment</strong> dropdown. Initially, your project will offer just one environment option. While it’s possible to add more by importing sample environments, <strong class="bold">DefaultSimulationEnvironment</strong> is usually adequate for <a id="_idIndexMarker381"/>most testing needs. Simply choose this option. Once you’ve made your selection, hit the play button to activate play mode and start the simulation. You’ll now be able to view your scene in real time, as depicted in <em class="italic">Figure 4</em><em class="italic">.16</em>.</p>
<div><div><img alt="Figure 4.16 – XR Simulation at runtime" src="img/B20869_04_16.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.16 – XR Simulation at runtime</p>
<p>Pressing the play button transfers you to the <strong class="bold">Game</strong> mode of <strong class="bold">XR Simulation</strong>, presenting the simulated AR scene (highlighted as <em class="italic">1</em> in <em class="italic">Figure 4</em><em class="italic">.16</em>). In this mode, you can adjust the viewpoint as needed, witnessing features such as plane detection in real time. Once satisfied with the viewpoint, you can shift from <strong class="bold">Game</strong> mode to <strong class="bold">Simulator</strong> mode (highlighted as <em class="italic">2</em> in <em class="italic">Figure 4</em><em class="italic">.16</em>) using the dropdown in the top-left corner. It’s important to switch to <strong class="bold">Simulator</strong> mode to access the <strong class="bold">XR Simulation</strong> functionalities, such as touch inputs. Observing the scene shown as <em class="italic">2</em> in <em class="italic">Figure 4</em><em class="italic">.16</em>, you’ll notice key features such as the distinct white point pattern, indicating the detected plane, and white capsules, marking the spots where mouse clicks occurred.</p>
<p>However, it’s crucial to be aware of the constraints surrounding <strong class="bold">XR Simulation</strong>. Some elements might seem smaller than expected, and the resolution may not be the sharpest. Although <strong class="bold">XR Simulation</strong> provides a convenient testing avenue, it’s not a complete <a id="_idIndexMarker382"/>substitute for deploying the AR scene on an actual device. Consider it a tool for iterative testing until you’re confident about deploying it onto your target mobile device.</p>
<p>Now, let’s proceed to launch the scene on our smartphone and observe the outcome.</p>
<h1 id="_idParaDest-95">Deploying AR experiences onto mobile devices</h1>
<p>It’s now<a id="_idIndexMarker383"/> time to launch your AR experiences onto smartphones or tablets. Primarily, you have two paths to accomplish this: deployment onto an Android or an iOS device.</p>
<p>For solo projects, where the AR application is meant for personal use, you may opt to deploy onto just Android or iOS, depending on your device’s operating system. However, for larger-scale projects that involve several users – be it academic, industrial, or any other group, irrespective of its size – it’s advisable to deploy and test the AR app on both Android and iOS platforms.</p>
<p>This strategy has multiple benefits. First, if your application gains momentum or its usage expands, it would already be compatible with both major platforms, eliminating the need for time-consuming porting later on. Second, making your app accessible on both platforms from the outset can draw in more users, and possibly attract increased funding or support.</p>
<p>Another key advantage to this is the cross-platform compatibility offered by Unity. This enables you to maintain a singular code base for both platforms, simplifying the management and updating process for your application. Any modifications made need to be done in one location and then deployed across both platforms.</p>
<p>In the next section, we’ll delve into the steps required to deploy your AR scene onto an Android device.</p>
<h2 id="_idParaDest-96">Deploying onto Android</h2>
<p>This <a id="_idIndexMarker384"/>section outlines the<a id="_idIndexMarker385"/> procedure to deploy your AR scene onto an Android device. The initial part of the process involves enabling some settings on your phone to prepare it for testing. Here’s a step-by-step guide:</p>
<ol>
<li>Confirm that <a id="_idIndexMarker386"/>your device is compatible with ARCore. ARCore is essential for AR Foundation to work correctly. You can find a list of supported devices at <a href="https://developers.google.com/ar/devices">https://developers.google.com/ar/devices</a>.</li>
<li>Install ARCore, which AR Fo<a href="https://play.google.com/store/apps/details?id=com.google.ar.core">undation uses to enable AR capabilities on Android devices. ARCo</a>re can be downloaded from the Google Play Store at <a href="https://play.google.com/store/apps/details?id=com.google.ar.core">https://play.google.com/store/apps/details?id=com.google.ar.core</a>.</li>
<li>Activate <strong class="bold">Developer Options</strong>. To do this, open <strong class="bold">Settings</strong> on your Android device, scroll down, and select <strong class="bold">About Phone</strong>. Find <strong class="bold">Build number</strong> and tap it seven times until a message appears stating <strong class="bold">You are now </strong><strong class="bold">a developer!</strong></li>
<li>Upon<a id="_idIndexMarker387"/> returning to the main <strong class="bold">Settings</strong> menu, you should now see an option called <strong class="bold">Developer Options</strong>. If it’s not present, perform an online search to find out how to enable developer mode for your specific device. Though the method described in the previous step is the most common, the variety of Android devices available might require slightly different steps.</li>
<li>With <strong class="bold">Developer Options</strong> enabled, turn on <strong class="bold">USB Debugging</strong>. This will allow you to transfer your AR scene to your Android device via a USB cable. Navigate to <strong class="bold">Settings</strong> | <strong class="bold">Developer options</strong>, scroll down to <strong class="bold">USB Debugging</strong>, and switch it on. Acknowledge any pop-up prompts.</li>
<li>Depending on your Android version, you might need to allow the installation of apps from unknown sources:<ul><li>For Android versions 7 (Nougat) and earlier: Navigate to <strong class="bold">Settings</strong> | <strong class="bold">Security Settings</strong> and then check the box next to <strong class="bold">Unknown Sources</strong> to allow the installation of apps from sources other than the Google Play Store.</li><li>For Android versions 8 (Oreo) and above: Select <strong class="bold">Settings</strong> |<strong class="bold">Apps &amp; Notifications</strong> | <strong class="bold">Special App Access</strong> | <strong class="bold">Install unknown apps</strong> and activate <strong class="bold">Unknown sources</strong>. You will see a list of apps that you can grant permission to install from unknown sources. This is where you select the <em class="italic">File Manager</em> app, as you’re using it to download the unknown app from Unity.</li></ul></li>
<li>Link your <a id="_idIndexMarker388"/>Android device to your computer using a USB cable. You can typically use your device’s charging cable for this. A prompt <a id="_idIndexMarker389"/>will appear on your Android device asking for permission to allow USB debugging from your computer. Confirm it.</li>
</ol>
<p>With your Android device properly prepared for testing AR scenes, you can now proceed to deploy your Unity AR scene. This involves adjusting several parameters in the Unity Editor’s <strong class="bold">Build Settings</strong> and <strong class="bold">Player Settings</strong>.</p>
<p>Here’s a step-by-step guide on how to do this:</p>
<ol>
<li>Select <strong class="bold">File</strong> | <strong class="bold">Build Settings</strong> | <strong class="bold">Android</strong>, then click the <strong class="bold">Switch Platform</strong> button. Now, your <strong class="bold">Build Settings</strong> should look something like what is illustrated in <em class="italic">Figure 4</em><em class="italic">.17</em>.</li>
</ol>
<div><div><img alt="Figure 4.17 – Unity’s Build Settings configuration for Android" src="img/B20869_04_17.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.17 – Unity’s Build Settings configuration for Android</p>
<ol>
<li value="2">Next, click<a id="_idIndexMarker390"/> on the <code>Android 7.0 Nougat (API level 24)</code> or above. This is crucial, as ARCore requires at least Android 7.0 to function properly.</li>
<li>Remaining in the <code>com.company_name.application_name</code>. This pattern is a widely adopted convention for naming application packages in Android and is used to ensure unique identification for each application on the Google Play Store.</li>
<li>Return<a id="_idIndexMarker392"/> to <code>Builds</code>. Upon selecting this folder, Unity will construct the scene within the newly created <code>Builds</code> folder.</li>
</ol>
<p>This is how you can set up your Android device for deploying AR scenes onto it. In the next section, you will learn how you can deploy your AR scene onto an iOS device, such as an iPhone or iPad.</p>
<h2 id="_idParaDest-97">Deploying onto iOS</h2>
<p>Before<a id="_idIndexMarker394"/> we <a id="_idIndexMarker395"/>delve into the process of deploying an AR scene onto an iOS device, it’s important to discuss certain hardware prerequisites. Regrettably, if you’re using a Windows PC and an iOS device, it’s not as straightforward as deploying an AR scene made in Unity. The reason for this is that Apple, in its characteristic style, requires the use of <em class="italic">Xcode</em>, its proprietary development environment, as an intermediary step. This is only available on Mac devices, not Windows or Linux.</p>
<p>If you don’t possess a Mac, there are still ways to deploy your AR scene onto an iOS device. Here are a few alternatives:</p>
<ul>
<li><em class="italic">Borrowing a Mac</em>: The simplest solution to gain access to Xcode and deploy your app onto an iOS device is to borrow a Mac from a friend or coworker. It’s also worth checking whether local libraries, universities, or co-working spaces offer public access to Macs. For commercial or academic projects, it’s highly recommended to invest in a Mac for testing your AR app on iOS.</li>
<li><em class="italic">Using a virtual machine</em>: Another no-cost alternative is to establish a macOS environment on your non-Apple PC. However, Apple neither endorses nor advises this method due to potential legal issues and stability concerns. Therefore, we won’t elaborate further or recommend it.</li>
<li><em class="italic">Employing a Unity plugin</em>: Fortunately, a widely used Unity plugin enables deployment of an AR scene onto your iOS device with relatively less hassle. Navigate to <code>iOS Project Builder for Windows</code> by Pierre-Marie Baty. Though this plugin costs $50, it is a much cheaper alternative than buying a Mac. After purchasing the plugin, import it into your AR scene and configure everything correctly by following the plugin’s documentation (<a href="https://www.pmbaty.com/iosbuildenv/documentation/unity.html">https://www.pmbaty.com/iosbuildenv/documentation/unity.html</a>).</li>
</ul>
<p>In this book, we<a id="_idIndexMarker396"/> focus exclusively on deploying AR applications onto iOS devices using a Mac for running Unity and Xcode. This is due to potential <a id="_idIndexMarker397"/>inconsistencies and maintenance concerns with other aforementioned methods.</p>
<p>Before you initiate the deployment setup, ensure that your Mac and iOS devices have the necessary software and settings. The following steps detail this preparatory process:</p>
<ol>
<li>Make sure the latest software versions are installed on your MacOS and iOS devices. Check for updates by navigating to <strong class="bold">Settings</strong> | <strong class="bold">General</strong> | <strong class="bold">Software Update</strong> on each device and install any that are available.</li>
<li>Confirm that your iOS device supports ARKit, which is crucial for the correct functioning of AR Foundation. You can check compatibility at <a href="https://developer.apple.com/documentation/arkit/">https://developer.apple.com/documentation/arkit/</a>. Generally, any device running on iPadOS 11 or iOS 11 and later versions are compatible.</li>
<li>You will <a id="_idTextAnchor015"/>need an Apple ID for the following steps. If you don’t have one, you can create it at <a href="https://appleid.apple.com/account">https://appleid.apple.com/account</a>.</li>
<li>Download the <strong class="bold">Xcode</strong> software onto your Mac from Apple’s developer website at <a href="https://developer.apple.com/xcode/">https://developer.apple.com/xcode/</a>.</li>
<li>Enable <strong class="bold">Developer Mode</strong> on your iOS device by going to <strong class="bold">Settings</strong> | <strong class="bold">Privacy &amp; Security</strong> | <strong class="bold">Developer Mode</strong>, activate <strong class="bold">Developer Mode</strong>, and then restart your device. If you don’t find the <strong class="bold">Developer Mode</strong> option, connect your iOS device to a Mac using a cable. Open <strong class="bold">Xcode</strong>, then navigate to <strong class="bold">Window</strong> | <strong class="bold">Devices and Simulator</strong>. If your device isn’t listed in the left pane, ensure you trust the computer on your device by acknowledging the prompt that appears after you connect your device to the Mac. Subsequently, you can enable <strong class="bold">Developer Mode</strong> on your iOS device.</li>
</ol>
<p>Having set up<a id="_idIndexMarker398"/> your<a id="_idIndexMarker399"/> Mac and iOS devices correctly, let’s now proceed with how to deploy your AR scene onto your iOS device. Each time you want to deploy your AR scene onto your iOS device, follow these steps:</p>
<ol>
<li>Use a USB cable to connect your iOS device to your Mac.</li>
<li>Within your Unity project, navigate to <strong class="bold">File</strong> | <strong class="bold">Build Settings</strong> and select <strong class="bold">iOS </strong>from <strong class="bold">Platform options</strong>. Click the <strong class="bold">Switch </strong><strong class="bold">Platform</strong> button.</li>
<li>Check the <strong class="bold">Development Build</strong> option in <strong class="bold">Build Settings</strong> for iOS. This enables you to deploy the app for testing purposes onto your iOS device. This step is crucial to avoid the annual subscription cost of an Apple Developer account.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">Deploying apps onto an iOS device with a free Apple Developer account has certain limitations. You can only deploy up to three apps onto your device at once, and they need to be redeployed every 7 days due to the expiration of the free provisioning profile. For industrial or academic purposes, we recommend subscribing to a paid Developer account after thorough testing using the <strong class="bold">Development </strong><strong class="bold">Build</strong> function.</p>
<ol>
<li value="4">Remain in <code>com.company_name.application_name</code>.</li>
<li>Return to <code>Builds</code> and select it.</li>
<li><strong class="bold">Xcode</strong> will<a id="_idIndexMarker400"/> open with the build, displaying an error message due to the need for a signing certificate. To create this, click on the error message, navigate to the <strong class="bold">Signing and Capabilities</strong> tab, and select the checkbox. In the <strong class="bold">Team </strong>drop-down menu, select <strong class="bold">New Team</strong>, and create a new team consisting solely of yourself. Now, select this newly-created team from the drop-down menu. Ensure that the information in the <strong class="bold">Bundle Identifier</strong> field matches your Unity Project found in <strong class="bold">Edit</strong> | <strong class="bold">Project Settings</strong> | <strong class="bold">Player</strong>.</li>
<li>While<a id="_idIndexMarker401"/> in <strong class="bold">Xcode</strong>, click on the <strong class="bold">Any iOS Device</strong> menu and select your specific iOS device as the output.</li>
<li>Click the <strong class="bold">Play</strong> button on the top left of <strong class="bold">Xcode</strong> and wait for a message indicating <strong class="bold">Build succeeded</strong>. Your AR application should now be on your iOS device. However, you won’t be able to open it until you trust the developer (in this case, yourself). Navigate to <strong class="bold">Settings</strong> | <strong class="bold">General</strong> | <strong class="bold">VPN &amp; Device Management</strong> on your iOS device, tap <strong class="bold">Developer App certificate</strong> under your <strong class="bold">Apple ID</strong>, and then tap <strong class="bold">Trust (Your </strong><strong class="bold">Apple ID)</strong>.</li>
<li>On your iOS device’s home screen, click the icon of your AR app. Grant the necessary permissions, such as camera access. Congratulations, you’ve successfully deployed your AR app onto your iOS device!</li>
</ol>
<p>You now know how to deploy your AR experiences onto both Android and iOS devices.</p>
<p>Let’s review what we have learned so far in this chapter before moving on to creating interactive XR experiences.</p>
<h1 id="_idParaDest-98">Summary</h1>
<p>In this chapter, we’ve delved into the complexities and intricacies that surround AR glasses, exploring why these devices face numerous physical and technological challenges before they can be fully embraced by the public at large. We’ve examined the critical choice between marker-based and markerless approaches in your AR application, and we’ve discussed how this seemingly simple decision can significantly influence not only the development journey of your application but also its accessibility, versatility, and user engagement.</p>
<p>Through the exploration and installation of Unity’s AR Foundation package, you are now empowered to create simple AR experiences of your own, and ready to deploy them across an extensive array of handheld, AR-compatible mobile devices.</p>
<p>We’ve also discovered that the deployment of an AR scene onto iOS devices can be a complex and time-intensive task, largely due to the various restrictions imposed by Apple’s ecosystem compared to the Android ecosystem. However, these constraints should not deter you from pursuing a cross-platform approach for your AR apps. By aiming for deployment across both operating systems, you ensure greater accessibility and increase the potential for reaching a broader audience.</p>
<p>By understanding the diverse aspects of AR development through Unity, you are well on your way to creating immersive AR experiences that truly stand out. In the next chapter, you will learn how you can use C# scripting and other techniques to add complex logic to your VR applications.</p>
</div>
</body></html>