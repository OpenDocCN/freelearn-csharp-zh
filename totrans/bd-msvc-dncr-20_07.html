<html><head></head><body>
        

                            
                    <h1 class="header-title">Monitoring Microservices</h1>
                
            
            
                
<p class="md-end-block md-heading md-focus">When something goes wrong in a system, stakeholders will want to know what has happened, why it has happened, any hint or clue you can give for how it might be fixed, and how to prevent the same problem from occurring again in the future. This is one of the primary uses of monitoring. However, monitoring can also do much more.</p>
<p>In .NET monoliths, there are multiple monitoring solutions available to choose from. The monitoring target is always centralized, and monitoring is certainly easy to set up and configure. If something breaks down we know what to look for and where to look for it, since only a finite number of components participate in a system, and they have a fairly long lifespan.</p>
<p>However, microservices are distributed systems and, by nature, more complex than monoliths. So resource utilization and health and performance monitoring are quite essential in a microservice production environment. We can use this diagnostic piece of information to detect and correct issues, and to also spot potential problems and prevent them from occurring. Monitoring microservices presents different challenges. In this chapter, we will primarily discuss the following topics:</p>
<ul class="ul-list">
<li>The need for monitoring</li>
<li>Monitoring and logging challenges in microservices</li>
<li>Monitoring strategies</li>
<li>Available tools and strategies for microservices in the .NET monitoring space </li>
<li>Use of Azure diagnostics and application insight</li>
<li>A brief overview of the ELK stack and Splunk</li>
</ul>
<p>What does monitoring really mean? There is no formal definition of monitoring; however, the following is appropriate:</p>
<p>''Monitoring provides information around the behavior of an entire system or different parts of a system in their operational environment. This information can be used for diagnosing and gaining insight into the different characteristics of a system.''</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Instrumentation and telemetry</h1>
                
            
            
                
<p>A monitoring solution is dependent upon instrumentation and telemetry. So it is natural that when we speak about monitoring microservices, we also discuss instrumentation and telemetry data. Logs are nothing more than an instrumentation mechanism.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Instrumentation</h1>
                
            
            
                
<p>Now let's look at what instrumentation is. Instrumentation is one of the ways through which you can add diagnostic features to applications. It can be formally defined as follows:</p>
<p>''Most applications will include diagnostic features that generate custom monitoring and debugging information, especially when an error occurs. This is referred to as instrumentation and is usually implemented by adding event and error handling code to the application." </p>
<p>-MSDN</p>
<p>Under normal conditions, data from informational events may not be required, thus reducing the cost of storage and the transactions required to collect it. However, when there is an issue with the application, you have to update the application configuration so that the diagnostic and instrumentation systems can collect informational event data as well as error and warning messages to assist in isolating and fixing faults. It may be necessary to run the application in this extended reporting mode for some time if the problem appears only intermittently.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Telemetry</h1>
                
            
            
                
<p>Telemetry, in its most basic form, is the process of gathering information generated by instrumentation and logging systems. Typically, it is performed using asynchronous mechanisms that support massive scaling and the wide distribution of application services. It can be defined as follows:</p>
<p>"The process of gathering remote information that is collected by instrumentation is usually referred to as telemetry."</p>
<p>-MSDN</p>
<p>In large and complex applications, information is usually captured in a data pipeline and stored in a form that makes it easier to analyze and display at different levels of granularity. This information is used to discover trends, gain insight into usage and performance, and detect and isolate faults.</p>
<p>Azure has no built-in system that directly provides a telemetry and reporting system of this type. However, a combination of the features exposed by all the Azure services, Azure diagnostics, and application insights allows you to create telemetry mechanisms that span the range of simple monitoring mechanisms to comprehensive dashboards. The complexity of the telemetry mechanism you require usually depends on the size of the application. This is based on several factors, such as the number of roles or virtual machine instances, the number of ancillary services it uses, the distribution of the application across different data centers, and other related factors.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The need for monitoring</h1>
                
            
            
                
<p>Microservices are complex, distributed systems. Microservice implementation is the backbone of any modern IT business. Understanding the internals of the services along with their interactions and behaviors will help you make the overall business more flexible and agile. The performance, availability, scale, and security of microservices can directly affect a business and also its revenue. Hence, monitoring microservices is vital. It helps us observe and manage the quality of the service attributes. Let's discuss the scenarios where it is required.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Health monitoring</h1>
                
            
            
                
<p>With health monitoring, we monitor the health of a system and its various components at a certain frequency, typically a few seconds. This ensures that the system and its components behave as expected. With the help of an exhaustive health monitoring system, we can keep tabs on the overall system health, including the CPU, memory utilization, and so on. It might be in the form of pings or extensive health monitoring endpoints, which emit the health status of services along with some useful metadata at that point in time.</p>
<p>For health monitoring, we can use the rate of request failures and successes; we can also utilize techniques such as synthetic user monitoring. We will see synthetic user monitoring a little later in this chapter.</p>
<p>The metrics for health monitoring are based on the threshold values of success or failure rates. If the parameter value goes beyond the configured threshold, an alert is triggered. It is quite possible that some preventive action to maintain the health of the system would be triggered due to this failure. This action could be restarting the service in the failure state, or provisioning some server resource.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Availability monitoring</h1>
                
            
            
                
<p>Availability monitoring is quite similar to health status monitoring, which we just discussed. However, the subtle difference is that in availability monitoring, the focus is on the availability of systems rather than a snapshot of the health at that point in time.</p>
<p>Availability of systems is dependent on various factors, such as the overall nature and domain of the application, services, and service dependencies as well as infrastructure or environment. The availability monitoring system captures low-level data points related to these factors and represents them so as to make a business-level feature available. Many times, availability monitoring parameters are used to track business metrics and <strong>service-level agreements</strong> (<strong>SLA</strong>).</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Performance monitoring</h1>
                
            
            
                
<p>The performance of a system is often measured by key performance indicators. Some of the key performance indicators of any large web-based system are as follows:</p>
<ul class="ul-list">
<li>The number of requests served per hour</li>
<li>The number of concurrent users served per hour</li>
<li>The average processing time required by users to perform business transactions, for example, placing an order</li>
</ul>
<p>Additionally, performance is also gauged by system-level parameters, such as:</p>
<ul class="ul-list">
<li>CPU utilization</li>
<li>Memory utilization</li>
<li>I/O rates</li>
<li>Number of queued messages</li>
</ul>
<p>If any of these key performance indicators are not met by the system, an alert is raised.</p>
<p>Often, while analyzing performance issues, historical data from previous benchmarks captured by the monitoring system is used to troubleshoot.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Security monitoring</h1>
                
            
            
                
<p>Monitoring systems can detect unusual data pattern requests, unusual resource consumption patterns, and detect attacks on the system. Specifically, in the case of DoS, attacks or injection attacks can be identified beforehand and teams can be alerted. Security monitoring also keeps audit trails of authenticated users and keeps a history of users who have checked in and out of the system. It also comes in handy for satisfying compliance requirements.</p>
<p>Security is a cross-cutting concern of distributed systems, including microservices, so there are multiple ways of generating this data in the system. Security monitoring can get data from various tools that are not part of the system but may be part of the infrastructure or environment in which the system is hosted. Different types of logs and database entries can serve as data sources. However, this really depends upon the nature of the system.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">SLA monitoring</h1>
                
            
            
                
<p>Systems with SLAs basically guarantee certain characteristics, such as performance and availability. For cloud-based services, this is a pretty common scenario. Essentially, SLA monitoring is all about monitoring those guaranteed SLAs for the system. SLA monitoring is enforced as a contractual obligation between a service provider and consumer.</p>
<p>It is often defined on the basis of availability, response time, and throughput. Data points required for SLA monitoring can come from performance endpoint monitoring or logging and availability of monitoring parameters. For internal applications, many organizations track the number of incidences raised due to server downtime. The action taken against these incidences' <strong>Root Cause Analysis</strong> (<strong>RCA</strong>) mitigates the risk of repeating those issues and helps meet the SLAs.</p>
<p>For internal purposes, an organization might also track the number and nature of incidents that had caused the service to fail. Learning how to resolve these issues quickly or eliminate them completely helps reduce downtime and meet SLAs.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Auditing sensitive data and critical business transactions</h1>
                
            
            
                
<p>For any legal obligations or compliance reasons, the system might need to keep audit trails of user activities in the system, and record all their data accesses and modifications. Since audit information is highly sensitive in nature, it might be disclosed only to a few privileged and trusted individuals in the system. Audit trails can be part of a security subsystem or separately logged. You may need to transfer and store audit trails in a specific format, as stated by the regulation or compliance specifications.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">End user monitoring</h1>
                
            
            
                
<p>In end user monitoring, the usage of the features of the system and/or the overall system usage by end users is tracked and logged. Usage monitoring can be done using various user-tracking parameters, such as the features used, the time required to complete a critical transaction for the specified user, or even enforced quotas. Enforced quotas are constraints or limits put on an end user in regard to system usage. In general, various pay-as-you-go services use enforced quotas; for example, a free trial, where you can upload files only up to 25 MB. The data source for this type of monitoring is typically collected in terms of logs and tracking user behavior.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Troubleshooting system failures</h1>
                
            
            
                
<p>The end users of a system might experience system failures. This can be in the form of either a system failure or a situation where users are not able to perform a certain activity. These kinds of issues are monitored using system logs; if not, the end user will need to provide a detailed information report. Also, sometimes server crash dumps or memory dumps can be immensely helpful. However, in the case of distributed systems, it will be a bit difficult to understand the exact root cause of the failures.</p>
<p>In many monitoring scenarios, using only one monitoring technique is not effective. It is better to use multiple monitoring techniques and tools for diagnostics. In particular, monitoring a distributed system is quite challenging and requires data from various sources. In addition to analyzing the situation properly and deciding on the action points, we must consider a holistic view of monitoring rather than looking into only one type of system perspective.</p>
<p>Now that we have a better idea about what needs to be done for general purpose monitoring, let's revisit the microservice perspective. So we will discuss the different monitoring challenges presented by the microservice architectural style.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Monitoring challenges</h1>
                
            
            
                
<p>Microservice monitoring presents different challenges. There will be scenarios where one service could depend upon another service, or a client sends a request to one service and the response comes from another service that would make the operation complex; hence scaling a microservice would be a challenging task here. Similarly, process implementation, let's say DevOps, would be a challenging job while implementing a huge enterprise microservice application. So, let's discuss these challenges in this section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Scale</h1>
                
            
            
                
<p>One service could be dependent upon the functionality provided by various other microservices. This yields complexity, which is not usual in the case of .NET monolith systems. Instrumenting all these dependencies is quite difficult. Another problem that comes along with scale is the rate of change. With the advancement of continuous deployment and container-based microservices, the code is always in a deployable state. Containers only live for minutes, if not seconds. The same is true for virtual machines. Virtual machines have a life of around a couple of minutes to a couple of hours.</p>
<p>In such a case, measuring regular signals, such as CPU usage and memory consumption usage per minute, does not make sense. Sometimes, container instances might not even be alive for a minute. Within a minute, the container instance might have already been disposed of. This is one of the challenges of microservice monitoring.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">DevOps mindset</h1>
                
            
            
                
<p>Traditionally, services or systems, once deployed, are owned and cared for by the operational teams. However, DevOps breaks down the silos between developers and operations teams. It comes with lots of practices, such as continuous integration and continuous delivery, as well as continuous monitoring. Along with these new practices come new tool sets.</p>
<p>However, DevOps is not just a set of practices or tools; it is, more importantly, a mindset. It is always a difficult and slow process to change the mindset of people. Microservice monitoring also requires a similar mindset shift. </p>
<p>With the emergence of autonomy of services, developer teams now have to own services. This also means that they have to work through and fix development issues as well as keep an eye on all the operational parameters and SLAs of the services. Development teams will not be transformed overnight just by using state-of-the-art monitoring tools. This is true for operational teams as well. It won't suddenly become a <em>core platform team</em> (or whatever fancy name you prefer) overnight.</p>
<p>To make microservices successful and meaningful for organizations, developers, and operations, teams need to help each other understand their own pain points and also think in the same direction, that is, how they can deliver value to the business together. Monitoring cannot happen without the instrumentation of services, which is where developer teams can help. Likewise, alerting and setting up of operational metrics and running books won't happen without the operational team's help. This is one of the challenges in delivering microservice monitoring solutions.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Data flow visualization</h1>
                
            
            
                
<p>There are a number of tools present on the market for data flow visualization. Some of them are AppDynamics, New Relic, and so on. These tools are capable of handling visualizations of 10 to, maybe, 100s of microservices. However, in larger settings, where there are thousands of microservices, these tools are unable to handle visualization. This is one of the challenges in microservice monitoring.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Testing of monitoring tools</h1>
                
            
            
                
<p>We trust monitoring tools with the understanding that they depict a factual representation of the big picture of our microservice implementation. However, to make sure that they remain true to this understanding, we will have to test the monitoring tools. This is never a challenge in monolith implementations. However, when it comes to microservices, visualization of microservices is required for monitoring purposes. This means generating fake/synthetic transactions and time and utilizing the entire infrastructure rather than serving the customer. Hence, the testing of monitoring tools is a costly affair and presents a significant challenge in microservice monitoring.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Monitoring strategies</h1>
                
            
            
                
<p>In this section, we will take a look at the monitoring strategies that make microservices observable. It is common to implement the following or more strategies to create a well-defined and holistic monitoring solution.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Application/system monitoring</h1>
                
            
            
                
<p>This strategy is also called a <strong>framework-based strategy</strong>. Here, the application, or in our case microservice, itself generates the monitoring information within the given context of execution. The application can be dynamically configured based on the thresholds or trigger points in the application data, which can generate tracing statements. It is also possible to have a probe-based framework (such as .NET CLR, which provides hooks to get more information) to generate monitoring data. So, effective instrumentation points themselves can be embedded into the application to facilitate this kind of monitoring. On top of this, the underlying infrastructure, where microservices are hosted, can also raise critical events. These events can be listened to and recorded by the monitoring agents present on the same host as that of the application.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Real user monitoring</h1>
                
            
            
                
<p>This strategy is based on a real end user's transactional flow across the system. While the end user is using the system in real time, the parameters related to response time and latency, as well as the number of errors experienced by the user, can be captured using it.</p>
<p>This is useful for specific troubleshooting and issue resolution. With this strategy, the system's hotspots and bottlenecks for service interactions can be captured as well. It is possible to record the entire end-to-end user flow or transactions to replay it at a later time. The benefits of this are that these kinds of recorded plays can be used for troubleshooting of issues as well as for various types of testing purposes.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Semantic monitoring and synthetic transactions</h1>
                
            
            
                
<p>The semantic monitoring strategy focuses on business transactions; however, it is implemented through the use of synthetic transactions. In semantic monitoring, as the name suggests, we try to emulate end user flows. However, this is done in a controlled fashion and with dummy data so you can differentiate the output of the flow from the actual end user flow data. This strategy is typically used for service dependency, health checking, and diagnostics of problems occurring across the system. To implement synthetic transactions, we need to be careful while planning the flow; also, we need to be careful enough not to stress the system out. Here's an example: creating fake orders for fake product catalogs and observing the response time and output across this whole transaction propagating in the system.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Profiling</h1>
                
            
            
                
<p>This approach is specifically focused on solving performance bottlenecks across the system. This approach is different from the preceding approaches. Real and semantic monitoring focuses on business transactions or functional aspects of the system and collects data around it. Rather, profiling is all about system-level or low-level information capture. A few of these parameters are response time, memory, or threads. This approach uses a probing technique in the application code or framework and collects data. Utilizing the data points captured during the profiling, the relevant DevOps team can identify the cause of the performance problem. Profiling using probing should be avoided in production environments. However, it is perfectly fine for generating call times and so on without overloading the system at runtime. A good example of profiling, in general, is an ASP.NET MVC application profiled with an ASP.NET MiniProfiler, or even with Glimpse.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Endpoint monitoring</h1>
                
            
            
                
<p>With this approach, we expose one or more endpoints of a service to emit diagnostic information related to the service itself as well as the infrastructure parameters. Generally, different endpoints focus on providing different information. For example, one endpoint can give the health status of the service, while the other could provide the HTTP 500 error information that occurred in that service execution. This is a very helpful technique for microservices since it inherently changes the monitoring from being a push model to a pull model and reduces the overhead of service monitoring. We can scrap data from these endpoints at a certain time interval and build a dashboard and collect data for operational metrics.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Logging</h1>
                
            
            
                
<p>Logging is a type of instrumentation made available by the system, its various components, or the infrastructure layer. In this section, we will first visit logging challenges and then discuss strategies to reach a solution for these challenges. </p>


            

            
        
    

        

                            
                    <h1 class="header-title">Logging challenges</h1>
                
            
            
                
<p>We will first try to understand the problem with log management in microservices:</p>
<ul>
<li>To log the information related to a system event and parameter as well as the infrastructure state, we will need to persist log files. In traditional .NET monoliths, log files are kept on the same machine where the application is deployed. In the case of microservices, they are hosted either on virtual machines or containers. But virtual machines and containers are both ephemeral, which means they do not persist states. In this situation, if we persist log files with virtual machines or containers, we will lose them. This is one of the challenges of log management in microservices.</li>
<li>In the microservice architecture, there are a number of services that constitute a transaction. Let's assume we have an order placement transaction where service A, service B, and service C take part in the transaction. If, say, service B fails during the transaction, how are we going to understand and capture this failure in the logs? Not only that but more importantly, how are we going to understand that a specific instance of service B has failed and it was taking part in a said transaction? This scenario presents another challenge to microservices.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Logging strategies</h1>
                
            
            
                
<p>So far in this section, we have discussed logging, its challenges, and why we should implement logging. Multiple calls at the same time are possible so when we implement logging, we should implement it in such a way that we know the exact source of the logged transaction. We would go with correlation ID for logging.</p>
<p>Logging is not related to microservices specifically; it is also important for monolithic applications.</p>
<p>To implement logging in microservices, we can use the keylogging strategies discussed in the following sections.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Centralized logging</h1>
                
            
            
                
<p>There is a difference between centralized logging and centralized monitoring. In centralized logging, we log all the details about the events that occur in our system—they may be errors or warnings or just for informational purposes—whereas in centralized monitoring, we monitor critical parameters, that is, specific information.</p>
<p>With logs, we can understand what has actually happened in the system or a specific transaction. We will have all the details about the specific transaction, such as why it started, who triggered it, what kind of data or resources it recorded, and so on. In a complex distributed system, such as microservices, this is really the key piece of information with which we can solve the entire puzzle of information flow or errors. We also need to treat timeouts, exceptions, and errors as events that we need to log.</p>
<p>The information we record regarding a specific event should also be structured, and this structure should be consistent across our system. So, for example, our structured log entry might contain level-based information to state whether the log entry is for information, an error, or whether it's debugged information or statistics that have been recorded as log entry events. The structured log entry must also have a date and time so we know when the event happened. We should also include the hostname within our structured log so that we know where exactly the log entry came from. We should also include the service name and the service instance so we know exactly which microservice made the log entry.</p>
<p>Finally, we should also include a message in our structured logging format, which is the key piece of information associated with the event. So, for example, for an error, this might be the call stack or details regarding the exception. The key thing is that we keep our structured logging format consistent. A consistent format will allow us to query the logging information. Then, we can basically search for specific patterns and issues using our centralized logging tool. Another key aspect of centralized logging within a microservice architecture is to make distributed transactions more traceable.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using a correlation ID in logging</h1>
                
            
            
                
<p>A correlation ID is a unique ID that is assigned to every transaction. So, when a transaction becomes distributed across multiple services, we can follow that transaction across different services using the logging information. The correlation ID is basically passed from service to service. All services that process that specific transaction receive the correlation ID and pass it to the next service, and so on, so that they can log any events associated with that transaction to our centralized logs. This helps us hugely when we have to visualize and understand what has happened with this transaction across different microservices.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Semantic logging</h1>
                
            
            
                
<p><strong>Event Tracing for Windows</strong> (<strong>ETW</strong>) is a structural logging mechanism where you can store a structured payload with the log entry. This information is generated by event listeners and may include typed metadata about the event. This is merely an example of semantic logging. Semantic logging passes additional data along with the log entry so that the processing system can get the context structured around the event. Hence, semantic logging is also referred to as structured logging or typed logging. </p>
<p>For more information, refer to <a href="https://docs.microsoft.com/en-us/windows-hardware/drivers/devtest/event-tracing-for-windows--etw-">https://docs.microsoft.com/en-us/windows-hardware/drivers/devtest/event-tracing-for-windows--etw-</a></p>
<p>As an example, an event that indicates an order was placed can generate a log entry that contains the number of items as an integer value, the total value as a decimal number, the customer identifier as a long value, and the city for delivery as a string value. An order monitoring system can read the payload and easily extract the individual values. ETW is the standard, shipped feature with Windows.</p>
<p>In Azure Cloud, it is possible to get your log data source from ETW. The Semantic Logging Application Block developed by Microsoft's patterns and practices team is an example of a framework that makes comprehensive logging easier. When you write events to the custom event source, the Semantic Logging Application Block detects this and allows you to write the event to other logging destinations, such as a disk file, database, email message, and more. You can use the Semantic Logging Application Block in Azure applications that are written in .NET and run on Azure websites, cloud services, and virtual machines.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Monitoring in Azure Cloud</h1>
                
            
            
                
<p>There is no single, off-the-shelf solution or offering in Azure, or for that matter any cloud provider, to the monitoring challenges presented by microservices. Interestingly enough, there are not too many open source tools available that can work with .NET-based microservices.</p>
<p>We are utilizing Microsoft Azure Cloud and cloud services for building our microservices, so it is useful to look for the monitoring capability it comes with. If you are looking to manage approximately a couple of hundred microservices, you can utilize a custom monitoring solution (mostly interweaving PowerShell scripts) based on a Microsoft Azure-based solution.</p>
<p>We will be primarily focusing on the following logging and monitoring solutions:</p>
<ul class="ul-list">
<li>Microsoft Azure Diagnostics: This helps in collecting and analyzing resources through resource and activity logs.</li>
<li>Application Insights: This helps in collecting all of the telemetry data about our microservices and analyzing them. This is a framework-based approach for monitoring.</li>
<li>Log Analytics: Log Analytics analyzes and displays data and provides scalable querying capability over collected logs.</li>
</ul>
<p>Let's look at these solutions from a different perspective. This perspective will help us visualize our Azure-based microservice monitoring solution. A microservice is composed of the following:</p>
<ul class="ol-list">
<li>Infrastructure layer: A virtual machine or an application container (for example, Docker container)</li>
<li class="">Application stack layer: Constitutes the operating system, .NET CLR, and the microservice application code</li>
</ul>
<p>Each of these layer components can be monitored as follows:</p>
<ul class="ul-list">
<li>Virtual machine: Using Azure Diagnostics Logs</li>
<li>Docker containers: Using container logs and Application Insights or a third-party container monitoring solution, such as cAdvisor, Prometheus, or Sensu</li>
<li>Windows operating system: Using Azure Diagnostics Logs and Activity Logs</li>
<li>A microservice application: Using Application Insights</li>
<li class="">Data visualization and metric monitoring: Using Log Analytics or third-party solutions, such as Splunk or ELK stack</li>
</ul>
<p>Various Azure services come with an activity ID in their log entries. This activity ID is a unique GUID assigned for each request, which can be utilized as a correlation ID during log analysis.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Microsoft Azure Diagnostics</h1>
                
            
            
                
<p>Azure diagnostics logs give us the ability to collect diagnostic data for a deployed microservice. We can also use a diagnostic extension to collect data from various sources. Azure Diagnostics is supported by web and worker roles, Azure virtual machines, and all Azure App services. Other Azure services have their own separate diagnostics.</p>
<p>Enabling Azure diagnostics logs and exploring various settings in the Azure app service is easy and available as a toggle switch, as shown in the following screenshot:</p>
<div><img class="image-border" height="362" src="img/2c18c643-fb74-4a97-a371-5427ddef4ec5.png" width="379"/></div>
<p>Azure diagnostics can collect data from the following sources:</p>
<ul class="ul-list">
<li class="">Performance counters</li>
<li class="">Application logs</li>
<li>Windows event logs</li>
<li>.NET event sources</li>
<li>IIS logs</li>
<li class="">Manifest-based ETW</li>
<li>Crash dumps</li>
<li>Custom error logs</li>
<li class="">Azure diagnostic infrastructure logs</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Storing diagnostic data using Azure storage</h1>
                
            
            
                
<p>Azure diagnostics logs are not permanently stored. They are rollover logs, that is, they are overwritten by newer ones. So, if we want to use them for any analysis work, we have to store them. Azure diagnostics logs can be either stored in a file system or transferred via FTP; better still, it can be stored in an Azure storage container.</p>
<p>There are different ways to specify an Azure storage container for diagnostics data for the specified Azure resource (in our case, microservices hosted on the Azure app service). These are as follows:</p>
<ul class="ul-list">
<li class="">CLI tools</li>
<li class="">PowerShell</li>
<li class="">Azure Resource Manager</li>
<li class="">Visual Studio 2017 with Azure SDK 2.9 or later</li>
<li class="">Azure portal</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Using Azure portal</h1>
                
            
            
                
<p>The following screenshot depicts the Azure storage container provisioned through the Azure portal:</p>
<div><img class="image-border" height="396" src="img/be4d9ee8-fc83-4d1a-845c-02b930ab472b.png" width="828"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Specifying a storage account</h1>
                
            
            
                
<p>Another way to specify the storage account for storing application-specific diagnostic data is by specifying the storage account in the <kbd>ServiceConfiguration.cscfg</kbd> file. This is also convenient as during development time itself, you can specify the storage account. It is also possible to specify an altogether different storage account during development and production. The Azure storage account might also be configured as one of the dynamic environment variables during the deployment process.</p>
<p>The account information is defined as a connection string in a configuration setting. The following example shows the default connection string created for a new microservice project in Visual Studio:</p>
<div><div><div><div><div><div><pre>&lt;ConfigurationSettings&gt;<br/>  &lt;Setting name="Microsoft.WindowsAzure.Plugins.<br/>  Diagnostics.ConnectionString" value="UseDevelopmentStorage=true" /&gt;<br/>&lt;/span&gt;&lt;/ConfigurationSettings&gt;</pre></div>
</div>
</div>
</div>
<p>You can change this connection string to provide account information for an Azure storage account.</p>
</div>
</div>
<p>Now, let's see how Azure storage stores the diagnostic data. All the log entries are stored in either a blob or table storage container. The storage choice can be specified while we create and associate the Azure storage container.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Azure storage schema for diagnostic data</h1>
                
            
            
                
<p>The structure of Azure table storage for storing diagnostic data is as follows:</p>
<p>If the storage is in the form of tables, we will see the following tables schema:</p>
<ul>
<li>WadLogsTable: This table stores the log statements written during code execution, using the trace listener.</li>
<li>WADDiagnosticInfrastructureLogsTable: This table specifies the diagnostic monitor and configuration changes.</li>
<li>WADDirectoriesTable: This table includes the directories that the diagnostic monitor is monitoring. This includes IIS logs, IIS-failed request logs, and custom directories. The location of the blob log file is specified in the container field and the name of the blob is in the RelativePath field. The AbsolutePath field indicates the location and the name of the file as it existed on the Azure virtual machine.</li>
<li>WADPerformanceCountersTable: This table contains data related to the configured performance counters.</li>
<li>WADWindowsEventLogsTable: This table contains Windows' event tracing log entries.</li>
</ul>
<p>For a blob storage container, the diagnostic storage schema is as follows:</p>
<ul class="ul-list">
<li>wad-control-container: This is only for SDK 2.4 and previous versions. It contains the XML configuration files that control Azure diagnostics.</li>
<li>wad-iis-failedreqlogfiles: This contains information from the IIS-failed request logs.</li>
<li>wad-iis-logfiles: This contains information about IIS logs.</li>
<li class="">custom: This is a custom container based on the configuring directories that are monitored by the diagnostic monitor. The name of this blob container will be specified in WADDirectoriesTable.</li>
</ul>
<p>An interesting fact to note here is that the WAD suffix, which can be seen on these container tables or blobs, comes from Microsoft Azure Diagnostics's previous product name, which is Windows Azure Diagnostics.</p>
<p>You can use <em>Cloud Explorer</em> from Visual Studio to explore the stored Azure diagnostics data.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Introduction of Application Insights</h1>
                
            
            
                
<p>Application Insights is an <strong>application performance management</strong> (<strong>APM</strong>) offering from Microsoft. It is a useful service offering for monitoring the performance of .NET-based microservices. It is useful for understanding the internal, operational behavior of individual microservices. Instead of just focusing on detecting and diagnosing issues, it will tune the service performance and understand the performance characteristics of your microservice. It is an example of the framework-based approach to monitoring. What that means is that during the development of a microservice, we will add the Application Insights package to the Visual Studio solution of our microservice. This is how Application Insights instruments your microservice for telemetry data. This might not always be an ideal approach for every microservice; however, it comes in handy if you have not given any good, thorough thought to monitoring your microservices. This way, monitoring comes out-of-the-box with your service.</p>
<p>With the help of Application Insights, you can collect and analyze the following types of telemetry data types:</p>
<ul class="ul-list">
<li class="">HTTP request rates, response times, and success rates</li>
<li>Dependency (HTTP and SQL) call rates, response times, and success rates</li>
<li>Exception traces from both server and client</li>
<li>Diagnostic log traces</li>
<li>Page view counts, user and session counts, browser load times, and exceptions</li>
<li>AJAX call rates, response times, and success rates</li>
<li>Server performance counters</li>
<li>Custom client and server telemetry</li>
<li>Segmentation by client location, browser version, OS version, server instance, custom dimensions, and more</li>
<li>Availability tests</li>
</ul>
<p>Along with the preceding types, there are associated diagnostic and analytics tools available for alerting and monitoring with various different customizable metrics. With its own query language and customizable dashboards, Application Insights forms a good monitoring solution for .NET microservices.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Other microservice monitoring solutions</h1>
                
            
            
                
<p>Now let's look at some of the popular monitoring solutions that can be used to build a custom microservice monitoring solution. Obviously, these solutions do not come out of-the-box; however, they are definitely time-tested by the open source community and can be easily integrated within .NET-based environments.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">A brief overview of the ELK stack</h1>
                
            
            
                
<p class="">As we saw, one of the fundamental tools for monitoring is logging. For microservices, there will be an astounding number of logs generated that are sometimes not even comprehensible to humans. The ELK stack (also referred to as the elastic stack) is the most popular log management platform. It is also a good candidate for microservice monitoring because of its ability to aggregate, analyze, visualize, and monitor. The ELK stack is a toolchain that includes three distinct tools, namely Elasticsearch, Logstash, and Kibana. Let's look at them one by one to understand their role in the ELK stack.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Elasticsearch</h1>
                
            
            
                
<p>Elasticsearch is a full-text search engine based on the Apache Lucene library. The project is open source and developed in Java. Elasticsearch supports horizontal scaling, multitenancy, and clustering approaches. The fundamental element of Elasticsearch is its search index. This index is stored in forms of JSON internally. A single Elasticsearch server stores multiple indexes (each index represents a database), and a single query can search data with multiple indexes.</p>
<p class="">Elasticsearch can really provide near real-time searches and can scale with very low latency. The search and results programming model is exposed through the Elasticsearch API and available over HTTP.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Logstash</h1>
                
            
            
                
<p class="">Logstash plays the role of a log aggregator in the ELK stack. It is a log aggregation engine that collects, parses, processes, and persists the log entries in its persistent store. Logstash is extensive due to its data-pipeline-based architecture pattern. It is deployed as an agent, and it sends the output to Elasticsearch.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Kibana</h1>
                
            
            
                
<p>Kibana is an open source data visualization solution. It is designed to work with Elasticsearch. You use Kibana to search, view, and interact with the data stored in the Elasticsearch indices.</p>
<p>It is a browser-based web application that lets you perform advanced data analysis and visualize your data in a variety of charts, tables, and maps. Moreover, it is a zero-configuration application. Therefore, it neither needs any coding nor additional infrastructure after the installation.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Splunk</h1>
                
            
            
                
<p>Splunk is one of the best commercial log management solutions. It can handle terabytes of log data very easily. Over time, it has added many additional capabilities and is now recognized as a full-fledged leading platform for operational intelligence. Splunk is used to monitor numerous applications and environments.</p>
<p>It plays a vital role in monitoring any infrastructure and application in real time and is essential for identifying issues, problems, and attacks before they impact customers, services, and profitability. Splunk's monitoring abilities, specific patterns, trends and thresholds, and so on can be established as events for Splunk to look out for. This is so that specific individuals don't have to do this manually.</p>
<p>Splunk has an alerting capability included in its platform. It can trigger alert notifications in real time so that appropriate action can be taken to avoid application or infrastructure downtime.</p>
<p>Based on a trigger of alert and action configured, Splunk can:</p>
<ul class="ul-list">
<li class="">Send an email</li>
<li class="">Execute a script or trigger a runbook</li>
<li class="">Create an organizational support or action ticket</li>
</ul>
<p>Typically, Splunk monitoring marks might include the following:</p>
<ul class="ul-list">
<li class="">Application logs</li>
<li class="">Active Directory changes event data</li>
<li class="">Windows event logs</li>
<li class="">Windows performance logs</li>
<li class="">WMI-based data</li>
<li class="">Windows registry information</li>
<li class="">Data from specific files and directories</li>
<li>Performance monitoring data</li>
<li class="">Scripted input to get data from the APIs and other remote data interfaces and message queues</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Alerting</h1>
                
            
            
                
<p>As with any monitoring solution, Splunk also has alert functionalities. It can be configured to set an alert based on any real-time or historical search patterns. These alert queries can be run periodically and automatically, and alerts can be triggered by the results of these real-time or historical queries.</p>
<p class="">You can base your Splunk alerts on a wide range of threshold-and trend-based situations, such as conditions, critical server or application errors, or threshold amounts of resource utilization.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Reporting</h1>
                
            
            
                
<p>Splunk can report on alerts that have been triggered and executed as well as if they meet certain conditions. Splunk's alert manager can be used to create a report based on the preceding alert data.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>Debugging and monitoring of microservices is not simple; it's a challenging problem. We have used the word <em>challenging</em> on purpose: there is no silver bullet for this. There is no single tool that you can install that works like magic. However, with Azure Diagnostics and Application Insights, or with ELK stack or Splunk, you can come up with solutions that will help you solve microservice monitoring challenges. Implementing microservice monitoring strategies, such as application/system monitoring, real user monitoring, synthetic transactions, centralized logging, semantic logging block, and implementation of correlation ID throughout transactional HTTP requests, is a helpful way to monitor microservice implementations.</p>
<p>In the next chapter, we will see how we can scale microservices, and the solutions and strategies for scaling microservice solutions.  </p>


            

            
        
    </body></html>