- en: Performance Analysis and Monitoring
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能分析和监控
- en: As you continue to develop your knowledge of building network software, we cannot
    overlook the key tasks of monitoring and performance tuning. Those two responsibilities
    will be the focus of this chapter, as we look at the tools that are available
    in .NET Core applications for monitoring and testing the performance and stability
    of your application. We'll be looking at the tools that are available to developers
    for putting your application under heavy load in controlled environments and observing
    its stability over time. We'll look at some naive logging and monitoring approaches,
    and consider how we can strengthen those approaches using some of the features
    of .NET Core.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 随着你继续发展构建网络软件的知识，我们不能忽视监控和性能调优的关键任务。这两个责任将是本章的重点，我们将探讨 .NET Core 应用程序中可用于监控和测试应用程序性能和稳定性的工具。我们将探讨开发者可用于在受控环境中对应用程序施加重负载并观察其稳定性的工具。我们将探讨一些简单的日志记录和监控方法，并考虑如何使用
    .NET Core 的一些功能来加强这些方法。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Identifying performance bottlenecks in your network architecture, and designing
    to minimize them
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别网络架构中的性能瓶颈，并设计以最小化它们
- en: Identifying end-to-end performance testing and reporting strategies
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别端到端性能测试和报告策略
- en: Establishing robust and resilient performance monitoring with C#
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 C# 建立稳健和有弹性的性能监控
- en: Technical requirements
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we''ll be writing a number of samples to demonstrate various
    aspects of performance tracing and monitoring, all of which can be found here:
    [https://github.com/PacktPublishing/Hands-On-Network-Programming-with-CSharp-and-.NET-Core](https://github.com/PacktPublishing/Hands-On-Network-Programming-with-CSharp-and-.NET-Core/tree/master/Chapter%2016)/tree/master/Chapter
    16.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将编写一些示例来展示性能跟踪和监控的各个方面，所有这些都可以在这里找到：[https://github.com/PacktPublishing/Hands-On-Network-Programming-with-CSharp-and-.NET-Core](https://github.com/PacktPublishing/Hands-On-Network-Programming-with-CSharp-and-.NET-Core/tree/master/Chapter%2016)/tree/master/Chapter
    16。
- en: Check out the following video to see the code in action: [http://bit.ly/2HYmD5r](http://bit.ly/2HYmD5r)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 观看以下视频以查看代码的实际效果：[http://bit.ly/2HYmD5r](http://bit.ly/2HYmD5r)
- en: 'To work with this code, you''ll want to use either of our trusty code editors:
    Visual Studio or Visual Studio Code. We''ll also be using the REST clients you''ve
    come to know and love, so make sure you''ve got either PostMan installed ([https://www.getpostman.com/downloads/](https://www.getpostman.com/downloads/))
    or the Insomnia REST client ([https://insomnia.rest/](https://insomnia.rest/)).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用此代码，您希望使用我们信任的代码编辑器之一：Visual Studio 或 Visual Studio Code。我们还将使用您所熟悉和喜爱的 REST
    客户端，所以请确保您已安装 PostMan ([https://www.getpostman.com/downloads/](https://www.getpostman.com/downloads/))
    或 Insomnia REST 客户端 ([https://insomnia.rest/](https://insomnia.rest/))。
- en: Network performance analysis
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络性能分析
- en: As you start to build more complicated distributed software systems, you begin
    to lose the granular control and fine-tuning you had over smaller, more isolated
    software projects. Each new network interaction introduces higher chances of systemic
    failure, reduces visibility on the source of bugs, and muddies the waters when
    searching for performance bottlenecks. The best way to mitigate these impacts
    is to get ahead of them, and design your distributed system with performance monitoring
    in mind from the start. So, how do you do that? What are the key metrics and interactions
    with which you should be concerned? What support does .NET Core provide to you?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始构建更复杂的分布式软件系统时，你开始失去对较小、更独立的软件项目所拥有的细粒度控制和微调。每个新的网络交互都增加了系统故障的风险，减少了查找错误来源的可见性，并在寻找性能瓶颈时使问题复杂化。减轻这些影响的最佳方式是提前做好准备，并从一开始就将性能监控纳入你的分布式系统设计中。那么，你该如何做呢？你应该关注哪些关键指标和交互？.NET
    Core为你提供了哪些支持？
- en: End-to-end performance impacts
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 端到端性能影响
- en: Imagine, if you will, that you're responsible for a cloud-hosted application
    suite. It includes a half-dozen microservices, each with a dependency on a half-dozen
    more. On top of that, each parallel resource is deployed behind a load-balancing
    network gateway responsible for routing requests to the server with the lowest
    current load.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，如果你负责一个云托管的应用程序套件。它包括半打微服务，每个微服务都依赖于另外半打服务。更不用说，每个并行资源都部署在负载均衡网络网关后面，该网关负责将请求路由到当前负载最低的服务器。
- en: Now, imagine that each component of that system was written almost entirely
    in isolation from the rest of the components. Your team of engineers was focused
    on the design principle of the separation of concerns, and so they thoroughly separated
    those concerns. Every data store was given its own, limited public API, and no
    other system was permitted direct access to its underlying database. All of the
    aggregation APIs are responsible for accessing each system of record to produce
    domain models relevant to your business use case. There's almost no overlap in
    responsibility.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象一下，该系统的每个组件几乎都是独立于其他组件编写的。你的工程师团队专注于关注点分离的设计原则，因此他们彻底地分离了这些关注点。每个数据存储都分配了自己的、有限的公共API，没有其他系统被允许直接访问其底层数据库。所有聚合API都负责访问每个记录系统，以产生与你的业务用例相关的领域模型。在责任上几乎没有重叠。
- en: Now, we'll generously imagine that your engineers were also disciplined in their
    testing strategies. Each service included a full suite of unit tests with nearly
    100% code coverage. Every unit test was fully isolated with well-defined mocks
    for each of its dependencies. Your engineers were so disciplined and thorough
    that those mocks were configured to return every possible permutation of valid
    and exceptional responses that the service they were mocking might return in the
    live system. The data contracts for each service in your ecosystem was well-defined,
    and all changes were well tracked and accounted for by any dependent systems.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们慷慨地假设你的工程师们在测试策略上也非常自律。每个服务都包含了一套完整的单元测试，代码覆盖率接近100%。每个单元测试都是完全隔离的，并为每个依赖项定义了良好的模拟。你的工程师们如此自律和细致，以至于那些模拟被配置为返回服务在实时系统中可能返回的每个可能的有效和异常响应的组合。你生态系统中的每个服务的数据契约都定义得很好，所有变更都得到了依赖系统的良好跟踪和记录。
- en: With all of this work diligently documented, maintained, and tested, you're
    finally ready to deploy version 1 of your application. Now, I want you to imagine
    that the first time someone tries to query your application, the entire ecosystem
    slows to a crawl, and the response doesn't return for a full 25 seconds.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些工作都得到仔细记录、维护和测试之后，你终于准备好部署你应用程序的版本1。现在，我想让你想象一下，当有人第一次尝试查询你的应用程序时，整个生态系统变得缓慢，响应需要整整25秒才返回。
- en: This may seem like an absurd case, but I have seen almost this exact scenario
    actually happen with a team of inexperienced cloud architects deploying their
    first fully distributed microservice-based applications. So, what went wrong in
    this imagined scenario of ours, and how can we avoid it in practice?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来像是一个荒谬的案例，但我确实见过几乎完全相同的场景，那是经验不足的云架构师团队在部署他们的第一个完全分布式的基于微服务应用时发生的。那么，在我们这个假设的场景中，到底出了什么问题，我们如何在实践中避免这种情况呢？
- en: Compounding latency
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 累积延迟
- en: It should be obvious at this point, but in this particular scenario, the primary
    culprit for the under-performance was isolation. With each service developed and
    tested entirely in a silo, there was no way that any engineers could measure the
    full impact of all of their backend dependencies. By always assuming a best-case
    scenario of near-zero latency with their upstream dependencies, the developers
    created unrealistic testing scenarios for their unit tests.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这一点应该是显而易见的，但在这种特定场景中，性能不佳的主要原因是隔离。由于每个服务都是完全在隔离的环境中开发和测试的，因此工程师们无法衡量所有后端依赖的全面影响。通过始终假设与上游依赖项的接近零延迟的最佳情况，开发者为他们单元测试创造了不切实际的测试场景。
- en: Integration and end-to-end testing are absolutely pivotal to designing a system
    that can withstand the strains and inconsistencies of a network-based hosting
    environment. With every new feature or microservice developed, the developers
    in our not-so-hypothetical development scenario should have deployed their solutions
    to an environment as close to their production configuration as possible. They
    should have been identifying bottlenecks in performance as they were implemented
    and mitigated those impacts.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 集成和端到端测试对于设计能够承受基于网络托管环境的压力和不一致性的系统至关重要。在我们这个并非完全假设的开发场景中，随着每个新功能或微服务的开发，开发者应该将他们的解决方案部署到一个尽可能接近生产配置的环境。他们应该在实施过程中识别性能瓶颈，并减轻这些影响。
- en: One of the more challenging aspects of optimizing network software is that your
    single application is almost never the last stop for a user's interactions with
    it. Typically, your .NET Core services are being called by a user's browser, or
    another application service, at the least. Moreover, it's not uncommon for them
    to also rely on other services accessed through network protocols such as HTTP
    or TCP. What the developers in this scenario didn't realize, and what you should
    always be mindful of as you build more and more complicated distributed systems,
    is that each network hop in your chain of dependencies introduces latency and
    risk of failure.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 优化网络软件的一个更具挑战性的方面是，你的单个应用程序几乎永远不会是用户与之交互的最后一站。通常，你的.NET Core服务至少会被用户的浏览器或其他应用程序服务调用。此外，它们也常常依赖于通过HTTP或TCP等网络协议访问的其他服务。在这个场景中，开发者没有意识到，而你作为构建越来越复杂的分布式系统时应该始终注意到的，是依赖链中的每个网络跳步都会引入延迟和失败的风险。
- en: With each upstream dependency, your application's latency is increased by the
    total average latency of your upstream dependency. Likewise, if your upstream
    dependency has an upstream dependency of its own, *its *latency will be whatever
    operational latency its own operations create, increased by the total average
    latency of *its *upstream dependency. This attribute of compounding the latency
    of networked systems is incredibly important to keep in mind whenever you're writing
    a network service that depends on another network service. This is why it's common
    with more experienced cloud architects to see strict enforcement of a three-tiered
    architectural model for the flow of request resolution.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 每个上游依赖都会使你的应用程序的延迟增加其上游依赖的总平均延迟。同样，如果你的上游依赖本身也有上游依赖，那么它的延迟将是其自身操作产生的操作延迟加上其上游依赖的总平均延迟。这种将网络系统延迟复合化的属性，在编写依赖于其他网络服务的网络服务时，是一个非常重要的属性需要记住。这就是为什么经验丰富的云架构师通常会对请求解析流程实施严格的分层架构模型。
- en: A three-tiered architecture
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 三层架构
- en: 'To minimize the impact of compounding latency, it''s not uncommon to minimize
    vertical dependencies in an architecture. For our purposes, we can think of horizontal
    dependencies as being the full suite of upstream dependencies that originate from
    the same source. Meanwhile, vertical dependencies describe any upstream dependencies
    that, themselves, have additional upstream dependencies:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化复合延迟的影响，在架构中减少垂直依赖性是很常见的。就我们的目的而言，我们可以将水平依赖性视为源自同一来源的完整上游依赖集。同时，垂直依赖性描述了任何具有额外上游依赖的上游依赖：
- en: '![](img/5f4b7da5-3327-488f-8c34-bf5a288435af.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5f4b7da5-3327-488f-8c34-bf5a288435af.png)'
- en: In this diagram, Architecture A has what we would call a three-tiered architecture.
    That is to say that there are at most three tiers of services encapsulating the
    entire vertical dependency graph of application interactions. This is an ideal
    organizational structure for cloud-hosted services. Since most hosting contexts
    (and, indeed, .NET Core itself) support a broad number of parallel requests being
    processed, the latency increase of horizontal dependencies within a given system
    will only ever be as large as the slowest of all of the horizontal dependencies.
    This is not entirely dissimilar from the benefits gained by parallelizing asynchronous
    operations, as we discussed in the section *Picking up the pace - multithreading
    data processing*, section of [Chapter 6](b5d28c0a-6e7c-4547-855d-e6c6d1842bd6.xhtml), *Streams,
    Threads, and Asynchronous Data Transfer*.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图中，架构A具有我们所说的三层架构。也就是说，最多有三层服务封装整个应用程序交互的垂直依赖图。这是云托管服务的理想组织结构。由于大多数托管环境（实际上，.NET
    Core 本身）支持处理大量的并行请求，因此给定系统内水平依赖性的延迟增加将永远不会超过所有水平依赖性中最慢的那个。这与我们在第6章的“加快速度 - 多线程数据处理”部分讨论的并行化异步操作带来的好处并不完全不同。[第6章](b5d28c0a-6e7c-4547-855d-e6c6d1842bd6.xhtml)，“流、线程和异步数据传输”。
- en: In a strict three-tiered architecture, there can only be at most two network
    hops between a user and any piece of data that is ultimately presented to them.
    The three tiers are easily conceptualized, and should be familiar to you after
    our discussion of the MVC design paradigm back in [Chapter 9](e93c024e-3366-46f3-b565-adc20317e6ec.xhtml),
    *HTTP in .NET*. The first tier is the user-interaction layer, which just defines
    any system or mechanism with which an external user is able to access the data
    or processes contained within your ecosystem. Next is the aggregation tier, or
    the domain tier. This is where all of the business logic of the parent user-interaction
    is performed on any data the user is interested in seeing. Last, but not least,
    is the data tier. In modern cloud systems, these are typically HTTP-based APIs
    that expose an internal database that serves as the system of record for an enterprise
    dataset.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在严格的分层架构中，用户与最终呈现给他们的任何数据之间最多只能有两个网络跳数。三层架构很容易理解，在我们讨论MVC设计范式（[第9章](e93c024e-3366-46f3-b565-adc20317e6ec.xhtml)，*HTTP
    in .NET*）之后，你应该已经熟悉了。第一层是用户交互层，它定义了任何外部用户能够访问你生态系统中的数据或过程的系统或机制。接下来是聚合层，或领域层。这是在用户感兴趣看到的所有数据上执行父用户交互的所有业务逻辑的地方。最后但同样重要的是，是数据层。在现代云系统中，这些通常是基于HTTP的API，它们公开一个内部数据库，作为企业数据集的记录系统。
- en: When the three-tiered paradigm is well enforced, no system is allowed to interact
    with a database except through its well-defined APIs. Meanwhile, if a UI needs
    the business logic defined in multiple aggregation services, it must either access
    all aggregation-tier systems itself, thus increasing its horizontal dependency
    map (which is considered acceptable) or a new aggregation service must be written
    that duplicates the work of the others. No aggregation system should ever call
    any other aggregation system. Doing so increases the vertical dependencies of
    that workflow and violates the three-tiered paradigm.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当三层架构得到良好执行时，不允许任何系统通过其定义良好的API之外的方式与数据库交互。同时，如果一个UI需要定义在多个聚合服务中的业务逻辑，它必须要么自己访问所有聚合层系统，从而增加其水平依赖图（这被认为是可接受的），或者必须编写一个新的聚合服务来重复其他服务的任务。任何聚合系统都不应该调用任何其他聚合系统。这样做会增加该工作流程的垂直依赖性，并违反三层架构。
- en: With this in mind, it should be easy to see why one of the most reliable ways
    to ensure high performance in your applications and implement a manageable monitoring
    system is by minimizing your vertical dependencies. If we can limit our search
    for bottlenecks to as few vertical tiers as possible, our ability to identify
    issues and resolve them can become almost trivial. And these compounding latency
    interactions only become more relevant as your software gets lower and lower in
    the network stack. Implementing a series of gateways or firewalls for any traffic
    moving between your local network and the wider internet will impact all traffic
    moving through your enterprise. Minimizing the vertical dependencies of such a
    system would need to be your first priority if you're to have any hope of success
    at minimizing the latency your gateway will introduce.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，应该很容易理解为什么确保应用程序高性能并实施可管理的监控系统最可靠的方法之一是通过最小化垂直依赖。如果我们能将瓶颈搜索限制在尽可能少的垂直层级中，我们识别问题和解决问题的能力几乎可以变得非常简单。随着你的软件在网络堆栈中的层级越来越低，这些累积的延迟交互变得更加相关。为任何在本地网络和更广泛的互联网之间移动的流量实施一系列网关或防火墙将影响企业中通过的所有流量。如果你希望有希望最小化网关引入的延迟，那么最小化此类系统的垂直依赖性应该是你的首要任务。
- en: Performance under stress
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 压力下的性能
- en: Another common issue that arises when an insufficiently tested system is first
    deployed to a production environment is the application's inability to handle
    the load placed upon it. Even a strictly enforced three-tiered architecture can't
    minimize the impact of multiple responses left unprocessed due to an extremely
    high network load. The capacity for high volumes of network requests to utterly
    cripple a network resource is so great that it's actually the basis of an extremely
    well-known software attack known as a **Dedicated Denial of Service** (**DDoS**)
    attacks. In these sorts of attack, a distributed network of malware sends out
    a coordinated barrage of simple network requests to a single host. The volume
    of incoming requests absolutely destroys the host's ability to continue responding
    to them, locking up the resource for legitimate users, and even destabilizing
    the host's OS and physical infrastructure.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个未经充分测试的系统首次部署到生产环境时，一个常见的问题就是应用程序无法处理施加在其上的负载。即使是严格执行的三层架构也无法最小化由于极高的网络负载而未处理的多个响应的影响。大量网络请求能够完全摧毁网络资源的能力如此之大，以至于这实际上成为了一种极其著名的软件攻击的基础，这种攻击被称为**专用拒绝服务**（**DDoS**）攻击。在这种攻击中，一个分布式的恶意软件网络向单个主机发送协调一致的简单网络请求。
    incoming requests absolutely destroys the host's ability to continue responding
    to them, locking up the resource for legitimate users, and even destabilizing
    the host's OS and physical infrastructure.
- en: While a DDoS attack might be an extreme and relatively rare example, the same
    effects can be felt on a system with insufficient bandwidth and horizontal scalability
    for handling a high volume of legitimate simultaneous requests. The challenge
    here is that it's not always possible to know ahead of time what kind of traffic
    your system will encounter when it's deployed.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然DDoS攻击可能是一个极端且相对罕见的例子，但同样的影响也可能出现在处理大量合法同时请求时带宽和水平可扩展性不足的系统上。这里的挑战在于，在部署时，你并不总是能提前知道你的系统将遇到什么样的流量。
- en: If you're writing an enterprise web service for a specific set of internal business
    users, you can probably define your operational parameters pretty clearly. Conducting
    a simple headcount, combined with user interviews to determine how frequently
    a given user will interact with your system, can give you a high degree of confidence
    that you know exactly how much traffic you can reasonably anticipate in a given
    day. However, if your application is being written for a general release to the
    broader public, then it could be impossible to know beforehand just how many users
    will hit your service in a given day.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在为特定的一组内部商业用户编写企业级网络服务，你很可能可以非常清楚地定义你的操作参数。进行简单的员工人数统计，结合用户访谈来确定特定用户将多久与你的系统交互一次，可以给你很高的信心，确保你知道在给定的一天里你可以合理地预期多少流量。然而，如果你的应用程序是为公开发布而编写的，那么事先知道在给定的一天里会有多少用户访问你的服务可能是不可能的。
- en: In the sorts of scenarios where you can't reasonably determine your maximum
    potential network traffic prior to experiencing high volumes of requests, then
    you will at least want to know roughly how much traffic your application can handle.
    This is where load testing comes in. With load testing, you should be targeting
    your best guess at the worst-case scenario for network traffic against your system.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在那些在遇到大量请求之前无法合理确定最大潜在网络流量的场景中，你至少想知道你的应用程序可以处理多少流量的大致情况。这就是负载测试的作用所在。通过负载测试，你应该针对网络流量对你的系统可能造成的最坏情况做出最佳猜测。
- en: Once you determine that maximum potential load, you execute a series of tests
    that generate as many interactions as possible with your system, up to and including
    your pre-determined maximum. To be truly valuable, your tests should be run on
    an instance of your software that's deployed as near to your production configuration
    as possible. Over the course of the tests, you should be logging and monitoring
    response times, and any exception responses. By the time the tests are complete,
    if they were designed well, you should have a robust set of metrics for how much
    traffic your current infrastructure can reasonably handle before it fails, and
    just what it looks like when that failure does finally occur.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你确定了最大潜在负载，你将执行一系列测试，尽可能多地与你的系统进行交互，包括你预定的最大值。为了真正有价值，你的测试应该在尽可能接近你的生产配置的软件实例上运行。在整个测试过程中，你应该记录和监控响应时间和任何异常响应。当测试完成时，如果设计得当，你应该有一套稳健的指标，这些指标可以告诉你当前基础设施在失败之前可以合理处理多少流量，以及当失败最终发生时它看起来是什么样子。
- en: Performance monitoring
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能监控
- en: Each of the performance bottlenecks and risks I've just discussed can be mitigated
    with good design and robust testing strategies. However, on distributed systems,
    failure is inevitable. As such, the risk of failure we seek to minimize with the
    testing and design of your system can never fully be eliminated. However, by using
    the results of our performance tests as a guide, we can minimize the impact of
    that inevitable failure. To do this, we'll need to implement a robust system of
    monitoring the health and availability of our services.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我刚才讨论的每个性能瓶颈和风险都可以通过良好的设计和稳健的测试策略来缓解。然而，在分布式系统中，失败是不可避免的。因此，我们通过系统的测试和设计来寻求最小化的失败风险永远无法完全消除。然而，通过使用我们的性能测试结果作为指导，我们可以最小化这种不可避免失败的影响。为此，我们需要实施一个稳健的系统来监控我们服务的健康和可用性。
- en: Naive monitoring strategies
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 天真的监控策略
- en: It's not uncommon for developers to confuse the concept of application monitoring
    with that of logging. And this is not an entirely unreasonable mistake. When done
    properly, a good logging strategy can serve as a relatively low-visibility monitoring
    system. The problem with that approach, though, is that logs are often incredibly *noisy.*
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者将应用程序监控的概念与日志记录混淆的情况并不少见。这并不是一个完全不合理的问题。当正确执行时，一个好的日志策略可以作为一个相对低可见性的监控系统。然而，那种方法的问题在于，日志往往非常**嘈杂**。
- en: Overuse of logging
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 日志记录过度使用
- en: Think for a second of what your initial approach is when catching an exception
    in your code. I'd be willing to bet that a fair number of you start with a basic
    **log and throw** approach. You simply log that an error occurred (usually without
    much else in the way of details or context) and then re-throw the same error,
    or perhaps throw a new error of a type the calling code might be configured to
    respond to.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 想想当你代码中捕获异常时的初始方法。我敢打赌，相当一部分人从基本的**记录并抛出**方法开始。你只是记录发生了错误（通常没有太多细节或上下文），然后重新抛出相同的错误，或者可能抛出一个调用代码可能配置为响应的新错误。
- en: Meanwhile, I'm sure at least a few of my readers have worked in an environment
    where every change in scope was logged. Enter a new method? Log it. Exit the current
    method? Log it. Send out a TCP packet to a host? Log it. Receive a TCP packet?
    Log it. I could keep going, but I'll spare you the frustration.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，我相信至少有几位读者在这样一个环境中工作过，那里的任何范围变化都会被记录下来。引入一个新方法？记录它。退出当前方法？记录它。向主机发送TCP数据包？记录它。接收TCP数据包？记录它。我可以继续说，但我会避免让你感到沮丧。
- en: This log everything approach is surprisingly common in large scale enterprises.
    What most people don't often consider, though, is that the more information you
    log, the less useful those logs become. As your log file grows with messages logging
    trivial changes in scope or context, it becomes increasingly harder to find the
    logs that are actually important. And, believe me, those are the logs you will
    be looking for when you find yourself going through a 200,000 line-long log file.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这种记录一切的方法在大规模企业中非常常见。然而，大多数人很少考虑的是，你记录的信息越多，那些日志就越不有用。随着你的日志文件随着记录范围或上下文中的琐碎变化而增长，找到真正重要的日志变得越来越困难。相信我，当你发现自己正在浏览一个200,000行长的日志文件时，你将需要找到那些日志。
- en: The term for this, if you've never heard of it, is the **signal-to-noise** ratio
    of your logs. The signal, in this ratio, describes the information that has meaning
    to you at the current moment. The noise, on the other hand, describes the information
    surrounding and obscuring your signal, which has no meaning to you at the current
    moment. Disciplined logging is not about logging everything diligently. Disciplined
    logging is about always logging only important things. This discipline should
    be applied to your logs, and any performance or health monitors you use in your
    software.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您从未听说过这个术语，那么这就是您日志的**信号与噪声比**。在这个比率中，信号描述的是您当前时刻有意义的信息。另一方面，噪声描述的是围绕并掩盖您信号的信息，这些信息在当前时刻对您没有意义。纪律性日志不是关于勤奋地记录所有事情。纪律性日志是关于始终只记录重要的事情。这种纪律应该应用于您的日志，以及您在软件中使用的任何性能或健康监控器。
- en: Delayed alerting
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 延迟警报
- en: Another common shortcoming with logging approaches is that the information we
    want to know about our applications isn't presented to us until it's too late.
    Typically, with a logging strategy for performance and health monitoring, the
    logs are written after the failure has occurred. While this is certainly more
    useful than no alerting system at all (when you're only notified about a system
    failure by receiving an angry phone call from a user), it's far from ideal.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的日志方法缺陷是，我们想要了解的应用程序信息直到太晚才呈现给我们。通常，对于性能和健康监控的日志策略，日志是在故障发生后编写的。虽然这当然比完全没有警报系统更有用（当您只能通过接到用户愤怒的电话通知系统故障时），但它远非理想。
- en: You should know about a system outage before anyone else can encounter it. However,
    that kind of visibility is impossible if you're only ever writing to a passive
    log file, and only when an exception has been thrown. With network software, your
    application is likely leveraged by a wide array of other resources in such a way
    that system outages could propagate outward across your network and cripple the
    businesses your software was written to support. So, how can you get out in front
    of those inevitable outages and respond before any of your users can feel an impact?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该在其他人遇到之前了解系统断电。然而，如果您只是被动地写入日志文件，并且只有在抛出异常时才这样做，那么这种可见性是不可能的。在网络软件中，您的应用程序可能被广泛的其他资源以某种方式利用，以至于系统断电可能会传播到您的网络并损害软件旨在支持的业务。那么，您如何能够提前应对那些不可避免的断电，并在任何用户感受到影响之前做出响应？
- en: Establishing proactive monitoring
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建立主动监控
- en: The best way to minimize the impact of an outage to your users is to identify
    when an outage has occurred, or is likely to occur, proactively. To do so, you'll
    need to define a policy for establishing health checks against your system. That
    strategy will have to determine a reasonable frequency for sending requests to
    your system. The frequency you determine will have to strike a balance between
    being frequent enough to have a high probability of detecting an outage before
    the outage could impact a customer, while being infrequent enough to not negatively
    impact the performance of your system under its heaviest load. On top of that,
    you'll need to identify all of the possible signals for an unhealthy system so
    that your monitors are well targeted for the most relevant aspects of your software's
    performance.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最有效地减少对用户造成断电影响的方法是主动识别断电何时发生，或可能发生。为此，您需要为对系统进行健康检查定义一个策略。该策略必须确定向系统发送请求的合理频率。您确定的频率必须在频繁到有高概率在断电影响客户之前检测到断电，以及不频繁到不会在系统最重负载下对系统性能产生负面影响之间取得平衡。除此之外，您还需要识别所有可能的不健康系统信号，以便您的监控器能够针对软件性能的最相关方面进行精准监控。
- en: Defining thresholds
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义阈值
- en: When defining a performance monitoring strategy, the first thing you should
    do is define your application's performance thresholds. These will tell you the
    traffic load, or resource demands, beyond which your application is likely to
    experience failure. For instance, if you've done extensive load testing on your
    application, you will likely have a deep pool of metrics about how much traffic
    your application server can handle. By using network tracing, which will provide
    a record of every request against a listening network application, you can identify
    when your traffic is experiencing a spike, sending alerts before the failure threshold
    is reached.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义性能监控策略时，你应该首先定义你应用程序的性能阈值。这些阈值将告诉你，超出这些阈值，你的应用程序很可能会出现故障。例如，如果你对你的应用程序进行了广泛的负载测试，你可能会有一大堆关于你的应用程序服务器可以处理多少流量的指标。通过使用网络跟踪，它将提供针对监听网络应用程序的每个请求的记录，你可以确定你的流量何时出现峰值，在达到故障阈值之前发送警报。
- en: Additionally, if you have upstream dependencies, you should take note of their
    performance over time, identifying any instance in which your dependencies are
    the source of your system failures. You should seek to leverage the same strategy
    of identifying signals in the content or latency of your dependency responses
    that are likely to result in a system failure for your user. Once you have that
    information, you are well positioned to respond to it.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你有上游依赖项，你应该注意它们随时间的变化，识别出任何你的依赖项成为系统故障来源的实例。你应该寻求利用识别依赖响应内容或延迟中的信号，这些信号可能导致你的用户系统出现故障的策略。一旦你有了这些信息，你就处于一个很好的位置来应对它们。
- en: Proactive health checks
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 积极的健康检查
- en: Once you get to know the potential points or conditions of failure are within
    your system, the robust monitoring strategy tests those points and conditions
    frequently. Services that are designed to actively monitor your system are commonly
    called **watchdogs**. Most cloud-hosting orchestrators or CI/CD software will
    allow you to configure a health-check access point into your software so that
    they can identify when an application has crashed using their own watchdog implementations.
    If your hosting solution provides these features, such as the Azure health verification
    tool, then you should leverage them to their fullest. If your host doesn't provide
    a reliable, regularly scheduled (or configurable schedule) health-check mechanism,
    then I would strongly advise you to roll your own. The impact on your development
    process is minimal, but the benefits are incredible.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你了解了系统内潜在的故障点或条件，鲁棒的监控策略就会频繁地测试这些点和条件。旨在积极监控你的系统的服务通常被称为**看门狗**。大多数云托管编排器或CI/CD软件都会允许你配置一个健康检查访问点到你的软件中，这样它们就可以使用自己的看门狗实现来识别应用程序崩溃的情况。如果你的托管解决方案提供了这些功能，例如Azure健康验证工具，那么你应该充分利用它们。如果你的主机不提供可靠、定期（或可配置的定期）的健康检查机制，那么我强烈建议你自行实现。这对你的开发过程的影响很小，但好处是巨大的。
- en: We'll look at this in our sample code in just a moment, but the typical pattern
    for this kind of proactive health and performance monitoring is to expose an endpoint
    that can be pinged by your hosting provider. That endpoint will then run a series
    of self-diagnostic checks against key potential points of failure, and return
    with either a success (healthy application, no likely failures imminent) or a
    failure (a system is down, or will soon be taken down).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的示例代码中查看这一点，但这种类型的积极健康和性能监控的典型模式是暴露一个可以被你的托管提供商ping的端点。然后，该端点将对关键潜在的故障点运行一系列自我诊断检查，并返回成功（健康的应用程序，没有即将发生的可能故障）或失败（系统已关闭，或很快将被关闭）。
- en: Active messaging and active recovery
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 活跃的消息传递和活跃的恢复
- en: A proactive health monitoring system requires a proactive response. For any
    error state detected by your health-monitoring system, you'll want to define a
    reasonable approach for responding to it. For instance, if your network traffic
    is spiking, a proactive approach may be configuring your health check system to
    automatically provision an additional parallel app server to respond to the additional
    request load. Meanwhile, if your health-check indicates that an upstream dependency
    has come offline, the active recovery system may attempt to restart the app server
    hosting that dependency in an effort to resolve the issue.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一个主动的健康监控系统需要主动的响应。对于你的健康监控系统检测到的任何错误状态，你将希望定义一个合理的响应方法。例如，如果你的网络流量激增，主动的方法可能是配置你的健康检查系统自动提供额外的并行应用服务器来响应额外的请求负载。同时，如果你的健康检查表明上游依赖已经离线，主动恢复系统可能会尝试重新启动托管该依赖的应用服务器，以解决该问题。
- en: Regardless of what your system does to respond (though it should define at least
    some kind of response to a failure), it should absolutely notify any engineers
    who are working on the application. Specifically, it should actively notify engineers.
    While the event should certainly be logged to whatever audit system you use to
    track the reliability of your application, remember that **a** **log** **is not
    an alert**. Logs are inherently passive. For an engineer responsible for a system
    to learn that it's failed from a logged message, that engineer who's has to take
    it upon themself to seek out and check the logs. Meanwhile, with an active messaging
    response, your monitoring platform can be configured to take the contact information
    of any engineers that should know about outages. Then, when an outage has occurred,
    the active monitor can push out notifications to the contact information for every
    engineer that it is configured to notify.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你的系统如何响应（尽管它应该至少定义某种类型的故障响应），它绝对应该通知任何正在处理该应用程序的工程师。具体来说，它应该**主动**通知工程师。虽然事件当然应该记录到你用来跟踪应用程序可靠性的任何审计系统中，但请记住，**日志****不是警报**。日志本质上是被动的。对于负责系统的工程师来说，要从日志消息中了解到系统已失败，该工程师必须自己寻找并检查日志。与此同时，通过主动的消息响应，你的监控平台可以配置为获取任何应该知道故障的工程师的联系方式。然后，当发生故障时，主动监控器可以将通知推送到它配置通知的每个工程师的联系方式。
- en: Performance and health monitoring in C#
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: C#中的性能和健康监控
- en: So, we should have a pretty clear understanding of what a robust and resilient
    performance and health-monitoring system looks like. It should be configured to
    respond to well-defined thresholds, so that potential outages can be spotted and
    mitigated before they occur. It should actively check the state of the application
    instead of passively waiting for user interactions to trigger a system failure.
    Finally, it should proactively respond to a failure once it has been identified,
    notifying anyone who might respond to it, and taking steps to bring the system
    back to a healthy state. So, what does this look like in .NET Core?
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们应该对什么是健壮和有弹性的性能和健康监控系统有一个相当清晰的理解。它应该配置为响应定义良好的阈值，以便在发生之前发现和缓解潜在的故障。它应该主动检查应用程序的状态，而不是被动地等待用户交互来触发系统故障。最后，一旦识别到故障，它应该主动响应，通知任何可能响应的人，并采取措施将系统恢复到健康状态。那么，在.NET
    Core中这看起来是什么样子呢？
- en: An unstable distributed architecture
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个不稳定的分布式架构
- en: 'To demonstrate how we can use performance monitoring to remain aware of the
    health of an unstable system, we''ll first have to build an unstable system. For
    this example, we''ll be using a three-tiered architecture, with PostMan (or Insomnia)
    as our interaction tier, and then two different web APIs to simulate our aggregation
    tier and our data tier. First, we''ll create our aggregation service:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示我们如何使用性能监控来保持对不稳定系统健康状态的了解，我们首先必须构建一个不稳定系统。在这个例子中，我们将使用三层架构，以PostMan（或Insomnia）作为我们的交互层，然后使用两个不同的Web
    API来模拟我们的聚合层和数据层。首先，我们将创建我们的聚合服务：
- en: '[PRE0]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We''ll be looking at this later, since this aggregator will be the application
    whose performance we monitor. For now, though, we''ll need to define an upstream
    dependency. This will be designed to have a negative impact on our aggregation
    service''s performance over time:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后会讨论这个问题，因为这个聚合器将是我们要监控性能的应用程序。不过，现在我们需要定义一个上游依赖。这将设计成随着时间的推移对我们的聚合服务性能产生负面影响：
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We''ll be focusing on the `DataAccessDemo` application to begin with. Here,
    our objective will be to create a destabilized system that contains a mechanism
    for its own recovery. We''ll use this in our Aggregator application, when poor
    performance is detected, to proactively recover from degrading system performance.
    To that end, our `DataAccessDemo` application will be relatively straightforward.
    We''ll provide two endpoints: one will be used as an upstream dependency for our
    aggregator application, and one will be used to recover from degrading performance.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将关注`DataAccessDemo`应用程序。在这里，我们的目标将是创建一个包含自身恢复机制的失稳系统。我们将使用这个系统在我们的聚合器应用程序中检测到性能不佳时，主动从系统性能下降中恢复。为此，我们的`DataAccessDemo`应用程序将相对简单。我们将提供两个端点：一个将用作聚合器应用程序的上游依赖项，另一个将用于从性能下降中恢复。
- en: In practice, it's not uncommon to provide an endpoint that serves as a performance
    management access-point to a live application. It might reinitialize a cache,
    force garbage collection, or dispose of any lingering threads in its thread pool.
    In our case, we'll simply reset a counter to restabilize our application's performance.
    That counter will then be used by our dependency endpoint to enforce an ever-increasing
    latency in the application.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，提供一个作为性能管理访问点的端点是常见的。它可能重新初始化缓存，强制垃圾回收，或者丢弃其线程池中的任何挂起的线程。在我们的情况下，我们只需重置一个计数器以重新稳定我们应用程序的性能。然后，这个计数器将被我们的依赖端点用来强制应用程序中不断增长的延迟。
- en: 'To initiate this, we''ll first define our listening ports to avoid a collision
    with our aggregator API:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了启动这个过程，我们首先定义我们的监听端口，以避免与我们的聚合器API冲突：
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, we''ll define a static class to hold on to our latency counter. This will
    be designed so that every time it''s accessed, it increases the latency for the
    next call. Additionally, to facilitate our recovery endpoint, we''ll provide a
    mechanism for resetting the counter to its initial state, as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将定义一个静态类来保存我们的延迟计数器。这将设计成每次访问时都会增加下一次调用的延迟。此外，为了方便我们的恢复端点，我们将提供一个机制来重置计数器到其初始状态，如下所示：
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, from within our controller, we''ll set a delay to our dependency request,
    after which we''ll return an arbitrary value. Then, we''ll expose a reset endpoint
    to stabilize our application, as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，从我们的控制器中，我们将设置一个延迟到我们的依赖请求，之后我们将返回一个任意值。然后，我们将公开一个重置端点以稳定我们的应用程序，如下所示：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This controller signature will define the vertical dependency of our aggregator
    API. So, Now, let's look at how we'll monitor the degrading performance of this
    application over time.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个控制器签名将定义我们的聚合器API的垂直依赖关系。因此，现在，让我们看看我们将如何监控这个应用程序随时间推移的性能下降。
- en: Performance-monitoring middleware
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能监控中间件
- en: To monitor the health of specific aspects of our application, we'll be creating
    an instance of the `IHealthCheck` **middleware**. This middleware is used to define
    the operations that are necessary for determining the relative health of your
    system. For our purposes, we'll want to define a health-check that confirms that
    the response time for our dependent data service is below two seconds. Once that
    threshold has been hit, we'll consider the system to be degraded. This will allow
    our watchdog to notify us that a restart may be necessary. However, if the service
    response time increases to five seconds, we'll consider it unhealthy, notifying
    our watchdog to initiate a reset.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了监控我们应用程序特定方面的健康状态，我们将创建一个`IHealthCheck`中间件的实例。这个中间件用于定义确定系统相对健康所需的操作。就我们的目的而言，我们希望定义一个健康检查，确认我们依赖的数据服务的响应时间低于两秒。一旦达到这个阈值，我们将认为系统已经退化。这将允许我们的看门狗通知我们可能需要重启。然而，如果服务响应时间增加到五秒，我们将认为它不健康，通知看门狗启动重置。
- en: 'This instance of the `IHealthCheck` middleware will be registered by our system
    in startup, when we add health-checks to our application. And, as with all things
    in .NET Core, adding a health-check is as easy as registering a service and configuring
    your app. So, first, within your `ConfigureServices(IServicesCollection services)`
    method, simply add the following line of code to set up all of the supporting
    classes and extensions that are necessary for using health checks:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`IHealthCheck`中间件的实例将在启动时由我们的系统注册，当我们向应用程序添加健康检查时。而且，正如.NET Core中的所有事情一样，添加健康检查就像注册一个服务和配置你的应用程序一样简单。所以，首先，在你的`ConfigureServices(IServicesCollection
    services)`方法中，只需添加以下代码行来设置所有必要的支持类和扩展，以便使用健康检查：
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, within your `Configure(IApplicationBuilder app, IHostingEnvironment env)`
    method, add the following code:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在你的 `Configure(IApplicationBuilder app, IHostingEnvironment env)` 方法中，添加以下代码：
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This simple registration pattern sets your application up to provide the results
    of all configured health-check middleware whenever a user sends a request to the
    path specified in `UseHealthChecks(<path>)`. Because we haven''t specified any
    specific system checks in our middleware, this endpoint will return nothing of
    interest. To test it, simply navigate to your application''s root host, and append
    the `/health` path. You should see the following output:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这种简单的注册模式将你的应用程序设置为在用户向 `UseHealthChecks(<path>)` 中指定的路径发送请求时提供所有配置的健康检查中间件的结果。因为我们没有在我们的中间件中指定任何特定的系统检查，所以此端点将返回无趣的内容。要测试它，只需导航到你的应用程序的主机根目录，并附加
    `/health` 路径。你应该看到以下输出：
- en: '![](img/307ebfcd-00ce-4798-a1a6-a7ac5da80981.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/307ebfcd-00ce-4798-a1a6-a7ac5da80981.png)'
- en: 'This confirms that our application is responding to health-checks. So, now,
    we need to define the specific check we want to use when validating the health
    of our system by defining our middleware. We''ll start by creating an implementation
    for the `IHealthCheck` interface, which we''ll call `DependencyHealthCheck` for
    simplicity''s sake. This class will contain the definitions for our response thresholds,
    measured in milliseconds, as shown in the following code:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这确认了我们的应用程序正在响应健康检查。因此，现在，我们需要通过定义我们的中间件来定义我们想要在验证系统健康时使用的特定检查。我们将首先创建 `IHealthCheck`
    接口的一个实现，为了简单起见，我们将称之为 `DependencyHealthCheck`。这个类将包含我们的响应阈值的定义，以毫秒为单位，如下面的代码所示：
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, we'll need to implement the public `CheckHealthAsync()` method to satisfy
    the requirements of the `IHealthCheck` interface. For this method, we'll be sending
    a request to our upstream dependency, and tracking the time it takes to resolve
    it using the `Stopwatch` class from the `System.Diagnostics` namespace.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要实现公共的 `CheckHealthAsync()` 方法以满足 `IHealthCheck` 接口的要求。对于这个方法，我们将向我们的上游依赖发送一个请求，并使用
    `System.Diagnostics` 命名空间中的 `Stopwatch` 类跟踪解决它所需的时间。
- en: 'The signature for the `CheckHealthAsync()` method accepts a `HealthCheckContext`
    instance that will provide our class with registration information, as well as
    a cancellation token. We''ll be ignoring those, but they are still required to
    satisfy the interface signature. Then, we''ll be creating our `HttpClient` instance
    (using the `HttpClientFactory` we discussed in [Chapter 9](e93c024e-3366-46f3-b565-adc20317e6ec.xhtml),
    *HTTP in .NET*) and building a request to the endpoint whose stability we want
    to validate. So, the first part of our `CheckHealthAsync()` method should look
    like this:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`CheckHealthAsync()` 方法的签名接受一个 `HealthCheckContext` 实例，该实例将为我们的类提供注册信息以及一个取消令牌。我们将忽略这些信息，但它们仍然是满足接口签名的必需条件。然后，我们将创建我们的
    `HttpClient` 实例（使用我们在第 9 章（e93c024e-3366-46f3-b565-adc20317e6ec.xhtml），*HTTP in
    .NET*）中讨论的 `HttpClientFactory`）并构建一个请求到我们想要验证其稳定性的端点。因此，我们的 `CheckHealthAsync()`
    方法的第一部分应该看起来像这样：'
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'At this point, we''ll want to set up our stopwatch to track how long a request
    takes so that we can confirm whether it falls beneath our designated thresholds:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们将想要设置我们的计时器来跟踪请求花费的时间，以便我们可以确认它是否低于我们指定的阈值：
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'And, finally, we''ll check the value of the response time against our thresholds,
    returning an instance of the `HealthCheckResult` according to the stability of
    our external dependency, as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将检查响应时间与我们的阈值，根据外部依赖的稳定性返回一个 `HealthCheckResult` 实例，如下所示：
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This completes our implementation of the `IHealthCheck` middleware, so now all
    that's left is to register it with our application services as a health-check.
    To do so, simply call the `AddCheck()` method of the `IHealthChecksBuilder` class
    that's returned by the `AddHealthChecks()` method in your `Startup.cs` file. This
    method takes a name, and an explicit instance of your `IHealthCheck` middleware
    implementation.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们的 `IHealthCheck` 中间件的实现，因此现在我们只剩下将其注册为健康检查到我们的应用程序服务。为此，只需调用 `Startup.cs`
    文件中 `AddHealthChecks()` 方法返回的 `IHealthChecksBuilder` 类的 `AddCheck()` 方法。此方法接受一个名称以及你的
    `IHealthCheck` 中间件实现的一个显式实例。
- en: 'Adding this, your `Startup.cs` file should have the following code:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 添加此代码后，你的 `Startup.cs` 文件应该包含以下代码：
- en: '[PRE11]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: And, with that, we've got a custom health check that will notify any monitors
    of instability in our system caused by failure of an upstream dependency! To confirm
    that it's returning the appropriate response, simply ping your `/health` endpoint
    5 to 10 times to increase the latency counter on your dependency service, and
    watch as the health status changes past the two- and five-second threshold.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 并且，有了这个，我们就得到了一个自定义的健康检查，它将通知任何监控系统我们系统中的不稳定状态，这是由于上游依赖项的失败引起的！为了确认它返回了适当的响应，只需ping你的
    `/health` 端点5到10次，以增加依赖服务上的延迟计数器，然后观察健康状态如何超过两秒和五秒的阈值。
- en: 'Here, you can see that the response from my service comes back as `Degraded`
    when the response time has broken the two-second threshold:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到当响应时间超过两秒阈值时，我的服务返回的响应为 `Degraded`：
- en: '![](img/db64ff6a-23e0-44eb-8397-d82147eda182.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/db64ff6a-23e0-44eb-8397-d82147eda182.png)'
- en: 'And after only a few more requests, the response decays to `Unhealthy`:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 并且在仅经过几次更多请求后，响应下降到 `Unhealthy`：
- en: '![](img/640a62af-d90c-4711-b80f-a0234bbd087b.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/640a62af-d90c-4711-b80f-a0234bbd087b.png)'
- en: So, now you've seen how, with only a few lines of code, you can implement an
    extensible system for monitoring your complete system health.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在你已经看到了如何仅用几行代码就实现了一个可扩展的系统，用于监控你整个系统的健康状态。
- en: Implementing a watchdog
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现看门狗
- en: 'The final piece of our proactive monitoring solution is to implement a watchdog
    service of our own. Since we''ve configured an external hook for resetting the
    performance of our unstable API, we''ll want to define a proactive recovery mechanism
    that takes advantage of it. So, for this service, we''ll set up a loop that pings
    the `/health` endpoint of our aggregator API at regular intervals, and then resets
    the data dependency API whenever the system comes back as `Degraded` or `Unhealthy`.
    Since this is a fairly trivial exercise in this case, we can keep our watchdog
    simple. We''ll start with a console app, created with our CLI:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主动监控解决方案的最后一部分是实现我们自己的看门狗服务。由于我们已经为不稳定 API 的性能重置配置了一个外部钩子，我们将想要定义一个利用它的主动恢复机制。因此，对于这个服务，我们将设置一个循环，定期ping我们的聚合器
    API 的 `/health` 端点，并在系统返回 `Degraded` 或 `Unhealthy` 时重置数据依赖 API。由于在这种情况下这是一个相当简单的练习，我们可以保持我们的看门狗简单。我们将从一个控制台应用程序开始，使用我们的
    CLI 创建：
- en: '[PRE12]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We''ll only be adding a few lines to our project. First, we''ll set up our
    `HttpClient` instance. Since the scope of our application never leaves the `Main()`
    method, we can safely create a private, static, single instance of the `HttpClient`
    class without needing to rely on an `HttpClientFactory` to manage multiple instances
    of our clients and prevent thread starvation. We''ll also assign our `Healthy`
    status code to a `private readonly` variable:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只会向我们的项目中添加几行代码。首先，我们将设置我们的 `HttpClient` 实例。由于我们应用程序的范围从未离开 `Main()` 方法，我们可以安全地创建一个私有、静态、单例的
    `HttpClient` 类，而无需依赖于 `HttpClientFactory` 来管理我们客户端的多个实例并防止线程饥饿。我们还将把我们的 `Healthy`
    状态代码分配给一个 `private readonly` 变量：
- en: '[PRE13]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Once that''s in place, our `Main()` method amounts to a dozen lines of relatively
    trivial code. First, we create an infinite loop so that our service runs until
    explicitly stopped. Then, we send a request to get the health status response
    from our aggregator API:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦设置好，我们的 `Main()` 方法就变成了十几行相对简单的代码。首先，我们创建一个无限循环，以便我们的服务在明确停止之前一直运行。然后，我们向我们的聚合器
    API 发送请求以获取健康状态响应：
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Once we''ve got the response back, we can confirm that it''s come back healthy.
    If it hasn''t, we simply send another request to the `/reset` endpoint of our
    data service. Meanwhile, if it has come back healthy, we can simply log the request
    and continue processing:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们收到响应，我们可以确认它已经返回健康状态。如果没有，我们只需向数据服务的 `/reset` 端点发送另一个请求。同时，如果它已经返回健康状态，我们只需记录请求并继续处理：
- en: '[PRE15]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Note that, for this example, I''ve determined that our health-check intervals
    should be 15 seconds. In our case, I''ve chosen this value mostly at random. However,
    as you configure or implement watchdog solutions in your own applications, you
    would do well to consider the impact of too-frequent or too-infrequent intervals,
    and configure your system accordingly. With all three of our applications up and
    running, you can see here that my watchdog system is performing according to expectations,
    resetting my data service whenever the response times have become unacceptably
    slow:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于这个例子，我确定我们的健康检查间隔应该是15秒。在我们的情况下，我主要是随机选择这个值。然而，当你配置或实现自己的应用程序中的看门狗解决方案时，你最好考虑过于频繁或过于稀疏的间隔的影响，并相应地配置你的系统。当所有三个应用程序都运行起来后，你可以看到我的看门狗系统正在按预期运行，当响应时间变得无法接受地慢时，它会重置我的数据服务：
- en: '![](img/47c772ca-0ba8-4a97-8b2e-ff33ac980bb7.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/47c772ca-0ba8-4a97-8b2e-ff33ac980bb7.png)'
- en: And, with that, our proactive monitoring system is complete. While we had to
    manufacture instability in this sample code, the strategies we devised to monitor
    and mitigate that instability will scale with any project you might have to write
    in the future.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如此一来，我们的主动监控系统就完成了。虽然在这个示例代码中我们不得不制造不稳定性，但我们制定的监控和缓解这种不稳定性的策略将适用于你未来可能要编写的任何项目。
- en: Summary
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter opened up with an unfortunately common example of a naively implemented
    distributed architecture. Using that as a touchstone, we considered the impact
    that architectural design decisions can have on an application's performance.
    We learned about vertical dependencies in our distributed architectures, and how
    we can mitigate their impact with a strictly-adhered-to three-tiered design.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 本章以一个不幸常见的、天真实现的分布式架构的例子开头。以此为基准，我们考虑了架构设计决策对应用程序性能的影响。我们了解了分布式架构中的垂直依赖，以及我们如何通过严格遵守的三层设计来减轻其影响。
- en: Next, we looked at how bad logging practices can make it untenable to use logging
    as a performance monitoring solution. We discussed the concept of a logging solution's
    signal-to-noise ratio, and we examined the practices we should all apply to our
    own logging strategies to maximize the effectiveness of our logs.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们探讨了糟糕的日志记录实践如何使将日志记录作为性能监控解决方案变得不可行。我们讨论了日志解决方案的信噪比的概念，并检查了我们应应用于我们自己的日志策略的实践，以最大限度地提高日志的有效性。
- en: Once we fully examined why logging constitutes an insufficient monitoring strategy,
    we dove into the attributes of a truly robust approach. We saw that a well-designed
    monitoring strategy should take advantage of the various unit and load tests you've
    performed, and the information they've provided. We learned that a robust strategy
    involves active monitoring, and should seek to identify problems systematically
    before a user ever encounters an issue. Finally, we saw how such a solution could,
    and should, take a proactive approach to try to resolve any health or performance
    issues, and notify the relevant engineers of the issues.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们全面分析了为什么日志记录构成一个不足够的监控策略，我们就深入研究了真正稳健方法的属性。我们了解到，一个设计良好的监控策略应该利用你进行的各种单元和负载测试，以及它们提供的信息。我们了解到，一个稳健的策略涉及主动监控，并且应该在用户遇到问题之前系统地识别问题。最后，我们看到了这样的解决方案如何以及应该采取主动方法来尝试解决任何健康或性能问题，并通知相关工程师问题。
- en: Last, but not least, we looked at precisely how to implement just such a monitoring
    strategy in .NET Core. Implementing our own monitoring middleware in an ASP.NET
    Core application, we saw the power and flexibility of the built-in monitoring
    solutions available from the framework. With this piece of the architectural puzzle
    in place, we're ready to start exploring some more advanced subjects. With that
    in mind, we'll use the next chapter to explore how we might devise our own application
    layer protocol using the concept of pluggable protocols in .NET Core.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，我们探讨了如何在.NET Core中精确地实现这样的监控策略。在ASP.NET Core应用程序中实现我们自己的监控中间件，我们看到了框架提供的内置监控解决方案的强大和灵活性。随着这个架构拼图块的到位，我们准备开始探索一些更高级的主题。考虑到这一点，我们将使用下一章来探讨我们如何利用.NET
    Core中的可插拔协议的概念来设计自己的应用程序层协议。
- en: Questions
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What is the difference between horizontal dependencies and vertical dependencies?
    How do they impact performance?
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 水平依赖和垂直依赖有什么区别？它们如何影响性能？
- en: What is a three-tiered architecture?
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是三层架构？
- en: What is a signal-to-noise ration? Why is it important in logging?
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 信号与噪声比是什么？为什么它在日志记录中很重要？
- en: What are some of the reasons logging is insufficient for performance monitoring?
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么日志记录对于性能监控来说是不足够的？
- en: What are the key attributes of a robust monitoring strategy?
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 稳健的监控策略的关键属性是什么？
- en: What is load testing? What information can it provide?
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 负载测试是什么？它能提供哪些信息？
- en: What does it mean to have a disciplined logging strategy?
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 严格的日志策略意味着什么？
- en: Further reading
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: For an additional resource on leveraging health checks and performance monitoring
    in your ASP.NET Core applications, I'd recommend *Learning ASP.NET Core 2.0* by
    Jason de Oliveira and Michel Bruchet, and I particular, the chapter entitled *Managing
    and Supervising ASP.NET Core 2.0 Applications*. It's an exceptionally good read
    and will provide a wealth of skills that are laterally transferable to any number
    of different contexts. It can be found from Packt, here: [https://www.packtpub.com/application-development/learning-aspnet-core-20](https://www.packtpub.com/application-development/learning-aspnet-core-20).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在您的 ASP.NET Core 应用程序中利用健康检查和性能监控的额外资源，我推荐 Jason de Oliveira 和 Michel Bruchet
    所著的 *Learning ASP.NET Core 2.0*，特别是其中名为 *Managing and Supervising ASP.NET Core
    2.0 Applications* 的章节。这是一本非常优秀的读物，它将提供大量可以横向迁移到不同情境的技能。您可以在 Packt 找到它：[https://www.packtpub.com/application-development/learning-aspnet-core-20](https://www.packtpub.com/application-development/learning-aspnet-core-20)。
- en: Additionally, if you'd like to continue down the path of learning architectural
    design, with a focus on microservices-based ecosystems, I'd recommend *Enterprise
    Application Architecture with .NET Core* by Ganesan Senthilvel, Ovais Mehboob
    Ahmed Khan, and Habib Ahmed Qureshi. The depth with which they cover a multi-layered
    architecture in a cloud environment is illuminating. It can be found, as always,
    through Packt: [https://www.packtpub.com/application-development/enterprise-application-architecture-net-core](https://www.packtpub.com/application-development/enterprise-application-architecture-net-core).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果您想继续学习架构设计，并专注于基于微服务的生态系统，我推荐 Ganesan Senthilvel、Ovais Mehboob Ahmed Khan
    和 Habib Ahmed Qureshi 所著的 *Enterprise Application Architecture with .NET Core*。他们深入探讨了在云环境中多层架构的广度，非常具有启发性。您可以通过
    Packt 找到它：[https://www.packtpub.com/application-development/enterprise-application-architecture-net-core](https://www.packtpub.com/application-development/enterprise-application-architecture-net-core)。
