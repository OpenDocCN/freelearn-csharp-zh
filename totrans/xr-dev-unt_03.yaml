- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: VR Development in Unity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's explore the world of VR, from creating your first VR project in Unity
    to deploying our first VR scene on a headset or simulator. In this chapter, we’ll
    present the most important VR toolkits and plugins available in Unity, helping
    you become familiar with each one’s capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll gain hands-on experience with the **XR Interaction Toolkit**’s demo scene
    , understand its most important components and scripts, and learn how to use them
    in your future VR projects. We’ll help you understand the nuances between VR development
    and traditional game development, as well as share different strategies to ensure
    a VR headset’s computing power.
  prefs: []
  type: TYPE_NORMAL
- en: Another important skill you will gain in this chapter is how to test and deploy
    VR experiences on various devices, from simulators to VR headsets. This chapter
    will give you a robust foundation that will equip you with the necessary skills
    and knowledge to create increasingly complex and immersive VR scenes using Unity.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll cover the following topics as we proceed:'
  prefs: []
  type: TYPE_NORMAL
- en: What is VR development?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a VR project in Unity and the XR Interaction Toolkit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the XR Interaction Toolkit demo scene
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying and testing VR experiences onto different VR platforms or simulators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To effectively follow along with this chapter on VR development in Unity, it
    is essential to have an appropriate hardware and software setup. For the most
    effective and seamless experience, we strongly recommend utilizing a Windows PC
    or a robust Windows laptop, even if your target platform is a standalone VR headset.
  prefs: []
  type: TYPE_NORMAL
- en: Windows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Windows is the most supported platform by the majority of VR headset manufacturers,
    including **Meta Quest** and **HTC**. This support stems from Windows’ comprehensive
    hardware support and its optimization for gaming. Whether you’re developing for
    PC-tethered or standalone VR headsets, a Windows environment will provide the
    most resources and compatibility for VR development in Unity.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a smooth experience, we recommend the following minimum requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating system: Windows 10 or above'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Processor: Intel i5-4590/AMD Ryzen 5 1500X or greater'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Memory: 8 GB RAM or more'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Graphics: NVIDIA GTX 1060/AMD Radeon RX 480 or greater'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: macOS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While macOS is rarely supported by VR headsets, Unity still enables VR development
    on Apple devices through platforms such as ARKit that predominantly target AR
    instead of VR. If you’re using macOS and targeting standalone VR headsets, consider
    setting up a Windows partition through **Boot Camp**, which allows you to use
    any Windows-compatible VR headset. This setup will enable you to follow along
    with the instructions in this chapter without significant issues.
  prefs: []
  type: TYPE_NORMAL
- en: Linux
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linux’s VR support is relatively limited, but **Valve** does extend support
    to the *Valve Index* and *HTC Vive* headsets via SteamVR on Linux platforms. If
    you’re targeting standalone VR headsets such as the *Oculus Quest*, Linux can
    be used for development by creating Android builds of your VR applications in
    Unity, then transferring these builds to the headset for testing. However, the
    standalone VR headsets typically run a version of Android, not Linux, which may
    bring about unique challenges.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re utilizing a Linux platform, please ensure your system meets the following
    minimum requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating system: Ubuntu 18.04 LTS or newer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Processor: Dual-core CPU with hyper-threading'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Memory: 8 GB RAM or more'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Graphics: Nvidia GeForce GTX 970, AMD RX480'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is VR development?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When embarking on the fascinating journey of VR development in Unity, it’s important
    that we understand the features and peculiarities of VR development. This includes
    appreciating the software and hardware limitations, as well as the challenges
    that exist within this field. In the forthcoming sections, we will dive into the
    contrasts between VR development and traditional game development for 2D computer
    screens.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, we will explore current trajectories and future trends of VR headset
    technology. This exploration includes the unique pros and cons that each VR headset
    brings to the table. Such understanding is key as it not only guides you in making
    a well-informed selection of a VR headset that suits your needs, but also equips
    you with the necessary knowledge to judge which VR headset would be ideal or less
    ideal, given a particular context.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this section, you will be well-equipped to navigate the immersive
    and dynamic world of VR development in Unity.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the contrasts between classical and VR game development
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Classical game development and VR development, although existing on different
    ends of the gaming spectrum, both emerged from the same desire to create immersive,
    virtual landscapes. Intriguingly, both categories share more than just the objective
    of captivating the audience; they’re built on similar foundational elements of
    game design and require a corresponding toolset.
  prefs: []
  type: TYPE_NORMAL
- en: Both domains of game creation demand careful planning and consideration of gameplay
    mechanics, user interfaces, objectives, levels, and storylines. Developers, whether
    they’re working on a traditional game or a VR project, often use the same suite
    of programming languages such as C++, C#, or Python for scriptwriting.
  prefs: []
  type: TYPE_NORMAL
- en: The creation of **game assets** – textures, models, animations, and sounds –
    is another commonality between the two fields. VR development may lean slightly
    more toward 3D models and spatial audio to enrich the immersive nature of the
    environment. However, the overarching process of creating and incorporating these
    assets remains congruous in both disciplines. Regardless of the gaming medium,
    be it a 2D screen or a VR headset, the implementation of realistic physics is
    paramount to an engrossing experience.
  prefs: []
  type: TYPE_NORMAL
- en: Though they share these similarities, the divergences between classical game
    development and VR development are just as fascinating, primarily due to VR’s
    distinct nature and abilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the significant contrasts lies in the design’s dimensionality. Classical
    2D games primarily operate on a flat plane, wherein objects are depicted in two
    dimensions: height and width. The game *Super Mario Bros*. serves as a classic
    example, with the action unfolding from left to right and the characters’ movements
    largely restricted within this plane.'
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, VR games invite depth as a vital third dimension into play, thus
    offering the user the freedom to explore the scene from any perspective or position,
    akin to their experience in the real world. For instance, in a VR game such as
    *Beat Saber*, players can look around, reach out, and interact with the game environment
    in a way that mimics real-world spatial interactions.
  prefs: []
  type: TYPE_NORMAL
- en: These subtle yet impactful differences pave the way for a varied range of experiences
    in classical game development and VR development, each with its own unique charm
    and challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Equally important, the scope of player movement exhibits a clear distinction
    between classical 2D games and VR gaming experiences. Traditional 2D games typically
    restrict player actions to the *x* and *y* axes, confining movement within a certain
    plane. Conversely, VR introduces varying degrees of freedom.
  prefs: []
  type: TYPE_NORMAL
- en: For example, **three degrees of freedom** (**3DoF**) devices, often paired with
    **Mobile VR** solutions such as *Google Cardboard*, allow the user to look around
    freely (pitch, yaw, and roll) but don’t track physical displacement within a space.
    This provides a basic level of immersion, enabling the user to view a scene or
    object from multiple perspectives while standing still.
  prefs: []
  type: TYPE_NORMAL
- en: A step further into the realm of VR are the **six degrees of freedom** (**6DoF**)
    devices. These not only track a player’s view but also register their physical
    movement along the *x*, *y*, and *z* axes, thus creating a more immersive and
    interactive environment. Crafting experiences for 6DoF can be intricate, necessitating
    full 3D spatial interactions. For instance, modern VR headsets such as the *Meta
    Quest 2* pair up with VR controllers or even offer hand-tracking capabilities.
    Users can interact with the virtual world in an incredibly intuitive manner, pressing
    buttons, twisting their hands, and conducting an array of movements that the AI-powered
    cameras on the VR headset accurately capture and translate in real time.
  prefs: []
  type: TYPE_NORMAL
- en: The imperative of **player safety** is a unique aspect that distinguishes VR
    development from traditional game creation. While physical safety concerns are
    virtually non-existent in classical 2D game development, the expanded range of
    movement in VR can potentially lead to real-world mishaps. As such, developers
    need to engineer safety features such as virtual boundaries or alert systems to
    forestall accidents.
  prefs: []
  type: TYPE_NORMAL
- en: The final distinguishing factor lies in **player conditioning**. Classical 2D
    games generally rely on players’ familiarity with standard game controls and mechanics
    – a safe assumption given the ubiquity of PCs in everyday life. VR, however, often
    pioneers novel forms of interactions that might confound first-time users. To
    bridge this gap, developers may need to incorporate tutorials or guides to help
    players navigate and interact with the VR gaming world.
  prefs: []
  type: TYPE_NORMAL
- en: Not only does VR development diverge significantly from traditional game development,
    but it also showcases a broader spectrum of techniques to power VR headsets. These
    diverse approaches, integral to the operation of VR technology, will be unraveled
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding different approaches to power VR headsets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The power of a virtual reality headset can dramatically impact the success of
    your VR application. Let’s delve into the most common.
  prefs: []
  type: TYPE_NORMAL
- en: PC VR (alternatively known as PC-based VR or tethered VR headsets)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**PC-based VR headsets**, including models such as *Valve Index* and *HTC Vive
    Pro*, hinge on the computational capacity of a desktop or laptop computer. They
    delegate the heavy lifting, such as intensive computations, graphics rendering,
    and 3D simulations, to the connected computer. To this day, a substantial portion
    of VR headsets are either wholly PC-based or support a PC VR mode alongside a
    standalone variant. This is primarily because PC VR, bolstered by the superior
    computational power and graphical prowess of PCs, provides the most graphically
    rich and immersive VR experiences. Attributes such as level of detail, frame rate,
    and responsiveness are top-notch in PC-based VR.'
  prefs: []
  type: TYPE_NORMAL
- en: PC-based VR systems predominantly employ two modes of connection between the
    VR headset and the computer. The wired cable connection, often utilizing HDMI
    or DisplayPort for video and USB for data, is the traditional route. A more recent
    innovation is a wireless connection, exemplified by the *Air Link* connection
    in the *Meta Quest* series. However, wireless PC VR connections are seldom used
    in reality due to their higher latency, which could hamper the responsiveness
    of the VR experience. Video quality can also fluctuate depending on network conditions,
    necessitating a robust and stable Wi-Fi signal for optimal operation.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the convenience of wireless connections, for a seamless VR experience,
    a wired connection is still advisable, even when the headset supports both. It’s
    essential to note, however, that a wired PC VR system sacrifices portability due
    to its physical attachment to a PC. Whether you choose wired or wireless, the
    mobility of your VR setup is limited. For example, you can’t easily pack your
    VR headset for a holiday, an event, a conference, or a friend’s gathering. If
    you decide to transport your VR gear, it necessitates bringing along a powerful
    PC or ensuring the destination has the required computational resources. Coupled
    with the potentially steep price of a high-performance or gaming PC, this could
    make PC VR systems a substantial investment for some users.
  prefs: []
  type: TYPE_NORMAL
- en: Standalone VR headsets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Addressing the limitations of PC-based VR headsets, **standalone VR devices**
    are making waves in the market, gaining immense popularity. Leading the pack is
    the *Meta Quest* series. These devices house all necessary computing components
    within the headsets themselves, making them autonomous units. Most modern standalone
    headsets are driven by *Qualcomm Snapdragon* chips, offering robust processing
    capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Standalone VR headsets champion portability and freedom of movement, eliminating
    the need to tether to a separate computing device. These all-in-one systems offer
    a cost-effective alternative to PC-based VR. Moreover, many standalone headsets,
    such as those in the *Meta Quest* series, support a PC VR mode, providing the
    flexibility to take on more computation-intensive applications such as *Half-Life
    Alyx* when desired.
  prefs: []
  type: TYPE_NORMAL
- en: One drawback, however, is the relatively short battery life of standalone headsets,
    usually topping out around two hours. Nevertheless, remedies exist, such as purchasing
    comfortable head straps equipped with additional battery life extenders. In most
    scenarios, we endorse the more affordable standalone headsets such as the *Meta
    Quest* series over their PC-based counterparts. For academic and business environments,
    standalone headsets are an efficient choice. A simple solution, such as purchasing
    multiple standalone headsets or a set of battery-enhancing head straps, can still
    offer substantial savings over traditional PC VR solutions. Standalone VR headsets
    also shine when used at events or conferences to showcase products or research,
    offering a hassle-free and portable solution.
  prefs: []
  type: TYPE_NORMAL
- en: Console-based VR headsets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Console-based VR headsets**, such as the *PlayStation VR 2*, cater mainly
    to the gaming community. They draw on the processing power of the gaming console,
    delivering high-quality VR experiences without the need for a separate PC. However,
    the cost of owning a gaming console isn’t trivial. Moreover, like their PC-based
    counterparts, console-based VR headsets sacrifice portability due to their tethered
    nature. The quality of VR experiences can also be limited by the console’s capabilities.
    In summary, unless your primary focus is gaming, console-based VR headsets may
    not be the best fit for creating VR experiences, whether in a personal, business,
    or academic setting. Even within gaming circles, there may be superior options
    to consider.'
  prefs: []
  type: TYPE_NORMAL
- en: Smartphone-based VR headsets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Smartphone-based VR headsets**, exemplified by *Google Cardboard* and *Samsung
    Gear VR*, harness the display and computing power of smartphones, which are devices
    most people already own. These headsets offer a cost-effective and highly portable
    solution. However, due to their limited computational and graphical prowess, the
    immersive experience they provide is inherently diminished. Furthermore, their
    interaction capabilities within the VR environment are curtailed due to a more
    restrictive range of sensors compared to other VR headset types. Thus, we seldom
    recommend them. Even for preliminary testing of VR experiences in the absence
    of a VR headset, tools such as Unity’s XR Device Simulator offer a free, convenient,
    and even superior solution.'
  prefs: []
  type: TYPE_NORMAL
- en: Cloud-based VR headsets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A growing chorus within the VR community is advocating the development of **cloud-based
    VR headsets**. This emergent technology capitalizes on robust servers to shoulder
    computational tasks, streaming the results to the VR headset via the internet.
    This arrangement could potentially deliver high-resolution VR experiences independent
    of local computational prowess, further enhancing immersion and user satisfaction.
  prefs: []
  type: TYPE_NORMAL
- en: A cloud-based approach could make VR headsets lighter and more user friendly,
    inviting a broader audience to invest in their own VR hardware. Another appealing
    aspect is that the hardware housed in the data center that powers the VR headset
    can be periodically upgraded by the provider, saving users from frequently investing
    in new equipment.
  prefs: []
  type: TYPE_NORMAL
- en: So, why isn’t this computational approach ubiquitous in contemporary VR headsets?
    The primary reason is the necessity of a stable, high-bandwidth, low-latency internet
    connection – a prerequisite that isn’t universally available. Furthermore, there
    are potential privacy and security concerns associated with transmitting sensitive
    data over the internet.
  prefs: []
  type: TYPE_NORMAL
- en: As technology evolves and these challenges are addressed, we may see more adoption
    of cloud-based VR systems. However, as of now, their use remains largely experimental
    and not yet mainstream.
  prefs: []
  type: TYPE_NORMAL
- en: Having journeyed through this exploration of the distinct features of VR headsets
    and the nuances of VR development, you are now well-prepared to embark on the
    creation of your inaugural VR scene in Unity. This milestone will allow you to
    play around with some of the engaging toolkits that Unity provides specifically
    for VR, including the highly versatile XR Interaction Toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a VR project in Unity and the XR Interaction Toolkit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following sections, you will acquire the knowledge to create a Unity
    project specifically tailored for VR development. Furthermore, you will become
    proficient in the installation and configuration of two highly significant plugins
    that are pivotal to VR development within Unity: **XR Plug-in Management** and
    the XR Interaction Toolkit.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a project in Unity for VR development
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s begin by creating a brand-new project in the latest version of Unity.
    To ensure compatibility with different devices and platforms, it is crucial to
    enable both `Installs` folder, clicking on the **Settings** icon of your Unity
    version, selecting **Add Modules**, and installing the missing build supports.
  prefs: []
  type: TYPE_NORMAL
- en: To bring a VR scene to life, we must first navigate to the **Projects** section
    in Unity and create a fresh project using the latest Unity version. While Unity
    does provide a VR template among the various project types to choose from, I recommend
    selecting **3D URP** **template** instead.
  prefs: []
  type: TYPE_NORMAL
- en: The rationale behind this recommendation is that Unity’s VR template does not
    include the XR Interaction Toolkit, which would lead to extensive setting modifications
    if you were to choose the VR template. As such, creating a 3D URP project is a
    more streamlined and trouble-free approach.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn how to install XR Plug-in Management and
    additional XR Plug-ins for PC-based and standalone VR headsets.
  prefs: []
  type: TYPE_NORMAL
- en: Installing XR Plug-in Management and XR Plug-ins
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let’s head over to **Edit** | **Project Settings** | **XR Plug-in Management**.
    From here, proceed by clicking on **Install XR Plug-in Management**. Think of
    XR Plug-in Management as the bridge between Unity and the different VR, MR, and
    AR systems you want to use for your scene. It acts like a middleman, allowing
    Unity to communicate and work effectively with these devices. Upon successful
    installation of XR Plug-in Management, you can navigate to **Edit** | **Project
    Settings** | **XR Plug-in Management** to select various plugin providers that
    will enable your VR application to run in PC VR mode on your VR headset. The following
    is a brief overview of some of the options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unity Mock HMD**: Ideal for game developers who may not have direct access
    to a VR headset. Unity’s Mock HMD mimics the functionalities of a VR headset,
    providing a platform to design, develop, and test your game within Unity’s environment
    without requiring physical hardware.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Oculus integration package**: A specialized Unity development package catering
    to Oculus devices. This includes platform-specific features, Avatar and LipSync
    SDKs, spatial audio, the Guardian system, and support for hand tracking. However,
    this package might restrict the portability of your project to non-Oculus platforms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenXR**: A remarkable open standard developed by the *Khronos Group* that
    offers developers the freedom to cater to a broad array of XR devices with the
    same input, thereby eliminating fragmentation. This obviates the need for separate
    Oculus or SteamVR plugins, simplifying maintenance by allowing a single code base
    to operate on any XR system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re an ambitious developer aiming to create a VR experience that appeals
    to a wide audience, irrespective of their VR hardware, OpenXR would be your go-to.
    For this reason, we’ll be utilizing the OpenXR plugin throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: To get started, enable the **OpenXR** checkboxes on both the **PC** and **Android**
    tabs.
  prefs: []
  type: TYPE_NORMAL
- en: This action triggers the download of OpenXR into your Unity project. Once the
    download is complete, a prompt requesting permission to restart the project will
    appear. Click **Yes**, as a restart is necessary to transition from Unity’s old
    input mode to the new one. After Unity restarts, you’ll find that the **OpenXR**
    option is now activated in XR Plug-in Management. However, an interaction profile
    still needs to be added, as indicated by the warning symbol next to it, as shown
    in *Figure 3**.1*.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 3.1 – \uFEFFHow the OpenXR plugin is selected for the PC tab in XR\
    \ Plug-in Management](img/B20869_03_01.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – How the OpenXR plugin is selected for the PC tab in XR Plug-in
    Management
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to **XR Plug-in Management** | **OpenXR** and click on the **+** icon
    in the **Interaction Profiles** section. Here, you can incorporate the interaction
    profiles of all headsets you aim to support. For instance, if you’re using a VR
    headset from the *Meta Quest* series, you should opt for the **Oculus Touch Controller
    Profile** option, as shown in *Figure 3**.2*.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 3.2 – \uFEFFThe OpenXR tab with the Oculus Touch Controller Profile\
    \ selected as an interaction profile](img/B20869_03_02.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – The OpenXR tab with the Oculus Touch Controller Profile selected
    as an interaction profile
  prefs: []
  type: TYPE_NORMAL
- en: OpenXR’s **interaction profiles** are standardized sets of user inputs for XR
    devices, ensuring consistency and compatibility across various platforms. They
    allow developers to program once while catering to multiple devices, making the
    development process more efficient. For instance, two VR controllers from different
    manufacturers might have different button layouts, but through OpenXR, they can
    be mapped to a standard set of interactions. This means that if an XR developer
    programs an action such as *grab* or *jump*, it will work irrespective of the
    exact controller used as long as the appropriate interaction profile is implemented.
    By adding only a few interaction profiles, developers limit their application’s
    compatibility to a specific set of devices, potentially excluding some users.
    Conversely, incorporating as many interaction profiles as possible broadens the
    application’s reach, ensuring that it can be used seamlessly across a wide array
    of XR devices.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to add interaction profiles for both **PC** and **Android**. If you
    can’t locate the **Android** tab, this implies that you missed downloading the
    module during the initial Unity setup. In such a case, you must return to Unity
    Hub and add the module to your installed Unity version.
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering why we must configure the same setting twice, once under
    the **Windows** tab and then under the **Android** tab. The reason is that the
    **Windows** tab enables VR for PC VR, while the **Android** tab permits the addition
    of plug-ins from different providers to run your VR application on your VR headset
    in standalone mode.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s discuss **Render Mode** for both the **Windows** and **Android**
    tabs. The default rendering mode is **single pass instanced** (**SPI**), but there’s
    also an option for the **multi pass** (**MP**) rendering mode. Both are tailored
    for VR environments, each with its own set of benefits and limitations.
  prefs: []
  type: TYPE_NORMAL
- en: SPI offers considerable performance advantages, most notably by reducing the
    number of draw calls by half. This is crucial for standalone VR headsets, where
    optimization is paramount to ensure the application runs smoothly. Maintaining
    a consistent frame rate and smoothness is vital in VR to avoid motion sickness.
    Additionally, SPI guarantees visual consistency as both eyes are rendered from
    the same instance, reducing the chances of visual discrepancies that can lead
    to discomfort. However, the catch is that SPI often requires shader modifications
    for accurate geometry processing, which can introduce another layer of development
    complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, MP boasts greater shader compatibility since it doesn’t
    demand specific shader modifications. This can be beneficial if your scene uses
    a variety of shaders. Each eye is rendered independently in MP, which can sometimes
    lead to superior visual quality by reducing visual artifacts. However, this comes
    with a performance trade-off: MP doubles the draw calls, which might overwhelm
    lower-end devices. Furthermore, rendering each eye separately might produce slight
    visual differences that can be jarring in VR.'
  prefs: []
  type: TYPE_NORMAL
- en: As for standalone VR development, SPI is often the preferred choice due to its
    optimization advantages. It’s crucial for delivering visually compelling experiences
    that also perform well on standalone VR headsets. That said, recent Unity versions
    tend to favor MP in desktop mode for editor testing.
  prefs: []
  type: TYPE_NORMAL
- en: Having made our project VR-ready, we can now proceed to install the XR Interaction
    Toolkit and its demo scene.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the XR Interaction Toolkit and samples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By installing XR Plug-in Management, we have enabled Unity to now communicate
    with our VR headset. But how can we enable Unity to receive information from our
    headset? And how can our application in Unity translate inputs it gets from the
    VR headsets into actions in the game? This is where the XR Interaction Toolkit
    comes into play. To download the XR Interaction Toolkit, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to **Window** | **Package Manager**. In **Package Manager**, you can
    see all the packages you have currently imported, such as the **OpenXR** plugin.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the XR Interaction Toolkit by clicking the `com.unity.xr.interaction.toolkit`
    as the `2.5.1` as the **Version**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Adding a specific version number is optional when installing packages. By default,
    if you don’t specify a version, the Package Manager will automatically install
    the latest version of the XR Interaction Toolkit. While staying updated with the
    latest versions is generally advisable for new projects, for the purpose of this
    book, we strongly recommend sticking to version *2.5.1* of the toolkit. Using
    this specific version ensures consistency and ease of understanding as you follow
    along with our tutorials. Even though newer versions might retain a similar structure,
    the additional features and elements introduced could make navigating our tutorials
    more challenging. So, to ensure a smooth learning experience, please use version
    *2.5.1* while working through this book.
  prefs: []
  type: TYPE_NORMAL
- en: Hit **Enter** and the XR Interaction Toolkit will be automatically downloaded
    into your project. If you see a pop-up window with a warning asking you to restart
    the Unity Editor, click on **Yes**. This just means that the XR Interaction Toolkit
    is using a new input system to validate interactions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Staying in the `Samples` folder to expand a list of useful add-ons to the XR
    Interaction Toolkit and import **Starter Assets**. This is a collection of tools
    that make it easier to set up behaviors. It includes a pre-made set of actions
    you can use as inputs, as well as presets specifically designed for XR Interaction
    Toolkit behaviors that rely on the input system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After closing `XR Interaction Toolkit` folder is the actual XR Interaction Toolkit,
    while the `Samples` folder includes the sample add-ons we downloaded. First, let’s
    open the **Demo Scene** asset that comes with **Starter Assets**. To do this,
    navigate to **Samples** | **XR Interaction Toolkit** | **2.5.1** | **Starter Assets**
    and double-click on **Demo Scene**.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore the different components of the **Demo**
    **Scene** asset.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the XR Interaction Toolkit demo scene
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will delve into the **Demo Scene** asset of the XR Interaction
    Toolkit. This scene offers a comprehensive overview of all the crucial concepts
    you need to grasp to develop your own immersive VR experiences. If you’ve followed
    the steps in the previous section, you should now be presented with the scene
    pictured in *Figure 3**.3*.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 3.3 – \uFEFFThe \uFEFFScene view \uFEFFand the Scene \uFEFFHierarchy\
    \ window of the XR Interaction Toolkit’s demo scene](img/B20869_03_03.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – The Scene view and the Scene Hierarchy window of the XR Interaction
    Toolkit’s demo scene
  prefs: []
  type: TYPE_NORMAL
- en: Beyond the `Climb Sample` via the project window’s search bar. Dragging and
    dropping this prefab from the project window into the scene hierarchy window will
    present the identical ladder and climbing wall in your new scene, combined with
    the associated scripts and functionalities. Similarly, the **XR Interaction Setup**
    prefab, the most important prefab of the XR Interaction Toolkit, will be a staple
    of many of the forthcoming XR projects discussed in this book. You will learn
    more about it in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Examining the pre-configured player
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the **Teleportation Environment**, **Interactables Sample**, **UI Sample**,
    and **Climb Sample** prefabs all provide an easy way for you to incorporate different
    types of interactions into your VR scenes, the **XR Interaction Setup** prefab
    is the entity that interacts with these interactable components. Essentially,
    it’s a pre-configured player, designed for seamless navigation and interaction
    within the scene. You can integrate it into any of your future VR scenes in which
    you import the XR Interaction Toolkit. Let’s begin to explore it.
  prefs: []
  type: TYPE_NORMAL
- en: By clicking on the arrow next to the **XR Interaction Setup** prefab in the
    scene hierarchy window, you can see its child components, which are **Input Action
    Manager**, **Interaction Manager**, **Event System**, and **XR Origin (XR Rig)**.
    By opening up all of their child components as well, you can examine the structure
    of this prefab, as shown in *Figure 3**.4*.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 3.4 – \uFEFFThe components of the complete XR Interaction Setup prefab](img/B20869_03_04.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – The components of the complete XR Interaction Setup prefab
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand these components and how they interact with each other, let’s
    imagine a VR game scenario where the player takes on the role of a chef in a restaurant
    and uses a VR headset and controllers to interact with the kitchen environment.
    In this imaginary example, the components of the **XR Interaction Setup** prefab
    would take on the following roles:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input Action Manager**: This component manages the processing of inputs from
    the VR controllers. For instance, a controller button press could initiate actions
    such as gripping a pan or picking up an ingredient.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**XR Interaction Manager**: This component governs interactions between the
    player and virtual objects. For instance, it manages the outcomes when a player’s
    VR hand interacts with a virtual button – perhaps a door opens or a light switches
    on. Similarly, if the player, holding a pan, moves their hand over the stove and
    releases the grip button, the pan could settle onto the stove.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**EventSystem**: This component oversees the triggering and processing of events
    within the game. For example, upon the successful preparation and serving of a
    dish, an event could be triggered perhaps the sound of applause or a visual congratulatory
    message illuminating the screen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**XR Origin (XR Rig)**: This component serves as the player’s anchor point
    in the virtual world, responsible for managing the user’s movement and positioning
    within the XR environment. By utilizing data from the VR device, **XR Origin**
    updates the position and orientation of the camera and controllers via its attached
    **XR Origin** script, thus creating the illusion of movement and interaction within
    the virtual environment. In our game context, **XR Origin** would represent the
    player’s location in the virtual kitchen; if the player advances a step, their
    avatar would correspondingly move a step closer to the stove.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Camera Offset** with **Main Camera**: This component tracks the player’s
    head movements. For instance, if the player turns their head toward a recipe book
    lying on a shelf, the game’s viewpoint adjusts accordingly to focus on the book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Left Controller** and **Right Controller**: These components monitor the
    player’s hand movements, which typically correspond with the position of the VR
    controllers. If the player extends their hand to grab a virtual pan using the
    controller, the VR hands within the scene would mimic this action. This is why
    the hands contain child objects such as **Poke Interactor** for touch, **Direct
    Interactor** for grabbing, **Ray Interactor** for pointing, and **Teleport Interactor**
    for swift movement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gaze Interactor**: This component facilitates interactions based on the player’s
    eye or head gaze direction. For instance, if the player focuses their gaze on
    a particular ingredient, a tooltip might appear to deliver more information about
    it. As not every VR headset supports eye tracking, **Gaze Interactor** also supports
    head tracking.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Locomotion System**: As its name already suggests, this GameObject with its
    associated **Locomotion System** script is responsible for equipping the player
    with different forms of movement such as turning, walking, teleporting, or climbing.
    Let’s have a look at each of its children:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Turn**: The **Turn** GameObject has two important scripts attached to it,
    which you can see by selecting **Turn** in the scene hierarchy window and navigating
    to its inspector window. First, there is the **Snap Turn Provider** script. This
    script enables the player to rotate their view in the VR environment in discrete
    steps, or “snaps,” using their VR controllers. This can be useful in cases where
    physically turning isn’t convenient or possible.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, there is the **Continuous Turn Provider** script. Allowing for smooth,
    continuous turning in the VR environment using the VR controllers, this script
    offers an alternative to the step-by-step turning provided by the **Snap Turn**
    **Provider** script.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Move**: In our VR restaurant game, for example, the player can move through
    the kitchen by navigating with the controllers of their VR headset. Even though
    they remain stationary in the physical world, their perspective shifts in the
    virtual space, giving the illusion of traversing the environment as if they were
    genuinely walking through the virtual kitchen. This functionality comes from the
    **Dynamic Move Provider** script, which is attached to the **Move** GameObject.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Grab Move**: This component of the XR Interaction Toolkit’s **XR Interaction
    Setup** prefab facilitates intuitive movement within the VR space. With two instances
    of the **Grab Move Provider** script being attached to this GameObject, the player
    can simulate the sensation of grabbing the virtual world with either hand. As
    the player moves the controller, the VR origin adjusts inversely, keeping the
    controller’s position consistent relative to the virtual environment. Complementing
    this, the **Two-Handed Grab Move Provider** script empowers players to manipulate
    their position using both controllers, similar to grabbing two bars and pulling
    or pushing oneself. This dual-hand interaction allows players to not just move
    through the VR restaurant game, but also to adjust their orientation and scale
    within the scene.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Teleportation**: Solely navigating the virtual world via continuous movement,
    as provided by the **Move** GameObject, can make some VR users motion sick, even
    if they regularly immerse themselves in VR. This is why most VR applications mainly
    work with teleportation, which enables the player to instantly appear in a new
    spot within the virtual space by moving the joysticks of the VR controllers in
    the desired direction. As they do so, a visual cue, often a circled overlay, indicates
    the target destination. Once the joystick is released, the player is directly
    teleported to that location. The **Teleportation** GameObject of the XR Interaction
    Toolkit offers the player of our imaginary VR restaurant game a rich variety of
    ways to teleport around the kitchen or dining area. For example, the player might
    only be able to navigate to distinct places in the VR restaurant, making the gameplay
    more straightforward and structured, which is ideal for educational settings or
    applications tailored to newbies to VR. If exploration and spontaneity are the
    core missions of the VR restaurant game, enabling the player to teleport to any
    part of the floor that is not occupied by chairs, tables, or kitchen cabinets
    might be more favorable.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Given the importance of teleportation in VR applications, we’ll delve deeper
    into which teleportation features the XR Interaction Toolkit offers in the upcoming
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Teleporting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Teleportation Environment** prefab of the **Demo Scene** asset consists
    of a **Teleport Area** prefab and four **Teleportation Anchor** prefabs. You can
    see these components in *Figure 3**.5*.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 3.5 – \uFEFFThe Teleportation Area and four Teleportation Anchors,\
    \ which are both part of the Teleportation Environment prefab](img/B20869_03_05.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – The Teleportation Area and four Teleportation Anchors, which are
    both part of the Teleportation Environment prefab
  prefs: []
  type: TYPE_NORMAL
- en: The **Teleport Area** script, equipped with a **Teleportation Area** prefab
    and a child cube, represents a space available for teleportation. Users can teleport
    to any pointed spot within this designated area, effectively teleporting within
    the bounds of the cube.
  prefs: []
  type: TYPE_NORMAL
- en: By contrast, a **Teleport Anchor** prefab serves as a specific teleportation
    point, allowing users to navigate to exact locations with ease.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a VR game where users must traverse a river by hopping from one stone
    lying in the water to another. On the other side of the river, there is a forest
    with gold hidden underneath some trees that they must find. In this case, **Teleport
    Anchors** could be used for each of the stones lying in the water. This would
    allow the users to traverse the river with ease and focus more on the actual gameplay.
    For the other side of the river, however, a **Teleport Area** script would be
    better suited, allowing users to explore the area as they wish.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn all about grabbable objects and their movement
    types.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring grabbable objects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Interactables Sample** prefab, another part of the **Demo Scene** asset,
    contains three 3D figures that highlight the capabilities of the **XR Grab Interactable**
    script, an essential tool within the XR Interaction Toolkit. This script will
    likely be a staple of your future VR projects whenever you want an object to be
    grabbable. Once you’ve integrated the XR Interaction Toolkit and configured XR
    Plug-in Management in any of your VR endeavors, making an object grabbable is
    a breeze. Simply highlight the object in the scene hierarchy window, click on
    the **Add Component** option in the Inspector, and then search for and choose
    the **XR Grab Interactable** script. Additionally, ensure the **XR Interaction
    Setup** prefab is in your scene, as it provides the required player for the interaction.
  prefs: []
  type: TYPE_NORMAL
- en: This single step of adding the **XR Grab Interactable** script to an object
    transforms it into a VR-interactable item. This ease of use and efficiency is
    a significant reason behind the XR Interaction Toolkit’s popularity among VR developers.
    Instead of manually coding numerous lines to achieve this functionality, you can
    focus your time on refining the user experience by tweaking the integrated physics
    settings. These adjustments can significantly enhance the immersion and realism
    of your VR application.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s delve into the various physics configurations offered by the **XR
    Grab Interactable** script to enhance the object-grabbing experience. Each of
    the three figures of the **Interactables Sample** prefab showcases a different
    **Movement Type** setting of the **XR Grab Interactable** script, as you can see
    in *Figure 3**.6*.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 3.6 – \uFEFFThe demo scene’s Interactables Sample prefab in the scene\
    \ view](img/B20869_03_06.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – The demo scene’s Interactables Sample prefab in the scene view
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go through the figures one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Interactable Kinematic Torus**: When you select this in the scene hierarchy
    window and inspect its properties, you will see that its **Movement Type** setting
    is labeled as **Kinematic**. A kinematic object is guided by the software’s calculations
    rather than by external forces. This means it moves smoothly based on the game
    engine’s instructions and isn’t affected by gravity or collisions like a regular
    object would be. This type of movement can be useful in applications such as puzzle
    games, where objects must fit into specific locations without bouncing or rolling
    away. It ensures that once placed, the objects remain static.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interactable Instant Pyramid**: This figure has **Instantaneous** selected
    as its **Movement Type** setting. Instantaneous movement is precisely as it sounds:
    immediate. Instead of creating a sensation of weight or inertia as you move it,
    this object will respond and relocate right away without any noticeable lag or
    drift. It’s like moving a cursor on a computer screen; where you point, it goes.
    This is ideal for **user interface** (**UI**) interactions in VR, such as selecting
    menu options or dragging and dropping virtual files. It’s also useful in time-sensitive
    scenarios, such as a fast-paced game where players must rapidly move objects to
    different locations without the delay of simulated physical interaction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interactable Velocity Tracked Wedge**: This figure’s **Movement Type** setting
    is set to **Velocity Tracking**. Here, the movement relies on the speed and direction
    of your hand motion. Think of it like tossing a ball; the speed and angle of your
    throw will determine how the ball flies. In the VR space, this object’s motion
    mirrors the speed and trajectory of your gesture. This movement type is perfect
    for sports simulations such as VR baseball or basketball, where the speed and
    direction of the player’s hand play a role in the performance of the in-game action.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can observe these three movement types firsthand by running the demo scene
    on your VR headset or via the XR Device Simulator and navigating to the **Grab
    Interactable** **Objects** booth.
  prefs: []
  type: TYPE_NORMAL
- en: The next section introduces you to the different types of UI elements provided
    by the XR Interaction Toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting different types of UI elements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **UI Sample** prefab in the **Demo Scene** asset showcases the different
    types of UI elements currently provided by the XR Interaction Toolkit. You can
    see them in *Figure 3**.7*.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 3.7 – \uFEFFThe demo scene’s UI Sample prefab in the scene view](img/B20869_03_07.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – The demo scene’s UI Sample prefab in the scene view
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the arrow next to the prefab in the scene hierarchy window to inspect
    its child components. Let’s go through them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ModalSingleButton**: This UI element presents a short text message accompanied
    by a button. This layout makes it an ideal UI prefab whenever you need to display
    important pop-up notifications to the user of your VR experience. For instance,
    it could serve to inform users at the outset of a VR session that they’re about
    to embark on a timed experience. Alternatively, it can ensure that users have
    understood instructions or confirm their consent for data usage in a VR research
    study.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interactive Controls**: This prefab contains multiple UI elements with more
    detailed input choices for users compared to the **ModalSingleButton** prefab.
    **MinMaxSlider**, for instance, lets the user pinpoint a specific state within
    a minimum-to-maximum range by sliding over it with the VR controller. The **Dropdown**
    prefab, **Text Toggle** prefab, and **Icon Toggle** prefab grant similarly precise
    ways for users to communicate their preferences in the VR world. These components
    of the **Interactive Controls** prefab shine in contexts requiring intricate user
    feedback or directives. For instance, the user might select the room they want
    to see in a VR apartment viewing experience via a **Dropdown** menu. They could
    adjust the room lighting intensity with **MinMaxSlider**, and toggle between floor
    materials and wall paintings using **Text Toggle** and **Icon Toggle**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scroll UI Sample**: This prefab features a simple text block that users can
    scroll through using their VR controllers. This element is a handy tool for educational
    segments within VR, such as diving deep into the history of an art piece in a
    virtual museum tour.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By right-clicking on the UI sample prefab in the scene hierarchy window and
    selecting one of the options offered under UI, you can easily add more UI elements
    onto the **Canvas** component as you please.
  prefs: []
  type: TYPE_NORMAL
- en: The **TrackedDeviceGraphicRaycaster** script is another essential component
    of all UI interactions in VR scenes with the XR Interaction Toolkit. It should
    be attached to the **Canvas** component showcasing the UI elements. In the case
    of the **Demo Scene** asset, the **UI Sample** prefab is the **Canvas** component,
    so the script is attached to it, as you can also see in its inspector window.
    The **TrackedDeviceGraphicRaycaster** script gives your **Canvas** component an
    X-ray vision to discern when a player’s controller is aiming at an element on
    the **Canvas** component. Within the VR context, it transforms controller movements
    into interactions with the UI elements displayed on the **Canvas** component.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn how you can interact with buttons in your
    VR scene.
  prefs: []
  type: TYPE_NORMAL
- en: Interacting via buttons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Poke Interactions Sample** GameObject within our **Demo Scene** asset
    features three unique buttons that are shown in *Figure 3**.8*.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 3.8 – \uFEFFThree unique buttons from the Poke Interaction Sample\
    \ of the demo scene](img/B20869_03_08.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – Three unique buttons from the Poke Interaction Sample of the demo
    scene
  prefs: []
  type: TYPE_NORMAL
- en: Unlike ordinary buttons, these are designed to react to your touch in distinct
    ways. The button on the left side triggers a particle animation once it is poked;
    the button in the middle plays a sound, and the button on the right side increments
    the number on the UI element that is attached to it every time it is pressed.
  prefs: []
  type: TYPE_NORMAL
- en: 'These buttons all have the same three scripts attached to them: **XR Simple
    Interactable**, **XR Poke Filter**, and **XR Poke Interactor**. These scripts
    provide important functionalities that all of these buttons need, despite their
    distinct flavors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**XR Simple Interactable**: This script is the magic that makes our buttons
    interactive and touch-responsive. Without it, they would simply be static objects
    in the virtual world, unreactive to our touch. In the scene hierarchy window,
    select the first **Push** button and navigate to the inspector window. Now, click
    on the arrow next to the **Interactable Events** menu of the **XR Simple Interactable**
    script to see all interactable events associated with this script. Scroll down
    to the **Select** section and observe how the **Particle System** event is played
    or stopped as the user enters or exits the selection of this button. Repeat this
    step for the other two buttons to see what their **Select** sections look like.
    In the upcoming chapters of this book, you will create your own unique game logics
    in the **Interactable** **Events** section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**XR Poke Filter**: This script assesses the authenticity of a touch. If the
    touch doesn’t meet its criteria, the poke filter blocks its entry.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**XR Poke Follow Affordance**: This script provides the buttons with a lifelike,
    tactile response. Upon poking, the respective button is pressed down, mirroring
    the movement of a real-world button. Upon release, it rebounds back to its original
    position.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Throughout this book, we’ll delve deeper into creating diverse poke interactions,
    teaching you to customize these components to fulfill your specific requirements.
  prefs: []
  type: TYPE_NORMAL
- en: On our last step of exploring the **Demo Scene** asset, you’ll get a glimpse
    of more advanced features of the XR Interaction Toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring gaze interactions and climbing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Gaze interaction** is an immersive feature in VR that enables the user to
    trigger events and control aspects of the VR environment simply by directing their
    eye or head gaze. The demo scene of the XR Interaction Toolkit contains an entire
    booth just to showcase different types of gaze-based interactions. These types
    of interactions are among the most advanced features of the XR Interaction Toolkit,
    which is why you will learn how to implement them into your VR scenes in [*Chapter
    8*](B20869_08.xhtml#_idTextAnchor026), alongside other advanced techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Climbing, while a demanding concept in real life, is easier to enable in VR
    scenes than one might think, especially when compared to techniques such as gaze
    tracking. The **Demo Scene** asset’s **Climb Sample** prefab showcases two climbable
    objects: a ladder and a climbing wall. You can observe them in *Figure 3**.9*.'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 3.9 – \uFEFFThe Climb Sample prefab, consisting of a ladder and a\
    \ climbing wall among other elements](img/B20869_03_09.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – The Climb Sample prefab, consisting of a ladder and a climbing
    wall among other elements
  prefs: []
  type: TYPE_NORMAL
- en: At first glance, they may seem distinct, but underneath, they share the same
    scripts.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand how you can make an object in your VR scene climbable, follow
    these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the **Climb Sample** prefab in the scene hierarchy window.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Explore every child component of both the **Ladder** and the **Climbing Wall**
    prefab that has the *Climbable* label. In the inspector window, you’ll notice
    each of these climbable components has the same two scripts from the XR Interaction
    Toolkit attached to them: the **Climb Interactable** script and the **XR Interactable
    Affordance State** **Provider** script.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Simply adding these two scripts to any object in your VR environment via the
    inspector window provides it with climbable properties. Now, let’s dive deeper
    into the roles of these two scripts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Climb Interactable**: This script essentially makes an object climbable.
    When a player interacts with an object that has this script, they can virtually
    climb it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**XR Interactable Affordance State Provider**: This script aids in fine-tuning
    the interaction between the player and climbable objects. It’s a system that dictates
    how the climbing experience should function and respond to the user’s interactions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve explored all the key components of the demo scene, it’s time
    for you to test and explore it. In the upcoming section, we’ll discuss several
    options to get you started on this exciting journey.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying and testing VR experiences onto different VR platforms or simulators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating a VR scene goes beyond just design; it involves testing and deployment
    as well. While testing focuses on ensuring the VR scene works correctly and offers
    a good user experience, deployment is about transferring and optimizing the scene
    for playback on specific VR headsets, such as the *Meta Quest* series, *HTC Vive*,
    or *Valve Index*. Early development stages might use tools such as the XR Device
    Simulator for rapid testing, but as a project nears completion, thorough testing
    on an actual VR headset becomes essential.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s go through the various options for testing and deploying your VR
    scene based on what hardware is available to you:'
  prefs: []
  type: TYPE_NORMAL
- en: '*No VR headset available*: In cases where a VR headset is unavailable, you
    can use the XR Interaction Toolkit’s XR Device Simulator to test the features
    of your VR scene. Even if you do have a VR headset at your disposal, the simulator
    can offer a quicker alternative to physically connecting and testing on a VR headset,
    particularly when you’re making simple modifications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Standalone VR headset*: Most standalone VR headsets support PC-based VR mode.
    This mode, when enabled, is often the most efficient and powerful way to test
    your VR scene. However, there might be times when you don’t have the necessary
    cable to activate PC-based VR mode. In such cases, you have two main alternatives
    for testing your VR scene. The XR Device Simulator is typically the preferred
    method, because testing directly on a standalone VR headset requires deploying
    it onto the device. This process can be lengthy and time-consuming. It happens
    on the Android platform via the project’s *Build Settings*. So, if PC-based VR
    mode isn’t an option, you might use the XR Device Simulator for quick tweaks and
    minor scene changes. For substantial modifications or a comprehensive user experience
    test, deploying directly to the headset would be advisable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*PC-based VR headset or Standalone VR headset with PC-based VR mode enabled*:
    If you have either a PC-based VR headset or a standalone VR headset with PC-based
    VR mode enabled, your options for testing and deployment are expanded. For minor
    modifications to your VR scene, the XR Device Simulator might be all you need.
    For moderate to major changes, it is easy to test the VR scene on your headset
    by pressing the **Play** button in your Unity scene, provided the VR headset is
    connected to either the *SteamVR* or *Oculus* software. For considerable changes,
    or when analyzing the overall user experience, you can deploy your VR scene through
    **Windows/Mac/Linux** in **Build Settings** while the VR headset is connected
    to the *SteamVR* or *Oculus* software.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You might be curious about what *SteamVR* or *Oculus* software entails and
    when you’ll need them. Simply put, both are software platforms that not only let
    you play PC VR games but also test and deploy VR experiences on your headset.
    You will learn more about both in the upcoming sections of this chapter. In general,
    we would recommend the following, depending on the type of headset you own:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Headset from the Meta Quest series*: Install both SteamVR and Oculus software
    on your computer and learn to deploy your VR scene to your VR headset through
    both VR platforms, as these headsets support both. Both types of deployment are
    very easy to do, but they enable you not only to test your scene more thoroughly,
    but also to easily reach a wider audience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Valve Index, HTC Vive series, or any other non-Meta headset*: Only install
    SteamVR and learn to deploy your VR scene onto your VR headset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following sections will delve deeper into each of these options, providing
    a detailed guide on installing the XR Device Simulator, SteamVR, and Oculus software.
    Feel free to skip to the sections that are relevant to you.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the XR Device Simulator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we’ll learn about the essentials of installing and using the XR Device
    Simulator, a valuable tool in the VR development workflow that lets you emulate
    user inputs to control XR devices in a simulated environment. From importing the
    toolkit to using it effectively in your Unity projects, we’ll detail every aspect
    using comprehensive examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'As it is part of the XR Interaction Toolkit, you can easily import the XR Device
    Simulator via Unity’s package manager. Here is a step-by-step guide to doing this:'
  prefs: []
  type: TYPE_NORMAL
- en: Inside your Unity project, navigate to **Windows** | **Package Manager** | **XR**
    **Interaction Toolkit**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By selecting the **Samples** tab of the **XR Interaction Toolkit** package,
    you will find **XR Device Simulator** among the listed elements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By clicking the **Import** button next to the XR Device Simulator, you import
    it into your project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After successfully installing the XR Device Simulator, let’s use it to test
    out the **Demo Scene** asset of the XR Interaction Toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: Using the XR Device Simulator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To incorporate the XR Device Simulator into the demo scene or any of your future
    VR scenes, search for `XR Device Simulator` via the project window’s search bar.
    This way, you will find the **XR Device Simulator** prefab, the asset you will
    need to add to scenes where you want to simulate XR input. Now, simply drag this
    prefab into the scene hierarchy window of your VR scene. Before continuing, make
    sure the **XR Interaction Setup** prefab is also added to your scene. Now, click
    on the **Play** button to explore the power of the XR Device Simulator. *Figure
    3**.10* shows what the XR Device Simulator looks like in the **Play** mode.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 3.10 – \uFEFFThe Play mode of the XR Interaction’s Toolkit demo scene\
    \ when the XR Device Simulator is added to the scene](img/B20869_03_10.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 – The Play mode of the XR Interaction’s Toolkit demo scene when
    the XR Device Simulator is added to the scene
  prefs: []
  type: TYPE_NORMAL
- en: You are now equipped to navigate your scene using the key bindings supplied
    by the simulator, specifically the *WASD* keys. Hit the *Tab* key to switch active
    control between the left controller, right controller, and VR headset. Utilizing
    your mouse, you can explore the scene or alter the angles of your controllers,
    contingent on whether you are operating in headset or controller mode. Pressing
    the *G* key enables you to interact with objects in the scene, including pressing
    buttons. After experimenting with the XR Device Simulator and familiarizing yourself
    with the different keys, you’ll swiftly appreciate its simplicity and effectiveness
    as a testing tool for your VR scene before moving on to real XR hardware deployment.
  prefs: []
  type: TYPE_NORMAL
- en: As powerful and convenient as the XR Device Simulator might be, there is no
    experience more immersive than testing and deploying your scene onto a VR headset.
    The following section explains how to do so using SteamVR.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up SteamVR
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SteamVR is a platform developed by *Valve Corporation* for VR applications and
    content. It is part of the larger **Steam** platform, a very popular online marketplace
    for games and software, known for its extensive library and active community.
    For VR developers, using SteamVR can thus be very advantageous as VR content in
    SteamVR can potentially reach millions of users worldwide. SteamVR supports a
    wide range of VR headsets, including those from *HTC Vive*, *Valve Index*, *Windows
    Mixed Reality*, and the *Meta Quest* series. At the time of writing the book,
    we are not aware of any commercially available VR headset that doesn’t support
    SteamVR. It is basically a default standard in the VR headset market.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, even the *Meta Quest* series, designed on Android and not primarily
    PC-based VR, allows developers to deploy VR scenes and applications not just on
    Meta’s proprietary Oculus platform, but also on the SteamVR platform. This versatility
    is immensely beneficial. If you’re a *Meta Quest* series headset owner, we highly
    recommend learning to test and deploy your scenes on both platforms. Let’s illustrate
    this with an example.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you’re creating a VR game requiring intricate hand interactions such
    as object manipulation and complex maneuvers. You aim to cater to users of *HTC
    Vive*, *Valve Index*, and headsets from the *Meta Quest* series. By harnessing
    SteamVR and its SteamVR Unity plugin, you can develop the game once and ensure
    compatibility across all these headsets. SteamVR’s precise tracking system enhances
    the game’s immersive and responsive nature across various devices. Post-development,
    you can launch your game on the Steam platform, thus accessing a vast user base,
    including users of the *Meta Quest* series who are employing SteamVR.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, you will learn how to install SteamVR.
  prefs: []
  type: TYPE_NORMAL
- en: Installing SteamVR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To target SteamVR, you must first install and setup SteamVR. To do this, perform
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Download Steam to your PC from the official website ([https://store.steampowered.com/about/](https://store.steampowered.com/about/)),
    create a Steam account, and log into this Steam account via the Steam software.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Inside Steam, search for `SteamVR` in the Steam Store, select the **Play Game**
    button, and follow through with the on-screen instructions to begin installing
    SteamVR.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the installation of SteamVR is completed, connect your headset to your
    PC via a cable or a wireless connection such as *Air Link* for the *Meta Quest*
    series. Then, either search for the SteamVR plugin inside the Steam software and
    click the `SteamVR` in your regular PC apps and clicking on it. Either way, you
    should quickly see that your VR headset is connected to SteamVR on your PC. Inside
    your VR headset, you should now see the SteamVR environment and game store.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While some VR headsets, such as *Valve Index*, only necessitate the installation
    of SteamVR software for testing and deploying VR experiences, many other headset
    manufacturers demand additional software to be installed alongside SteamVR. For
    instance, *Windows Mixed Reality* devices, including the *HP Reverb* series, also
    require the *Mixed Reality Portal*. These installations are typically straightforward
    and usually don’t involve any further steps from you other than opening the downloaded
    software every time you want to connect your VR headset to the PC. As these software
    requirements can change over time, we recommend you check whether any manufacturer-specific
    software is needed for your particular VR headset model to test or deploy on it.
  prefs: []
  type: TYPE_NORMAL
- en: After installing and setting up SteamVR and potentially some other manufacturer-specific
    software, the next section reveals how you can finally test and deploy your scene
    onto your VR headset.
  prefs: []
  type: TYPE_NORMAL
- en: Testing and deploying your VR scene with SteamVR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When developing for SteamVR in Unity, you can test your VR scene directly in
    the Editor using the **Play** button. With the SteamVR plugin installed, the Unity
    Editor will recognize a connected SteamVR-compatible headset, and the scene will
    be rendered in VR. This enables you to quickly evaluate changes, troubleshoot
    issues, and refine your VR content without leaving the Unity environment.
  prefs: []
  type: TYPE_NORMAL
- en: When you’re ready to deploy your scene, you can build the project into an executable
    file that can be run outside of Unity.
  prefs: []
  type: TYPE_NORMAL
- en: After connecting your VR headset to SteamVR, go back into your Unity project.
    Here, go to **File** | **Build Settings** in your Unity project. Make sure that
    your preferred **Target Platform** field, such as Windows, is selected and that
    the **Architecture** field is set to 64-bit. Finally, you can deploy your VR scene
    onto your headset by clicking the **Build and Run** button in **Build Settings**.
    Once built, the project can be launched in SteamVR through the Steam client, allowing
    you to experience the VR scene on your headset as end users would.
  prefs: []
  type: TYPE_NORMAL
- en: If you own a VR headset from the *Meta Quest* series, setting up your VR scene
    for Oculus will be just as important to you as setting it up for SteamVR. The
    next section explains how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Oculus
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll explore the steps for setting up Oculus for your VR development
    projects. We’ll detail the installation of the Oculus app on your PC, how to test
    your VR scenes with Oculus, and how to deploy your VR scenes with Oculus.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Oculus
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To work with Oculus in Unity, you’ll first need to download and install the
    Oculus app onto your PC. Complete the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to [www.oculus.com/setup](http://www.oculus.com/setup) in your web
    browser.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Below the VR headset of your choice, such as *Meta Quest Pro*, click **Download
    Software**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the downloaded Oculus app and click the **Install** **Now** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Follow the on-screen instructions to create an Oculus account and set up your
    VR headset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After the installation and setup are completed, it is finally time to test and
    deploy your VR scene with Oculus. This process is described in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Testing and deploying your VR scene with Oculus
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once the Oculus software is installed on your PC, you can test your VR scenes
    directly in the Unity Editor. The following steps show you how:'
  prefs: []
  type: TYPE_NORMAL
- en: Connect your Oculus-compatible VR headset to your PC. Ensure the Oculus app
    recognizes it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open your VR project in Unity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Play** button in the Unity Editor to start the scene. Unity will
    automatically render the scene in VR, allowing you to interact with the scene
    using your Oculus headset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After testing and refining your VR scene, you can build and deploy it so it
    can be experienced outside of Unity. Complete the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: With your VR project open in Unity, go to **File** | **Build Settings**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make sure your preferred **Target Platform** field, such as Windows, is selected.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ensure the **Architecture** is set to 64-bit.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Build and Run** button. Unity will build an executable file of your
    project and run it. After the build is complete, the executable can be launched
    with the Oculus software. Now, you can fully experience your VR scene with your
    Oculus headset, just as your end users will.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this chapter, you’ve navigated the exciting landscape of VR development
    in Unity. You’ve learned to create a fundamental VR scene from scratch and mastered
    the deployment process on various devices. The utilization of the XR Interaction
    Toolkit and the informative elements from its demo scene should now be within
    your repertoire, empowering you to generate basic VR scenes with increased confidence
    and proficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, these acquired skills aren’t limited to a specific genre. Whether
    it be for industrial applications or academic research, your new proficiency in
    VR development should allow you to replicate this process effectively to fit any
    use case that you encounter.
  prefs: []
  type: TYPE_NORMAL
- en: As we venture into the next chapter, we will broaden our horizon to include
    the creation and deployment of AR scenes within Unity.
  prefs: []
  type: TYPE_NORMAL
