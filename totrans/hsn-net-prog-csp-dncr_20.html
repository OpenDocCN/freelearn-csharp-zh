<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Performance Analysis and Monitoring</h1>
                </header>
            
            <article>
                
<p>As you continue to develop your knowledge of building network software, we cannot overlook the key tasks of monitoring and performance tuning. Those two responsibilities will be the focus of this chapter, as we look at the tools that are available in .NET Core applications for monitoring and testing the performance and stability of your application. We'll be looking at the tools that are available to developers for putting your application under heavy load in controlled environments and observing its stability over time. We'll look at some naive logging and monitoring approaches, and consider how we can strengthen those approaches using some of the features of .NET Core.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Identifying performance bottlenecks in your network architecture, and designing to minimize them</li>
<li>Identifying end-to-end performance testing and reporting strategies</li>
<li>Establishing robust and resilient performance monitoring with C#</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>In this chapter, we'll be writing a number of samples to demonstrate various aspects of performance tracing and monitoring, all of which can be found here: <a href="https://github.com/PacktPublishing/Hands-On-Network-Programming-with-CSharp-and-.NET-Core/tree/master/Chapter%2016">https://github.com/PacktPublishing/Hands-On-Network-Programming-with-CSharp-and-.NET-Core</a><span>/tree/master/Chapter 16.</span></p>
<p><span>Check out the following video to see the code in action: <a href="http://bit.ly/2HYmD5r"/><a href="http://bit.ly/2HYmD5r"/><a href="http://bit.ly/2HYmD5r"/><a href="http://bit.ly/2HYmD5r">http://bit.ly/2HYmD5r</a></span></p>
<p class="mce-root">To work with this code, you'll want to use either of our trusty code editors: Visual Studio or Visual Studio Code. We'll also be using the REST clients you've come to know and love, so make sure you've got either PostMan installed (<a href="https://www.getpostman.com/downloads/">https://www.getpostman.com/downloads/</a>) or the Insomnia REST client (<a href="https://insomnia.rest/">https://insomnia.rest/</a>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Network performance analysis</h1>
                </header>
            
            <article>
                
<p>As you start to build more complicated distributed software systems, you begin to lose the granular control and fine-tuning you had over smaller, more isolated software projects. Each new network interaction introduces higher chances of systemic failure, reduces visibility on the source of bugs, and muddies the waters when searching for performance bottlenecks. The best way to mitigate these impacts is to get ahead of them, and design your distributed system with performance monitoring in mind from the start. So, how do you do that? What are the key metrics and interactions with which you should be concerned? What support does .NET Core provide to you?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">End-to-end performance impacts</h1>
                </header>
            
            <article>
                
<p>Imagine, if you will, that you're responsible for a cloud-hosted application suite. It includes a half-dozen microservices, each with a dependency on a half-dozen more. On top of that, each parallel resource is deployed behind a load-balancing network gateway responsible for routing requests to the server with the lowest current load.</p>
<p>Now, imagine that each component of that system was written almost entirely in isolation from the rest of the components. Your team of engineers was focused on the design principle of the separation of concerns, and so they thoroughly separated those concerns. Every data store was given its own, limited public API, and no other system was permitted direct access to its underlying database. All of the aggregation APIs are responsible for accessing each system of record to produce domain models relevant to your business use case. There's almost no overlap in responsibility.</p>
<p>Now, we'll generously imagine that your engineers were also disciplined in their testing strategies. Each service included a full suite of unit tests with nearly 100% code coverage. Every unit test was fully isolated with well-defined mocks for each of its dependencies. Your engineers were so disciplined and thorough that those mocks were configured to return every possible permutation of valid and exceptional responses that the service they were mocking might return in the live system. The data contracts for each service in your ecosystem was well-defined, and all changes were well tracked and accounted for by any dependent systems.</p>
<p>With all of this work diligently documented, maintained, and tested, you're finally ready to deploy version 1 of your application. Now, I want you to imagine that the first time someone tries to query your application, the entire ecosystem slows to a crawl, and the response doesn't return for a full 25 seconds.</p>
<p>This may seem like an absurd case, but I have seen almost this exact scenario actually happen with a team of inexperienced cloud architects deploying their first fully distributed microservice-based applications. So, what went wrong in this imagined scenario of ours, and how can we avoid it in practice?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Compounding latency</h1>
                </header>
            
            <article>
                
<p>It should be obvious at this point, but in this particular scenario, the primary culprit for the under-performance was isolation. With each service developed and tested entirely in a silo, there was no way that any engineers could measure the full impact of all of their backend dependencies. By always assuming a best-case scenario of near-zero latency with their upstream dependencies, the developers created unrealistic testing scenarios for their unit tests.</p>
<p>Integration and end-to-end testing are absolutely pivotal to designing a system that can withstand the strains and inconsistencies of a network-based hosting environment. With every new feature or microservice developed, the developers in our not-so-hypothetical development scenario should have deployed their solutions to an environment as close to their production configuration as possible. They should have been identifying bottlenecks in performance as they were implemented and mitigated those impacts.</p>
<p>One of the more challenging aspects of optimizing network software is that your single application is almost never the last stop for a user's interactions with it. Typically, your .NET Core services are being called by a user's browser, or another application service, at the least. Moreover, it's not uncommon for them to also rely on other services accessed through network protocols such as HTTP or TCP. What the developers in this scenario didn't realize, and what you should always be mindful of as you build more and more complicated distributed systems, is that each network hop in your chain of dependencies introduces latency and risk of failure.</p>
<p class="mce-root">With each upstream dependency, your application's latency is increased by the total average latency of your upstream dependency. Likewise, if your upstream dependency has an upstream dependency of its own, <em>its </em>latency will be whatever operational latency its own operations create, increased by the total average latency of <em>its </em>upstream dependency. This attribute of compounding the latency of networked systems is incredibly important to keep in mind whenever you're writing a network service that depends on another network service. This is why it's common with more experienced cloud architects to see strict enforcement of a three-tiered architectural model for the flow of request resolution.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A three-tiered architecture</h1>
                </header>
            
            <article>
                
<p>To minimize the impact of compounding latency, it's not uncommon to minimize vertical dependencies in an architecture. For our purposes, we can think of horizontal dependencies as being the full suite of upstream dependencies that originate from the same source. Meanwhile, vertical dependencies describe any upstream dependencies that, themselves, have additional upstream dependencies:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5f4b7da5-3327-488f-8c34-bf5a288435af.png" style="width:38.58em;height:22.75em;"/></p>
<p>In this diagram, Architecture A has what we would call a three-tiered architecture. That is to say that there are at most three tiers of services encapsulating the entire vertical dependency graph of application interactions. This is an ideal organizational structure for cloud-hosted services. Since most hosting contexts (and, indeed, .NET Core itself) support a broad number of parallel requests being processed, the latency increase of horizontal dependencies within a given system will only ever be as large as the slowest of all of the horizontal dependencies. This is not entirely dissimilar from the benefits gained by parallelizing asynchronous operations, as we discussed in the section <em>Picking up the pace - multithreading data processing</em>, <span>section of</span> <a href="b5d28c0a-6e7c-4547-855d-e6c6d1842bd6.xhtml">Chapter 6</a>, <em>Streams, Threads, and Asynchronous Data Transfer</em>.</p>
<p>In a strict three-tiered architecture, there can only be at most two network hops between a user and any piece of data that is ultimately presented to them. The three tiers are easily conceptualized, and should be familiar to you after our discussion of the MVC design paradigm back in <a href="e93c024e-3366-46f3-b565-adc20317e6ec.xhtml">Chapter 9</a>, <em>HTTP in .NET</em>. The first tier is the user-interaction layer, which just defines any system or mechanism with which an external user is able to access the data or processes contained within your ecosystem. Next is the aggregation tier, or the domain tier. This is where all of the business logic of the parent user-interaction is performed on any data the user is interested in seeing. Last, but not least, is the data tier. In modern cloud systems, these are typically HTTP-based APIs that expose an internal database that serves as the system of record for an enterprise dataset.</p>
<p>When the three-tiered paradigm is well enforced, no system is allowed to interact with a database except through its well-defined APIs. Meanwhile, if a UI needs the business logic defined in multiple aggregation services, it must either access all aggregation-tier systems itself, thus increasing its horizontal dependency map (which is considered acceptable) or a new aggregation service must be written that duplicates the work of the others. No aggregation system should ever call any other aggregation system. Doing so increases the vertical dependencies of that workflow and violates the three-tiered paradigm.</p>
<p>With this in mind, it should be easy to see why one of the most reliable ways to ensure high performance in your applications and implement a manageable monitoring system is by minimizing your vertical dependencies. If we can limit our search for bottlenecks to as few vertical tiers as possible, our ability to identify issues and resolve them can become almost trivial. And these compounding latency interactions only become more relevant as your software gets lower and lower in the network stack. Implementing a series of gateways or firewalls for any traffic moving between your local network and the wider internet will impact all traffic moving through your enterprise. Minimizing the vertical dependencies of such a system would need to be your first priority if you're to have any hope of success at minimizing the latency your gateway will introduce.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performance under stress</h1>
                </header>
            
            <article>
                
<p>Another common issue that arises when an insufficiently tested system is first deployed to a production environment is the application's inability to handle the load placed upon it. Even a strictly enforced three-tiered architecture can't minimize the impact of multiple responses left unprocessed due to an extremely high network load. The capacity for high volumes of network requests to utterly cripple a network resource is so great that it's actually the basis of an extremely well-known software attack known as a <strong>Dedicated Denial of Service</strong> (<strong>DDoS</strong>) attacks. In these sorts of attack, a distributed network of malware sends out a coordinated barrage of simple network requests to a single host. The volume of incoming requests absolutely destroys the host's ability to continue responding to them, locking up the resource for legitimate users, and even destabilizing the host's OS and physical infrastructure.</p>
<p>While a DDoS attack might be an extreme and relatively rare example, the same effects can be felt on a system with insufficient bandwidth and horizontal scalability for handling a high volume of legitimate simultaneous requests. The challenge here is that it's not always possible to know ahead of time what kind of traffic your system will encounter when it's deployed.</p>
<p>If you're writing an enterprise web service for a specific set of internal business users, you can probably<span> </span><span>define your operational parameters</span><span> </span><span>pretty clearly</span><span>. Conducting a simple headcount, combined with user interviews to determine how frequently a given user will interact with your system, can give you a high degree of confidence that you know exactly how much traffic you can reasonably anticipate in a given day. However, if your application is being written for a general release to the broader public, then it could be impossible to know beforehand just how many users will hit your service in a given day.</span></p>
<p>In the sorts of scenarios where you can't reasonably determine your maximum potential network traffic prior to experiencing high volumes of requests, then you will at least want to know roughly how much traffic your application can handle. This is where load testing comes in. With load testing, you should be targeting your best guess at the worst-case scenario for network traffic against your system.</p>
<p>Once you determine that maximum potential load, you execute a series of tests that generate as many interactions as possible with your system, up to and including your pre-determined maximum. To be truly valuable, your tests should be run on an instance of your software that's <span>deployed as near to your production configuration as possible. </span>Over the course of the tests, you should be logging and monitoring response times, and any exception responses. By the time the tests are complete, if they were designed well, you should have a robust set of metrics for how much traffic your current infrastructure can reasonably handle before it fails, and just what it looks like when that failure does finally occur.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performance monitoring</h1>
                </header>
            
            <article>
                
<p>Each of the performance bottlenecks and risks I've just discussed can be mitigated with good design and robust testing strategies. However, on distributed systems, failure is inevitable. As such, the risk of failure we seek to minimize with the testing and design of your system can never fully be eliminated. However, by using the results of our performance tests as a guide, we can minimize the impact of that inevitable failure. To do this, we'll need to implement a robust system of monitoring the health and availability of our services.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Naive monitoring strategies</h1>
                </header>
            
            <article>
                
<p>It's not uncommon for developers to confuse the concept of application monitoring with that of logging. And this is not an entirely unreasonable mistake. When done properly, a good logging strategy can serve as a relatively low-visibility monitoring system. The problem with that approach, though, is that logs are often incredibly <em>noisy.</em></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overuse of logging</h1>
                </header>
            
            <article>
                
<p>Think for a second of what your initial approach is when catching an exception in your code. I'd be willing to bet that a fair number of you start with a basic <strong>log and throw</strong> approach. You simply log that an error occurred (usually without much else in the way of details or context) and then re-throw the same error, or perhaps throw a new error of a type the calling code might be configured to respond to.</p>
<p>Meanwhile, I'm sure at least a few of my readers have worked in an environment where every change in scope was logged. Enter a new method? Log it. Exit the current method? Log it. Send out a TCP packet to a host? Log it. Receive a TCP packet? Log it. I could keep going, but I'll spare you the frustration.</p>
<p>This log everything approach is surprisingly common in large scale enterprises. What most people don't often consider, though, is that the more information you log, the less useful those logs become. As your log file grows with messages logging trivial changes in scope or context, it becomes increasingly harder to find the logs that are actually important. And, believe me, those are the logs you will be looking for when you find yourself going through a 200,000 line-long log file.</p>
<p>The term for this, if you've never heard of it, is the <strong>signal-to-noise</strong> ratio of your logs. The signal, in this ratio, describes the information that has meaning to you at the current moment. The noise, on the other hand, describes the information surrounding and obscuring your signal, which has no meaning to you at the current moment. Disciplined logging is not about logging everything diligently. Disciplined logging is about always logging only important things. This discipline should be applied to your logs, and any performance or health monitors you use in your software.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Delayed alerting</h1>
                </header>
            
            <article>
                
<p>Another common shortcoming with logging approaches is that the information we want to know about our applications isn't presented to us until it's too late. Typically, with a logging strategy for performance and health monitoring, the logs are written after the failure has occurred. While this is certainly more useful than no alerting system at all (when you're only notified about a system failure by receiving an angry phone call from a user), it's far from ideal.</p>
<p>You should know about a system outage before anyone else can encounter it. However, that kind of visibility is impossible if you're only ever writing to a passive log file, and only when an exception has been thrown. With network software, your application is likely leveraged by a wide array of other resources in such a way that system outages could propagate outward across your network and cripple the businesses your software was written to support. So, how can you get out in front of those inevitable outages and respond before any of your users can feel an impact?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Establishing proactive monitoring</h1>
                </header>
            
            <article>
                
<p class="mce-root">The best way to minimize the impact of an outage to your users is to identify when an outage has occurred, or is likely to occur, proactively. To do so, you'll need to define a policy for establishing health checks against your system. That strategy will have to determine a reasonable frequency for sending requests to your system. The frequency you determine will have to strike a balance between being frequent enough to have a high probability of detecting an outage before the outage could impact a customer, while being infrequent enough to not negatively impact the performance of your system under its heaviest load. On top of that, you'll need to identify all of the possible signals for an unhealthy system so that your monitors are well targeted for the most relevant aspects of your software's performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining thresholds</h1>
                </header>
            
            <article>
                
<p>When defining a performance monitoring strategy, the first thing you should do is define your application's performance thresholds. These will tell you the traffic load, or resource demands, beyond which your application is likely to experience failure. For instance, if you've done extensive load testing on your application, you will likely have a deep pool of metrics about how much traffic your application server can handle. By using network tracing, which will provide a record of every request against a listening network application, you can identify when your traffic is experiencing a spike, sending alerts before the failure threshold is reached.</p>
<p class="mce-root"/>
<p>Additionally, if you have upstream dependencies, you should take note of their performance over time, identifying any instance in which your dependencies are the source of your system failures. You should seek to leverage the same strategy of identifying signals in the content or latency of your dependency responses that are likely to result in a system failure for your user. Once you have that information, you are well positioned to respond to it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Proactive health checks</h1>
                </header>
            
            <article>
                
<p>Once you get to know the potential points or conditions of failure are within your system, the robust monitoring strategy tests those points and conditions frequently. <span>Services that are designed to actively monitor your system are commonly called </span><strong>watchdogs</strong><span>. </span>Most cloud-hosting orchestrators or CI/CD software will allow you to configure a health-check access point into your software so that they can identify when an application has crashed using their own watchdog implementations. If your hosting solution provides these features, such as the Azure health verification tool, then you should leverage them to their fullest. If your host doesn't provide a reliable, regularly scheduled (or configurable schedule) health-check mechanism, then I would strongly advise you to roll your own. The impact on your development process is minimal, but the benefits are incredible.</p>
<p>We'll look at this in our sample code in just a moment, but the typical pattern for this kind of proactive health and performance monitoring is to expose an endpoint that can be pinged by your hosting provider. That endpoint will then run a series of self-diagnostic checks against key potential points of failure, and return with either a success (healthy application, no likely failures imminent) or a failure (a system is down, or will soon be taken down).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Active messaging and active recovery</h1>
                </header>
            
            <article>
                
<p><span> A proactive health monitoring system requires a proactive response. For any error state detected by your health-monitoring system, you'll want to define a reasonable approach for responding to it. For instance, if your network traffic is spiking, a proactive approach may be configuring your health check system to automatically provision an additional parallel app server to respond to the additional request load. Meanwhile, if your health-check indicates that an upstream dependency has come offline, the active recovery system may attempt to restart the app server hosting that dependency in an effort to resolve the issue.</span></p>
<p><span>Regardless of what your system does to respond (though it should define at least some kind of response to a failure), it should absolutely notify any engineers who are working on the application. Specifically, it should actively notify engineers. While the event should certainly be logged to whatever audit system you use to track the reliability of your application, remember that <strong>a</strong> <strong>log</strong> <strong>is not an alert</strong>. Logs are inherently passive. For an engineer responsible for a system to learn that it's failed from a logged message, that engineer who's has to take it upon themself to seek out and check the logs. Meanwhile, with an active messaging response, your monitoring platform can be configured to take the contact information of any engineers that should know about outages. Then, when an outage has occurred, the active monitor can push out notifications to the contact information for every engineer that it is configured to notify.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performance and health monitoring in C#</h1>
                </header>
            
            <article>
                
<p class="mce-root">So, we should have a pretty clear understanding of what a robust and resilient performance and health-monitoring system looks like. It should be configured to respond to well-defined thresholds, so that potential outages can be spotted and mitigated before they occur. It should actively check the state of the application instead of passively waiting for user interactions to trigger a system failure. Finally, it should proactively respond to a failure once it has been identified, notifying anyone who might respond to it, and taking steps to bring the system back to a healthy state. So, what does this look like in .NET Core?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An unstable distributed architecture</h1>
                </header>
            
            <article>
                
<p>To demonstrate how we can use performance monitoring to remain aware of the health of an unstable system, we'll first have to build an unstable system. For this example, we'll be using a three-tiered architecture, with PostMan (or Insomnia) as our interaction tier, and then two different web APIs to simulate our aggregation tier and our data tier. First, we'll create our aggregation service:</p>
<pre><strong>dotnet new webapi -n AggregatorDemo</strong></pre>
<p>We'll be looking at this later, since this aggregator will be the application whose performance we monitor. For now, though, we'll need to define an upstream dependency. This will be designed to have a negative impact on our aggregation service's performance over time:</p>
<pre><strong>dotnet new webapi -n DataAccessDemo</strong></pre>
<p>We'll be focusing on the <kbd>DataAccessDemo</kbd> application to begin with. Here, our objective will be to create a destabilized system that contains a mechanism for its own recovery. We'll use this in our Aggregator application, when poor performance is detected, to proactively recover from degrading system performance. To that end, our <kbd>DataAccessDemo</kbd> application will be relatively straightforward. We'll provide two endpoints: one will be used as an upstream dependency for our aggregator application, and one will be used to recover from degrading performance.</p>
<p>In practice, it's not uncommon to provide an endpoint that serves as a performance management access-point to a live application. It might reinitialize a cache, force garbage collection, or dispose of any lingering threads in its thread pool. In our case, we'll simply reset a counter to restabilize our application's performance. That counter will then be used by our dependency endpoint to enforce an ever-increasing latency in the application.</p>
<p>To initiate this, we'll first define our listening ports to avoid a collision with our aggregator API:</p>
<pre>public static IWebHostBuilder CreateWebHostBuilder(string[] args) =&gt;<br/>    WebHost.CreateDefaultBuilder(args)<br/>        .UseUrls("https://[::]:33333")<br/>        .UseStartup&lt;Startup&gt;();</pre>
<p>Now, we'll define a static class to hold on to our latency counter. This will be designed so that every time it's accessed, it increases the latency for the next call. Additionally, to facilitate our recovery endpoint, we'll provide a mechanism for resetting the counter to its initial state, as follows:</p>
<pre>public static class Latency {<br/>    private static int initialLatency = 1;<br/>    private static int counter = 1;<br/><br/>    public static int GetLatency() {<br/>        //milliseconds of latency. increase by .5 second per request<br/>        return counter++ * 500; <br/>    }<br/><br/>    public static void ResetLatency() {<br/>        counter = initialLatency;<br/>    }<br/>}</pre>
<p>Finally, from within our controller, we'll set a delay to our dependency request, after which we'll return an arbitrary value. Then, we'll expose a reset endpoint to stabilize our application, <span>as follows:</span></p>
<pre>[Route("api/[controller]")]<br/>[ApiController]<br/>public class DependencyController : ControllerBase {<br/>    [HttpGet("new-data")]<br/>    public ActionResult&lt;string&gt; GetDependentValue() {<br/>        Thread.Sleep(Latency.GetLatency());<br/>        return $"requested data: {new Random().Next() }";<br/>    }<br/>        <br/>    [HttpGet("reset")]<br/>    public ActionResult&lt;string&gt; Reset() {<br/>        Latency.ResetLatency();<br/>        return "success";<br/>    }<br/>}</pre>
<p>This controller signature will define the vertical dependency of our aggregator API. So, Now, let's look at how we'll monitor the degrading performance of this application over time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performance-monitoring middleware</h1>
                </header>
            
            <article>
                
<p>To monitor the health of specific aspects of our application, we'll be creating an instance of the <kbd>IHealthCheck</kbd> <strong>middleware</strong>. This middleware is used to define the operations that are necessary for determining the relative health of your system. For our purposes, we'll want to define a health-check that confirms that the response time for our dependent data service is below two seconds. Once that threshold has been hit, we'll consider the system to be degraded. This will allow our watchdog to notify us that a restart may be necessary. However, if the service response time increases to five seconds, we'll consider it unhealthy, notifying our watchdog to initiate a reset.</p>
<p>This instance of the <kbd>IHealthCheck</kbd> middleware will be registered by our system in startup, when we add health-checks to our application. And, as with all things in .NET Core, adding a health-check is as easy as registering a service and configuring your app. So, first, within your <kbd>ConfigureServices(IServicesCollection services)</kbd> method, simply add the following line of code to set up all of the supporting classes and extensions that are necessary for using health checks:</p>
<pre>services.AddHealthChecks();</pre>
<p>Then, within your <kbd>Configure(IApplicationBuilder app, IHostingEnvironment env)</kbd> method, add the following code:</p>
<pre>app.UseHealthChecks("/health");</pre>
<p>This simple registration pattern sets your application up to provide the results of all configured health-check middleware whenever a user sends a request to the path specified in <kbd>UseHealthChecks(&lt;path&gt;)</kbd>. Because we haven't specified any specific system checks in our middleware, this endpoint will return nothing of interest. To test it, simply navigate to your application's root host, and append the <kbd>/health</kbd> path. You should see the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-943 image-border" src="assets/307ebfcd-00ce-4798-a1a6-a7ac5da80981.png" style="width:80.17em;height:30.17em;"/></p>
<p>This confirms that our application is responding to health-checks. So, now, we need to define the specific check we want to use when validating the health of our system by defining our middleware. We'll start by creating an implementation for the <kbd>IHealthCheck</kbd> interface, which we'll call <kbd>DependencyHealthCheck</kbd> for simplicity's sake. This class will contain the definitions for our response thresholds, measured in milliseconds, as shown in the following code:</p>
<pre>public class DependencyHealthCheck : IHealthCheck {<br/>    private readonly int DEGRADING_THRESHOLD = 2000;<br/>    private readonly int UNHEALTHY_THRESHOLD = 5000;</pre>
<p>Now, we'll need to implement the public <kbd>CheckHealthAsync()</kbd> method to satisfy the requirements of the <kbd>IHealthCheck</kbd> interface. For this method, we'll be sending a request to our upstream dependency, and tracking the time it takes to resolve it using the <kbd>Stopwatch</kbd> class from the <kbd>System.Diagnostics</kbd> namespace.</p>
<p>The signature for the <kbd>CheckHealthAsync()</kbd> method accepts a <kbd>HealthCheckContext</kbd> instance that will provide our class with registration information, as well as a cancellation token. We'll be ignoring those, but they are still required to satisfy the interface signature. Then, we'll be creating our <kbd>HttpClient</kbd> instance (using the <kbd>HttpClientFactory</kbd> we discussed in <a href="e93c024e-3366-46f3-b565-adc20317e6ec.xhtml">Chapter 9</a>, <em>HTTP in .NET</em>) and building a request to the endpoint whose stability we want to validate. So, the first part of our <kbd>CheckHealthAsync()</kbd> method should look like this:</p>
<pre>public async Task&lt;HealthCheckResult&gt; CheckHealthAsync(<br/>    HealthCheckContext context,<br/>    CancellationToken token = default(CancellationToken)<br/>) {<br/>    var httpClient = HttpClientFactory.Create();<br/>    httpClient.BaseAddress = new Uri("https://localhost:33333");<br/>    var request = new HttpRequestMessage(HttpMethod.Get, "/api/dependency/new-data");</pre>
<p>At this point, we'll want to set up our stopwatch to track how long a request takes so that we can confirm whether it falls beneath our designated thresholds:</p>
<pre>Stopwatch sw = Stopwatch.StartNew();<br/>var response = await httpClient.SendAsync(request);<br/>sw.Stop();<br/>var responseTime = sw.ElapsedMilliseconds;</pre>
<p>And, finally, we'll check the value of the response time against our thresholds, returning an instance of the <kbd>HealthCheckResult</kbd> according to the stability of our external dependency, as follows:</p>
<pre>if (responseTime &lt; DEGRADING_THRESHOLD) {<br/>    return HealthCheckResult.Healthy("The dependent system is performing within acceptable parameters");<br/>} else if (responseTime &lt; UNHEALTHY_THRESHOLD) {<br/>    return HealthCheckResult.Degraded("The dependent system is degrading and likely to fail soon");<br/>} else {<br/>    return HealthCheckResult.Unhealthy("The dependent system is unacceptably degraded. Restart.");<br/>}</pre>
<p>This completes our implementation of the <kbd>IHealthCheck</kbd> middleware, so now all that's left is to register it with our application services as a health-check. To do so, simply call the <kbd>AddCheck()</kbd> method of the <kbd>IHealthChecksBuilder</kbd> class that's returned by the <kbd>AddHealthChecks()</kbd> method in your <kbd>Startup.cs</kbd> file. This method takes a name, and an explicit instance of your <kbd>IHealthCheck</kbd> middleware implementation.</p>
<p>Adding this, your <kbd>Startup.cs</kbd> file should have the following code:</p>
<pre>services.AddHealthChecks()<br/>    .AddCheck("DataDependencyCheck", new DependencyHealthCheck());</pre>
<p>And, with that, we've got a custom health check that will notify any monitors of instability in our system caused by failure of an upstream dependency! To confirm that it's returning the appropriate response, simply ping your <kbd>/health</kbd> endpoint 5 to 10 times to increase the latency counter on your dependency service, and watch as the health status changes past the two- and five-second threshold.</p>
<p>Here, you can see that the response from my service comes back as <kbd>Degraded</kbd> when the response time has broken the two-second threshold:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-944 image-border" src="assets/db64ff6a-23e0-44eb-8397-d82147eda182.png" style="width:78.83em;height:30.50em;"/></p>
<p>And after only a few more requests, the response decays to <kbd>Unhealthy</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-945 image-border" src="assets/640a62af-d90c-4711-b80f-a0234bbd087b.png" style="width:79.50em;height:31.33em;"/></p>
<p class="mce-root">So, now you've seen how, with only a few lines of code, you can implement an extensible system for monitoring your complete system health.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing a watchdog</h1>
                </header>
            
            <article>
                
<p>The final piece of our proactive monitoring solution is to implement a watchdog service of our own. Since we've configured an external hook for resetting the performance of our unstable API, we'll want to define a proactive recovery mechanism that takes advantage of it. So, for this service, we'll set up a loop that pings the <kbd>/health</kbd> endpoint of our aggregator API at regular intervals, and then resets the data dependency API whenever the system comes back as <kbd>Degraded</kbd> or <kbd>Unhealthy</kbd>. Since this is a fairly trivial exercise in this case, we can keep our watchdog simple. We'll start with a console app, created with our CLI:</p>
<pre><strong>dotnet new console -n WatchdogDemo</strong></pre>
<p>We'll only be adding a few lines to our project. First, we'll set up our <kbd>HttpClient</kbd> instance. Since the scope of our application never leaves the <kbd>Main()</kbd> method, we can safely create a private, static, single instance of the <kbd>HttpClient</kbd> class without needing to rely on an <kbd>HttpClientFactory</kbd> to manage multiple instances of our clients and prevent thread starvation. We'll also assign our <kbd>Healthy</kbd> status code to a <kbd>private readonly</kbd> variable:</p>
<pre>public class Program {<br/>    private static readonly HttpClient client = new HttpClient();<br/>    private static readonly string HEALTHY_STATUS = "Healthy";</pre>
<p>Once that's in place, our <kbd>Main()</kbd> method amounts to a dozen lines of relatively trivial code. First, we create an infinite loop so that our service runs until explicitly stopped. Then, we send a request to get the health status response from our aggregator API:</p>
<pre>static async Task Main(string[] args) {<br/>  while(true) {<br/>    var healthRequest = new HttpRequestMessage(HttpMethod.Get, "https://localhost:44444/health");<br/>    var healthResponse = await client.SendAsync(healthRequest);<br/>    var healthStatus = await healthResponse.Content.ReadAsStringAsync();</pre>
<p>Once we've got the response back, we can confirm that it's come back healthy. If it hasn't, we simply send another request to the <kbd>/reset</kbd> endpoint of our data service. Meanwhile, if it has come back healthy, we can simply log the request and continue processing:</p>
<pre>if (healthStatus != HEALTHY_STATUS) {<br/>  Console.WriteLine($"{ DateTime.Now.ToLocalTime().ToLongTimeString()} : Unhealthy API. Restarting Dependency");<br/>  var resetRequest = new HttpRequestMessage(HttpMethod.Get, "https://localhost:33333/api/dependency/reset");<br/>  var resetResponse = await client.SendAsync(resetRequest);<br/>} else {<br/>  Console.WriteLine($"{DateTime.Now.ToLocalTime().ToLongTimeString()} : Healthy API");<br/>}<br/>Thread.Sleep(15000);</pre>
<p>Note that, for this example, I've determined that our health-check intervals should be 15 seconds. In our case, I've chosen this value mostly at random. However, as you configure or implement watchdog solutions in your own applications, you would do well to consider the impact of too-frequent or too-infrequent intervals, and configure your system accordingly. With all three of our applications up and running, you can see here that my watchdog system is performing according to expectations, resetting my data service whenever the response times have become unacceptably slow:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-946 image-border" src="assets/47c772ca-0ba8-4a97-8b2e-ff33ac980bb7.png" style="width:42.83em;height:14.92em;"/></p>
<p>And, with that, our proactive monitoring system is complete. While we had to manufacture instability in this sample code, the strategies we devised to monitor and mitigate that instability will scale with any project you might have to write in the future.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This chapter opened up with an unfortunately common example of a naively implemented distributed architecture. Using that as a touchstone, we considered the impact that architectural design decisions can have on an application's performance. We learned about vertical dependencies in our distributed architectures, and how we can mitigate their impact with a strictly-adhered-to three-tiered design.</p>
<p>Next, we looked at how bad logging practices can make it untenable to use logging as a performance monitoring solution. We discussed the concept of a logging solution's signal-to-noise ratio, and we examined the practices we should all apply to our own logging strategies to maximize the effectiveness of our logs.</p>
<p>Once we fully examined why logging constitutes an insufficient monitoring strategy, we dove into the attributes of a truly robust approach. We saw that a well-designed monitoring strategy should take advantage of the various unit and load tests you've performed, and the information they've provided. We learned that a robust strategy involves active monitoring, and should seek to identify problems systematically before a user ever encounters an issue. Finally, we saw how such a solution could, and should, take a proactive approach to try to resolve any health or performance issues, and notify the relevant engineers of the issues.</p>
<p>Last, but not least, we looked at precisely how to implement just such a monitoring strategy in .NET Core. Implementing our own monitoring middleware in an ASP.NET Core application, we saw the power and flexibility of the built-in monitoring solutions available from the framework. With this piece of the architectural puzzle in place, we're ready to start exploring some more advanced subjects. With that in mind, we'll use the next chapter to explore how we might devise our own application layer protocol using the concept of pluggable protocols in .NET Core.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What is the difference between horizontal dependencies and vertical dependencies? How do they impact performance?</li>
<li>What is a three-tiered architecture?</li>
<li>What is a signal-to-noise ration? Why is it important in logging?</li>
<li>What are some of the reasons logging is insufficient for performance monitoring?</li>
<li>What are the key attributes of a robust monitoring strategy?</li>
<li>What is load testing? What information can it provide?</li>
<li>What does it mean to have a disciplined logging strategy?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p><span>For an additional resource on leveraging health checks and performance monitoring in your ASP.NET Core applications, I'd recommend <em>Learning ASP.NET Core 2.0</em> by Jason de Oliveira and Michel Bruchet, and I particular, the chapter entitled <em>Managing and Supervising ASP.NET Core 2.0 Applications</em>. It's an exceptionally good read and will provide a wealth of skills that are laterally transferable to any number of different contexts. It can be found from Packt, here: </span><a href="https://www.packtpub.com/application-development/learning-aspnet-core-20">https://www.packtpub.com/application-development/learning-aspnet-core-20</a>.</p>
<p>Additionally, if you'd like to continue down the path of learning architectural design, with a focus on microservices-based ecosystems, I'd recommend <em>Enterprise Application Architecture with .NET Core</em> by Ganesan Senthilvel, Ovais Mehboob Ahmed Khan, and Habib Ahmed Qureshi. The depth with which they cover a multi-layered architecture in a cloud environment is illuminating. It can be found, as always, through Packt: <a href="https://www.packtpub.com/application-development/enterprise-application-architecture-net-core">https://www.packtpub.com/application-development/enterprise-application-architecture-net-core</a>.</p>


            </article>

            
        </section>
    </body></html>