<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-132"><a id="_idTextAnchor131"/>8</h1>
<h1 id="_idParaDest-133"><a id="_idTextAnchor132"/>Writing Structured and Correlated Logs</h1>
<p>Distributed tracing is a great tool to describe and correlate operations, but sometimes, we need to record things such as callbacks and startup configurations, or conditionally write debug information. In this chapter, we’re going to explore logs – the oldest and most popular telemetry signal that can describe anything.</p>
<p>First, we’ll talk about logging use cases and discover different APIs available in .NET, and then we’ll focus on <code>ILogger</code> – a common logging façade. We’ll learn how to use it efficiently to write structured events. We’ll see how to export logs with OpenTelemetry and write rich queries over them. Finally, we’ll explore log sampling and cost-saving strategies.</p>
<p>In this chapter, you’ll learn the following:</p>
<ul>
<li>When to write logs and which .NET API to use</li>
<li>How to write logs with the <code>Microsoft.Extentions.Logging.ILogger</code> class</li>
<li>How to capture and export logs with OpenTelemetry</li>
<li>Cost-management strategies with the OpenTelemetry Collector</li>
</ul>
<p>By the end of this chapter, you will be able to efficiently instrument your application with logs and events to debug and analyze service behavior.</p>
<h1 id="_idParaDest-134"><a id="_idTextAnchor133"/>Technical requirements</h1>
<p>The code for this chapter is available in the book’s repository on GitHub at <a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter8">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter8</a>.</p>
<p>To run samples and perform analysis, we’ll need the following tools:</p>
<ul>
<li>.NET SDK 7.0 or later</li>
<li>Docker and <code>docker-compose</code></li>
</ul>
<h1 id="_idParaDest-135"><a id="_idTextAnchor134"/>Logging evolution in .NET</h1>
<p>Logs are the<a id="_idIndexMarker464"/> most flexible<a id="_idIndexMarker465"/> telemetry signal and usually include a timestamp, a level, a category, a message, and sometimes attributes.</p>
<p>Logs are frequently intended to be human-readable and don’t have a strict structure. Here’s an example of a log record written to <code>stdout</code> by an ASP.NET Core application:</p>
<pre class="source-code">
info: Microsoft.Hosting.Lifetime[14]
      Now listening on: http://localhost:5050</pre>
<p>If we need to investigate something, we’d first look for logs describing interesting operations and then read the filtered logs. Our ability to understand what happened depends on how much context is logged and how searchable it is, with tools such as <code>grep</code>.</p>
<p>Structured logs<a id="_idIndexMarker466"/> are sometimes called <strong class="bold">events</strong>. Events are intended to be queried, potentially across<a id="_idIndexMarker467"/> multiple requests and based on any property, and need a strict and consistent structure. Here’s the previous log record in the OpenTelemetry JSON format:</p>
<pre class="source-code">
"timeUnixNano":"1673832588236902400",
"severityNumber":9, "severityText":"Information",
"body":{"stringValue":"Now listening on: {address}"},
"attributes":[
  {"key":"dotnet.ilogger.category",
     "value":{"stringValue":"Microsoft.Hosting.Lifetime"}},
  {"key":"Id","value":{"intValue":"14"},
  {"key":"address",
     "value":{"stringValue":"http://[::]:5050"}}], "traceId":"",
        "spanId":""}</pre>
<p>It’s not human-readable, but even when written to <code>stdout</code> or a file, it can be easily parsed into structured records without any prior knowledge of the semantics of the event.</p>
<p>As we started exploring in <a href="B19423_01.xhtml#_idTextAnchor016"><em class="italic">Chapter 1</em></a>, <em class="italic">Observability Needs of Modern Applications</em>, the difference between logs and events is semantical – the same information can be optimized and printed in human-readable format or stored and indexed in a database in a structured format.</p>
<p>We’re going to learn<a id="_idIndexMarker468"/> how to write <a id="_idIndexMarker469"/>such structured logs with the <code>Microsoft.Extensions.Logging.ILogger</code> class, but first, let’s take a quick look at other logging APIs in .NET.</p>
<h2 id="_idParaDest-136"><a id="_idTextAnchor135"/>Console</h2>
<p>We can use the <code>System.Console</code> class as a logger<a id="_idIndexMarker470"/> and write everything to <code>stdout</code>. We’d need to implement all logging primitives from scratch and forward <code>stdout</code> to the log management system, parsing it along the way to bring the original log structure back. Logging to <code>Console</code> is neither a convenient nor an efficient solution.</p>
<h2 id="_idParaDest-137"><a id="_idTextAnchor136"/>Trace</h2>
<p>The <code>System.Diagnostics.Trace</code> and <code>System.Diagnostics.TraceSource</code> classes provide<a id="_idIndexMarker471"/> methods to write messages, along with some arguments, and support logging levels. We can also listen to them with the <code>TraceListener</code> class to export them to a log management system.</p>
<p>It seems like a good start, but there are a couple of limitations:</p>
<ul>
<li>The <code>TraceSource</code> API does not provide a standard way to write arguments. So, it’s easy to format a message as a string, but we need to know specific event semantics to know argument names.</li>
<li>By default, <code>TraceSource</code> and <code>TraceListener</code> use a global lock on every operation. It’s possible to use them in a lock-free way, but it might be easy to overlook until the load becomes<a id="_idIndexMarker472"/> high enough.</li>
</ul>
<p>So, <code>Trace</code> APIs solve some logging problems but introduce new ones.</p>
<h2 id="_idParaDest-138"><a id="_idTextAnchor137"/>EventSource</h2>
<p><code>System.Diagnostics.Tracing.EventSource</code> is another logging<a id="_idIndexMarker473"/> API in .NET. It’s designed for high-performance scenarios, supports logging levels, and rich payloads, and captures the names and values of arguments. It’s possible to listen to it by implementing the <code>EventListener</code> class or with .NET diagnostics tools running as a side-car process.</p>
<p><code>EventSource</code> is a part of the .NET platform and can be used directly without any extra dependencies. <code>EventSource</code> is a perfect candidate to log in libraries that don’t want to add any new dependencies.</p>
<p>When it comes to consumption, many observability vendors provide custom packages to listen to event sources, but there is no integration with OpenTelemetry yet, which is likely to change by the time you read it.</p>
<p>EventSource events can also be captured with <code>dotnet</code> diagnostics tools – <code>dotnet-trace</code> and <code>dotnet-monitor</code> – as we saw in <a href="B19423_04.xhtml#_idTextAnchor068"><em class="italic">Chapter 4</em></a>, <em class="italic">Low-Level Performance Analysis with Diagnostic Tools</em>, and <a href="B19423_02.xhtml#_idTextAnchor038"><em class="italic">Chapter 2</em></a>, <em class="italic">Native Monitoring </em><em class="italic">in .NET</em>.</p>
<h2 id="_idParaDest-139"><a id="_idTextAnchor138"/>ILogger</h2>
<p><code>Microsoft.Extensions.Logging.ILogger</code> is a common logging<a id="_idIndexMarker474"/> façade integrated<a id="_idIndexMarker475"/> with ASP.NET Core. It supports structured logging and levels and has a rich ecosystem, making it easy to configure and send data to any provider, local or remote.</p>
<p>Logs written with <code>ILogger</code> can be consumed from other logging libraries, such as <code>Serilog</code> or <code>NLog</code>, and it’s also supported by OpenTelemetry. Many observability backends support <code>ILogger</code>, making it a perfect tool to write application logs with.</p>
<p><code>ILogger</code> logs can be captured out of process with .NET diagnostics tools. This is done by forwarding logs to <code>EventSource</code> first with the <code>Microsoft.Extensions.Logging.EventSource .EventSourceLoggingProvider</code> class. This provider is enabled by default in ASP.NET Core applications, and you can configure it manually with the <code>AddEventSourceLogger</code> extension method for the <code>ILoggingBuilder</code> interface. We used this mechanism to capture<a id="_idIndexMarker476"/> logs with <code>dotnet-monitor</code> and control log verbosity<a id="_idIndexMarker477"/> dynamically in <a href="B19423_02.xhtml#_idTextAnchor038"><em class="italic">Chapter 2</em></a>, <em class="italic">Native Monitoring </em><em class="italic">in .NET</em>.</p>
<p>Let’s go through the <code>ILogger</code> usage in more detail.</p>
<h1 id="_idParaDest-140"><a id="_idTextAnchor139"/>Logging with ILogger</h1>
<p>The <code>ILogger</code> class<a id="_idIndexMarker478"/> is part<a id="_idIndexMarker479"/> of the <code>Microsoft.Extensions.Logging.Abstractions</code> NuGet package. If you work on an ASP.NET Core application, worker service, or use other <code>Microsoft.Extensions</code> packages, you already depend on it transitively.</p>
<p>The <code>ILogger</code> interface exposes a few methods:</p>
<ul>
<li><code>Log</code> records a log message with a given level, ID, exception, state, and formatter. The state type is generic but should contain a message, along with all the parameters and their names.</li>
<li><code>IsEnabled</code> checks whether logging at this level is enabled.</li>
<li><code>BeginScope</code> adds an object to the logging scope, allowing you to enrich nested log records with it. We saw scopes in action in <a href="B19423_02.xhtml#_idTextAnchor038"><em class="italic">Chapter 2</em></a>, <em class="italic">Native Monitoring in .NET</em>, where we annotated console logs with trace context and ASP.NET Core request information.</li>
</ul>
<p>It’s common to use convenient extension methods defined in the <code>Microsoft.Extensions.Logging.LoggerExtensions</code> class instead of the vanilla <code>ILogger.Log</code> method.</p>
<p>Before writing any logs, let’s first obtain an instance of <code>ILogger</code> – in ASP.NET Core applications, we can do it with constructor parameter injection, as shown in this example:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">frontend/RetryHandler.cs</p>
<pre class="source-code">
private readonly ILogger&lt;RetryHandler&gt; _logger;
public RetryHandler(ILogger&lt;RetryHandler&gt; logger) =&gt;
    _logger = logger;</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter8/memes/frontend/RetryHandler.cs ">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter8/memes/frontend/RetryHandler.cs</a></p>
<p>Here, we obtain an <code>ILogger</code> instance with the <code>RetryHandler</code> type parameter. The full name of the type<a id="_idIndexMarker480"/> parameter translates into the logging <strong class="bold">category</strong>, which is important to control verbosity and query logs, as we’ll see later in the <em class="italic">Capturing logs with </em><em class="italic">OpenTelemetry</em> section.</p>
<p class="callout-heading">Note</p>
<p class="callout">Please refer<a id="_idIndexMarker481"/> to the .NET documentation at <a href="https://learn.microsoft.com/aspnet/core/fundamentals/logging">https://learn.microsoft.com/aspnet/core/fundamentals/logging</a> to learn how to create and configure loggers.</p>
<p>Now, we can finally<a id="_idIndexMarker482"/> log things. For example, we<a id="_idIndexMarker483"/> can write an information log with <code>_logger.LogInformation("hello world")</code>.</p>
<p>If you use standard logging implementation, this call is broadcast to all registered logging providers that have the <code>Information</code> level enabled for this logging category.</p>
<p>Filters are provided at configuration time and can be global or specific to the logging provider. For example, here’s a global logging configuration in our memes application:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">frontend/appsettings.json</p>
<pre class="source-code">
"Logging": {
  "LogLevel": {
    "frontend": "Information",
    "Microsoft.Hosting.Lifetime": "Information",
    "Default": "Warning"
  }
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter8/memes/frontend/appsettings.json ">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter8/memes/frontend/appsettings.json</a></p>
<p>This global configuration sets the <code>Information</code> level for the <code>frontend</code> and <code>Microsoft.Hosting.Lifetime</code> categories and <code>Warning</code> for everything else.</p>
<p>Let’s get back to the <code>ILogger</code> API and see how<a id="_idIndexMarker484"/> we can write more useful logs. For example, let’s log<a id="_idIndexMarker485"/> debug messages for error responses that include a response body.</p>
<p>We should be cautious here – a body stream usually can only be read once and can be very long, but in any case, we should be able to control any overhead that is introduced:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">frontend/RetryHandler.cs</p>
<pre class="source-code">
if (!response.IsSuccessStatusCode &amp;&amp;
     _logger.IsEnabled(LogLevel.Debug))
  _logger.LogDebug("got response: {status} {body} {url}",
    (int)response.StatusCode,
    await response.Content.ReadAsStringAsync(),
    response.RequestMessage?.RequestUri);
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter8/memes/frontend/RetryHandler.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter8/memes/frontend/RetryHandler.cs</a></p>
<p>Here, we write the log record at the <code>Debug</code> level and check whether the level is enabled <em class="italic">before</em> reading<a id="_idIndexMarker486"/> the response stream. We also use <strong class="bold">semantic</strong> (aka structured) logging, providing parameter names in curly brackets in the message string and their values as arguments.</p>
<p class="callout-heading">Note</p>
<p class="callout">Make sure to use semantic logging. String interpolation or explicit formatting for <code>ILogger</code> messages removes the structure and makes performance optimization based on logging level impossible.</p>
<p>Arguments are passed as objects. <code>ILogger</code> implementations, such as <code>OpenTelemetryLogger</code>, support some types and usually call the <code>ToString</code> method on everything else. If logging at this level is not enabled, <code>ToString</code> is never called, saving you some CPU cycles and memory allocations.</p>
<p>Guarding logging<a id="_idIndexMarker487"/> calls, along with the retrieval or computation of arguments<a id="_idIndexMarker488"/>, with an <code>IsEnabled</code> check, is a great way to keep the performance impact of disabled categories very low.</p>
<h2 id="_idParaDest-141"><a id="_idTextAnchor140"/>Optimizing logging</h2>
<p>Logging-related code<a id="_idIndexMarker489"/> frequently becomes a source of performance degradation. Avoiding memory allocations and the computation of arguments, especially when logging at this level is disabled, is the first step, but we should also optimize logging on the hot path when it’s enabled. Here’re a few tips for it:</p>
<ul>
<li><strong class="bold">Avoid excessive logging</strong>: You might need to write a log record when entering an important code branch, a callback is called, or an exception is caught. Avoid logging exceptions multiple times as they propagate, or logging the same callback in nested methods.</li>
<li><strong class="bold">Avoid duplication</strong>: Unify multiple logs related to the same operation, and use logs coming from ASP.NET Core and other libraries when they are available, instead of adding your own.</li>
<li><strong class="bold">Avoid calculating any values for logging purposes only</strong>: It’s common to serialize objects and parse or format strings, but this can usually be optimized by reusing existing objects, caching values, or formatting text at query time.</li>
</ul>
<p>Finally, when log volume and arguments are optimized, we can do some micro-optimizations. One of them uses compile-time logging source generation and is demonstrated in the following example:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">StorageService.cs</p>
<pre class="source-code">
[LoggerMessage(EventId = 1, Level = LogLevel.Information,
  Message = "download {memeSize} {memeName}")]
private partial void DownloadMemeEvent(long? memeSize,
  string memeName);</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter8/memes/frontend/StorageService.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter8/memes/frontend/StorageService.cs</a></p>
<p>Here, we defined a partial method<a id="_idIndexMarker490"/> and annotated it with the <code>LoggerMessage</code> attribute, providing an event ID, level, and message. The implementation of this method is generated<a id="_idIndexMarker491"/> at compile time (you can find more information on it in the .NET documentation at <a href="https://learn.microsoft.com/dotnet/core/extensions/logger-message-generator">https://learn.microsoft.com/dotnet/core/extensions/logger-message-generator</a>).</p>
<p>If we check the generated code, we can see that it caches logger calls along with their static arguments. Please<a id="_idIndexMarker492"/> refer to the .NET documentation available at <a href="https://learn.microsoft.com/dotnet/core/extensions/high-performance-logging">https://learn.microsoft.com/dotnet/core/extensions/high-performance-logging</a> for more details on this approach.</p>
<p>We can compare the performance of different logging approaches by running <code>logging-benchmark$ dotnet run -c Release</code> and checking the results in the <code>BenchmarkDotNet.Artifacts</code> folder. The benchmark uses a dummy logger and measures the instrumentation side only. If we compare results for compile-time logging source generation and the <code>LogInformation</code> (or similar) method, we’ll see the following results:</p>
<ul>
<li>Compile-time logging source generation eliminates memory allocations on the instrumentation side, even when logging is enabled. As a result, GC becomes less frequent, leading to higher throughput and smaller P95 latency.</li>
<li>With compile-time logging source generation, an <code>IsEnabled</code> check is not needed if the argument values are readily available.</li>
<li>The duration of an individual log call, when logging is enabled, does not depend much on the approach used.</li>
</ul>
<p>These results may vary, depending<a id="_idIndexMarker493"/> on the argument types and values. Make sure to run performance, stress, and load tests, or profile your application with a similar logging configuration as used in production.</p>
<p>Now, you’re fully equipped to write logs, so it’s time to explore the consumption side.</p>
<h1 id="_idParaDest-142"><a id="_idTextAnchor141"/>Capturing logs with OpenTelemetry</h1>
<p>By default, ASP.NET Core<a id="_idIndexMarker494"/> applications write logs to <code>stdout</code>, but since we want<a id="_idIndexMarker495"/> to correlate them with traces and query them by any attribute, we should export them to the observability backend or the log management tool that supports it. If your vendor supports <code>ILogger</code>, you can send logs directly to your vendor by configuring the corresponding logging provider. It will be up to this logging provider to annotate logs with trace context or environment information. By collecting logs with OpenTelemetry, we can annotate them consistently with other signals.</p>
<p>Let’s see how to collect logs from the meme application with OpenTelemetry. To get the most<a id="_idIndexMarker496"/> out of the structure, we’ll export them to <strong class="bold">ClickHouse</strong> – an open source database that supports SQL queries.</p>
<p>Here’s an example<a id="_idIndexMarker497"/> of a configuration that exports logs with the <strong class="bold">OpenTelemetry Protocol</strong> (<strong class="bold">OTLP</strong>) exporter to the OpenTelemetry Collector first:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">frontend/Program.cs</p>
<pre class="source-code">
builder.Logging.AddOpenTelemetry(b =&gt; {
  b.SetResourceBuilder(resource);
  b.ParseStateValues = true;
  b.AddOtlpExporter();
});</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter8/memes/frontend/Program.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter8/memes/frontend/Program.cs</a></p>
<p>Here, we added an OpenTelemetry logging provider to the application’s <code>ILoggingBuilder</code> instance and then configured the provider. We configured resource attributes, enabled parsing state values to populate arguments, and added the OTLP exporter. The exporter endpoint is configured with the <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> environment variable.</p>
<p>The OpenTelemetry Collector is configured to send all logs to a file and write sampled logs to ClickHouse – we’ll look into its configuration in the next section.</p>
<p>Let’s go ahead and run the memes application with <code>memes$ docker-compose up --build</code>. Then, we’ll hit the frontend at <code>http://localhost:5051/</code> to upload and download some memes.</p>
<p>To query logs in ClickHouse, run <code>$ docker exec -it memes-clickhouse-1 /usr/bin/clickhouse-client</code> – this will start a client<a id="_idIndexMarker498"/> where we can write SQL<a id="_idIndexMarker499"/> queries, such as the following one, that return all log records:</p>
<pre class="console">
$ select * from otel_logs order by Timestamp desc</pre>
<p>Here’s an example of the output – the download meme log we added earlier in this chapter (if you don’t see it, keep in mind that logs are sampled and you might need to download more memes):</p>
<pre class="source-code">
│ 2023-01-17 03:28:37.446217500 │ 1bf63116f826fcc34a1e255
4b633580e │ 2a6bbdfee21d260a │         1 │ Information │ 9
│ frontend │ download {memeSize} {memeName}│
{'service.instance.id':'833fb55a4717','service.name':'front
end'} │ {'dotnet.ilogger.category':'frontend
.StorageService',
'Id':'1','Name':'DownloadMemeEvent',
'memeSize':'65412', 'memeName':'this is fine'}</pre>
<p>It’s barely readable but easy to query, as it includes a timestamp, a trace context, a log level, a body, resource information, and attributes – an event name, an ID, a meme size, and a name.</p>
<p class="callout-heading">Note</p>
<p class="callout">At the time of writing, OpenTelemetry log specification is still experimental, so .NET implementation is minimal and details might change; the ClickHouse exporter is in alpha status, and the table schema could change in later versions.</p>
<p>We didn’t enable capturing logging scopes; otherwise, we’d also see a few of them as attributes. They’re populated by ASP.NET Core and describe incoming HTTP request properties. As we saw in <a href="B19423_02.xhtml#_idTextAnchor038"><em class="italic">Chapter 2</em></a>, <em class="italic">Native Monitoring in .NET</em>, scopes include trace-context, which OpenTelemetry captures for us anyway.</p>
<p>With this, we can correlate logs using trace context or any attributes. For example, we can find the most popular memes with a query such as this:</p>
<pre class="source-code">
select LogAttributes['memeName'], count(*) as downloads
from otel_logs
where ServiceName='frontend' and
  LogAttributes['Name']='DownloadMemeEvent'
group by LogAttributes['memeName'] order by downloads desc
limit 3</pre>
<p>This can be useful when making business or technical decisions. For example, it helps to optimize caching or partitioning strategy, or plan capacity.</p>
<p>We can write queries<a id="_idIndexMarker500"/> such as these because we have enough structure<a id="_idIndexMarker501"/> in our logs, including an optional event ID and name. If we didn’t have them, we’d have to filter logs based on message text, which is neither efficient nor reliable. For example, when someone changes the message when fixing a typo or adding new arguments, all saved queries need to be changed to reflect this.</p>
<p class="callout-heading">Tip</p>
<p class="callout">To make logs queryable, make sure to use semantic logging. Provide a static event ID and name. Use consistent (across the whole system) attribute names.</p>
<p>By following this approach, we can change observability vendors, print logs in human-readable format, and, at the same time, store them in a structured form, post-process them, or aggregate them if needed.</p>
<p>Structured logs<a id="_idIndexMarker502"/> combined with traces allow us to report business<a id="_idIndexMarker503"/> telemetry and run queries, but it brings new costs – let’s see how we can control them.</p>
<h1 id="_idParaDest-143"><a id="_idTextAnchor142"/>Managing logging costs</h1>
<p>Similarly to tracing<a id="_idIndexMarker504"/> and metrics, logging increases the compute resources needed to run an application, the cost of running a logging pipeline (if any), and the costs associated with using (or running) an observability backend. Vendor pricing is frequently based on a combination of telemetry volume, retention time, and API calls, including queries.</p>
<p>We already know how to write logs efficiently, so let’s talk about pipelines and backends.</p>
<h2 id="_idParaDest-144"><a id="_idTextAnchor143"/>Pipelines</h2>
<p>A logging pipeline<a id="_idIndexMarker505"/> consists of the infrastructure needed to send logs to the backend<a id="_idIndexMarker506"/> of your choice. It’s typical to do some grokking, parsing, transformations, buffering, throttling, and hardening on the way to the backend.</p>
<p>In a simple case, it’s all done by your vendor’s logging provider or the OpenTelemetry processors and exporter inside the process.</p>
<p>In many cases, we need logging pipelines to capture logs and events coming from outside – the OS, self-hosted third-party services, proxies, and other infrastructure pieces. They could be structured such as Kubernetes events, have a well-known configurable format such as HTTP server logs, or have no structure at all.</p>
<p>A logging pipeline can help parse such logs and transform them into a common format. In the OpenTelemetry world, this could be done on the Collector.</p>
<p>We would  receive logs from <code>files</code>, <code>syslog</code>, <code>journald</code>, <code>fluentd</code>, other<a id="_idIndexMarker507"/> systems, or collectors with a <strong class="bold">receiver</strong>, then massage, filter, and route them with a <strong class="bold">processor</strong>, and finally, export them<a id="_idIndexMarker508"/> to the final destination.</p>
<p>Cost-saving strategies for pipelines start with a typical approach to minimize log volume and avoid duplication and complex transformations, as we discussed earlier in this chapter.</p>
<p>For example, you might enable HTTP tracing, metrics, and logs from both a client and server and logs from the HTTP proxy as well. Do you need logs from the proxy? Do you use them?</p>
<p>Eliminate duplicates by potentially substituting them with metrics, less verbose events, or attributes on other signals. If some information is rarely needed, process it lazily.</p>
<p>It’s also important to monitor your logging pipeline – measure the error rate and estimate the end-to-end latency and throughput. The OpenTelemetry Collector helps by exposing its own metrics and logs.</p>
<p>Once, the team I worked<a id="_idIndexMarker509"/> on discovered that some logs had dropped at an ~80% rate within<a id="_idIndexMarker510"/> the logging pipeline. We published them in a fire-and-forget manner and didn’t even know they were dropped  until we were not able to investigate incidents in production.</p>
<h2 id="_idParaDest-145"><a id="_idTextAnchor144"/>Backends</h2>
<p>Backend<a id="_idIndexMarker511"/> cost optimization<a id="_idIndexMarker512"/> also starts by producing as few logs as possible. Then, costs can be controlled in different ways, depending on your constraints and observability backend pricing model:</p>
<ul>
<li>The log volume can be reduced with sampling. Aggregations based on sampled events would need to be scaled accordingly but would provide unskewed results when unbiased sampling is used. Logs can be sampled consistently with traces at the same or a higher rate.</li>
<li>Logs can stay in hot storage for a short period of time and then move to cold storage. During the first few days, logs in hot storage can be used for urgent ad hoc queries, but after that, query speed becomes less important.</li>
</ul>
<p>This strategy can be combined with sampling – logs could be sent to the cold (and cheap) storage, while sampled in logs would go to hot storage.</p>
<ul>
<li>Certain logs can be post-processed and aggregated into metrics or reports for frequent queries.</li>
</ul>
<p>All these strategies and combinations of them can be implemented with the OpenTelemetry Collector. For example, in our memes application, we use a combination of sampling and hot/cold storage, as shown in <em class="italic">Figure 8</em><em class="italic">.1</em>:</p>
<div><div><img alt="Figure 8.1 – Logging pipelines with sampling and hot and cold storage" src="img/B19423_08_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – Logging pipelines with sampling and hot and cold storage</p>
<p>We have two different logging pipelines<a id="_idIndexMarker513"/> here. Both start with the OTLP receiver and batch<a id="_idIndexMarker514"/> processor. Then, one pipeline writes all logs to a file, and another one runs a filter based on log record properties. It checks <code>trace-flags</code> and drops logs when the parent span is not recorded. Logs with a recorded parent (or those that have no parent at all, such as startup logs) end up in ClickHouse. Here’s the corresponding logging pipeline configuration:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">otel-collector-config.yml</p>
<pre class="source-code">
logs:
  receivers: [otlp]
  processors: [batch]
  exporters: [file]
logs/sampled:
  receivers: [otlp]
  processors: [batch, filter]
  exporters: [clickhouse]</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter8/memes/configs/otel-collector-config.yml">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter8/memes/configs/otel-collector-config.yml</a></p>
<p>The filter processor, and many other<a id="_idIndexMarker515"/> processors, leverage a rich transformation language – <strong class="bold">OTTL</strong>. OTTL can be used to rename<a id="_idIndexMarker516"/> attributes, change their values, drop metrics<a id="_idIndexMarker517"/> and spans, create derived metrics, or add and drop attributes. Here’s the filter processor configuration:</p>
<pre class="source-code">
filter:
  logs:
    log_record:
      - 'flags == 0 and trace_id != TraceID
        (0x00000000000000000000000000000000)'</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter8/memes/configs/otel-collector-config.yml">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter8/memes/configs/otel-collector-config.yml</a></p>
<p>The collector can solve many common post-processing needs and take this burden away from your service.</p>
<p>That brings us to the end of this chapter. Let’s recollect what we’ve learned so far.</p>
<h1 id="_idParaDest-146"><a id="_idTextAnchor145"/>Summary</h1>
<p>Logs are the most flexible telemetry signal – they can be used to write information in human-readable format, complement traces with more information, or record structured events to analyze usage or performance.</p>
<p>To write logs, we can use different logging APIs – <code>ILogger</code> works best for application code, while <code>EventSource</code> is usually the best choice for libraries.</p>
<p><code>ILogger</code> makes it easy to write structured logs efficiently, but it depends on application authors to do so by minimizing log volume and the operations needed to calculate logging arguments.</p>
<p><code>ILogger</code> has a rich ecosystem of integrations with .NET frameworks, libraries, and providers that can send logs almost anywhere in a flat or structured format.</p>
<p>Collecting and exporting <code>ILogger</code> logs with OpenTelemetry produces logs that are consistent and correlated with other telemetry signals.</p>
<p>In addition to application logs, we usually also need to collect logs from infrastructure or legacy systems. We can do it with the OpenTelemetry Collector, which allows us to collect and unify logs from multiple destinations. The Collector’s logging pipelines can throttle, aggregate, or route logs to help you manage your logging costs.</p>
<p>You should now be ready to efficiently instrument your application with structured logs and export them with OpenTelemetry. You’re also prepared to build logging pipelines with OpenTelemetry to add observability to your infrastructure and control logging costs.</p>
<p>This concludes our deep dive into individual telemetry signals. In the next chapter, we’ll talk about choosing a good set of telemetry signals, depending on a scenario, and adding the right level of information, based on OpenTelemetry semantic conventions.</p>
<h1 id="_idParaDest-147"><a id="_idTextAnchor146"/>Questions</h1>
<ol>
<li>Is the following code snippet correct? How would you improve it?<pre class="console">
var foo = 42;</pre><pre class="console">
var bar = "bar";</pre><pre class="console">
logger.LogInformation($"hello world: {foo}, {bar}");</pre></li>
<li>Let’s say your application writes usage events using the <code>ILogger</code> APIs. Events are exported somewhere and then used to build business-critical reports. As your application evolves, you will probably refactor code, rename namespaces and classes, improve log messages, and add more arguments. How can you write logs to keep the usage report resilient to logging changes?</li>
<li>Assuming that traces for HTTP requests are collected, do you also need to write logs for the same HTTP calls?</li>
</ol>
</div>


<div><h1 id="_idParaDest-148"><a id="_idTextAnchor147"/>Part 3: Observability for Common Cloud Scenarios</h1>
</div>
<div><p>This part provides instrumentation recipes for common scenarios such as network calls, async messaging, databases, and web clients. It demonstrates how to write your own instrumentation or cover a gap in an automatic one, and, most importantly, how to investigate performance issues using a combination of distributed tracing, metrics, and logs.</p>
<p>This part has the following chapters:</p>
<ul>
<li><a href="B19423_09.xhtml#_idTextAnchor148"><em class="italic">Chapter 9</em></a>, <em class="italic">Best Practices</em></li>
<li><a href="B19423_10.xhtml#_idTextAnchor161"><em class="italic">Chapter 10</em></a>, <em class="italic">Tracing Network Calls</em></li>
<li><a href="B19423_11.xhtml#_idTextAnchor174"><em class="italic">Chapter 11</em></a>, <em class="italic">Instrumenting Messaging Scenarios</em></li>
<li><a href="B19423_12.xhtml#_idTextAnchor192"><em class="italic">Chapter 12</em></a>, <em class="italic">Instrumenting Database Calls</em></li>
</ul>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
</body></html>