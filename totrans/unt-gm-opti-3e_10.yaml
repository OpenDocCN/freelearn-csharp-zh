- en: Optimizations for Virtual and Augmented Reality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Two whole new entertainment mediums have entered the world stage in the forms
    of **virtual reality** (**VR**), where users are transported into a virtual space
    through the use of a **head-mounted device** (**HMD**), and **augmented reality**
    (**AR**), where virtual elements are superimposed on top of a display showing
    the real world. For the sake of brevity, these two terms are often combined into
    the singular term – **extended reality** (**XR**). There is also **Mixed Reality**
    (**MR**) (also known as **Hybrid Reality** (**HR**)), where an application mixes
    the real and virtual worlds together; this encompasses all of the previously mentioned
    formats, while also including AR, where real-world objects are scanned and superimposed
    inside a mostly virtual world.
  prefs: []
  type: TYPE_NORMAL
- en: The markets for these media formats have sprung up very fast and are continuing
    to grow rapidly, with huge investments from the technology industry's biggest
    players. Naturally, game engines such as Unity jumped on the bandwagon quickly,
    providing ample support for most of the top contending platforms, such as Google's
    Cardboard, HTC's VIVE, Oculus Rift, Microsoft's HoloLens, and Samsung's Gear VR
    platforms, as well as the more recent entries, such as Apple's ARKit, Google's
    ARCore, Microsoft's Windows Mixed Reality platform, PTC's (originally Qualcomm's)
    Vuforia, and Sony's PlayStation VR.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of XR technology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: XR offers a whole new realm for developers and creatives to explore. This includes
    entertainment products such as games and 360-degree videos (or 360 video for short),
    where a series of cameras are bundled together, each facing a different direction—the
    various captures from those cameras are stitched together and later played back
    like a movie in a VR headset, with visibility in all directions. Creative industry
    tools are also common in XR, such as 3D-modeling software, workflow visualizations,
    and quality-of-life gadgets. There are very few rules that have been set in stone,
    so there are plenty of opportunities to innovate, contribute to this new wave
    of technology, and become the one to create those rules. This has led to a lot
    of buzz and excitement as people explore what is possible and try to make their
    mark on the future of entertainment and interactive experiences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, almost every new and budding technology goes through the **Hype
    Cycle** (from the Gartner Hype Cycle, which you can see at [https://www.gartner.com/technology/research/methodologies/hype-cycle.jsp](https://www.gartner.com/technology/research/methodologies/hype-cycle.jsp)).
    The Hype Cycle starts with its honeymoon period of excessive hype, where early
    adopters will evangelize its benefits. Later, there is an eventual cooling of
    emotions as it enters the trough of disillusionment since it hasn''t quite hit
    the mainstream, and its benefits are not taking hold just yet*.* This continues until
    either the technology fails to capture hearts and minds, thereby falling out of
    existence, or takes a firm hold and continues steady adoption. The following diagram
    shows the essentials of the Gartner Hype Cycle:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/45d26bb2-2e3b-4d15-bef9-61dc1b20923d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Arguably, XR has recently been passing through this final phase and is starting
    to enjoy much better support and much higher quality experiences than it did during
    the early days, although it is true that the adoption rate of XR has been slower
    than initially predicted. It remains to be seen whether XR will grow into a multibillion
    dollar industry or fade into a niche market of gadgets. Consequently, developing
    within this new medium is not without its risks, and we can find industry analysts
    who will agree with our opinions, regardless of where we stand on the future of
    XR. One thing is for certain, though: every time someone experiences firsthand
    what VR and AR are capable of, they''re blown away by the level of immersion and
    the medium''s ability to convincingly transport them into another world. This
    level of immersion and interactivity is unparalleled and teases many more possibilities
    as support for the platforms matures and technology continues to advance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will explore the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The concerns to keep in mind when developing VR or AR projects in Unity and
    what must be avoided
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance enhancements specific to the XR medium
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing XR products
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developing an XR product in Unity involves importing one of several XR **software
    development kits** (**SDKs**) into our Unity project and making some specialized
    API calls to configure and use the platform at runtime. Each SDK is different
    in its own way and offers a different set of features. For instance, the Oculus
    Rift and HTC VIVE SDKs provide APIs to control VR HMDs and their respective controllers,
    whereas Apple's ARKit provides utilities to determine spatial positioning and
    superimpose objects on the display. Unity Technologies have been working hard
    to create APIs that support all of these variations, so the APIs for XR development
    in Unity have changed a lot over the past few years.
  prefs: []
  type: TYPE_NORMAL
- en: The early days of Unity VR development meant dropping native plugins into our
    Unity projects, importing SDKs directly from an external developer portal (involving
    all kinds of annoying grunt work in the setup), and applying updates manually. Since
    then, however, Unity has incorporated several of these SDKs directly into the
    editor. In addition, since AR has become more popular recently, the main API has
    been renamed from `UnityEngine.VR` to `UnityEngine.XR` in Unity 2017.2.0 and later,
    and modified so that it can work with several AR SDKs.
  prefs: []
  type: TYPE_NORMAL
- en: The Unity XR system is currently transitioning from the legacy model to a new
    package-based model. By default, Unity supports a limited set of XR platforms.
    To import other XR SDKs and to configure them (such as ARKit or the Hololens),
    you need first to install them using Package Manager by going to Window | Package
    Manager.
  prefs: []
  type: TYPE_NORMAL
- en: The development experience of working on XR products is a bit of a mixed bag
    right now. It involves working on some top-of-the-line hardware and software,
    which means that there are constant changes, redesigns, breakages, patches, bugs,
    crashes, compatibility issues, performance problems, rendering artifacts, a lack
    of feature parity between platforms, and so on. All of these problems serve to
    slow down our progress, which makes gaining a competitive advantage extraordinarily
    difficult in the XR field. On the bright side, pretty much everyone is having
    the same issues, so they get a lot of attention from their developers, making
    them easier to develop with all the time. Lessons are learned, APIs are cleaned
    up, and new features, tools, and optimizations are made available with every passing
    update.
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance problems limit an XR product''s success, perhaps more so than non-XR
    projects because of the current state of the medium. Let''s take a look at a few
    of these performance problems:'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, our users will be spending significant amounts of money to purchase
    VR HMDs and sensor equipment or AR-capable hardware. Both of these platforms can
    be very resource intensive, requiring similarly expensive graphics hardware to
    support them. This typically leads users to expect a much higher level of quality
    compared to typical games so that the investment feels worthwhile. To put it another
    way, this makes poor user experiences understandably less forgivable due to the
    monetary investment required by the user.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secondly, perhaps more so for VR projects than AR ones, poor application performance
    can lead to serious physical user discomfort, quickly turning even the staunchest
    advocate into a detractor. In particular, if the frame rate of the XR application
    is not enough, there will be a discrepancy between the motion the players feel
    (for example, by rotating the head) and what they see (we will learn about this
    in more detail later). This leads to the common issue of motion sickness that,
    in some cases, can last for hours*. *
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thirdly, the XR platform's primary draw is its immersiveness, and nothing breaks
    that faster than frame drops, flickering, or any kind of application breakdown
    that forces the user to remove their headset or reboot the app.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ultimately, we must be prepared to profile our XR applications early to make
    sure we aren't exceeding our runtime budget, as it will be stretched thin by the
    complex and resource-intensive nature of the technology behind these media.
  prefs: []
  type: TYPE_NORMAL
- en: User comfort
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike typical games and apps, VR apps need to consider user comfort as a metric
    to use to optimize themselves. Dizziness, motion sickness, eye strain, headaches,
    and even physical injuries from loss of balance have unfortunately been all too
    common for early VR adopters, and the onus is on us to limit these negative effects
    for users. In essence, content is just as important to user comfort as the hardware
    is, and we need to take the matter seriously if we are building for the medium.
  prefs: []
  type: TYPE_NORMAL
- en: Not everyone experiences these issues, and there are a lucky few who have experienced
    none of them; however, the overwhelming majority of users have reported these
    problems at one point or another. Also, just because our game doesn't trigger
    these problems in ourselves when we're testing them doesn't mean they won't trigger
    them in someone else. In fact, we will be the most biased test subject for our
    game due to familiarity. Without realizing it, we might start to predict our way
    around the most nauseating behavior our app generates, making it an unfair test
    compared to a new user experiencing the same situation. This, unfortunately, raises
    the costs of VR app development further, as a lot of testing with different unbiased
    individuals is required if we want to figure out whether our experience will cause
    discomfort, which may be needed each time we make significant changes that affect
    motion and frame rate.
  prefs: []
  type: TYPE_NORMAL
- en: There are several things that users can do to improve their VR comfort, such
    as starting with small sessions and working their way up to get practice in balancing
    and training their brain to expect the mismatched motion. A more drastic option
    is to take motion sickness medication or drink a little ginger tea beforehand
    to settle the stomach. However, we will hardly convince users to try our app if
    we promise it'll only take a few sessions of motion sickness before it starts
    to get enjoyable.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three main kinds of discomfort that users can experience in VR:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Motion sickness**: The first problem, nausea caused by motion sickness, typically
    happens when there is a sensory disconnect between where the user''s eyes think
    the horizon is and what their other senses are telling their brain, such as the
    inner ear''s sense of balance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Eye strain**: The second problem, eye strain, comes from the fact that the
    user is staring at a screen mere inches from their eyes, which tends to lead to
    a lot of eye strain and, ultimately, headaches after prolonged use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disorientation**: Finally, disorientation typically occurs because a user
    in VR is sometimes standing within a confined space, so if a game features any
    kind of acceleration-based motion, the user will instinctively try to offset that
    acceleration by adjusting their balance, which can lead to disorientation, falling
    over, and the user hurting themselves if we are not careful in ensuring that the
    user experiences smooth and predictable motion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the term **acceleration** is used intentionally since it is a vector,
    which means it has both magnitude and direction. Any kind of acceleration can
    cause disorientation, which includes not only accelerating forward, backward,
    and sideways, but also an acceleration in a rotational fashion (turning around),
    falling, jumping, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Another potential problem for VR apps is the possibility of invoking seizures.
    VR is in the unique position of being able to blast images into the user's eyes
    at a close range, which opens up some risks that we might unintentionally trigger
    seizures in vulnerable users if rendering behavior breaks down and starts flickering.
    These are all things we need to keep in mind during development that need to be
    tested for and fixed sooner rather than later.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps the most important performance metric to reach in a VR app is having
    a high number of **frames-per-second** (**FPS**), preferably 90 FPS or more, as
    this will generate a smooth viewing experience since there will be a very small
    disconnection between the user's head motion and the motion of the world. Any
    period of extended frame drops or having an FPS value consistently below this
    value is likely to cause a lot of problems for our users, making it critical that
    our application performs well at all times. Also, we should be very careful about
    how we control the user's viewpoint. We should avoid changing an HMD's field of
    view ourselves (let the user dictate the direction they are facing), generating
    acceleration over long periods, or causing uncontrolled world rotation and horizon
    motion, since these are extremely likely to trigger motion sickness and balance
    problems for the user.
  prefs: []
  type: TYPE_NORMAL
- en: A strict rule that is not up for debate is that we should never apply any kind
    of gain, multiplier effect, or acceleration effect to the positional tracking
    of an HMD in the final build of our product. Doing so for the sake of testing
    is fine, but if a real user moves their head two inches to the side, then it should
    feel like it moved the same relative distance inside the application and should
    stop the moment their head stops. Doing otherwise is not only going to cause a
    disconnect between where the player's head feels like it should be and where it
    is, but may also cause some serious discomfort if the camera becomes offset with
    respect to the player's orientation and the angle of their neck.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to use acceleration for the motion of the player character, but
    it should be incredibly short and rapid before the user starts to self-adjust
    too quickly. It would be wisest to stick to motion that relies on constant velocities
    and/or teleportation.
  prefs: []
  type: TYPE_NORMAL
- en: Placing banked turns in racing games seems to improve user comfort a great deal
    since the user naturally tilts their head and adjusts their balance to match the
    turn.
  prefs: []
  type: TYPE_NORMAL
- en: All of the previous rules apply just as well to 360 video content as they do
    to VR games. Frankly, there has been an embarrassing number of 360 videos released
    to the market that are not taking the aforementioned points into account—they
    feature too many jerking movements, a lack of camera stabilization, manual viewport
    rotation, and so on. These hacks are often used to ensure the user is facing in
    the direction we intend; however, we must spend more effort on doing this without
    hacking to avoid nausea-inducing behavior. Humans are naturally very curious about
    things that move. If they notice something moving in the corner of their eye,
    then they will most likely turn to face it. This can be used to great effect to
    keep the user facing in the direction we intend as they watch the video.
  prefs: []
  type: TYPE_NORMAL
- en: Laziness is not the way to go when generating VR content. Don't just slap a
    360 camera on top of a dirt rally car and hack an unexpected camera rotation into
    the video to keep the action in the center. The motion needs to be smooth and
    predictable. During production, we need to constantly keep in mind where we expect
    the user to be looking so that we capture action shots correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, for the 360 video format, it seems as though industry-standard
    frame rates, such as 24 FPS or 29.97 FPS, do not have a disastrous effect on user
    comfort, but note that this frame rate applies to video playback only. Our rendering
    FPS is a separate FPS value and dictates how smooth positional head tracking will
    be. The rendering FPS must always be very high to avoid discomfort (ideally, 90
    FPS).
  prefs: []
  type: TYPE_NORMAL
- en: Other problems arise when building VR apps—different HMDs and controllers support
    different inputs and behavior, making feature-parity across VR platforms difficult.
    A problem called **stereo fighting** can occur if we try to merge 2D and 3D content
    together, where 2D objects appear to be rendering deep inside 3D objects since
    the eyes can't distinguish the distance correctly. This is typically a big problem
    for the user interface of VR applications and 360 video playback, which tends
    to be a series of flat panels superimposed over a 3D background. Stereo fighting
    does not usually lead to nausea, but it can cause additional eye strain.
  prefs: []
  type: TYPE_NORMAL
- en: Although the effects of discomfort are not quite as pronounced in the AR platform,
    it's still important not to ignore it. Since AR apps tend to consume a lot of
    resources, low frame rate applications can cause some discomfort. This is especially
    true if an AR app makes use of superimposing objects onto a camera image (which
    is the majority of them), where there will probably be a disconnect in the frame
    rate between the background camera image and the objects we're superimposing over
    it. We should try to synchronize these frame rates to limit that disconnect.
  prefs: []
  type: TYPE_NORMAL
- en: Performance enhancements in XR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: That's enough talk about the industry and XR development. In the next section,
    we will cover some performance enhancements that can be applied to XR projects,
    such as choosing between the different kinds of stereo rendering algorithms and
    how to apply antialiasing and other effects to VR games
  prefs: []
  type: TYPE_NORMAL
- en: The kitchen sink
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since AR and VR apps are built using the same engine, the same subsystems, assets,
    tools, and utilities as any other Unity game, literally every other performance
    enhancement mentioned in this book, can help VR and AR apps in some fashion, and
    we should try them all before getting too in-depth with XR-specific enhancements.
    This is reassuring, as there are a lot of potential performance enhancements we
    could apply. The downside is that we may need to apply many of them to reach the
    level of performance we need for our app.
  prefs: []
  type: TYPE_NORMAL
- en: The biggest threat to a VR app's performance is the GPU fill rate, which is
    already one of the more likely bottlenecks in any other game, but significantly
    more so for VR, since we will always be trying to render a high-resolution image
    to a much larger frame buffer (since we're effectively rendering the scene twice—once
    for each eye). AR apps are typically going to find extreme consumption in both
    the CPU and the GPU since AR platforms make heavy use of the GPU's parallel pipeline
    to resolve the spatial locality of objects and perform tasks such as image recognition,
    as well as requiring a lot of draw calls to support those activities.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, certain performance-enhancing techniques are not going to be particularly
    effective in XR. **Occlusion Culling** in a VR app may be difficult to set up
    since the user can look under, around, and sometimes through objects in the scene
    (although it can still be enormously beneficial). Meanwhile, AR apps normally
    render objects at reachable distances; LOD enhancements – that is, using simpler
    meshes for objects far away – may be fairly pointless to set up.
  prefs: []
  type: TYPE_NORMAL
- en: We must use our better judgment to determine whether a performance optimization
    technique is worth implementing before we start implementing it, since many of
    them take a lot of time to implement and support.
  prefs: []
  type: TYPE_NORMAL
- en: Single Pass versus Multi Pass Stereo rendering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For VR apps, Unity provides three rendering modes: Multi Pass, Single Pass,
    and Single Pass Instanced. This can be configured under Edit | Project Settings
    | Player | XR Settings | Stereo Rendering Method (note that the checkbox of Virtual
    Reality Supported must be enabled for this to show up):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b8a87d9-d916-4d80-a272-4cd90b725ad8.png)'
  prefs: []
  type: TYPE_IMG
- en: Multi Pass rendering will render the scene to two different images, which are
    displayed separately for each eye. Single Pass Stereo rendering combines both
    images into a single double-width render texture, where only the relevant half
    is displayed to each eye.
  prefs: []
  type: TYPE_NORMAL
- en: Note that XR Settings are enabled only for the legacy system. If you installed
    a new experimental **XR Managment** package, you will find the rendering modes
    by going to Edit | Project Settings | XR Plugin Management.
  prefs: []
  type: TYPE_NORMAL
- en: Multi Pass Stereo rendering is the default case. The advantage of Single Pass
    rendering is that it provides significant savings in the CPU work in the main
    thread (by reducing draw call setup) and in the GPU since less texture swapping
    needs to occur. Of course, the GPU will need to work just as hard to render the
    objects since each object is still rendered twice from two different perspectives
    (there are no freebies here). The disadvantage is that this effect can currently
    only be used when using OpenGL ES3.0 or higher, and so it is not available on
    all platforms.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, its effects on the rendering pipeline require extra care and effort,
    particularly surrounding any shaders that are making use of screen-space effects
    (effects that only use data already drawn to the framebuffer). With Single Pass
    Stereo rendering enabled, shader code can no longer make the same assumptions
    about the incoming screen space information. The following image shows how screen
    space coordinates vary between **Multi-Pass S****tereo Rendering** and **Single-Pass
    Stereo Rendering**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b3e23a2-0307-4d87-8b17-03a82c06f16e.png)'
  prefs: []
  type: TYPE_IMG
- en: The shader is always informed of the screen space coordinates relative to the
    entire output render texture rather than just the portion it is interested in—for
    example, we would normally expect an **x** value of **0.5** to correspond to the
    horizontal halfway point of the screen, which would be the case when we use **Multi-Pass
    Stereo Rendering**; however, if we use **Single-Pass Stereo Rendering**, then
    an **x** value of **0.5** would correspond to the halfway point between the rendering
    of both eyes (the right edge of the left eye or the left edge of the right eye).
  prefs: []
  type: TYPE_NORMAL
- en: Unity provides some useful helper methods for screen space conversion for shaders,
    which can be found at [https://docs.unity3d.com/Manual/SinglePassStereoRendering.html](https://docs.unity3d.com/Manual/SinglePassStereoRendering.html).
  prefs: []
  type: TYPE_NORMAL
- en: Another problem to worry about is post-processing effects. We essentially always
    pay double the cost for any post-processing effect applied to the scene in VR
    since it needs to be evaluated once for each eye. Single Pass Stereo rendering
    can reduce the draw calls needed to set up our effect, but we can't blindly apply
    a post-processing effect to both images simultaneously. Consequently, the post-processing
    effect shaders must also be adjusted to ensure that they render to the correct
    half of the output render texture. Without doing this, a post-processing effect
    will be stretched over both eyes twice, which might look incredibly bizarre for
    effects such as lens flares.
  prefs: []
  type: TYPE_NORMAL
- en: '**Single-Pass instancing **(also known as **stereo instancing**), on the other
    hand, is an experimental rendering mode that has some advantages over the standard
    single pass. Instead of doubling draw calls by rendering the same object for the
    right and the left eye, single-pass instances makes heavy use of **GPU instancing**.
    In short, GPU instancing allows Unity to issue a single draw call to the GPU,
    but with the instruction that the mesh must be drawn in two different positions.
    Therefore, single-pass instances can offer a dramatic improvement compared with
    CPU performances; however, custom shaders need to be ready for GPU instantiation:
    this involves adding a position parameter to the shader (so that the GPU knows
    how to move the mesh) and several Unity utility functions.'
  prefs: []
  type: TYPE_NORMAL
- en: Enabling GPU instancing on a custom shader is a hard task, and it is not recommended
    unless you already have some experience with the Unity shading language. If you
    have, then the right place to start is [https://docs.unity3d.com/Manual/SinglePassInstancing.html](https://docs.unity3d.com/Manual/SinglePassInstancing.html).
  prefs: []
  type: TYPE_NORMAL
- en: The single-pass rendering (both traditional and instanced) feature is not supported for
    all platforms. We can expect it to be rolled out to more platforms eventually,
    but for platforms supporting it, we will need to perform some profiling and sensible
    sanity checks on our screen space shaders to ensure that we are making positive
    gains from enabling this option.
  prefs: []
  type: TYPE_NORMAL
- en: Applying antialiasing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Applying antialiasing is less of a performance enhancement and more of a requirement.
    Antialiasing significantly improves the fidelity of XR projects since objects
    will blend better and appear less pixelated, improving immersion, which can cost
    a lot of fill-rate. We should enable this feature early and try to reach our performance
    goals with the assumption that it is simply always there, only disabling it as
    an absolute last resort.
  prefs: []
  type: TYPE_NORMAL
- en: Using forward rendering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The advantage of deferred rendering is the ability to resolve many light sources
    with minimal draw calls. Unfortunately, if we follow the preceding advice and
    apply antialiasing effects, then this must be done as a post-processing screen
    space shader when deferred rendering is used. This can cost a considerable amount
    of performance compared to how the same technique is applied as a multisampling
    effect in forward rendering, potentially making forward rendering the more performant
    of the two options.
  prefs: []
  type: TYPE_NORMAL
- en: Applying image effects in VR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The effects applied by normal maps tend to break down easily in VR, where the
    texture appears painted on to the surface instead of giving the illusion of depth.
    Normal maps normally break down very quickly with viewing angles that are very
    oblique (shallow) with the surface, which is not particularly common in a typical
    game; however, in VR, since most HMDs allow users to move their heads around in
    a 3D space via positional tracking (which, granted, not all of them do), they
    will quickly find positions that break the effect for any objects close to the
    camera. Normal maps have been known to improve the quality of high polygon count
    objects in VR, but it rarely provides a benefit for those with a low polygon count,
    so we should perform a little testing to make sure that any visual improvement
    is worth the costs in memory bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, we cannot rely on normal mapping to provide a quick and cheap increase
    in graphical fidelity for low polygon count objects that we might expect from
    a non-VR scene, so testing is required to establish whether the illusion is working
    as intended. Displacement maps, tessellation, and/or parallax mapping should be
    used instead to create a more believable appearance of depth. Unfortunately, all
    of these techniques are more expensive than a typical normal map, but it is a
    burden we must suffer in order to achieve good graphical quality in VR.
  prefs: []
  type: TYPE_NORMAL
- en: Other post-processing effects, such as depth of field, blurring, and lens flares,
    are effects that look good in a typical 3D game, but are generally not effects
    we witness in the real world, and will seem out of place in VR (at least until
    eye-tracking support is available), and so should generally be avoided.
  prefs: []
  type: TYPE_NORMAL
- en: Backface culling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Backface culling (removing faces from objects that will never be visible) can
    be tricky for VR and AR projects since the player's viewing angle could potentially
    come from any direction for objects near the camera. Assets near the camera should
    be a fully closed shape if we want to avoid immersion-breaking viewpoints. We
    should also think carefully about applying backface culling for distant objects,
    particularly if the user travels by teleportation since it can be tricky to restrict
    a user's location completely. Ensure that you test your game world's bounding
    volumes to ensure that the user cannot escape.
  prefs: []
  type: TYPE_NORMAL
- en: Spatialized audio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The audio industry is abuzz with new techniques to present audio experiences
    for VR (or, more accurately, old techniques that have finally found a good use)
    in the form of spatial audio. Audio data for these formats no longer represents
    audio data from specific channels, but instead contains data for certain audio
    harmonics that are merged at runtime to create a more believable audio experience
    depending on the current camera viewport, particularly vertical orientations.
    The key word from the previous sentence is runtime, meaning that this effect has
    a continuous nontrivial cost associated with it. These techniques will require
    CPU activity, but may also use GPU acceleration to generate their effects, so
    we should double-check the behavior of both devices if we're experiencing performance
    problems when we're making use of spatial audio.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding camera physics collisions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In VR and AR, the user can move the camera through objects, which can break
    their immersion. Although it may be tempting to add physics colliders to such
    surfaces to prevent the camera from moving through them, this will cause disorientation
    in VR since the camera will not move in unison with the user's movements. This
    could also break the positional-tracking calibration of an AR app. A better approach
    is to either allow the user to see into objects or to maintain a safe buffer zone
    between the camera and such surfaces. If we don't allow the player to teleport
    too close to them in the first place, then there's no risk of sticking their head
    through walls.
  prefs: []
  type: TYPE_NORMAL
- en: This will save on performance because of a reduced number of colliders, but
    should be followed as more of a quality-of-life issue. We shouldn't be too concerned
    about risking immersion-breaking behavior by doing this, as research has shown
    that users tend to avoid looking into objects once they realize they can do it.
    They may experience a moment of confusion or hilarity when it happens initially,
    but fortunately, people tend to want to remain in the immersive experience we've
    created and will tend to avoid putting their heads through walls. However, the
    ability to do so provides the gameplay advantage of seeing through a wall to observe
    where enemies are about to come from, so we should develop our scenes with that
    in mind.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding Euler angles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Avoid using Euler angles for any kind of orientation behavior. Quaternions are
    designed to be much better for representing angles (the only downside is that
    they are more abstract and harder to visualize when debugging) and maintaining
    accuracy whenever there are changes, while also avoiding the dreaded gimbal lock.
    Using Euler angles for calculations could eventually lead to inaccuracies after
    there are a lot of rotation changes, which is incredibly likely since the user's
    viewpoint will change by tiny amounts many times per second in both VR and AR.
  prefs: []
  type: TYPE_NORMAL
- en: Gimbal lock is a problem that can occur with Euler angles. Since Euler angles
    represent orientation via three axes, and there are overlaps when one of these
    axes is rotated 90 degrees, we could accidentally lock them together, becoming
    mathematically inseparable and causing future orientation changes to affect both
    axes simultaneously. Of course, a human being can figure out how to rotate the
    object to solve this problem, but gimbal lock is a purely mathematical problem.
    The classic example is the orientation bubble in a fighter jet. The pilot never
    has problems with gimbal lock, but the orientation instruments in their heads-up
    display could become inaccurate because of it. Quaternions solve this problem
    by including a fourth value that effectively allows overlapping axes to still be distinguishable
    from one another.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise restraint
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performance targets for VR apps are very difficult to reach. It is, therefore,
    important to recognize when we are simply trying to cram too much quality into
    our app than is tolerable for the current generation of XR devices and typical
    user hardware. The last resort is always to cull objects from our scenes until
    we reach our performance goals. We should be more willing to do so for an XR app
    than a non-XR one since the costs of poor performance often far outweigh the gains
    of higher quality. We must refrain from adding more detail to our scenes if it
    has become apparent that the rendering budget has been exhausted. This can be
    difficult to admit with immersive VR content, where we want to create as much
    compelling immersion as we can, but until the technology catches up with our ambition,
    we need to remain frugal.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping up to date with the latest developments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unity provides a list of useful articles and tutorials containing VR design
    and optimization tips, which will likely get updated as the medium and market
    matures and new techniques are discovered. This list can be kept more up to date
    than this book ever could be, so check them out from time to time to catch the
    latest tips. As usual, the articles and tutorials in question can be found at [https://learn.unity.com](https://learn.unity.com).
  prefs: []
  type: TYPE_NORMAL
- en: We should also keep an eye on Unity blogs to make sure that we don't miss anything
    important with regard to XR API changes, performance enhancements, and performance
    suggestions.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hopefully, this brief guide will help you improve the performance of your XR
    applications. The reassuring news is that you have many performance optimization
    options to choose from since Unity XR apps are built on the same underlying platform
    we've been exploring throughout this book. Less reassuring is the fact that we
    might have to test and implement all of them in order to stand a chance of reaching
    our quality goals. We can expect hardware to get more powerful over time, prices
    to come down, and adoption to increase as a result; however, until then, we need
    to pull out all the stops if we're going to compete in the tech world's latest
    craze.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll dig into Unity's underlying engine, along with the
    various frameworks, layers, and languages that it is built from. In essence, we
    will take a more in-depth look at our script code and investigate some methods
    to improve our CPU and memory management across the board.
  prefs: []
  type: TYPE_NORMAL
