<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<meta charset="utf-8"/>
<meta content="packt" name="generator"/>
<title>20 Modular Monolith</title>


<!-- kobo-style -->
<style id="koboSpanStyle" type="text/css" xmlns="http://www.w3.org/1999/xhtml">.koboSpan { -webkit-text-combine: inherit; }</style>
</head>
<body>

<h1 data-number="21">20 Modular Monolith</h1>

<h2 data-number="21.1">Before you begin: Join our book community on Discord</h2>
<p>Give your feedback straight to the author himself and chat to other early readers on our Discord server (find the "architecting-aspnet-core-apps-3e" channel under EARLY ACCESS SUBSCRIPTION).</p>
<p><a href="https://packt.link/EarlyAccess">https://packt.link/EarlyAccess</a></p>
<p><img alt="Qr code Description automatically generated" src="img/file172.png" style="width:10em"/></p>
<p>In the ever-evolving software development landscape, choosing the right architecture is like laying the foundation for a building. The architecture dictates how the software is structured, impacting its scalability, maintainability, and overall success. Traditional monolithic architecture and microservices have long been the dominant paradigms, each with advantages and challenges.However, a new architectural style has been gaining traction—Modular Monoliths. This approach aims to offer the best of both worlds by combining the simplicity of monoliths with the flexibility of microservices. It serves as a middle ground that addresses some of the complexities associated with microservices, making it particularly appealing for small to medium-sized projects or teams transitioning from a traditional monolithic architecture.</p>
<blockquote>
<p>I wrote an article about this in 2017 entitled <em>Microservices Aggregation</em>. I recently read the name <em>Modular Monolith</em> and loved it better. This architectural style is gaining traction, yet it is not entirely new. The Modular Monolith style is a way to modularize and organize an application, lowering our chances of creating a big ball of mud.</p>
</blockquote>
<p>This chapter aims to provide a comprehensive understanding of Modular Monoliths. We delve into its core principles, advantages, and key components and explore when and how to implement this architecture. We build upon our nano e-commerce application to get hands-on insights into Modular Monoliths' practical applications. Additionally, we discuss how they compare with other architectural styles to help you make informed decisions for your next projects.By the end of this chapter, you should have a solid grasp of Modular Monoliths, why they might be the right choice for your project, and how to implement them.In this chapter, we cover the following topics:</p>
<ul>
<li>What is a Modular Monolith?</li>
<li>Advantages of Modular Monoliths</li>
<li>Key Components of a Modular Monolith</li>
<li>Implementing a Modular Monolith</li>
<li>Project—Modular Monolith</li>
<li>Transitioning to Microservices</li>
<li>Challenges and Pitfalls</li>
</ul>
<p>Let’s start by exploring what is a Modular Monolith.</p>


<h2 data-number="21.2">What is a Modular Monolith?</h2>
<p>A Modular Monolith is an architectural style that aims to combine the best aspects of traditional monolithic architectures and microservices. It organizes the software application into well-defined, loosely coupled modules. Each is responsible for a specific business capability. However, unlike microservices, all these modules are deployed as a single unit like a monolith.The core principles of a Modular Monolith are:</p>
<ul>
<li>Treat each module as a microservice.</li>
<li>Deploy the application as a single unit.</li>
</ul>
<p>Here are the fundamental principles of a successful microservice as studied in <em>Chapter 19</em>, <em>Introduction to Microservices Architecture</em>:</p>
<ul>
<li>Each microservice should be a cohesive unit of business.</li>
<li>Each microservice should own its data.</li>
<li>Each microservice should be independent of the others.</li>
</ul>
<p>In a nutshell, we get the best of both worlds. Yet, understanding how a Modular Monolith compares with other architectural styles is crucial for making informed decisions.</p>

<h3 data-number="21.2.1">What are traditional Monoliths?</h3>
<p>In a traditional monolithic architecture, we build the application as a single, indivisible unit. This leads to the functionalities being tightly coupled together, making it difficult to make changes or scale specific features. This approach makes creating a big ball of mud easier, particularly when the team invests little effort in domain modeling and analysis before and during development.On top of that, while this approach is simple and straightforward, it lacks the flexibility and scalability of more modern architectures.</p>
<blockquote>
<p>A monolith does not have to be indivisible, yet most end up this way because it is easy to create tight coupling in a single application.</p>
</blockquote>
<p>Let’s look at microservices next.</p>


<h3 data-number="21.2.2">What are microservices?</h3>
<p>Microservices architecture, on the other hand, takes modularity to the extreme. Each service is a completely independent unit, running in its own environment. Microservices architecture allows for high scalability and flexibility but comes at the cost of increased operational complexity.</p>
<blockquote>
<p><em>Chapter 19</em>, <em>Introduction to Microservices Architecture</em>, covers this topic more in-depth.</p>
</blockquote>
<p>The following sections delve deeper into this emerging architectural style, starting with its advantages.</p>



<h2 data-number="21.3">Advantages of Modular Monoliths</h2>
<p>One of the best things about Modular Monoliths is that they are easy to manage. You don't have to worry about many moving parts like with microservices. Everything is in one place but still separated into modules. This makes it easier for us to keep track of things and to work with them.With Modular Monoliths, each module is like its own small project. We can test, fix, or improve a module without affecting the others. This is great because it lets us focus on one thing at a time, which improves productivity by lowering the cognitive load required to work on that feature.When it's time to release the software, we only have one application to deploy. Even though it has many modules, we treat them like one deployable unit. This makes managing deployments much more straightforward as we don’t have to juggle multiple services like we would with microservices.Modular Monoliths can save us money because we don't need as many resources since we deploy a single monolith. Because of that, we don't need a team to manage and run complicated infrastructure. We don't need to worry about distributed tracing between our services, which reduces the upfront monitoring cost. This deployment style is highly beneficial when starting a project with a small team or if the team is not proficient with microservices architecture.</p>
<blockquote>
<p>Modular Monoliths can still be valuable even if you are part of a large team, part of a larger organization, or have experience with microservices architecture. It is not one or the other kind of scenario.</p>
</blockquote>
<p>As we just explored, Modular Monoliths bring many advantages:</p>
<ul>
<li>The reduced operational complexity makes deploying a complex application as a single unit easier than microservices.</li>
<li>They improve our development and testing experience because of their simplicity, improving our efficiency.</li>
<li>They are easier to manage than most traditional monoliths because modules are well segregated.</li>
<li>They are more cost-effective than microservices.</li>
</ul>
<p>The following section looks at what makes up a Modular Monolith.</p>


<h2 data-number="21.4">Key Components of a Modular Monolith</h2>
<p>The first thing to know about Modular Monoliths is that they are composed of different parts called <strong>modules</strong>. Each module is like a mini-app that performs a specific job. This job is related to a particular business capability. A business capability comes from the business domain and aims at a cohesive group of scenarios. Such a group is called a bounded context in DDD. Think of each module as a microservice or a well-defined chunk of the domain.That separation means everything we need to complete a specific task is in one module, which makes it easier for us to understand and work on the software because we don’t have to jump from one place to another to get things done. So, if one module needs an update or has a problem, we can fix it without touching the others. This segregation is also perfect for testing since we can test each module in isolation to ensure it works well. For example, one module might handle a shopping cart while another takes care of the shipping, but we piece all modules together in a final aggregated application.</p>
<blockquote>
<p>We can create the modular monolith as a single project. However, in a .NET-specific environment, when each module comprises one or more assemblies, it is harder to create unwanted coupling between modules. We explore this in the project we are about to build.</p>
</blockquote>
<p>The <strong>module aggregator</strong>—the monolith—is responsible for loading and serving the modules as if they were just one application.Even though each module is independent, they still need to talk to each other sometimes. For example, the product catalog module might need to tell the shopping cart module that an employee has added a new product to the catalog. There are many ways to make this communication happen. One of the best ways to keep a low coupling level between the modules is to leverage event-driven architecture. On top of loose coupling, this opens doors like scaling the Modular Monolith by migrating one or more modules to a microservices architecture; more on that later.Here's a diagram that represents these relationships:</p>
<figure>
<img alt="Figure 20.1: Diagram representing the relationships between the module aggregator, the modules, and an event broker." src="img/file173.png"/><figcaption aria-hidden="true">Figure 20.1: Diagram representing the relationships between the module aggregator, the modules, and an event broker.</figcaption>
</figure>
<p>Now that we have explored the key components of a Modular Monolith, the next section discusses planning and implementing one.</p>


<h2 data-number="21.5">Implementing a Modular Monolith</h2>
<p>Planning is essential before building a Modular Monolith. We must consider what each module does and how modules work together. A good plan helps us avoid problems later on.Choosing the right tools to create a lean stack is also essential. The good news is that we don't need to define a large shared stack since each module is independent. Like a slice in Vertical Slice architecture, each module can determine its patterns and data sources. Yet, we must define a few common elements to assemble a Modular Monolith successfully. Here are a few items to consider to improve the chances of success of a Modular Monolith:</p>
<ul>
<li>The modules share a URL space.</li>
<li>The modules share the configuration infrastructure.</li>
<li>The modules share a single dependency injection object graph (one container).</li>
<li>The modules share the inter-module communication infrastructure (event broker).</li>
</ul>
<p>We can mitigate the first two elements using the module name as a discriminator. For example, using the <code>/{module name}/{module space}</code> URI space would yield the following results (<code>products</code>, <code>baskets</code>, and <code>customers</code> are modules):</p>
<ul>
<li><code>/products</code></li>
<li><code>/products/123</code></li>
<li><code>/baskets</code></li>
<li><code>/customers</code></li>
</ul>
<p>Using the module name as the top-level key of the configuration also makes it easy to avoid conflicts, like <code>{module name}:{key}</code>, or like the following JSON (say from the <code>appsettings.json</code> file):</p>
<div><pre><code>"{module name}": {
    "{key}": "Module configs"
}</code></pre>
</div>
<p>We can mitigate the last two elements by managing the shared code in a certain way. For example, limiting the amount of shared code reduces the chances of conflict between the modules. Yet, multiple modules globally configuring ASP.NET Core or a third-party library can result in conflicts; centralizing those configurations into the aggregator instead and treating them as a convention will help mitigate most issues. Cross-cutting concerns like exception handling, JSON serialization, logging, and security are great candidates for this.Lastly, sharing a single way to communicate between modules and configuring it in the aggregator will help mitigate the communication issues.Let's start planning the project next.</p>

<h3 data-number="21.5.1">Planning the project</h3>
<p>Planning is a crucial part of any software. Without a plan, you increase your chances of building the wrong thing. A plan does not guarantee success, but it improves your chances. Overplanning is the other side of this coin. It can lead to <strong>analysis paralysis</strong>, which means you may never even deliver your project or that it will take you so long to deliver it that it will be the wrong product because the needs changed along the way and you did not adapt.Here's a high-level way to accomplish planning a Modular Monolith. You don't have to execute all the steps in order. You can perform many of them iteratively, or multiple people or teams can even work on them in parallel:</p>
<ol>
<li>Analyse and model the domain.</li>
<li>Identify and design the modules.</li>
<li>Identify the interactions between modules and design the integration events that cover those interactions.</li>
</ol>
<p>Once we are done planning, we can develop and operate the application. Here are a few high-level steps:</p>
<ol>
<li>Build and test the modules in isolation.</li>
<li>Build and test the module aggregator application that integrates one or more modules.</li>
<li>Deploy, operate, and monitor the monolith.</li>
</ol>
<p>Implementing a Modular Monolith, like any program, is a step-by-step process. We plan, build, test, and then deploy it. Each part is simple enough on its own, and when we piece all of them together, we get an easy-to-maintain system. Even if continuously improving the application and refining the analysis and the model over time should yield the best results, having a good idea of the high-level domain—the modules—and at least a vague view of their interactions before starting will help avoid mistakes and potentially significant refactoring further down the road. Here’s a general representation of this process:</p>
<p>∞</p>
Figure 20.2: A partial Agile and DevOps view of the Modular Monolith phases
<p>Next, we plan our nano e-commerce application.</p>

<h4 data-number="21.5.1.1">Analyzing the domain</h4>
<p>As we continue to iterate over the nano e-commerce application we built in <em>Chapter 18</em> and <em>Chapter 19</em>, the domain analysis will be very short. Moreover, we are not expanding the application further than products and baskets because the application is already too large to fit in a single chapter. Of course, this time, we are making it a Modular Monolith. Here are the high-level entities and their relationships:</p>
<figure>
<img alt="Figure 20.3: The high-level entities of our nono e-commerce app and their relationships" src="img/file174.png"/><figcaption aria-hidden="true">Figure 20.3: The high-level entities of our nono e-commerce app and their relationships</figcaption>
</figure>
<p>As the diagram shows, we have a <code>Product</code> entity that could benefit from more details like categories, hence the <code>...</code> box. We also have the <code>BasketItem</code> entity we use for people to save their shopping baskets to the database. Finally, a <code>Customer</code> entity represents the person to whom a shopping basket belongs.</p>
<blockquote>
<p>We did not implement a <code>Customer</code> class, yet the customer is conceptually present through the <code>CustomerId</code> property.</p>
</blockquote>
<p>Next, we split this subset of the domain into modules.</p>


<h4 data-number="21.5.1.2">Identifying the modules</h4>
<p>Now that we have identified the entities, it is time to map them into modules. Let’s start with the following diagram:</p>
<figure>
<img alt="Figure 20.4: Modules (bounded context) separation and entities relationships." src="img/file175.png"/><figcaption aria-hidden="true">Figure 20.4: Modules (bounded context) separation and entities relationships.</figcaption>
</figure>
<p>As expected, we have a product catalog, a shopping basket, and a customer management modules. What’s new here is the relationships between the entities. The catalog mainly manages the <code>Product</code> entity, yet the shopping basket needs to know about it to operate. The same logic applies to the <code>Customer</code> entity. In this case, the shopping basket only needs to know the unique identifier of each entity, but another module could need more information.Based on that high-level view, we need to create three modules. In our case, we continue with only two modules. In an actual application, we’d have more than three modules since we’d have to manage the purchases, the shipping, the inventory, and more.Let’s look at the interactions between the modules.</p>


<h4 data-number="21.5.1.3">Identifying the interactions between modules</h4>
<p>Based on our analysis and limited to the two modules we are building, the shopping basket module needs to know about the products. Here’s our <code>BasketItem</code> class:</p>
<div><pre><code>public record class BasketItem(int CustomerId, int ProductId, int Quantity);</code></pre>
</div>
<p>The preceding class shows that we only need to know about the unique product identifier. So, with an event-driven mindset, the shopping basket module wants to be notified when:</p>
<ul>
<li>A product is created.</li>
<li>A product is deleted.</li>
</ul>
<p>With those two events, the basket module can manage its cache of products and only allow customers to add existing items to their shopping basket. It can also remove items from customers’ shopping baskets when unavailable. Here’s a diagram representing these flows:</p>
<figure>
<img alt="Figure 20.5: The integration event flows between the catalog and the basket modules." src="img/file176.png"/><figcaption aria-hidden="true">Figure 20.5: The integration event flows between the catalog and the basket modules.</figcaption>
</figure>
<p>Now that we have analyzed the domain and the modules, before building anything, let's define our stack.</p>



<h3 data-number="21.5.2">Defining our stack</h3>
<p>We know we are using ASP.NET Core and C#. We continue leveraging minimal APIs, yet MVC could achieve the same. We also continue to leverage <em>EF Core</em>, <em>ExceptionMapper</em>, <em>FluentValidation</em>, and <em>Mapperly</em>. But what about the modules and the other shared aspects of the project? Let's have a look.</p>

<h4 data-number="21.5.2.1">The module structure</h4>
<p>We are using a flexible yet straightforward module structure. You can organize your projects however you like; this is not a prescriptive approach. For example, you can get inspired by other architectural styles, like Clean Architecture, or invent your own based on your own experience, context, and work environment.In our case, I opted for the following directory structure:</p>
<ul>
<li>The <code>applications</code> directory contains the deployable applications, like the aggregator. We could add user interfaces in this directory or other deployable applications, like the BFF we built in Chapter 19. Each application is contained within its own subdirectory.</li>
<li>The <code>modules</code> directory contains the modules, each within its own subdirectory.</li>
<li>The <code>shared</code> directory contains the shared projects.</li>
</ul>
<blockquote>
<p>In real-world software, we could extend this setup and add <code>infrastructure</code>, <code>docs</code>, and <code>pipelines</code> directories to store our Infrastructure as Code (IaC), documentation, and CI/CD pipelines next to our code.</p>
</blockquote>
<p>What I like about this mono-repo-inspired structure is that each module and application is self-contained. For example, the aggregator’s API, contracts, and tests are next to each other:</p>
<figure>
<img alt="Figure 20.6: The aggregator’s directory and project hierarchy." src="img/file177.png"/><figcaption aria-hidden="true">Figure 20.6: The aggregator’s directory and project hierarchy.</figcaption>
</figure>
<p>The modules are organized similarly:</p>
<figure>
<img alt="Figure 20.7: The modules' subdirectories and catalog’s project hierarchy." src="img/file178.png"/><figcaption aria-hidden="true">Figure 20.7: The modules' subdirectories and catalog’s project hierarchy.</figcaption>
</figure>
<p>I kept the REPR prefix since it’s based on Chapter 18’s code, but I changed the code structure a bit. In this version, I got rid of the nested classes and created one class per file. This follows a more classic .NET convention and allows us to extract the API contracts to another assembly. If you remember, in Chapter 19, the BFF project referenced the two APIs to reuse their <code>Query</code>, <code>Command</code>, and <code>Response</code> contracts. We fix this problem through the <code>Contracts</code> class library projects in this solution.</p>
<blockquote>
<p>Why is it a problem? The BFF depends on all the microservices, including their logic and dependencies. This is a recipe to introduce unwanted coupling. Moreover, since it inherits all the dependencies transitively, it increases its deployment size and vulnerability surface; more dependencies and more code means more possibilities for a malicious actor to find an exploitable hole.</p>
</blockquote>
<p>On top of the API contracts, the <code>Contracts</code> projects also contain the integration events. We could have separated the API contracts and the integration events if the application was larger; in this case, we only have two integration events. Design choices must be taken relative to the current project and context.Let’s explore the URI space next.</p>


<h4 data-number="21.5.2.2">The URI space</h4>
<p>The modules of this application follow the previously discussed URI space: <code>/{module name}/{module space}</code>. Each module has a <code>Constants</code> file at its root that looks like this:</p>
<div><pre><code>namespace REPR.Baskets;
public sealed class Constants
{
    public const string ModuleName = nameof(Baskets);
}</code></pre>
</div>
<p>We use the <code>ModuleName</code> constant in the <code>{module name}ModuleExtensions</code> files to set the URI prefix and tag the endpoints like this:</p>
<div><pre><code>namespace REPR.Baskets;
public static class BasketModuleExtensions
{
    public static IEndpointRouteBuilder MapBasketModule(this IEndpointRouteBuilder endpoints)
    {
        _ = endpoints
            .MapGroup(Constants.ModuleName.ToLower())
            .WithTags(Constants.ModuleName)
            .AddFluentValidationFilter()
            // Map endpoints
            .MapFetchItems()
            .MapAddItem()
            .MapUpdateQuantity()
            .MapRemoveItem()
        ;
        return endpoints;
    }
}</code></pre>
</div>
<p>With this in place, both modules self-register themselves in the correct URI space.</p>
<blockquote>
<p>We can apply these types of conventions in many different ways. In this case, we opted for simplicity, which is the most error-prone, leaving the responsibility to the mercy of each module. With a more framework-oriented mindset, we could create a strongly typed module contract that gets loaded automatically, like an <code>IModule</code> interface. The aggregator could also create the root groups and enforce the URI space.</p>
</blockquote>
<p>Next, we explore the data space.</p>


<h4 data-number="21.5.2.3">The data space</h4>
<p>Since we are following the microservices architecture tenets and each module should own its data, we must find a way to ensure our data contexts do not conflict.The project uses the EF Core in-memory provider to develop locally. For production, we plan on using SQL Server. One excellent way to ensure our <code>DbContext</code> classes do not conflict with each other is to create one database schema per context. Each module has one context, so one schema per module. We don’t have to overthink this; we can reuse the same idea as the URI and leverage the module name. So, each module will group its tables under the <code>{module name}</code> schema instead of <code>dbo</code> (the default SQL Server schema).</p>
<blockquote>
<p>We can apply different security rules and permissions to each schema in SQL Server, so we could craft a very secure database model by expanding this. For instance, we could employ multiple users possessing minimal privileges, utilize different connection strings within the modules, etc.</p>
</blockquote>
<p>In code, doing this is reflected by setting the default schema name in the <code>OnModelCreating</code> method of each <code>DbContext</code>. Here’s the <code>ProductContext</code> class:</p>
<div><pre><code>namespace REPR.Products.Data;
public class ProductContext : DbContext
{
    public ProductContext(DbContextOptions&lt;ProductContext&gt; options)
        : base(options) { }
    protected override void OnModelCreating(ModelBuilder modelBuilder)
    {
        base.OnModelCreating(modelBuilder);
        modelBuilder.HasDefaultSchema(Constants.ModuleName.ToLower());
    }
    public DbSet&lt;Product&gt; Products =&gt; Set&lt;Product&gt;();
}</code></pre>
</div>
<p>The preceding code makes all <code>ProductContext</code>’s tables part of the <code>products</code> schema. We then apply the same for the basket module:</p>
<div><pre><code>namespace REPR.Baskets.Data;
public class BasketContext : DbContext
{
    public BasketContext(DbContextOptions&lt;BasketContext&gt; options)
        : base(options) { }
    public DbSet&lt;BasketItem&gt; Items =&gt; Set&lt;BasketItem&gt;();
    public DbSet&lt;Product&gt; Products =&gt; Set&lt;Product&gt;();
    protected override void OnModelCreating(ModelBuilder modelBuilder)
    {
        base.OnModelCreating(modelBuilder);
        modelBuilder.HasDefaultSchema(Constants.ModuleName.ToLower());
        modelBuilder
            .Entity&lt;BasketItem&gt;()
            .HasKey(x =&gt; new { x.CustomerId, x.ProductId })
        ;
    }
}</code></pre>
</div>
<p>The preceding code makes all <code>BasketContext</code>’s tables part of the <code>baskets</code> schema.Due to the schema, both contexts are safe from hindering the other. But wait! Both contexts have a <code>Products</code> table; what happens then? The catalog module uses the <code>products.products</code> table, while the basket module uses the <code>baskets.products</code> table. Different schema, different tables, case closed!</p>
<blockquote>
<p>We can apply these notions to more than Modular Monolith as it is general SQL Server and EF Core knowledge.</p>
</blockquote>
<p>If you are using another relational database engine that does not offer schema or a NoSQL database, you must also think about this. Each NoSQL database has different ways to think about the data, and it would be impossible to cover them all here. The important piece is to find a discriminator that segregates the data of your modules. At the limit, it can even be one different database per module; however, this increases the operational complexity of the application.Next, we explore the message broker.</p>


<h4 data-number="21.5.2.4">The message broker</h4>
<p>To handle the integration events between the catalog and the basket modules, I decided to pick MassTransit. To quote their GitHub project:</p>
<blockquote>
<em>MassTransit is a free, open-source distributed application framework for .NET. MassTransit makes it easy to create applications and services that leverage message-based, loosely-coupled asynchronous communication for higher availability, reliability, and scalability.</em>
</blockquote>
<p>I picked MassTransit because it is a popular project with 5,800 GitHub stars as of 2023 and supports many providers, including in-memory. Moreover, it offers many features that are way above our needs. Once again, we could have used anything. For example, MediatR could have also done the job.The <code>REPR.API</code> project—the aggregator—and the modules depend on the following NuGet package to use MassTransit:</p>
<div><pre><code>&lt;PackageReference Include="MassTransit" Version="8.1.0" /&gt;</code></pre>
</div>
<p>Our usage is very simple; the aggregator registers and configures MassTransit like this:</p>
<div><pre><code>builder.Services.AddMassTransit(x =&gt;
{
    x.SetKebabCaseEndpointNameFormatter();
    x.UsingInMemory((context, cfg) =&gt;
    {
        cfg.ConfigureEndpoints(context);
    });
    x.AddBasketModuleConsumers();
});</code></pre>
</div>
<p>The highlighted line delegates the registration of event consumers to the basket module. The <code>AddBasketModuleConsumers</code> method is part of the <code>BasketModuleExtensions</code> class and contains the following code:</p>
<div><pre><code>public static void AddBasketModuleConsumers(this IRegistrationConfigurator configurator)
{
    configurator.AddConsumers(typeof(ProductEventsConsumers));
}</code></pre>
</div>
<p>The <code>ProductEventsConsumers</code> class manages the two events. The <code>AddConsumers</code> method is part of MassTransit. We explore the <code>ProductEventsConsumers</code> class in the project section.</p>
<blockquote>
<p>We would register the event consumers of other modules here if we had more. Yet the delegation of that registration to each module makes it modular.</p>
</blockquote>
<p>Next, we write some C# code to transform our microservices application into a Modular Monolith.</p>




<h2 data-number="21.6">Project—Modular Monolith</h2>
<p>This project has the same building block as <em>Chapter 18</em> and <em>Chapter 19</em>, but we use the Modular Monolith approach. On top of the previous versions, we leverage events to enable the shopping basket to validate the existence of a product before allowing customers to add it to their shopping basket.</p>
<blockquote>
<p>The complete source code is available on GitHub: <a href="https://adpg.link/gyds">https://adpg.link/gyds</a></p>
<blockquote>
<p>The test projects in the solution are empty. They only exist for the organizational aspect of the solution. As an exercise, you can migrate the tests from <em>Chapter 18</em> and adapt them to this new architectural style.</p>
</blockquote>
</blockquote>
<p>Let’s start with the communication piece.</p>

<h3 data-number="21.6.1">Sending events from the catalog module</h3>
<p>For the catalog to communicate the events that the basket module needs, it must define the following new operations:</p>
<ul>
<li>Create products</li>
<li>Delete products</li>
</ul>
<p>Here are the API contracts we must create in the <code>REPR.Products.Contracts</code> project to support those two operations:</p>
<div><pre><code>namespace REPR.Products.Contracts;
public record class CreateProductCommand(string Name, decimal UnitPrice);
public record class CreateProductResponse(int Id, string Name, decimal UnitPrice);
public record class DeleteProductCommand(int ProductId);
public record class DeleteProductResponse(int Id, string Name, decimal UnitPrice);</code></pre>
</div>
<p>The API contracts should look very familiar by now and are similar to those from previous chapters. We then need the following two event contracts:</p>
<div><pre><code>namespace REPR.Products.Contracts;
public record class ProductCreated(int Id, string Name, decimal UnitPrice);
public record class ProductDeleted(int Id);</code></pre>
</div>
<p>The two event classes are also very straightforward, but their name is in the past tense because an event happened in the past. So, the module creates the product and then notifies its subscribers that a product was created (in the past). Exactly like we studied in <em>Chapter 19</em>, <em>Introduction to Microservices Architecture</em>. Moreover, the events are simple data containers, like an API contract—a DTO—is.How do we send those events? Let’s have a look at the <code>CreateProductHandler</code> class:</p>
<div><pre><code>namespace REPR.Products.Features;
public class CreateProductHandler
{
    private readonly ProductContext _db;
    private readonly CreateProductMapper _mapper;
    private readonly IBus _bus;
    public CreateProductHandler(ProductContext db, CreateProductMapper mapper, IBus bus)
    {
        _db = db ?? throw new ArgumentNullException(nameof(db));
        _mapper = mapper ?? throw new ArgumentNullException(nameof(mapper));
        _bus = bus ?? throw new ArgumentNullException(nameof(bus));
    }
    public async Task&lt;CreateProductResponse&gt; HandleAsync(CreateProductCommand command, CancellationToken cancellationToken)
    {
        var product = _mapper.Map(command);
        var entry = _db.Products.Add(product);
        await _db.SaveChangesAsync(cancellationToken);
        var productCreated = _mapper.MapToIntegrationEvent(entry.Entity);
        await _bus.Publish(productCreated, CancellationToken.None);
        var response = _mapper.MapToResponse(entry.Entity);
        return response;
    }
}</code></pre>
</div>
<p>In the preceding code, we inject an <code>IBus</code> interface from MassTransit in the <code>CreateProductHandler</code> class. We also inject an object mapper and an EF Core <code>ProductContext</code>. The <code>HandleAsync</code> method does the following:</p>
<ol>
<li>Creates the product and saves it to the database.</li>
<li>Publishes a <code>ProductCreated</code> event (highlighted code).</li>
<li>Returns a <code>CreateProductResponse</code> instance based on the new product.</li>
</ol>
<p>The <code>Publish</code> method sends the event to the configured pipe, which is in memory in our case. The code passes a <code>CancellationToken.None</code> argument here because we don’t want this operation to be canceled by any external force because the change is already saved in the database.Because of the mapping code, the publishing code may be hard to understand in a book. The <code>MapToIntegrationEvent</code> method converts a <code>Product</code> object to a <code>ProductCreated</code> instance, so the <code>productCreated</code> variable is of type <code>ProductCreated</code>. Here’s the <em>Mapperly</em> mapper class with that method highlighted:</p>
<div><pre><code>namespace REPR.Products.Features;
[Mapper]
public partial class CreateProductMapper
{
    public partial Product Map(CreateProductCommand product);
    public partial ProductCreated MapToIntegrationEvent(Product product);
    public partial CreateProductResponse MapToResponse(Product product);
}</code></pre>
</div>
<p>The <code>DeleteProductHandler</code> class follows a similar pattern but publishes the <code>ProductDeleted</code> event instead.Now, let’s explore how the basket module consumes those events.</p>


<h3 data-number="21.6.2">Consuming the events from the basket module</h3>
<p>The basket module wants to cache existing products. We can achieve this in different ways. In this case, we create the following <code>Product</code> class that we persist in the database:</p>
<div><pre><code>namespace REPR.Baskets.Data;
public record class Product(int Id);</code></pre>
</div>
<p>To make use of it, we must expose the following property from the <code>BasketContext</code> class:</p>
<div><pre><code>public DbSet&lt;Product&gt; Products =&gt; Set&lt;Product&gt;();</code></pre>
</div>
<p>Then, we can start to leverage this cache. Firstly, we must populate it when the catalog module creates a product and remove that product when deleted. The <code>ProductEventsConsumers</code> class handles both events. Here’s the skeleton of this class:</p>
<div><pre><code>using REPR.Products.Contracts;
namespace REPR.Baskets.Features;
public class ProductEventsConsumers : IConsumer&lt;ProductCreated&gt;, IConsumer&lt;ProductDeleted&gt;
{
    private readonly BasketContext _db;
    private readonly ILogger _logger;
    public ProductEventsConsumers(BasketContext db, ILogger&lt;ProductEventsConsumers&gt; logger)
    {
        _db = db ?? throw new ArgumentNullException(nameof(db));
        _logger = logger ?? throw new ArgumentNullException(nameof(logger));
    }
    public async Task Consume(ConsumeContext&lt;ProductCreated&gt; context) 
    {...}
    public async Task Consume(ConsumeContext&lt;ProductDeleted&gt; context) 
    {...}
}</code></pre>
</div>
<p>The highlighted code represents the two event handlers. The <code>IConsumer&lt;TMessage&gt;</code> interface contains the <code>Consume</code> method. Implementing the interface twice with a different <code>TMessage</code> generic parameter prescribes implementing the two <code>Consume</code> methods in the <code>ProductEventsConsumers</code> class. Each method handles its own event.When a product is created in the product module, the following method is executed in the basket module (I removed the logging code for brevity):</p>
<div><pre><code>public async Task Consume(ConsumeContext&lt;ProductCreated&gt; context)
{
    var product = await _db.Products.FirstOrDefaultAsync(
        x =&gt; x.Id == context.Message.Id,
        cancellationToken: context.CancellationToken
    );
    if (product is not null)
    {
        return;
    }
    _db.Products.Add(new(context.Message.Id));
    await _db.SaveChangesAsync();
}</code></pre>
</div>
<p>The preceding code adds a new product to the database if it does not exist. If it does exist, it does nothing.</p>
<blockquote>
<p>We could add telemetry here to log and flag the conflict and explore how to solve it. If the product already exists, the event was received twice, which may indicate an issue with our system.</p>
</blockquote>
<p>Here’s the flow of what happens when an employee creates a new product:</p>
<figure>
<img alt="Figure 20.8: A diagram showcasing the sequence of operations when someone adds a product to the system." src="img/file179.png"/><figcaption aria-hidden="true">Figure 20.8: A diagram showcasing the sequence of operations when someone adds a product to the system.</figcaption>
</figure>
<p>Let’s review the operations:</p>
<ol>
<li>A client sends a POST request to the API (the aggregator application). The catalog module receives the request.</li>
<li>The catalog creates the product and adds it to the database.</li>
<li>The catalog then publishes the <code>ProductCreated</code> event using MassTransit.</li>
<li>In the basket module, the <code>Consume</code> method of the <code>ProductEventsConsumers</code> class gets called by MassTransit in reaction to the <code>ProductCreated</code> event.</li>
<li>The basket module adds the product's identifier to the database to its materialized view.</li>
</ol>
<p>Now that we know how the <code>ProductEventsConsumers</code> class handles the <code>ProductCreated</code> events, let’s explore the <code>ProductDeleted</code> events (logging code omitted for brevity):</p>
<div><pre><code>public async Task Consume(ConsumeContext&lt;ProductDeleted&gt; context)
{
    var item = await _db.Products.FirstOrDefaultAsync(
        x =&gt; x.Id == context.Message.Id,
        cancellationToken: context.CancellationToken
    );
    if (item is null)
    {
        return;
    }
    // Remove the products from existing baskets
    var existingItemInCarts = _db.Items
        .Where(x =&gt; x.ProductId == context.Message.Id);
    var count = existingItemInCarts.Count();
    _db.Items.RemoveRange(existingItemInCarts);
    // Remove the product from the internal cache
    _db.Products.Remove(item);
    // Save the changes
    await _db.SaveChangesAsync();
}</code></pre>
</div>
<p>The preceding <code>Consume</code> method removes the products from people’s shopping baskets and the materialized view. If the product does not exist, the method does nothing because there’s nothing to handle.A similar flow happens when an employee deletes a product from the catalog and publishes a <code>ProductDeleted</code> event. The basket module then reacts to the event and updates its cache (materialized view).Now that we have explored this exciting part of the project, let’s look at the aggregator.</p>


<h3 data-number="21.6.3">Inside the aggregator</h3>
<p>The aggregator application is like an empty shell that loads the other assemblies and configures the cross-cutting concerns. It references the modules, then assembles and boots the application. Here’s the first part of the <code>Program.cs</code> file:</p>
<div><pre><code>using FluentValidation;
using FluentValidation.AspNetCore;
using MassTransit;
using REPR.API.HttpClient;
using REPR.Baskets;
using REPR.Products;
using System.Reflection;
var builder = WebApplication.CreateBuilder(args);
// Register fluent validation
builder.AddFluentValidationEndpointFilter();
builder.Services
    .AddFluentValidationAutoValidation()
    .AddValidatorsFromAssemblies(new[] {
        Assembly.GetExecutingAssembly(),
        Assembly.GetAssembly(typeof(BasketModuleExtensions)),
        Assembly.GetAssembly(typeof(ProductsModuleExtensions)),
    })
;
builder.AddApiHttpClient();
builder.AddExceptionMapper();
builder
    .AddBasketModule()
    .AddProductsModule()
;
builder.Services.AddMassTransit(x =&gt;
{
    x.SetKebabCaseEndpointNameFormatter();
    x.UsingInMemory((context, cfg) =&gt;
    {
        cfg.ConfigureEndpoints(context);
    });
    x.AddBasketModuleConsumers();
});</code></pre>
</div>
<p>The preceding code registers the following:</p>
<ul>
<li><em>FluentValidation</em>; it also scans the assemblies for validators.</li>
<li>The <code>IWebClient</code> interface that we use to seed the database (the <code>AddApiHttpClient</code> method).</li>
<li><em>ExceptionMapper</em>.</li>
<li>The modules' dependencies (highlighted)</li>
<li><em>MassTransit</em>; it also registers the consumers from the basket module (highlighted).</li>
</ul>
<p>The container we register those dependencies into is also used for and by the modules because the aggregator is the host. Those dependencies are shared across the modules.The second part of the <code>Program.cs</code> file is the following:</p>
<div><pre><code>var app = builder.Build();
app.UseExceptionMapper();
app
    .MapBasketModule()
    .MapProductsModule()
;
// Convenience endpoint, seeding the catalog
app.MapGet("/", async (IWebClient client, CancellationToken cancellationToken) =&gt;
{
    await client.Catalog.CreateProductAsync(new("Banana", 0.30m), cancellationToken);
    await client.Catalog.CreateProductAsync(new("Apple", 0.79m), cancellationToken);
    await client.Catalog.CreateProductAsync(new("Habanero Pepper", 0.99m), cancellationToken);
    return new
    {
        Message = "Application started and catalog seeded. Do not refresh this page, or it will reseed the catalog."
    };
});
app.Run();</code></pre>
</div>
<p>The preceding code registers the <code>ExceptionMapper</code> middleware, then the modules. It also adds a seeding endpoint (highlighted). If you remember, we were seeding the database using the <code>DbContext</code> in the previous versions. However, since the basket module needs to receive the events from the catalog to build a materialized view, it is more convenient to seed the database through the catalog module. In this version, the program seeds the database when a client hits the <code>/</code> endpoint. By default, when starting the application, Visual Studio should open a browser at that URL, which will seed the database.</p>
<blockquote>
<p>Seeding the database by sending a GET request to the <code>/</code> endpoint is very convenient for an academic scenario where we use in-memory databases. However, this could be disastrous in a production environment because it would reseed the database whenever someone hits that endpoint.</p>
</blockquote>
<p>Let’s explore the <code>IWebClient</code>, next.</p>


<h3 data-number="21.6.4">Exploring the REST API HttpClient</h3>
<p>In the <code>shared</code> directory, the <code>REPR.API.HttpClient</code> project contains the REST API client code. The code is very similar to the previous project, but the <code>IProductsClient</code> now exposes a create and delete method (highlighted):</p>
<div><pre><code>using Refit;
using REPR.Products.Contracts;
namespace REPR.API.HttpClient;
public interface IProductsClient
{
    [Get("/products/{query.ProductId}")]
    Task&lt;FetchOneProductResponse&gt; FetchProductAsync(
        FetchOneProductQuery query,
        CancellationToken cancellationToken);
    [Get("/products")]
    Task&lt;FetchAllProductsResponse&gt; FetchProductsAsync(
        CancellationToken cancellationToken);
    [Post("/products")]
    Task&lt;CreateProductResponse&gt; CreateProductAsync(
        CreateProductCommand command,
        CancellationToken cancellationToken);
    [Delete("/products/{command.ProductId}")]
    Task&lt;DeleteProductResponse&gt; DeleteProductAsync(
        DeleteProductCommand command,
        CancellationToken cancellationToken);
}</code></pre>
</div>
<p>On top of this, the project only references the <code>Contracts</code> projects, limiting its dependencies to the classes it needs. It does not reference the complete modules anymore. This makes this project easy to reuse. For example, we could build another project, like a user interface (UI), then reference and use this typed client to query the API (modular monolith) from that .NET UI.Since we created this client for a microservices application, we have two base downstream service URLs—one for the product microservice and one for the basket microservice. This nuance suits us well since we may want to migrate our monolith to microservices later. In the meantime, all we have to do is set the two keys to the same host, like the following <code>appsettings.json</code> file from the aggregator:</p>
<div><pre><code>{
  "Downstream": {
    "Baskets": {
      "BaseAddress": "https://localhost:7164/"
    },
    "Products": {
      "BaseAddress": "https://localhost:7164/"
    }
  }
}</code></pre>
</div>
<p>With these configurations, the <code>AddApiHttpClient</code> method configures two <code>HttpClient</code> with the same <code>BaseAddress</code> value, like this:</p>
<div><pre><code>using Microsoft.AspNetCore.Builder;
using Microsoft.Extensions.Configuration;
using Microsoft.Extensions.DependencyInjection;
using Refit;
namespace REPR.API.HttpClient;
public static class ApiHttpClientExtensions
{
    public static WebApplicationBuilder AddApiHttpClient(this WebApplicationBuilder builder)
    {
        const string basketsBaseAddressKey = "Downstream:Baskets:BaseAddress";
        const string productsBaseAddressKey = "Downstream:Products:BaseAddress";
        var basketsBaseAddress = builder.Configuration
            .GetValue&lt;string&gt;(basketsBaseAddressKey) ?? throw new BaseAddressNotFoundException(basketsBaseAddressKey);
        var productsBaseAddress = builder.Configuration
            .GetValue&lt;string&gt;(productsBaseAddressKey) ?? throw new BaseAddressNotFoundException(productsBaseAddressKey);
        builder.Services
            .AddRefitClient&lt;IBasketsClient&gt;()
            .ConfigureHttpClient(c =&gt; c.BaseAddress = new Uri(basketsBaseAddress))
        ;
        builder.Services
            .AddRefitClient&lt;IProductsClient&gt;()
            .ConfigureHttpClient(c =&gt; c.BaseAddress = new Uri(productsBaseAddress))
        ;
        builder.Services.AddTransient&lt;IWebClient, DefaultWebClient&gt;();
        return builder;
    }
}</code></pre>
</div>
<p>And voilà, we have a functional typed client we can reuse and a working modular monolith. Let’s test this out next.</p>


<h3 data-number="21.6.5">Sending HTTP requests to the API</h3>
<p>Now that we have a working modular monolith, we can reuse similar HTTP requests than the previous versions.At the root of the REPR.API project, we can use the following:</p>
<ul>
<li>The <code>API-Products.http</code> file contains requests to the product module.</li>
<li>The <code>API-Baskets.http</code> file contains requests to the basket module.</li>
</ul>
<p>A new outcome in this API compared to the previous versions is when we try to add a product that does not exist in the catalog to our basket, like the following request:</p>
<div><pre><code>POST https://localhost:7164/baskets
Content-Type: application/json
{
    "customerId": 1, 
    "productId": 5, 
    "quantity": 99
}</code></pre>
</div>
<p>Since the product <code>5</code> does not exist, the API returns the following:</p>
<div><pre><code>HTTP/1.1 400 Bad Request
Content-Type: application/problem+json
{
  "type": "https://tools.ietf.org/html/rfc9110#section-15.5.1",
  "title": "One or more validation errors occurred.",
  "status": 400,
  "errors": {
    "productId": [
      "The Product does not exist."
    ]
  }
}</code></pre>
</div>
<p>That error confirms that the validation works as expected. But how are we validating this?</p>


<h3 data-number="21.6.6">Validating the existence of a product</h3>
<p>In the <em>add item</em> feature, the <code>AddItemValidator</code> class ensures the product exists while validating the <code>AddItemCommand</code> object. To achieve this, we leverage the <code>MustAsync</code> and <code>WithMessage</code> methods from FluentValidation. Here’s the code:</p>
<div><pre><code>namespace REPR.Baskets.Features;
public class AddItemValidator : AbstractValidator&lt;AddItemCommand&gt;
{
    private readonly BasketContext _db;
    public AddItemValidator(BasketContext db)
    {
        _db = db ?? throw new ArgumentNullException(nameof(db));
        RuleFor(x =&gt; x.CustomerId).GreaterThan(0);
        RuleFor(x =&gt; x.ProductId)
            .GreaterThan(0)
            .MustAsync(ProductExistsAsync)
            .WithMessage("The Product does not exist.")
        ;
        RuleFor(x =&gt; x.Quantity).GreaterThan(0);
    }
    private async Task&lt;bool&gt; ProductExistsAsync(int productId, CancellationToken cancellationToken)
    {
        var product = await _db.Products
            .FirstOrDefaultAsync(x =&gt; x.Id == productId, cancellationToken);
        return product is not null;
    }
}</code></pre>
</div>
<p>The preceding code implements the same rules as the previous versions but also calls the <code>ProductExistsAsync</code> method that fetches the requested product from the cache. If the result is <code>false</code>, the validation fails with the message “<em>The Product does not exist.</em>”.Here’s the resulting flow of this change:</p>
<ol>
<li>The client calls the API.</li>
<li>The validation middleware calls the validator.</li>
<li>The validator fetches the record from the database and validates its existence.</li>
<li>If the product does not exist, the request is short-circuited here and returned to the client. Otherwise, it continues to the <code>AddItemHandler</code> class.</li>
</ol>
<p>With this in place, we covered all new scenarios from this project. We also explored how the aggregator connects the modules together and how easy it is to keep our modules independent.Next, we get the BFF back into the project and explore how to transition our modular monolith to a microservices architecture.</p>



<h2 data-number="21.7">Transitioning to Microservices</h2>
<p>You don’t have to transition your monolith to microservices; deploying a monolith is fine if it fits your needs. However, if ever you need to, you could shield your aggregator with a gateway or a reverse proxy so you can extract modules into their own microservices and reroute the requests without impacting the clients. This would also allow you to gradually transfer the traffic to the new service while keeping the monolith intact in case of an unexpected failure.In a modular monolith, the aggregator registers most dependencies, so when migrating, you must also migrate this shared setup. One way to not duplicate code would be to create and reference a shared assembly containing those registrations. This makes it easier to manage the dependencies and shared configurations, but it is also coupling between the microservices and the aggregator. Leveraging the code of this shared assembly is even easier when the microservices are part of the same mono-repo; you can reference the project directly without managing a NuGet package. Yet, when you update the library, it updates all microservices and the monolith, voiding the deployment independence of each application.Once again, consider keeping the monolith intact versus the operational complexity that deploying microservices would bring.In the solution, the <code>REPR.BFF</code> project is a migration of <em>Chapter 19</em> code. The code and the logic are almost the same. It leverages the <code>REPR.API.HttpClient</code> project and configures the downstream base addresses in its <code>appsettings.Development.json</code> file. The <code>REPR.BFF.http</code> file at the root of the project contains a few requests to test the application.</p>
<blockquote>
<p>The code is available on GitHub: <a href="https://adpg.link/bn1F">https://adpg.link/bn1F</a>. I suggest playing with the live application, which will yield a better result than copying the code in the book a second time. The relationships between the projects are also more evident in Visual Studio than written in a book. Feel free to refactor the projects, add tests, or add features. The best way to learn is to practice!</p>
</blockquote>
<p>Next, let’s have a look at challenges and pitfalls.</p>


<h2 data-number="21.8">Challenges and Pitfalls</h2>
<p>It is a misconception that monoliths are only suitable for small projects or small teams. A well-built monolith can go a long way, which is what a modular structure and a good analysis bring. Modular Monoliths can be very powerful and work well for different sizes of projects. It is essential to consider the pros and cons of each approach when selecting the application pattern for a project. Maybe a cloud-native, serverless, microservices application is what your next project needs, but perhaps a well-designed Modular Monolith would achieve the same performance at a tremendously lower cost.To help you make those decisions, here are some potential challenges and how to avoid or mitigate them:</p>
<ul>
<li><strong>Too much complexity within a module:</strong> One risk is making the modules too complex. Like any piece of code, it becomes harder to manage a module if it does too many things. Moreover, the bigger the piece of software, the more chance the shiny initial design starts to bleed into a big ball of mud. We can avoid this by keeping each module focused on a specific part of the domain and by applying the SRP.</li>
<li><strong>Poorly defined module boundaries:</strong> If the boundaries between modules are unclear, it can create problems. For example, if two modules do similar things, it can confuse developers and lead to one part of the system depending on the wrong module. Another example is when two modules that should be part of the same bounded context are separated. In that case, the chances are that the two modules will interact a lot with each other, creating chattiness and significantly increasing the coupling between them. We avoid this with good planning, domain analysis, and ensuring each module has a specific job (SRP).</li>
<li><strong>Scaling:</strong> Even though Modular Monoliths are easier to manage, they carry the monolith issues when they must scale up. We must deploy the whole application, so we can't scale modules independently. Moreover, even if modules own their data, they probably share one database, which must also be scaled as a whole. If scaling the whole system is impossible, we can migrate specific modules to microservices. We can also extract in-memory services to distributed ones, like using a distributed cache instead of a memory cache, leveraging a cloud-based event broker instead of an in-memory one, or even having compute or data-intensive modules start using their own database so they partially scale independently. However, these solutions make the deployment and the infrastructure more complex, gradually moving towards complex infrastructure, which defeats the monolith’s benefits.</li>
<li><strong>Eventual consistency:</strong> Keeping data in sync between modules can be challenging. We can handle this using event-driven architecture. Using an in-memory message broker is low-latency and high-fidelity (no network involved), which is a good first step towards learning and dealing with eventual consistency and breaking the monolith into microservices (if the transition is ever needed). However, a more resilient transport is recommended for production, so the system resists to failures, increasing the synchronization latency. You do not have to use events if you prefer to remove this complexity; a well-designed relational database can do the trick.</li>
<li><strong>Transition to microservices:</strong> Moving an application to microservices architecture after the fact can be a significant undertaking. However, starting with an event-powered Modular Monolith should make the journey less painful.</li>
</ul>
<p>While Modular Monoliths offer many benefits, they can also pose challenges. The key is to plan well, keep things simple, and be ready to adapt as the program evolves.</p>


<h2 data-number="21.9">Conclusion</h2>
<p>In this chapter, we learned about the Modular Monolith architectural style, which blends the simplicity of monolithic architectures with the flexibility of microservices. This architectural style organizes software applications into distinct, loosely coupled modules, each responsible for a specific business capability. Unlike microservices, we deploy these modules as a single unit, like a monolith. We discussed the benefits of Modular Monoliths, including easier overall management, sound development and testing experiences, cost-effectiveness, and a simplified deployment model.We saw that a Modular Monolith comprises modules, a module aggregator, and an inter-module communication infrastructure—event-driven in this case.We learned that analyzing the domain, designing the modules, and identifying the interactions between modules before starting the development improves the chances of success of the product.We touched on transitioning from a Modular Monolith to a microservices architecture, which involves extracting modules into separate microservices and rerouting requests.We also highlighted the importance of knowing potential challenges and pitfalls. These include module complexity, poorly defined module boundaries, scaling limitations, and eventual consistency caused by an asynchronous communication model.</p>


<h2 data-number="21.10">Questions</h2>
<p>Let’s take a look at a few practice questions:</p>
<ol>
<li>What are the core principles of a Modular Monolith?</li>
<li>What are the advantages of Modular Monoliths?</li>
<li>What are traditional Monoliths?</li>
<li>Are poorly defined module boundaries indeed beneficial to a Modular Monolith?</li>
<li>Is it true that transitioning an application to microservices architecture can be a significant undertaking?</li>
</ol>


<h2 data-number="21.11">Further reading</h2>
<p>Here is a link to build upon what we learned in the chapter:</p>
<ul>
<li>Microservices Aggregation (Modular Monolith): <a href="https://adpg.link/zznM">https://adpg.link/zznM</a></li>
<li>When (modular) monolith is the better way to build software: <a href="https://adpg.link/KBGB">https://adpg.link/KBGB</a></li>
<li>Source code: <a href="https://adpg.link/gyds">https://adpg.link/gyds</a></li>
</ul>


<h2 data-number="21.12">An end is simply a new beginning</h2>
<p>This may be the end of the book, but it is also the continuation of your journey into software architecture and design. I hope you found this to be a refreshing view of design patterns and how to design SOLID apps.Depending on your goals and current situation, you may want to explore one or more application-scale design patterns in more depth, start your next personal project, start a business, apply for a new job, or all of those. No matter your goals, keep in mind that designing software is technical but also an art. There is rarely one way to implement a feature but multiple acceptable ways. Every decision has trade-offs, and experience is your best friend, so keep programming, learn from your mistakes, become better, and continue. The path to mastery is a never-ending continuous learning loop. Remember that we are all born knowing next to nothing, so not knowing something is expected; we need to learn. Please ask questions, read, experiment, learn, and share your knowledge with others. Explaining a concept to someone is extremely rewarding and reinforces your own learning and knowledge.Now that this book is complete, you may find interesting articles on my blog (<a href="https://adpg.link/blog">https://adpg.link/blog</a>). Feel free to reach out on Discord, Twitter <code>@CarlHugoM</code> (<a href="https://adpg.link/twit">https://adpg.link/twit</a>), or LinkedIn (<a href="https://adpg.link/edin">https://adpg.link/edin</a>). I hope you found the book educational and approachable and that you learned many things. I wish you success in your career.</p>


<h2 data-number="21.13">Answers</h2>
<ol>
<li>We must treat each module as a microservice and deploy the application as a single unit—a monolith.</li>
<li>A few moving parts make the application simpler. Each module is independent, making modules loosely coupled. Its simple deployment model leads to cost-effective hosting and ease of deployment.</li>
<li>Traditional monolithic architectures build the application as a single, indivisible unit, often resulting in tightly coupled functionalities.</li>
<li>False. Poorly defined module boundaries hinder the health of the application.</li>
<li>True. Even if a well-conceived Modular Monolith can help, the transition will be a journey.</li>
</ol>


</body>
</html>
