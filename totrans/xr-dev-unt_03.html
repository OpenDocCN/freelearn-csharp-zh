<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-49"><a id="_idTextAnchor009"/>3</h1>
<h1 id="_idParaDest-50">VR Development in Unity</h1>
<p>Let's explore the world of VR, from creating your first VR project in Unity to deploying our first VR scene on a headset or simulator. In this chapter, we’ll present the most important VR toolkits and plugins available in Unity, helping you become familiar with each one’s capabilities.</p>
<p>You’ll gain hands-on experience with the <strong class="bold">XR Interaction Toolkit</strong>’s demo scene , understand its most important components and scripts, and learn how to use them in your future VR projects. We’ll help you understand the nuances between VR development and traditional game development, as well as share different strategies to ensure a VR headset’s computing power.</p>
<p>Another important skill you will gain in this chapter is how to test and deploy VR experiences on various devices, from simulators to VR headsets. This chapter will give you a robust foundation that will equip you with the necessary skills and knowledge to create increasingly complex and immersive VR scenes using Unity.</p>
<p>We’ll cover the following topics as we proceed:</p>
<ul>
<li>What is VR development?</li>
<li>Setting up a VR project in Unity and the XR Interaction Toolkit</li>
<li>Exploring the XR Interaction Toolkit demo scene</li>
<li>Deploying and testing VR experiences onto different VR platforms or simulators</li>
</ul>
<h1 id="_idParaDest-51">Technical requirements</h1>
<p>To effectively follow along with this chapter on VR development in Unity, it is essential to have an appropriate hardware and software setup. For the most effective and seamless experience, we strongly recommend utilizing a Windows PC or a robust Windows laptop, even if your target platform is a standalone VR headset.</p>
<h2 id="_idParaDest-52">Windows</h2>
<p>Windows is <a id="_idIndexMarker169"/>the most supported platform by the majority of VR headset manufacturers, including <strong class="bold">Meta Quest</strong> and <strong class="bold">HTC</strong>. This <a id="_idIndexMarker170"/>support stems from Windows’ comprehensive<a id="_idIndexMarker171"/> hardware support and its optimization for gaming. Whether you’re developing for PC-tethered or standalone VR headsets, a Windows environment will provide the most resources and compatibility for VR development in Unity.</p>
<p>For a smooth experience, we recommend the following minimum requirements:</p>
<ul>
<li>Operating system: Windows 10 or above</li>
<li>Processor: Intel i5-4590/AMD Ryzen 5 1500X or greater</li>
<li>Memory: 8 GB RAM or more</li>
<li>Graphics: NVIDIA GTX 1060/AMD Radeon RX 480 or greater</li>
</ul>
<h2 id="_idParaDest-53">macOS</h2>
<p>While macOS <a id="_idIndexMarker172"/>is rarely <a id="_idIndexMarker173"/>supported by VR headsets, Unity still enables VR development on Apple devices through platforms such as ARKit that predominantly target AR instead of VR. If you’re using macOS and targeting standalone VR headsets, consider setting up a Windows partition through <strong class="bold">Boot Camp</strong>, which allows you to use any Windows-compatible VR headset. This setup will enable you to follow along with the instructions in this chapter without significant issues.</p>
<h2 id="_idParaDest-54">Linux</h2>
<p>Linux’s VR<a id="_idIndexMarker174"/> support is relatively limited, but <strong class="bold">Valve</strong> does<a id="_idIndexMarker175"/> extend support to the <em class="italic">Valve Index</em> and <em class="italic">HTC Vive</em> headsets<a id="_idIndexMarker176"/> via SteamVR on Linux platforms. If you’re targeting standalone VR headsets such as<a id="_idIndexMarker177"/> the <em class="italic">Oculus Quest</em>, Linux can be used for development by creating Android builds of your VR applications in Unity, then transferring these builds to the headset for testing. However, the standalone VR headsets typically run a version of Android, not Linux, which may bring about unique challenges.</p>
<p>If you’re utilizing a Linux platform, please ensure your system meets the following minimum requirements:</p>
<ul>
<li>Operating system: Ubuntu 18.04 LTS or newer</li>
<li>Processor: Dual-core CPU with hyper-threading</li>
<li>Memory: 8 GB RAM or more</li>
<li>Graphics: Nvidia GeForce GTX 970, AMD RX480</li>
</ul>
<h1 id="_idParaDest-55">What is VR development?</h1>
<p>When<a id="_idIndexMarker178"/> embarking on the fascinating journey of VR development in Unity, it’s important that we understand the features and peculiarities of VR development. This includes appreciating the software and hardware limitations, as well as the challenges that exist within this field. In the forthcoming sections, we will dive into the contrasts between VR development and traditional game development for 2D computer screens.</p>
<p>Moreover, we will explore current trajectories and future trends of VR headset technology. This exploration includes the unique pros and cons that each VR headset brings to the table. Such understanding is key as it not only guides you in making a well-informed selection of a VR headset that suits your needs, but also equips you with the necessary knowledge to judge which VR headset would be ideal or less ideal, given a particular context.</p>
<p>By the end of this section, you will be well-equipped to navigate the immersive and dynamic world of VR development in Unity.</p>
<h2 id="_idParaDest-56">Exploring the contrasts between classical and VR game development</h2>
<p>Classical game development and VR development, although existing on different ends of the <a id="_idIndexMarker179"/>gaming spectrum, both emerged from the same desire to create immersive, virtual landscapes. Intriguingly, both categories share more than just the objective of captivating the audience; they’re built on similar foundational elements of game design and require a corresponding toolset.</p>
<p>Both domains of game creation demand careful planning and consideration of gameplay mechanics, user interfaces, objectives, levels, and storylines. Developers, whether they’re working on a traditional game or a VR project, often use the same suite of programming languages such as C++, C#, or Python for scriptwriting.</p>
<p>The <a id="_idIndexMarker180"/>creation of <strong class="bold">game assets</strong> – textures, models, animations, and sounds – is another commonality between the two fields. VR development may lean slightly more toward 3D models and spatial audio to enrich the immersive nature of the environment. However, the overarching process of creating and incorporating these assets remains congruous in both disciplines. Regardless of the gaming medium, be it a 2D screen or a VR headset, the implementation of realistic physics is paramount to an engrossing experience.</p>
<p>Though they share these similarities, the divergences between classical game development and VR development are just as fascinating, primarily due to VR’s distinct nature and abilities.</p>
<p>One of the significant contrasts lies in the design’s dimensionality. Classical 2D games primarily operate on a flat plane, wherein objects are depicted in two dimensions: height and width. The <a id="_idIndexMarker181"/>game <em class="italic">Super Mario Bros</em>. serves as a classic example, with the action unfolding from left to right and the characters’ movements largely restricted within this plane.</p>
<p>Conversely, VR games invite depth as a vital third dimension into play, thus offering the user the freedom to explore the scene from any perspective or position, akin to their experience in the real world. For instance, in a VR game<a id="_idIndexMarker182"/> such as <em class="italic">Beat Saber</em>, players can look around, reach out, and interact with the game environment in a way that mimics real-world spatial interactions.</p>
<p>These subtle yet impactful differences pave the way for a varied range of experiences in classical game development and VR development, each with its own unique charm and challenges.</p>
<p>Equally important, the scope of player movement exhibits a clear distinction between classical 2D games and VR gaming experiences. Traditional 2D games typically restrict player actions to the <em class="italic">x</em> and <em class="italic">y</em> axes, confining movement within a certain plane. Conversely, VR introduces varying degrees of freedom.</p>
<p>For example, <strong class="bold">three degrees of freedom</strong> (<strong class="bold">3DoF</strong>) devices, often paired with <strong class="bold">Mobile VR</strong> solutions <a id="_idIndexMarker183"/>such as <em class="italic">Google Cardboard</em>, allow <a id="_idIndexMarker184"/>the user to look around freely (pitch, yaw, and roll) but<a id="_idIndexMarker185"/> don’t track physical displacement within a space. This provides a basic level of immersion, enabling the user to view a scene or object from multiple perspectives while standing still.</p>
<p>A step <a id="_idIndexMarker186"/>further into the realm of VR are the <strong class="bold">six degrees of freedom</strong> (<strong class="bold">6DoF</strong>) devices. These not only track a <a id="_idIndexMarker187"/>player’s view but also register their physical movement along the <em class="italic">x</em>, <em class="italic">y</em>, and <em class="italic">z</em> axes, thus creating a more immersive and interactive environment. Crafting experiences for 6DoF can be intricate, necessitating full 3D spatial interactions. For instance, modern VR headsets such as the <em class="italic">Meta Quest 2</em> pair up with VR controllers or even offer hand-tracking capabilities. Users can interact with the virtual world in an incredibly intuitive manner, pressing buttons, twisting their hands, and conducting an array of movements that the AI-powered cameras on the VR headset accurately capture and translate in real time.</p>
<p>The<a id="_idIndexMarker188"/> imperative of <strong class="bold">player safety</strong> is a unique aspect that distinguishes VR development from traditional game creation. While physical safety concerns are virtually non-existent in classical 2D game development, the expanded range of movement in VR can potentially lead to real-world mishaps. As such, developers need to engineer safety features such as virtual boundaries or alert systems to forestall accidents.</p>
<p>The final distinguishing<a id="_idIndexMarker189"/> factor lies in <strong class="bold">player conditioning</strong>. Classical 2D games generally rely on players’ familiarity with standard game controls and mechanics – a safe assumption given the ubiquity of PCs in everyday life. VR, however, often pioneers novel forms of interactions that might confound first-time users. To bridge this gap, developers may need to incorporate tutorials or guides to help players navigate and interact with the VR gaming world.</p>
<p>Not only does VR development diverge significantly from traditional game development, but it also showcases a broader spectrum of techniques to power VR headsets. These diverse approaches, integral to the operation of VR technology, will be unraveled in the next section.</p>
<h2 id="_idParaDest-57">Understanding different approaches to power VR headsets</h2>
<p>The <a id="_idIndexMarker190"/>power of a virtual reality headset can dramatically impact the success of your VR application. Let’s delve into the most common.</p>
<h3>PC VR (alternatively known as PC-based VR or tethered VR headsets)</h3>
<p><strong class="bold">PC-based VR headsets</strong>, including<a id="_idIndexMarker191"/> models such as <em class="italic">Valve Index</em> and <em class="italic">HTC Vive Pro</em>, hinge on the computational<a id="_idIndexMarker192"/> capacity of a desktop or laptop computer. They delegate the heavy lifting, such as intensive computations, graphics rendering, and 3D simulations, to the connected computer. To this day, a substantial portion of VR headsets are either wholly PC-based or support a PC VR mode alongside a standalone variant. This is primarily because PC VR, bolstered by the superior computational power and graphical prowess of PCs, provides the most graphically rich and immersive VR experiences. Attributes such as level of detail, frame rate, and responsiveness are top-notch in PC-based VR.</p>
<p>PC-based VR systems predominantly employ two modes of connection between the VR headset and the computer. The wired cable connection, often utilizing HDMI or DisplayPort for video and USB for data, is the traditional route. A more recent innovation is a wireless connection, exemplified by the <em class="italic">Air Link</em> connection in the <em class="italic">Meta Quest</em> series. However, wireless PC VR connections are seldom used in reality due to their higher latency, which could hamper the responsiveness of the VR experience. Video quality can also fluctuate depending on network conditions, necessitating a robust and stable Wi-Fi signal for optimal operation.</p>
<p>Despite the convenience of wireless connections, for a seamless VR experience, a wired connection is still advisable, even when the headset supports both. It’s essential to note, however, that a wired PC VR system sacrifices portability due to its physical attachment to a PC. Whether you choose wired or wireless, the mobility of your VR setup is limited. For example, you can’t easily pack your VR headset for a holiday, an event, a conference, or a friend’s gathering. If you decide to transport your VR gear, it necessitates bringing along a powerful PC or ensuring the destination has the required computational resources. Coupled with the potentially steep price of a high-performance or gaming PC, this could make PC VR systems a substantial investment for some users.</p>
<h3>Standalone VR headsets</h3>
<p>Addressing <a id="_idIndexMarker193"/>the limitations of PC-based VR headsets, <strong class="bold">standalone VR devices</strong> are making waves in the market, gaining <a id="_idIndexMarker194"/>immense popularity. Leading the pack is the <em class="italic">Meta Quest</em> series. These devices house all necessary computing components within the headsets themselves, making them autonomous units. Most modern standalone headsets are driven by <em class="italic">Qualcomm Snapdragon</em> chips, offering robust processing capabilities.</p>
<p>Standalone VR headsets champion portability and freedom of movement, eliminating the need to tether to a separate computing device. These all-in-one systems offer a cost-effective alternative to PC-based VR. Moreover, many standalone headsets, such as those in the <em class="italic">Meta Quest</em> series, support a PC VR mode, providing the flexibility to take on more computation-intensive applications such as <em class="italic">Half-Life Alyx</em> when desired.</p>
<p>One drawback, however, is the relatively short battery life of standalone headsets, usually topping out around two hours. Nevertheless, remedies exist, such as purchasing comfortable head straps equipped with additional battery life extenders. In most scenarios, we endorse the more affordable standalone headsets such as the <em class="italic">Meta Quest</em> series over their PC-based counterparts. For academic and business environments, standalone headsets are an efficient choice. A simple solution, such as purchasing multiple standalone headsets or a set of battery-enhancing head straps, can still offer substantial savings over traditional PC VR solutions. Standalone VR headsets also shine when used at events or conferences to showcase products or research, offering a hassle-free and portable solution.</p>
<h3>Console-based VR headsets</h3>
<p><strong class="bold">Console-based VR headsets</strong>, such<a id="_idIndexMarker195"/> as the <em class="italic">PlayStation VR 2</em>, cater mainly to the gaming community. They draw on the<a id="_idIndexMarker196"/> processing power of the gaming console, delivering high-quality VR experiences without the need for a separate PC. However, the cost of owning a gaming console isn’t trivial. Moreover, like their PC-based counterparts, console-based VR headsets sacrifice portability due to their tethered nature. The quality of VR experiences can also be limited by the console’s capabilities. In summary, unless your primary <a id="_idIndexMarker197"/>focus is gaming, console-based VR headsets may not be the best fit for creating VR <a id="_idIndexMarker198"/>experiences, whether in a personal, business, or academic setting. Even within gaming circles, there may be superior options to consider.</p>
<h3>Smartphone-based VR headsets</h3>
<p><strong class="bold">Smartphone-based VR headsets</strong>, exemplified by <em class="italic">Google Cardboard</em> and <em class="italic">Samsung Gear VR</em>, harness <a id="_idIndexMarker199"/>the display and computing power of smartphones, which are devices most people <a id="_idIndexMarker200"/>already own. These headsets offer a cost-effective and highly portable solution. However, due to their limited computational and graphical prowess, the immersive experience they provide is inherently diminished. Furthermore, their interaction capabilities within the VR environment are curtailed due to a more restrictive range of sensors compared to other VR headset types. Thus, we seldom recommend them. Even for preliminary testing of VR experiences in the absence of a VR headset, tools such as Unity’s XR Device Simulator offer a free, convenient, and even superior solution.</p>
<h3>Cloud-based VR headsets</h3>
<p>A growing chorus within the <a id="_idIndexMarker201"/>VR community is<a id="_idIndexMarker202"/> advocating the development of <strong class="bold">cloud-based VR headsets</strong>. This emergent technology capitalizes on robust servers to shoulder computational tasks, streaming the results to the VR headset via the internet. This arrangement could potentially deliver high-resolution VR experiences independent of local computational prowess, further enhancing immersion and user satisfaction.</p>
<p>A cloud-based approach could make VR headsets lighter and more user friendly, inviting a broader audience to invest in their own VR hardware. Another appealing aspect is that the hardware housed in the data center that powers the VR headset can be periodically upgraded by the provider, saving users from frequently investing in new equipment.</p>
<p>So, why isn’t this computational approach ubiquitous in contemporary VR headsets? The primary reason is the necessity of a stable, high-bandwidth, low-latency internet connection – a prerequisite that isn’t universally available. Furthermore, there are potential privacy and security concerns associated with transmitting sensitive data over the internet.</p>
<p>As technology evolves and these challenges are addressed, we<a id="_idIndexMarker203"/> may see more adoption of cloud-based <a id="_idIndexMarker204"/>VR systems. However, as of now, their use remains largely experimental and not yet mainstream.</p>
<p>Having journeyed through this exploration of the distinct features of VR headsets and the nuances of VR development, you are now well-prepared to embark on the creation of your inaugural VR scene in Unity. This milestone will allow you to play around with some of the engaging toolkits that Unity provides specifically for VR, including the highly versatile XR Interaction Toolkit.</p>
<h1 id="_idParaDest-58">Setting up a VR project in Unity and the XR Interaction Toolkit</h1>
<p>In the following sections, you will acquire the knowledge to create a Unity project specifically tailored for VR development. Furthermore, you will become proficient in the installation and configuration of two highly significant plugins that are pivotal to VR development within Unity: <strong class="bold">XR Plug-in Management</strong> and the XR Interaction Toolkit.</p>
<h2 id="_idParaDest-59">Creating a project in Unity for VR development</h2>
<p>Let’s <a id="_idIndexMarker205"/>begin by creating a brand-new project in the latest version of Unity. To ensure compatibility with different devices and platforms, it is crucial to enable both <code>Installs</code> folder, clicking on the <strong class="bold">Settings</strong> icon of your Unity version, selecting <strong class="bold">Add Modules</strong>, and installing the missing build supports.</p>
<p>To bring a VR scene to life, we must first navigate to the <strong class="bold">Projects</strong> section in Unity and create a fresh project using the latest Unity version. While Unity does provide a VR template among the various project types to choose from, I recommend selecting <strong class="bold">3D URP </strong><strong class="bold">template</strong> instead.</p>
<p>The<a id="_idIndexMarker206"/> rationale behind this recommendation is that Unity’s VR template does not include the XR Interaction Toolkit, which would lead to extensive setting modifications if you were to choose the VR template. As such, creating a 3D URP project is a more streamlined and trouble-free approach.</p>
<p>In the next section, you will learn how to install XR Plug-in Management and additional XR Plug-ins for PC-based and standalone VR headsets.</p>
<h2 id="_idParaDest-60">Installing XR Plug-in Management and XR Plug-ins</h2>
<p>Now, let’s <a id="_idIndexMarker207"/>head over to <strong class="bold">Edit</strong> | <strong class="bold">Project Settings</strong> | <strong class="bold">XR Plug-in Management</strong>. From here, proceed by clicking on <strong class="bold">Install XR Plug-in Management</strong>. Think of XR Plug-in Management as the bridge between Unity and the different VR, MR, and AR systems you want to use for your scene. It acts like a middleman, allowing Unity to communicate and work effectively with these devices. Upon successful installation of XR Plug-in Management, you can navigate to <strong class="bold">Edit</strong> | <strong class="bold">Project Settings</strong> | <strong class="bold">XR Plug-in Management</strong> to select various plugin providers that will enable your VR application to run in PC VR mode on your VR headset. The following is a brief overview of some of the options:</p>
<ul>
<li><strong class="bold">Unity Mock HMD</strong>: Ideal<a id="_idIndexMarker208"/> for game developers<a id="_idIndexMarker209"/> who may not have direct access to a VR headset. Unity’s Mock HMD mimics the functionalities of a VR headset, providing a platform to design, develop, and test your game within Unity’s environment without requiring physical hardware.</li>
<li><strong class="bold">Oculus integration package</strong>: A<a id="_idIndexMarker210"/> specialized <a id="_idIndexMarker211"/>Unity development package catering to Oculus devices. This includes platform-specific features, Avatar and LipSync SDKs, spatial audio, the Guardian system, and support for hand tracking. However, this package might restrict the portability of your project to non-Oculus platforms.</li>
<li><strong class="bold">OpenXR</strong>: A<a id="_idIndexMarker212"/> remarkable open standard developed by the <em class="italic">Khronos Group</em> that offers developers the freedom to cater to a broad array of XR <a id="_idIndexMarker213"/>devices with the same input, thereby eliminating fragmentation. This obviates the need for separate Oculus or SteamVR plugins, simplifying maintenance by allowing a single code base to operate on any XR system.</li>
</ul>
<p>If you’re an<a id="_idIndexMarker214"/> ambitious developer aiming to create a VR experience that appeals to a wide audience, irrespective of their VR hardware, OpenXR would be your go-to. For this reason, we’ll be utilizing the OpenXR plugin throughout this book.</p>
<p>To get started, enable the <strong class="bold">OpenXR</strong> checkboxes on both the <strong class="bold">PC</strong> and <strong class="bold">Android</strong> tabs.</p>
<p>This action<a id="_idIndexMarker215"/> triggers the download of OpenXR into your Unity project. Once the download is complete, a prompt requesting permission to restart the project will appear. Click <strong class="bold">Yes</strong>, as a restart is necessary to transition from Unity’s old input mode to the new one. After Unity restarts, you’ll find that the <strong class="bold">OpenXR</strong> option is now activated in XR Plug-in Management. However, an interaction profile still needs to be added, as indicated by the warning symbol next to it, as shown in <em class="italic">Figure 3</em><em class="italic">.1</em>.</p>
<div><div><img alt="Figure 3.1 – ﻿How the OpenXR plugin is selected for the PC tab in XR Plug-in Management" src="img/B20869_03_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – How the OpenXR plugin is selected for the PC tab in XR Plug-in Management</p>
<p>Navigate to <strong class="bold">XR Plug-in Management</strong> | <strong class="bold">OpenXR</strong> and click on the <strong class="bold">+</strong> icon in the <strong class="bold">Interaction Profiles</strong> section. Here, you<a id="_idIndexMarker216"/> can incorporate the interaction profiles of all headsets you aim to support. For instance, if you’re using a VR headset from the <em class="italic">Meta Quest</em> series, you should opt for the <strong class="bold">Oculus Touch Controller Profile</strong> option, as shown in <em class="italic">Figure 3</em><em class="italic">.2</em>.</p>
<div><div><img alt="Figure 3.2 – ﻿The OpenXR tab with the Oculus Touch Controller Profile selected as an interaction profile" src="img/B20869_03_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2 – The OpenXR tab with the Oculus Touch Controller Profile selected as an interaction profile</p>
<p>OpenXR’s <strong class="bold">interaction profiles</strong> are<a id="_idIndexMarker217"/> standardized sets of user inputs for XR devices, ensuring consistency and compatibility across various platforms. They allow developers to program once while<a id="_idIndexMarker218"/> catering to multiple devices, making the development process more efficient. For instance, two VR controllers from different manufacturers might have different button layouts, but through OpenXR, they can be mapped to a standard set of interactions. This means that if an XR developer programs an action such as <em class="italic">grab</em> or <em class="italic">jump</em>, it will work irrespective of the exact controller used as long as the appropriate interaction profile is implemented. By adding only a few interaction profiles, developers limit their application’s compatibility to a specific set of devices, potentially excluding some users. Conversely, incorporating as many interaction profiles as possible broadens the application’s reach, ensuring that it can be used seamlessly across a wide array of XR devices.</p>
<p>Make sure to <a id="_idIndexMarker219"/>add interaction profiles for both <strong class="bold">PC</strong> and <strong class="bold">Android</strong>. If you can’t locate the <strong class="bold">Android</strong> tab, this implies that you missed downloading the module during the initial Unity setup. In such a case, you must return to Unity Hub and add the module to your installed Unity version.</p>
<p>You might be wondering why we must configure the same setting twice, once under the <strong class="bold">Windows</strong> tab and then under the <strong class="bold">Android</strong> tab. The reason is that the <strong class="bold">Windows</strong> tab enables VR for PC VR, while the <strong class="bold">Android</strong> tab permits the addition of plug-ins from different providers to run your VR application on your VR headset in standalone mode.</p>
<p>Next, let’s discuss <strong class="bold">Render Mode</strong> for both the <strong class="bold">Windows</strong> and <strong class="bold">Android</strong> tabs. The default rendering<a id="_idIndexMarker220"/> mode is <strong class="bold">single pass instanced</strong> (<strong class="bold">SPI</strong>), but there’s also an option<a id="_idIndexMarker221"/> for the <strong class="bold">multi pass</strong> (<strong class="bold">MP</strong>) rendering mode. Both are tailored for VR environments, each with its own set of benefits and limitations.</p>
<p>SPI offers considerable performance advantages, most notably by reducing the number of draw calls by half. This is crucial for standalone VR headsets, where optimization is paramount to ensure the application runs smoothly. Maintaining a consistent frame rate and smoothness is vital in VR to avoid motion sickness. Additionally, SPI guarantees visual consistency as both eyes are rendered from the same instance, reducing the chances of visual discrepancies that can lead to discomfort. However, the catch is that SPI often requires shader modifications for accurate geometry processing, which can introduce another layer of development complexity.</p>
<p>On the other hand, MP boasts greater shader compatibility since it doesn’t demand specific shader modifications. This can be beneficial if your scene uses a variety of shaders. Each eye is rendered independently in MP, which can sometimes lead to superior visual quality by reducing visual artifacts. However, this comes with a performance trade-off: MP doubles the draw calls, which might overwhelm lower-end devices. Furthermore, rendering each eye separately might produce slight visual differences that can be jarring in VR.</p>
<p>As for standalone VR development, SPI is often the preferred choice due to its optimization advantages. It’s crucial for delivering visually compelling experiences that also perform well on standalone VR headsets. That said, recent Unity versions tend to favor MP in desktop mode for editor testing.</p>
<p>Having made our project VR-ready, we can now proceed to install the XR Interaction Toolkit and its demo scene.</p>
<h2 id="_idParaDest-61">Installing the XR Interaction Toolkit and samples</h2>
<p>By installing XR Plug-in Management, we have enabled Unity to now communicate with our VR headset. But how can we enable Unity to receive information from our headset? And how can our application in Unity translate inputs it gets from the VR headsets into actions in the game? This is where the XR Interaction Toolkit comes into play. To download the<a id="_idIndexMarker222"/> XR Interaction Toolkit, do the following:</p>
<ol>
<li>Navigate to <strong class="bold">Window</strong> | <strong class="bold">Package Manager</strong>. In <strong class="bold">Package Manager</strong>, you can see all the packages you have currently imported, such as the <strong class="bold">OpenXR</strong> plugin.</li>
<li>Download the XR Interaction Toolkit by clicking the <code>com.unity.xr.interaction.toolkit</code> as the <code>2.5.1</code> as the <strong class="bold">Version</strong>.</li>
</ol>
<p class="callout-heading">Important note</p>
<p class="callout">Adding a specific version number is optional when installing packages. By default, if you don’t specify a version, the Package Manager will automatically install the latest version of the XR Interaction Toolkit. While staying updated with the latest versions is generally advisable for new projects, for the purpose of this book, we strongly recommend sticking to version <em class="italic">2.5.1</em> of the toolkit. Using this specific version ensures consistency and ease of understanding as you follow along with our tutorials. Even though newer versions might retain a similar structure, the additional features and elements introduced could make navigating our tutorials more challenging. So, to ensure a smooth learning experience, please use version <em class="italic">2.5.1</em> while working through this book.</p>
<ol>
<li value="3">Hit <strong class="bold">Enter</strong> and the XR Interaction Toolkit will be automatically downloaded into your project. If you see a pop-up window with a warning asking you to restart the <a id="_idIndexMarker223"/>Unity Editor, click on <strong class="bold">Yes</strong>. This just means that the XR Interaction Toolkit is using a new input system to validate interactions.</li>
<li>Staying in the <code>Samples</code> folder to expand a list of useful add-ons to the XR Interaction Toolkit and import <strong class="bold">Starter Assets</strong>. This is a collection of tools that make it easier to set up behaviors. It includes a pre-made set of actions you can use as inputs, as well as presets specifically designed for XR Interaction Toolkit behaviors that rely on the input system.</li>
</ol>
<p>After closing <code>XR Interaction Toolkit</code> folder is the actual XR Interaction Toolkit, while the <code>Samples</code> folder includes the sample add-ons we downloaded. First, let’s open the <strong class="bold">Demo Scene</strong> asset that comes with <strong class="bold">Starter Assets</strong>. To do this, navigate to <strong class="bold">Samples</strong> | <strong class="bold">XR Interaction Toolkit</strong> | <strong class="bold">2.5.1</strong> | <strong class="bold">Starter Assets</strong> and double-click on <strong class="bold">Demo Scene</strong>.</p>
<p>In the next section, we will explore the different components of the <strong class="bold">Demo </strong><strong class="bold">Scene</strong> asset.</p>
<h1 id="_idParaDest-62">Exploring the XR Interaction Toolkit demo scene</h1>
<p>In this<a id="_idIndexMarker224"/> section, we will delve into the <strong class="bold">Demo Scene</strong> asset of the XR Interaction Toolkit. This scene offers a comprehensive overview of all the crucial concepts you need to grasp to develop your own immersive VR experiences. If you’ve followed the steps in the previous section, you should now be presented with the scene pictured in <em class="italic">Figure 3</em><em class="italic">.3</em>.</p>
<div><div><img alt="Figure 3.3 – ﻿The ﻿Scene view ﻿and the Scene ﻿Hierarchy window of the XR Interaction Toolkit’s demo scene" src="img/B20869_03_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3 – The Scene view and the Scene Hierarchy window of the XR Interaction Toolkit’s demo scene</p>
<p>Beyond <a id="_idIndexMarker225"/>the <code>Climb Sample</code> via the project window’s search bar. Dragging and dropping this prefab from the project window into the scene hierarchy window will present the identical ladder and climbing wall in your new scene, combined with the associated scripts and functionalities. Similarly, the <strong class="bold">XR Interaction Setup</strong> prefab, the most important prefab of the XR Interaction Toolkit, will be a staple of many of the forthcoming XR projects discussed in this book. You will learn more about it in the next section.</p>
<h2 id="_idParaDest-63">Examining the pre-configured player</h2>
<p>While <a id="_idIndexMarker226"/>the <strong class="bold">Teleportation Environment</strong>, <strong class="bold">Interactables Sample</strong>, <strong class="bold">UI Sample</strong>, and <strong class="bold">Climb Sample</strong> prefabs all provide an easy way for you to incorporate different types of interactions into your VR scenes, the <strong class="bold">XR Interaction Setup</strong> prefab is the entity that interacts with these interactable components. Essentially, it’s a pre-configured player, designed for seamless navigation and interaction within the scene. You can integrate it into any of your future VR scenes in which you import the XR Interaction Toolkit. Let’s begin to explore it.</p>
<p>By clicking <a id="_idIndexMarker227"/>on the arrow next to the <strong class="bold">XR Interaction Setup</strong> prefab in the scene hierarchy window, you can see its child components, which are <strong class="bold">Input Action Manager</strong>, <strong class="bold">Interaction Manager</strong>, <strong class="bold">Event System</strong>, and <strong class="bold">XR Origin (XR Rig)</strong>. By opening up all of their child components as well, you can examine the structure of this prefab, as shown in <em class="italic">Figure 3</em><em class="italic">.4</em>.</p>
<div><div><img alt="Figure 3.4 – ﻿The components of the complete XR Interaction Setup prefab" src="img/B20869_03_04.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.4 – The components of the complete XR Interaction Setup prefab</p>
<p>To understand these components and how they interact with each other, let’s imagine a VR game scenario where the player takes on the role of a chef in a restaurant and uses a VR headset and controllers to interact with the kitchen environment. In this imaginary example, the components of the <strong class="bold">XR Interaction Setup</strong> prefab would take on the following roles:</p>
<ul>
<li><strong class="bold">Input Action Manager</strong>: This component manages the processing of inputs from the VR controllers. For instance, a controller button press could initiate actions such as gripping a pan or picking up an ingredient.</li>
<li><strong class="bold">XR Interaction Manager</strong>: This component governs interactions between the player and virtual objects. For instance, it manages the outcomes when a player’s VR<a id="_idIndexMarker228"/> hand interacts with a virtual button – perhaps a door opens or a light switches on. Similarly, if the player, holding a pan, moves their hand over the stove and releases the grip button, the pan could settle onto the stove.</li>
<li><strong class="bold">EventSystem</strong>: This component oversees the triggering and processing of events within the game. For example, upon the successful preparation and serving of a dish, an event could be triggered  perhaps the sound of applause or a visual congratulatory message illuminating the screen.</li>
<li><strong class="bold">XR Origin (XR Rig)</strong>: This component serves as the player’s anchor point in the virtual world, responsible for managing the user’s movement and positioning within the XR environment. By utilizing data from the VR device, <strong class="bold">XR Origin </strong>updates the position and orientation of the camera and controllers via its attached <strong class="bold">XR Origin</strong> script, thus creating the illusion of movement and interaction within the virtual environment. In our game context, <strong class="bold">XR Origin</strong> would represent the player’s location in the virtual kitchen; if the player advances a step, their avatar would correspondingly move a step closer to the stove.</li>
<li><strong class="bold">Camera Offset</strong> with <strong class="bold">Main Camera</strong>: This component tracks the player’s head movements. For instance, if the player turns their head toward a recipe book lying on a shelf, the game’s viewpoint adjusts accordingly to focus on the book.</li>
<li><strong class="bold">Left Controller</strong> and <strong class="bold">Right Controller</strong>: These components monitor the player’s hand movements, which typically correspond with the position of the VR controllers. If the player extends their hand to grab a virtual pan using the controller, the VR hands within the scene would mimic this action. This is why the hands contain child objects such as <strong class="bold">Poke Interactor</strong> for touch, <strong class="bold">Direct Interactor</strong> for grabbing, <strong class="bold">Ray Interactor</strong> for pointing, and <strong class="bold">Teleport Interactor</strong> for swift movement.</li>
<li><strong class="bold">Gaze Interactor</strong>: This component facilitates interactions based on the player’s eye or head gaze direction. For instance, if the player focuses their gaze on a particular ingredient, a<a id="_idIndexMarker229"/> tooltip might appear to deliver more information about it. As not every VR headset supports eye tracking, <strong class="bold">Gaze Interactor</strong> also supports head tracking.</li>
<li><strong class="bold">Locomotion System</strong>: As its name already suggests, this GameObject with its associated <strong class="bold">Locomotion System</strong> script is responsible for equipping the player with different forms of movement such as turning, walking, teleporting, or climbing. Let’s have a look at each of its children:<ul><li><strong class="bold">Turn</strong>: The <strong class="bold">Turn</strong> GameObject has two important scripts attached to it, which you can see by selecting <strong class="bold">Turn</strong> in the scene hierarchy window and navigating to its inspector window. First, there is the <strong class="bold">Snap Turn Provider</strong> script. This script enables the player to rotate their view in the VR environment in discrete steps, or “snaps,” using their VR controllers. This can be useful in cases where physically turning isn’t convenient or possible.</li><li>Second, there is the <strong class="bold">Continuous Turn Provider</strong> script. Allowing for smooth, continuous turning in the VR environment using the VR controllers, this script offers an alternative to the step-by-step turning provided by the <strong class="bold">Snap Turn </strong><strong class="bold">Provider</strong> script.</li><li><strong class="bold">Move</strong>: In our VR restaurant game, for example, the player can move through the kitchen by navigating with the controllers of their VR headset. Even though they remain stationary in the physical world, their perspective shifts in the virtual space, giving the illusion of traversing the environment as if they were genuinely walking through the virtual kitchen. This functionality comes from the <strong class="bold">Dynamic Move Provider</strong> script, which is attached to the <strong class="bold">Move</strong> GameObject.</li><li><strong class="bold">Grab Move</strong>: This component of the XR Interaction Toolkit’s <strong class="bold">XR Interaction Setup</strong> prefab facilitates intuitive movement within the VR space. With two instances of the <strong class="bold">Grab Move Provider</strong> script being attached to this GameObject, the player can simulate the sensation of grabbing the virtual world with either hand. As the player moves the controller, the VR origin adjusts inversely, keeping the controller’s position consistent relative to the virtual environment. Complementing this, the <strong class="bold">Two-Handed Grab Move Provider</strong> script empowers players to manipulate their position using both controllers, similar to grabbing two bars and pulling<a id="_idIndexMarker230"/> or pushing oneself. This dual-hand interaction allows players to not just move through the VR restaurant game, but also to adjust their orientation and scale within the scene.</li><li><strong class="bold">Teleportation</strong>: Solely navigating the virtual world via continuous movement, as provided by the <strong class="bold">Move</strong> GameObject, can make some VR users motion sick, even if they regularly immerse themselves in VR. This is why most VR applications mainly work with teleportation, which enables the player to instantly appear in a new spot within the virtual space by moving the joysticks of the VR controllers in the desired direction. As they do so, a visual cue, often a circled overlay, indicates the target destination. Once the joystick is released, the player is directly teleported to that location. The <strong class="bold">Teleportation</strong> GameObject of the XR Interaction Toolkit offers the player of our imaginary VR restaurant game a rich variety of ways to teleport around the kitchen or dining area. For example, the player might only be able to navigate to distinct places in the VR restaurant, making the gameplay more straightforward and structured, which is ideal for educational settings or applications tailored to newbies to VR. If exploration and spontaneity are the core missions of the VR restaurant game, enabling the player to teleport to any part of the floor that is not occupied by chairs, tables, or kitchen cabinets might be more favorable.</li></ul></li>
</ul>
<p>Given the importance of teleportation in VR applications, we’ll delve deeper into which teleportation features the XR Interaction Toolkit offers in the upcoming section.</p>
<h2 id="_idParaDest-64">Teleporting</h2>
<p>The <strong class="bold">Teleportation Environment</strong> prefab<a id="_idIndexMarker231"/> of the <strong class="bold">Demo Scene</strong> asset consists of a <strong class="bold">Teleport Area</strong> prefab and four <strong class="bold">Teleportation Anchor</strong> prefabs. You can see these components in <em class="italic">Figure 3</em><em class="italic">.5</em>.</p>
<div><div><img alt="Figure 3.5 – ﻿The Teleportation Area and four Teleportation Anchors, which are both part of the Teleportation Environment prefab" src="img/B20869_03_05.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.5 – The Teleportation Area and four Teleportation Anchors, which are both part of the Teleportation Environment prefab</p>
<p>The <strong class="bold">Teleport Area</strong> script, equipped with a <strong class="bold">Teleportation Area</strong> prefab and a child cube, represents a space available for teleportation. Users can teleport to any pointed spot within this designated area, effectively teleporting within the bounds of the cube.</p>
<p>By contrast, a <strong class="bold">Teleport Anchor</strong> prefab serves as a specific teleportation point, allowing users to navigate to exact locations with ease.</p>
<p>Imagine a VR game where users must traverse a river by hopping from one stone lying in the water to another. On the other side of the river, there is a forest with gold hidden underneath some trees that they must find. In this case, <strong class="bold">Teleport Anchors</strong> could be used for each of the stones lying in the water. This would allow the users to traverse the river with ease and focus more on the actual gameplay. For the other side of the river, however, a <strong class="bold">Teleport Area</strong> script <a id="_idIndexMarker232"/>would be better suited, allowing users to explore the area as they wish.</p>
<p>In the next section, you will learn all about grabbable objects and their movement types.</p>
<h2 id="_idParaDest-65">Exploring grabbable objects</h2>
<p>The <strong class="bold">Interactables Sample</strong> prefab, another part of the <strong class="bold">Demo Scene</strong> asset, contains three <a id="_idIndexMarker233"/>3D figures that highlight the capabilities of the <strong class="bold">XR Grab Interactable</strong> script, an essential tool within the XR Interaction Toolkit. This script will likely be a staple of your future VR projects whenever you want an object to be grabbable. Once you’ve integrated the XR Interaction Toolkit and configured XR Plug-in Management in any of your VR endeavors, making an object grabbable is a breeze. Simply highlight the object in the scene hierarchy window, click on the <strong class="bold">Add Component</strong> option in the Inspector, and then search for and choose the <strong class="bold">XR Grab Interactable</strong> script. Additionally, ensure the <strong class="bold">XR Interaction Setup</strong> prefab is in your scene, as it provides the required player for the interaction.</p>
<p>This single step of adding the <strong class="bold">XR Grab Interactable</strong> script to an object transforms it into a VR-interactable item. This ease of use and efficiency is a significant reason behind the XR Interaction Toolkit’s popularity among VR developers. Instead of manually coding numerous lines to achieve this functionality, you can focus your time on refining the user experience by tweaking the integrated physics settings. These adjustments can significantly enhance the immersion and realism of your VR application.</p>
<p>Now, let’s delve into the various physics configurations offered by the <strong class="bold">XR Grab Interactable</strong> script to enhance the object-grabbing experience. Each of the three figures of the <strong class="bold">Interactables Sample</strong> prefab showcases a different <strong class="bold">Movement Type</strong> setting of the <strong class="bold">XR Grab Interactable</strong> script, as you can see in <em class="italic">Figure 3</em><em class="italic">.6</em>.</p>
<div><div><img alt="Figure 3.6 – ﻿The demo scene’s Interactables Sample prefab in the scene view" src="img/B20869_03_06.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.6 – The demo scene’s Interactables Sample prefab in the scene view</p>
<p>Let’s go through<a id="_idIndexMarker234"/> the figures one by one:</p>
<ul>
<li><strong class="bold">Interactable Kinematic Torus</strong>: When you select this in the scene hierarchy window and inspect its properties, you will see that its <strong class="bold">Movement Type</strong> setting is labeled as <strong class="bold">Kinematic</strong>. A kinematic object is guided by the software’s calculations rather than by external forces. This means it moves smoothly based on the game engine’s instructions and isn’t affected by gravity or collisions like a regular object would be. This type of movement can be useful in applications such as puzzle games, where objects must fit into specific locations without bouncing or rolling away. It ensures that once placed, the objects remain static.</li>
<li><strong class="bold">Interactable Instant Pyramid</strong>: This figure has <strong class="bold">Instantaneous</strong> selected as its <strong class="bold">Movement Type</strong> setting. Instantaneous movement is precisely as it sounds: immediate. Instead of creating a sensation of weight or inertia as you move it, this object will respond and relocate right away without any noticeable lag or drift. It’s like moving a cursor on a computer screen; where you point, it goes. This is ideal for <strong class="bold">user interface</strong> (<strong class="bold">UI</strong>) interactions in VR, such as selecting menu options or dragging and dropping virtual files. It’s also useful in time-sensitive scenarios, such as a fast-paced game where players must rapidly move objects to different locations without the delay of simulated physical interaction.</li>
<li><strong class="bold">Interactable Velocity Tracked Wedge</strong>: This figure’s <strong class="bold">Movement Type</strong> setting is set to <strong class="bold">Velocity Tracking</strong>. Here, the movement relies on the speed and direction of your hand motion. Think of it like tossing a ball; the speed and angle of <a id="_idIndexMarker235"/>your throw will determine how the ball flies. In the VR space, this object’s motion mirrors the speed and trajectory of your gesture. This movement type is perfect for sports simulations such as VR baseball or basketball, where the speed and direction of the player’s hand play a role in the performance of the in-game action.</li>
</ul>
<p>You can observe these three movement types firsthand by running the demo scene on your VR headset or via the XR Device Simulator and navigating to the <strong class="bold">Grab Interactable </strong><strong class="bold">Objects</strong> booth.</p>
<p>The next section introduces you to the different types of UI elements provided by the XR Interaction Toolkit.</p>
<h2 id="_idParaDest-66">Inspecting different types of UI elements</h2>
<p>The <strong class="bold">UI Sample</strong> prefab <a id="_idIndexMarker236"/>in the <strong class="bold">Demo Scene</strong> asset showcases the different types of UI elements currently provided by the XR Interaction Toolkit. You can see them in <em class="italic">Figure 3</em><em class="italic">.7</em>.</p>
<div><div><img alt="Figure 3.7 – ﻿The demo scene’s UI Sample prefab in the scene view" src="img/B20869_03_07.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.7 – The demo scene’s UI Sample prefab in the scene view</p>
<p>Click on<a id="_idIndexMarker237"/> the arrow next to the prefab in the scene hierarchy window to inspect its child components. Let’s go through them:</p>
<ul>
<li><strong class="bold">ModalSingleButton</strong>: This UI element presents a short text message accompanied by a button. This layout makes it an ideal UI prefab whenever you need to display important pop-up notifications to the user of your VR experience. For instance, it could serve to inform users at the outset of a VR session that they’re about to embark on a timed experience. Alternatively, it can ensure that users have understood instructions or confirm their consent for data usage in a VR research study.</li>
<li><strong class="bold">Interactive Controls</strong>: This prefab contains multiple UI elements with more detailed input choices for users compared to the <strong class="bold">ModalSingleButton</strong> prefab. <strong class="bold">MinMaxSlider</strong>, for instance, lets the user pinpoint a specific state within a minimum-to-maximum range by sliding over it with the VR controller. The <strong class="bold">Dropdown</strong> prefab, <strong class="bold">Text Toggle</strong> prefab, and <strong class="bold">Icon Toggle</strong> prefab grant similarly precise ways for users to communicate their preferences in the VR world. These components of the <strong class="bold">Interactive Controls</strong> prefab shine in contexts requiring intricate user feedback or directives. For instance, the user might select the room they want to see in a VR apartment viewing experience via a <strong class="bold">Dropdown</strong> menu. They could adjust the room lighting intensity with <strong class="bold">MinMaxSlider</strong>, and toggle between floor materials and wall paintings using <strong class="bold">Text Toggle</strong> and <strong class="bold">Icon Toggle</strong>.</li>
<li><strong class="bold">Scroll UI Sample</strong>: This prefab features a simple text block that users can scroll through using their VR controllers. This element is a handy tool for educational segments within VR, such as diving deep into the history of an art piece in a virtual museum tour.</li>
</ul>
<p>By <a id="_idIndexMarker238"/>right-clicking on the UI sample prefab in the scene hierarchy window and selecting one of the options offered under UI, you can easily add more UI elements onto the <strong class="bold">Canvas</strong> component as you please.</p>
<p>The <strong class="bold">TrackedDeviceGraphicRaycaster</strong> script is another essential component of all UI interactions in VR scenes with the XR Interaction Toolkit. It should be attached to the <strong class="bold">Canvas</strong> component showcasing the UI elements. In the case of the <strong class="bold">Demo Scene</strong> asset, the <strong class="bold">UI Sample</strong> prefab is the <strong class="bold">Canvas</strong> component, so the script is attached to it, as you can also see in its inspector window. The <strong class="bold">TrackedDeviceGraphicRaycaster</strong> script gives your <strong class="bold">Canvas</strong> component an X-ray vision to discern when a player’s controller is aiming at an element on the <strong class="bold">Canvas</strong> component. Within the VR context, it transforms controller movements into interactions with the UI elements displayed on the <strong class="bold">Canvas</strong> component.</p>
<p>In the next section, you will learn how you can interact with buttons in your VR scene.</p>
<h2 id="_idParaDest-67">Interacting via buttons</h2>
<p>The <strong class="bold">Poke Interactions Sample</strong> GameObject within our <strong class="bold">Demo Scene</strong> asset features three<a id="_idIndexMarker239"/> unique buttons that are shown in <em class="italic">Figure 3</em><em class="italic">.8</em>.</p>
<div><div><img alt="Figure 3.8 – ﻿Three unique buttons from the Poke Interaction Sample of the demo scene" src="img/B20869_03_08.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.8 – Three unique buttons from the Poke Interaction Sample of the demo scene</p>
<p>Unlike <a id="_idIndexMarker240"/>ordinary buttons, these are designed to react to your touch in distinct ways. The button on the left side triggers a particle animation once it is poked; the button in the middle plays a sound, and the button on the right side increments the number on the UI element that is attached to it every time it is pressed.</p>
<p>These buttons all have the same three scripts attached to them: <strong class="bold">XR Simple Interactable</strong>, <strong class="bold">XR Poke Filter</strong>, and <strong class="bold">XR Poke Interactor</strong>. These scripts provide important functionalities that all of these buttons need, despite their distinct flavors:</p>
<ul>
<li><strong class="bold">XR Simple Interactable</strong>: This script is the magic that makes our buttons interactive and touch-responsive. Without it, they would simply be static objects in the virtual world, unreactive to our touch. In the scene hierarchy window, select the first <strong class="bold">Push</strong> button and navigate to the inspector window. Now, click on the arrow next to the <strong class="bold">Interactable Events</strong> menu of the <strong class="bold">XR Simple Interactable</strong> script to see all interactable events associated with this script. Scroll down to the <strong class="bold">Select</strong> section and observe how the <strong class="bold">Particle System</strong> event is played or stopped as the user enters or exits the selection of this button. Repeat this step for the other two buttons to see what their <strong class="bold">Select</strong> sections look like. In the upcoming chapters of this book, you will create your own unique game logics in the <strong class="bold">Interactable </strong><strong class="bold">Events</strong> section.</li>
<li><strong class="bold">XR Poke Filter</strong>: This script assesses the authenticity of a touch. If the touch doesn’t meet its criteria, the poke filter blocks its entry.</li>
<li><strong class="bold">XR Poke Follow Affordance</strong>: This script provides the buttons with a lifelike, tactile response. Upon poking, the respective button is pressed down, mirroring the<a id="_idIndexMarker241"/> movement of a real-world button. Upon release, it rebounds back to its original position.</li>
</ul>
<p>Throughout this book, we’ll delve deeper into creating diverse poke interactions, teaching you to customize these components to fulfill your specific requirements.</p>
<p>On our last step of exploring the <strong class="bold">Demo Scene</strong> asset, you’ll get a glimpse of more advanced features of the XR Interaction Toolkit.</p>
<h2 id="_idParaDest-68">Exploring gaze interactions and climbing</h2>
<p><strong class="bold">Gaze interaction</strong> is an<a id="_idIndexMarker242"/> immersive feature in VR that enables the user to trigger events and control aspects of the<a id="_idIndexMarker243"/> VR environment simply by directing their eye or head gaze. The demo scene of the XR Interaction Toolkit contains an entire booth just to showcase different types of gaze-based interactions. These types of interactions are among the most advanced features of the XR Interaction Toolkit, which is why you will learn how to implement them into your VR scenes in <a href="B20869_08.xhtml#_idTextAnchor026"><em class="italic">Chapter 8</em></a>, alongside other advanced techniques.</p>
<p>Climbing, while a demanding concept in real life, is easier to enable in VR scenes than one might think, especially when compared to techniques such as gaze tracking. The <strong class="bold">Demo Scene</strong> asset’s <strong class="bold">Climb Sample</strong> prefab showcases two climbable objects: a ladder and a climbing wall. You can observe them in <em class="italic">Figure 3</em><em class="italic">.9</em>.</p>
<div><div><img alt="Figure 3.9 – ﻿The Climb Sample prefab, consisting of a ladder and a climbing wall among other elements" src="img/B20869_03_09.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.9 – The Climb Sample prefab, consisting of a ladder and a climbing wall among other elements</p>
<p>At first glance, they<a id="_idIndexMarker244"/> may seem distinct, but underneath, they share the same scripts.</p>
<p>To understand <a id="_idIndexMarker245"/>how you can make an object in your VR scene climbable, follow these steps:</p>
<ol>
<li>Open the <strong class="bold">Climb Sample</strong> prefab in the scene hierarchy window.</li>
<li>Explore every child component of both the <strong class="bold">Ladder</strong> and the <strong class="bold">Climbing Wall</strong> prefab that has the <em class="italic">Climbable</em> label. In the inspector window, you’ll notice each of these climbable components has the same two scripts from the XR Interaction Toolkit attached to them: the <strong class="bold">Climb Interactable</strong> script and the <strong class="bold">XR Interactable Affordance State </strong><strong class="bold">Provider</strong> script.</li>
</ol>
<p>Simply adding these two scripts to any object in your VR environment via the inspector window provides <a id="_idIndexMarker246"/>it with climbable properties. Now, let’s dive deeper into the roles of these two scripts:</p>
<ul>
<li><strong class="bold">Climb Interactable</strong>: This script essentially makes an object climbable. When a player interacts with an object that has this script, they can virtually climb it.</li>
<li><strong class="bold">XR Interactable Affordance State Provider</strong>: This script aids in fine-tuning the interaction between the player and climbable objects. It’s a system that dictates how the climbing experience should function and respond to the user’s interactions.</li>
</ul>
<p>Now that we’ve explored all the key components of the demo scene, it’s time for you to test and explore it. In the upcoming section, we’ll <a id="_idTextAnchor010"/>discuss several options to get you started on this exciting journey.</p>
<h1 id="_idParaDest-69">Deploying and testing VR experiences onto different VR platforms or simulators</h1>
<p>Creating<a id="_idIndexMarker247"/> a VR scene goes beyond just design; it involves testing and deployment as well. While testing focuses on ensuring the VR scene works correctly and offers a good user experience, deployment is about transferring and optimizing the scene for playback on specific VR headsets, such as the <em class="italic">Meta Quest</em> series, <em class="italic">HTC Vive</em>, or <em class="italic">Valve Index</em>. Early development stages might use tools such as the XR Device Simulator for rapid testing, but as a project nears completion, thorough testing on an actual VR headset becomes essential.</p>
<p>Now, let’s go through the various options for testing and deploying your VR scene based on what hardware is available to you:</p>
<ul>
<li><em class="italic">No VR headset available</em>: In cases where a VR headset is unavailable, you can use the XR Interaction Toolkit’s XR Device Simulator to test the features of your VR scene. Even if you do have a VR headset at your disposal, the simulator can offer a quicker alternative to physically connecting and testing on a VR headset, particularly when you’re making simple modifications.</li>
<li><em class="italic">Standalone VR headset</em>: Most standalone VR headsets support PC-based VR mode. This mode, when enabled, is often the most efficient and powerful way to test your VR scene. However, there might be times when you don’t have the necessary cable to activate PC-based VR mode. In such cases, you have two main alternatives for testing your VR scene. The XR Device Simulator is typically the preferred method, because testing directly on a standalone VR headset requires deploying it onto the device. This process can be lengthy and time-consuming. It happens on the Android platform via the project’s <em class="italic">Build Settings</em>. So, if PC-based VR mode isn’t an option, you might use the XR Device Simulator for quick tweaks and minor scene changes. For substantial modifications or a comprehensive user experience test, deploying directly to the headset would be advisable.</li>
<li><em class="italic">PC-based VR headset or Standalone VR headset with PC-based VR mode enabled</em>: If you have either a PC-based VR headset or a standalone VR headset with PC-based VR mode enabled, your options for testing and deployment are expanded. For minor modifications to your VR scene, the XR Device Simulator might be all you need. For moderate to major changes, it is easy to test the VR scene on your headset by pressing the <strong class="bold">Play</strong> button in your Unity scene, provided the VR headset is connected to either the <em class="italic">SteamVR</em> or <em class="italic">Oculus</em> software. For considerable changes, or when analyzing the overall user experience, you can deploy your VR scene through <strong class="bold">Windows/Mac/Linux</strong> in <strong class="bold">Build Settings</strong> while the VR headset is connected to the <em class="italic">SteamVR</em> or <em class="italic">Oculus</em> software.</li>
</ul>
<p>You might be <a id="_idIndexMarker248"/>curious about what <em class="italic">SteamVR</em> or <em class="italic">Oculus</em> software entails and when you’ll need them. Simply put, both are software platforms that not only let you play PC VR games but also test and deploy VR experiences on your headset. You will learn more about both in the upcoming sections of this chapter. In general, we would recommend the following, depending on the type of headset you own:</p>
<ul>
<li><em class="italic">Headset from the Meta Quest series</em>: Install both SteamVR and Oculus software on your computer and learn to deploy your VR scene to your VR headset through both VR platforms, as these headsets support both. Both types of deployment are very easy to do, but they enable you not only to test your scene more thoroughly, but also to easily reach a wider audience.</li>
<li><em class="italic">Valve Index, HTC Vive series, or any other non-Meta headset</em>: Only install SteamVR and <a id="_idIndexMarker249"/>learn to deploy your VR scene onto your VR headset.</li>
</ul>
<p>The following sections will delve deeper into each of these options, providing a detailed guide on installing the XR Device Simulator, SteamVR, and Oculus software. Feel free to skip to the sections that are relevant to you.</p>
<h2 id="_idParaDest-70">Installing the XR Device Simulator</h2>
<p>Now, we’ll learn <a id="_idIndexMarker250"/>about the essentials of installing and using the XR Device Simulator, a valuable tool in the VR development workflow that lets you emulate user inputs to control XR devices in a simulated environment. From importing the toolkit to using it effectively in your Unity projects, we’ll detail every aspect using comprehensive examples.</p>
<p>As it is part of the XR Interaction Toolkit, you can easily import the XR Device Simulator via Unity’s package manager. Here is a step-by-step guide to doing this:</p>
<ol>
<li>Inside your Unity project, navigate to <strong class="bold">Windows</strong> | <strong class="bold">Package Manager</strong> | <strong class="bold">XR </strong><strong class="bold">Interaction Toolkit</strong>.</li>
<li>By selecting the <strong class="bold">Samples</strong> tab of the <strong class="bold">XR Interaction Toolkit</strong> package, you will find <strong class="bold">XR Device Simulator</strong> among the listed elements.</li>
<li>By clicking the <strong class="bold">Import</strong> button next to the XR Device Simulator, you import it into your project.</li>
</ol>
<p>After successfully installing the XR Device Simulator, let’s use it to test out the <strong class="bold">Demo Scene</strong> asset of the XR Interaction Toolkit.</p>
<h2 id="_idParaDest-71">Using the XR Device Simulator</h2>
<p>To<a id="_idIndexMarker251"/> incorporate the XR Device Simulator into the demo scene or any of your future VR scenes, search for <code>XR Device Simulator</code> via the project window’s search bar. This way, you will find the <strong class="bold">XR Device Simulator</strong> prefab, the asset you will need to add to scenes where you want to simulate XR input. Now, simply drag this prefab into the scene hierarchy window of your VR scene. Before continuing, make sure the <strong class="bold">XR Interaction Setup</strong> prefab is also added to your scene. Now, click on the <strong class="bold">Play</strong> button to explore the power of the XR Device Simulator. <em class="italic">Figure 3</em><em class="italic">.10</em> shows <a id="_idIndexMarker252"/>what the XR Device Simulator looks like in the <strong class="bold">Play</strong> mode.</p>
<div><div><img alt="Figure 3.10 – ﻿The Play mode of the XR Interaction’s Toolkit demo scene when the XR Device Simulator is added to the scene" src="img/B20869_03_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.10 – The Play mode of the XR Interaction’s Toolkit demo scene when the XR Device Simulator is added to the scene</p>
<p>You are now equipped to navigate your scene using the key bindings supplied by the simulator, specifically the <em class="italic">WASD</em> keys. Hit the <em class="italic">Tab</em> key to switch active control between the left controller, right controller, and VR headset. Utilizing your mouse, you can explore the scene or alter the angles of your controllers, contingent on whether you are operating in headset or controller mode. Pressing the <em class="italic">G</em> key enables you to interact with objects in the scene, including pressing buttons. After experimenting with the XR Device Simulator and familiarizing yourself with the different keys, you’ll swiftly appreciate its simplicity and effectiveness as a testing tool for your VR scene before moving on to real XR hardware deployment.</p>
<p>As powerful and convenient as the XR Device Simulator might be, there is no experience more immersive than testing and deploying your scene onto a VR headset. The following section explains how to do so using SteamVR.</p>
<h2 id="_idParaDest-72">Setting up SteamVR</h2>
<p>SteamVR is <a id="_idIndexMarker253"/>a platform developed by <em class="italic">Valve Corporation</em> for VR applications and content. It is part of the <a id="_idIndexMarker254"/>larger <strong class="bold">Steam</strong> platform, a very popular online marketplace for games and software, known for its extensive library and active community. For VR developers, using SteamVR can thus be very advantageous as VR content in SteamVR can potentially reach millions of users worldwide. SteamVR supports a wide range of VR headsets, including those from <em class="italic">HTC Vive</em>, <em class="italic">Valve Index</em>, <em class="italic">Windows Mixed Reality</em>, and the <em class="italic">Meta Quest</em> series. At the time of writing the book, we are not aware of any commercially available VR headset that doesn’t support SteamVR. It is basically a default standard in the VR headset market.</p>
<p>Interestingly, even<a id="_idIndexMarker255"/> the <em class="italic">Meta Quest</em> series, designed on Android and not primarily PC-based VR, allows developers to deploy VR scenes and applications not just on Meta’s proprietary Oculus platform, but also on the SteamVR platform. This versatility is immensely beneficial. If you’re a <em class="italic">Meta Quest</em> series headset owner, we highly recommend learning to test and deploy your scenes on both platforms. Let’s illustrate this with an example.</p>
<p>Suppose you’re creating a VR game requiring intricate hand interactions such as object manipulation and complex maneuvers. You aim to cater to users of <em class="italic">HTC Vive</em>, <em class="italic">Valve Index</em>, and headsets from the <em class="italic">Meta Quest</em> series. By harnessing SteamVR and its SteamVR Unity plugin, you can develop the game once and ensure compatibility across all these headsets. SteamVR’s precise tracking system enhances the game’s immersive and responsive nature across various devices. Post-development, you can launch your game on the Steam platform, thus accessing a vast user base, including users of the <em class="italic">Meta Quest</em> series who are employing SteamVR.</p>
<p>In the following section, you will learn how to install SteamVR.</p>
<h3>Installing SteamVR</h3>
<p>To <a id="_idIndexMarker256"/>target SteamVR, you must first install and setup SteamVR. To do this, perform the following steps:</p>
<ol>
<li>Download Steam to your PC from the official website (<a href="https://store.steampowered.com/about/">https://store.steampowered.com/about/</a>), create<a id="_idIndexMarker257"/> a Steam account, and log into this Steam account via the Steam software.</li>
<li>Inside Steam, search for <code>SteamVR</code> in the Steam Store, select the <strong class="bold">Play Game</strong> button, and follow through with the on-screen instructions to begin installing SteamVR.</li>
<li>Once the<a id="_idIndexMarker258"/> installation of SteamVR is completed, connect your headset to your PC via a cable or a wireless connection such as <em class="italic">Air Link</em> for the <em class="italic">Meta Quest</em> series. Then, either search for the SteamVR plugin inside the Steam software and click the <code>SteamVR</code> in your regular PC apps and clicking on it. Either way, you should quickly see that your VR headset is connected to SteamVR on your PC. Inside your VR headset, you should now see the SteamVR environment and game store.</li>
</ol>
<p>While some VR headsets, such as <em class="italic">Valve Index</em>, only necessitate the installation of SteamVR software for testing and deploying VR experiences, many other headset manufacturers demand additional software to be installed alongside SteamVR. For instance, <em class="italic">Windows Mixed Reality</em> devices, including the <em class="italic">HP Reverb</em> series, also require the <em class="italic">Mixed Reality Portal</em>. These installations are typically straightforward and usually don’t involve any further steps from you other than opening the downloaded software every time you want to connect your VR headset to the PC. As these software requirements can change over time, we recommend you check whether any manufacturer-specific software is needed for your particular VR headset model to test or deploy on it.</p>
<p>After installing and setting up SteamVR and potentially some other manufacturer-specific software, the next section reveals how you can finally test and deploy your scene onto your VR headset.</p>
<h3>Testing and deploying your VR scene with SteamVR</h3>
<p>When<a id="_idIndexMarker259"/> developing for <a id="_idIndexMarker260"/>SteamVR in Unity, you can test your VR scene directly in the Editor using the <strong class="bold">Play</strong> button. With the SteamVR plugin installed, the Unity Editor will recognize a connected SteamVR-compatible headset, and the scene will be rendered in VR. This enables you to quickly evaluate changes, troubleshoot issues, and refine your VR content without leaving the Unity environment.</p>
<p>When you’re ready to <a id="_idIndexMarker261"/>deploy your scene, you can build the project into an executable file that can be run outside of Unity.</p>
<p>After<a id="_idIndexMarker262"/> connecting <a id="_idIndexMarker263"/>your VR headset to SteamVR, go back into your Unity project. Here, go to <strong class="bold">File</strong> | <strong class="bold">Build Settings</strong> in your Unity project. Make sure that your preferred <strong class="bold">Target Platform</strong> field, such as Windows, is selected and that the <strong class="bold">Architecture</strong> field is set to 64-bit. Finally, you can deploy your VR scene onto your headset by clicking the <strong class="bold">Build and Run</strong> button in <strong class="bold">Build Settings</strong>. Once built, the project can be launched in SteamVR through the Steam client, allowing you to experience the VR scene on your headset as end users would.</p>
<p>If you own a VR headset from the <em class="italic">Meta Quest</em> series, setting up your VR scene for Oculus will be just as important to you as setting it up for SteamVR. The next section explains how to do this.</p>
<h2 id="_idParaDest-73">Setting up Oculus</h2>
<p>In this<a id="_idIndexMarker264"/> section, we’ll explore the steps for setting up Oculus for your VR development projects. We’ll detail the installation of the Oculus app on your PC, how to test your VR scenes with Oculus, and how to deploy your VR scenes with Oculus.</p>
<h3>Installing Oculus</h3>
<p>To work <a id="_idIndexMarker265"/>with Oculus in Unity, you’ll first need to download and install the Oculus app onto your PC. Complete the following steps:</p>
<ol>
<li>Navigate to <a href="http://www.oculus.com/setup">www.oculus.com/setup</a> in<a id="_idIndexMarker266"/> your web browser.</li>
<li>Below the VR headset of your choice, such as <em class="italic">Meta Quest Pro</em>, click <strong class="bold">Download Software</strong>.</li>
<li>Open the downloaded Oculus app and click the <strong class="bold">Install </strong><strong class="bold">Now</strong> button.</li>
<li>Follow the on-screen instructions to create an Oculus account and set up your VR headset.</li>
</ol>
<p>After the<a id="_idIndexMarker267"/> installation and setup are completed, it is finally time to test and deploy your VR scene with Oculus. This process is described in the next section.</p>
<h3>Testing and deploying your VR scene with Oculus</h3>
<p>Once the <a id="_idIndexMarker268"/>Oculus software is installed on your PC, you can <a id="_idIndexMarker269"/>test your VR scenes directly in the Unity Editor. The following steps show you how:</p>
<ol>
<li>Connect your Oculus-compatible VR headset to your PC. Ensure the Oculus app recognizes it.</li>
<li>Open your VR project in Unity.</li>
<li>Click the <strong class="bold">Play</strong> button in the Unity Editor to start the scene. Unity will automatically render the scene in VR, allowing you to interact with the scene using your Oculus headset.</li>
</ol>
<p>After testing and refining your VR scene, you can build and deploy it so it can be experienced outside of Unity. Complete the following steps:</p>
<ol>
<li>With your VR project open in Unity, go to <strong class="bold">File</strong> | <strong class="bold">Build Settings</strong>.</li>
<li>Make sure your preferred <strong class="bold">Target Platform</strong> field, such as Windows, is selected.</li>
<li>Ensure the <strong class="bold">Architecture</strong> is set to 64-bit.</li>
</ol>
<p>Click the <strong class="bold">Build and Run</strong> button. Unity will build an executable file of your project and run it. After the <a id="_idIndexMarker270"/>build is complete, the executable can be launched <a id="_idIndexMarker271"/>with the Oculus software. Now, you can fully experience your VR scene with your Oculus headset, just as your end users will.</p>
<h1 id="_idParaDest-74">Summary</h1>
<p>Throughout this chapter, you’ve navigated the exciting landscape of VR development in Unity. You’ve learned to create a fundamental VR scene from scratch and mastered the deployment process on various devices. The utilization of the XR Interaction Toolkit and the informative elements from its demo scene should now be within your repertoire, empowering you to generate basic VR scenes with increased confidence and proficiency.</p>
<p>Furthermore, these acquired skills aren’t limited to a specific genre. Whether it be for industrial applications or academic research, your new proficiency in VR development should allow you to replicate this process effectively to fit any use case that you encounter.</p>
<p>As we venture into the next chapter, we will broaden our horizon to include the creation and deployment of AR scenes within Unity.</p>
</div>
</body></html>