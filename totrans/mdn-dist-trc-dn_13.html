<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-207"><a id="_idTextAnchor206"/>13</h1>
<h1 id="_idParaDest-208"><a id="_idTextAnchor207"/>Driving Change</h1>
<p>Throughout the book, we have talked about the technical side of observability and discussed how to trace calls, record metrics, report events, or use auto-collected telemetry provided by platforms and libraries. Here, we’re going to talk about the organizational aspects of implementing observability solutions.</p>
<p>First, we’ll look into the benefits of and reasons for changing your existing solution and discuss associated costs. Then, we’ll go through the implementation stages and come up with a brief. Finally, we’ll see how to leverage observability to drive and improve the development process.</p>
<p>in this chapter, you’ll learn how to do the following:</p>
<ul>
<li>Decide whether you need a better observability solution and which level is right for you</li>
<li>Develop an onboarding plan and start implementing it</li>
<li>Use observability to help with daily development tasks</li>
</ul>
<p>By the end of this chapter, you should be ready to propose an observability solution and onboarding plan for your organization.</p>
<h1 id="_idParaDest-209"><a id="_idTextAnchor208"/>Understanding the importance of observability</h1>
<p>If you’re reading this book, you’re probably<a id="_idIndexMarker669"/> at least entertaining the idea of improving the observability story of your application. Maybe it’s hard to understand how customers use your system or it takes a lot of time to understand what exactly went wrong when someone reports an issue. In the worst case, it takes a lot of time to just notice that the system is unhealthy and users are affected. Or, maybe you want to minimize such risks in your future projects.</p>
<p>In any case, these pain points brought you here and they should guide you further to find the right observability level and approach for your system.</p>
<p>Even if we clearly see the problem and how it can be solved with better observability, we usually still need to get other people working on the system onboard with this vision. Astoundingly, they might have quite different feelings about the same problems and might not consider them worthy of solving.</p>
<p>Let me share a few common points I have heard arguing that a problem is not important:</p>
<ul>
<li>When a customer reports an issue, we can ask for a timestamp and find operations at that time by customer identifier. Then we can find any suspicious logs, get the request ID, and then find correlated logs on other services.</li>
<li>When we see an issue in production, we open related dashboards and start visually correlating metrics until we can guess what’s broken and then mitigate it. We have experts and an excellent set of runbooks for typical issues.</li>
<li>We can do a user study or customer research to get extensive information on how people use the system.</li>
<li>We ask customers to enable verbose logs and reproduce the problem, then send us logs that we’ll parse based on our expert knowledge of the system.</li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">Each of these approaches is totally valid. They <em class="italic">already</em> solve the problem, your team <em class="italic">already</em> knows how to use them, and some of them are still necessary and quite useful even with perfect observability solutions in place.</p>
<p>So, essentially, when we consider<a id="_idIndexMarker670"/> the approach to observability, we need to break the status quo and convince ourselves and our organization that it’s worth it. To achieve this, we first need to clearly outline the pain points and understand the cost of keeping things as they are.</p>
<h2 id="_idParaDest-210"><a id="_idTextAnchor209"/>The cost of insufficient observability</h2>
<p>Your<a id="_idIndexMarker671"/> organization<a id="_idIndexMarker672"/> might already<a id="_idIndexMarker673"/> have some common<a id="_idIndexMarker674"/> incident metrics<a id="_idIndexMarker675"/> in place that we can rely on, such as <strong class="bold">MTTM</strong> (<strong class="bold">mean time to mitigate</strong>), <strong class="bold">MTTR</strong> (<strong class="bold">mean time to recover</strong>), <strong class="bold">MTBF</strong> (<strong class="bold">mean time between failures</strong>), or others. They are somewhat subjective and depend on what qualifies as an incident, or what recovery means, but roughly show how fast we can investigate incidents and how frequently they happen.</p>
<p>If incidents take a lot of time to resolve and happen frequently, it’s likely that our organization cares deeply about them and would be interested in improving the situation.</p>
<p>Ironically, we need at least some level of observability to notice there is an incident and to measure how long it takes to resolve. If we don’t have even this in place, we can start to manually track when things get broken and how long it takes us to discover and resolve issues. However subjective it is, it’s better than nothing.</p>
<p>Some things rarely appear in such metrics directly: how bad is your on-call experience? How much time does onboarding take before someone can be on-call independently? How many issues end up closed with something such as “cannot reproduce,” “not enough information,” “probably a network or hardware error”; get lost in ping-pong between teams; or get moved to backlogs and never resolved?</p>
<p>It should be feasible<a id="_idIndexMarker676"/> to measure some of such things. For example, we can label issues<a id="_idIndexMarker677"/> that can’t be investigated further due to a lack of telemetry. If they represent a significant portion of your bugs, it’s something worth improving.</p>
<p>As a team, you can also do an experiment for a week or two to roughly measure the time spent investigating issues. How much time does it take to investigate when there is enough data? Or, how much time is wasted investigating issues and meeting a dead end due to a lack of telemetry or finding a trivial transient network issue?</p>
<p>By minimizing the time necessary to find the root cause of an issue, we improve the user experience. We notice incidents earlier and resolve them faster. We also improve our work-life balance and focus on creative work instead of grepping megabytes of logs.</p>
<p class="callout-heading">Note</p>
<p class="callout">There could be other data, such as business analytics, support stats, public reviews, or anything else, showing that a noticeable number of users are leaving us because of unresolved technical issues. If you need to convince your organization to invest in observability, finding such data and showing how a better observability story can improve things could be a good way to approach it.</p>
<p>So, the first step is to understand whether current tools and processes are effective and have a rough understanding of how better<a id="_idIndexMarker678"/> observability could improve things. The next step is to understand<a id="_idIndexMarker679"/> the cost of the solution.</p>
<h2 id="_idParaDest-211"><a id="_idTextAnchor210"/>The cost of an observability solution</h2>
<p>We can roughly break down<a id="_idIndexMarker680"/> the costs into two groups: implementation<a id="_idIndexMarker681"/> and telemetry backend costs.</p>
<p>We need to add instrumentation, tune and customize telemetry collection, learn how to use new tools, create alerts and dashboards, and build new processes around them. When onboarding a mature and stable system, we should also consider risks – we might break something and temporarily make it less reliable.</p>
<p>As we discussed in <a href="B19423_09.xhtml#_idTextAnchor148"><em class="italic">Chapter 9</em></a>, <em class="italic">Best Practices</em>, we can always choose the level of detail and amount of customization to help us keep the costs within the given budget.</p>
<p>The minimalistic approach would be to start with network-level auto-instrumentation for actively developed services and then add context, customizations, and manual instrumentation as we go.</p>
<p>By using OpenTelemetry and shared instrumentation libraries, we can also rely on vendors to provide common visualizations, alerts, dashboards, queries, and analysis for typical technologies. As a result, it’s almost free to get started.</p>
<h3>Telemetry backend</h3>
<p>We can host the observability<a id="_idIndexMarker682"/> stack ourselves or use one of the available platforms. Either way, there will be recurring costs associated with using the solution.</p>
<p>These costs depend on telemetry volume, retention period, the number of services and instances, and many other factors, including the support plan.</p>
<p>Throughout the book, we have discussed how to optimize telemetry collection while keeping the system observable enough for our needs: traces can be sampled, metrics should have low cardinality, and events and logs can be sampled too or kept in cold but indexed storage.</p>
<p>It’s a good idea to try a few different backends – luckily, many platforms have a free tier or trial period and, most importantly, you can instrument the system once with OpenTelemetry and pump data into multiple backends to compare the experience and get an idea of what the cost of using them would look like. Once you start relying on a specific backend for alerts or daily tasks, it will be more difficult to switch between vendors.</p>
<p>During this experiment, you will also get a better understanding of the necessary data retention period, sampling rate, and other parameters, and will be able to pick them along with the vendor.</p>
<p class="callout-heading">Note</p>
<p class="callout">When running a modern cloud application under scale, it’s not possible to operate it without an observability solution, so it’s not a question of whether you need one but rather how many details you need to collect and which observability vendor out there works best for your system and budget.</p>
<p>Essentially, we can start small and incrementally tune collection to add or remove details, while keeping it within a reasonable budget.</p>
<p>We should also define<a id="_idIndexMarker683"/> what success means – it could be an MTTR improvement, subjective user experience, on-call engineer happiness, anything else that matters for your organization, or any combination of these.</p>
<p>Let’s now talk more about the implementation details and try to make this journey less painful.</p>
<h1 id="_idParaDest-212"><a id="_idTextAnchor211"/>The onboarding process</h1>
<p>The need for distributed tracing<a id="_idIndexMarker684"/> and visibility of all parts of the system comes from the complexity of modern applications. For example, we need to know how a serverless environment interacts with cloud storage to debug configuration issues or optimize performance. Or, maybe we want to know why certain requests fail in the downstream service without asking someone to help.</p>
<p>To make the most of distributed tracing, we have to onboard the whole system (or at least a significant part of it), making sure all services create correlated and coherent telemetry, write it to a place where different teams can access it, and reuse the same tooling to do analysis.</p>
<p>So, implementing an observability<a id="_idIndexMarker685"/> solution is an organization-wide effort, and it makes sense to start with a pilot project instrumenting a small part of the system. Let’s outline its scope and goals.</p>
<h2 id="_idParaDest-213"><a id="_idTextAnchor212"/>The pilot phase</h2>
<p>The goal of this project<a id="_idIndexMarker686"/> is to get hands-on experience<a id="_idIndexMarker687"/> with observability, discover any significant technical or process issues early, and better understand the scope and effort needed for the rest of the system.</p>
<p>We’ll need a few (at least two) services<a id="_idIndexMarker688"/> to start instrumenting:</p>
<ul>
<li>That are in active development</li>
<li>That interact with each other</li>
<li>That have no (or few) internal dependencies except on each other</li>
<li>That have teams working on them in close contact</li>
</ul>
<p>From the technical side, we’ll use this phase to make the following decisions:</p>
<ul>
<li><strong class="bold">Instrumentation SDKs</strong>: I hope I convinced you to use .NET platform<a id="_idIndexMarker689"/> capabilities and OpenTelemetry, but you probably need to decide what to do with existing instrumentation code and tooling.</li>
<li><strong class="bold">Context propagation standards</strong>: Using W3C Trace Context would be a good start. We may also need to decide whether and how to propagate baggage, or how to pass context over non-HTTP/proprietary protocols.</li>
<li><strong class="bold">Sampling strategy</strong>: Rate-based, percentage-based, parent-based, tail-based – these are good things<a id="_idIndexMarker690"/> to decide early on and identify whether you need an OpenTelemetry collector or can rely on an observability vendor.</li>
<li><strong class="bold">Which vendor to use and a migration plan from your current one</strong>: We’ll discuss technical aspects and trade-offs when instrumenting existing systems in <a href="B19423_15.xhtml#_idTextAnchor233"><em class="italic">Chapter 15</em></a>, <em class="italic">Instrumenting </em><em class="italic">Brownfield Applications</em>.</li>
</ul>
<p>By the end of the pilot phase, we should have a clear understanding of what onboarding takes, what the challenges are, and how we will solve them.</p>
<p>We will also have<a id="_idIndexMarker691"/> a small part of the system instrumented – it’s a good time to check whether we see any improvement.</p>
<h3>Tracking progress</h3>
<p>In the perfect world, after instrumentation<a id="_idIndexMarker692"/> is deployed, we’d be able to resolve all incidents in no time and investigate all the tricky bugs we’ve hunted down for months. I wish that was the case.</p>
<p>There are at least several challenges along the way:</p>
<ul>
<li><strong class="bold">Change is hard</strong>: People prefer to use tools they know well, especially when they’re dealing with incidents in production. It would be a good exercise to do the same investigation with the new observability solution after the incident is resolved and compare the experiences.</li>
</ul>
<p>It’s best to start playing with new tools at development time or when investigating low-priority failures. In any case, it takes time and practice to learn about and trust new tools.</p>
<ul>
<li><strong class="bold">You’ll discover new issues</strong>: Looking at traces or a service map for the first time, I always learn something new about my code. It’s common to discover calls to external systems you didn’t expect (for example, auth calls made under the hood), unnecessary network calls, wrong retry logic, calls that should run in parallel but run sequentially, and so on.</li>
<li><strong class="bold">Basic auto-instrumentation is not sufficient</strong>: Without application context or manual instrumentation for certain libraries and scenarios, our ability to find, understand, and aggregate related telemetry is limited.</li>
</ul>
<p>It will take a few iterations to see an improvement – make sure to collect feedback and understand what’s working and what’s not.</p>
<p>It also takes time and dedication<a id="_idIndexMarker693"/>. Demos, success stories, shared case studies, and documentation on how to get started should create awareness and help people get curious and learn faster.</p>
<h3>Iterating</h3>
<p>So, after the initial instrumentation, we’re not quite<a id="_idIndexMarker694"/> ready to roll it out to the rest of the system. Here’re a few things to do first:</p>
<ul>
<li><strong class="bold">Tune instrumentation libraries</strong>: Remove verbose and noisy signals or enable useful attributes that are off by default. If some parts of your stack don’t have auto-instrumentation available, start writing your own.</li>
<li><strong class="bold">Add essential application context</strong>: Finding common properties and standardizing attribute names or baggage keys across your organization will have a huge impact down the road. We’ll talk more about it in <a href="B19423_14.xhtml#_idTextAnchor220"><em class="italic">Chapter 14</em></a>, <em class="italic">Creating Your </em><em class="italic">Own Conventions</em>.</li>
<li><strong class="bold">Start building backend-specific tooling</strong>: Alerts, dashboards, and workbooks will help us validate whether we have enough context and telemetry to run our system and migrate to the new solution.</li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">By the end of this stage, you should be able to see positive outcomes. It probably will not yet have moved the needle for the whole system, and there might not be enough data for the services involved in the experiment, but you should at least see some success stories and be able to show examples of where the new solution shone.</p>
<p>If you see cases where the new observability story should have helped but has not, it is a good idea to investigate why and tune instrumentation further. While iterating, it’s also worth paying attention to backend costs and optimizing telemetry collection if you see the potential for a significant reduction without noticeable impact.</p>
<p>The goal here is to create a good enough instrumentation approach and any necessary tooling around it. We iterate fast and keep the number of participating services small so we can still change direction and make breaking changes.</p>
<p>Once we have finalized all the decisions<a id="_idIndexMarker695"/> and implemented and validated them on a small part of the system, we should be able to rely on new observability solutions for most of the monitoring and debugging needs. Before we roll them out, we still need to document them and create reusable artifacts.</p>
<h3>Documenting and standardizing</h3>
<p>The main outcome<a id="_idIndexMarker696"/> of the pilot phase<a id="_idIndexMarker697"/> is clarity on how to make the rest of the system observable and the specific benefits it will bring.</p>
<p>To maximize the impact of this phase, we need to make it easier for other services to be onboarded. We can help them by doing the following:</p>
<ul>
<li>Documenting new solutions and processes</li>
<li>Providing demos and starters showing how to use backends and configure them, and add alerts and dashboards</li>
<li>Producing common artifacts that include any customizations:<ul><li>Context propagators, samplers, or instrumentations</li><li>Attribute names or helpers that efficiently populate them</li><li>Starter packs that bring all OpenTelemetry dependencies and enable telemetry collection in a uniform manner</li><li>Common configuration options</li></ul></li>
</ul>
<p>Finally, we’re ready to instrument and onboard the rest of the system. It will probably take some time to align attribute names, configuration, or backend plans. We also need to keep tracking progress toward original goals and apply necessary changes when things don’t work. Let’s talk about a few things that can slow us down.</p>
<h2 id="_idParaDest-214"><a id="_idTextAnchor213"/>Avoiding pitfalls</h2>
<p>The challenge with distributed tracing<a id="_idIndexMarker698"/> and observability is that they’re most effective when a distributed application produces coherent signals: trace context is propagated, sampling algorithms are aligned to produce full traces, all signals use the same attribute names, and so on.</p>
<p>While OpenTelemetry solves most of these concerns, it still relies on the application to bring all the pieces together and use coherent signals. It becomes a problem for huge organizations where one service deviating from the standard breaks the correlation for the rest of the system.</p>
<p>Here are a few things to avoid when onboarding your system:</p>
<ul>
<li><strong class="bold">Starting too big</strong>: If multiple teams work on instrumentation independently, they will inevitably develop different solutions optimized for their services. Aligning these solutions after onboarding is finalized would be a difficult project on its own. Each team would have an impression that things work for them, but end-to-end customer issues would still take months to resolve.</li>
<li><strong class="bold">Not sharing telemetry across the system</strong>: When investigating issues or analyzing performance and usage, it’s beneficial to be able to see how other services process requests. Without doing so, we will end up in the same place where each cross-service problem involves some amount of ping-pong and problems are not resolved fast enough.</li>
<li><strong class="bold">Not enforcing standards</strong>: Inconsistent telemetry would make us come back to grepping logs and make customers and ourselves unhappy.</li>
<li><strong class="bold">Not using new tools and capabilities</strong>: We talked about how migrating from familiar tooling is hard. We need to put enough effort into advocating, promoting, explaining, documenting, and improving things to make sure people use them. Sunsetting old tools, once new ones are more capable and fast, is one (sometimes unpopular) way to make sure everyone switches.</li>
<li><strong class="bold">Not using the observability stack at development or test time</strong>: Investigating flaky tests is one of the cheapest ways to debug tricky issues. Traces can help a lot, so make sure tests send traces and logs by default and it’s super easy to enable tracing on dev machines.</li>
<li><strong class="bold">Building things that are hard to use or not reliable</strong>: While some friction is expected, we should make sure most of it happens during the pilot phase. If you decide to build your own observability stack based on OSS solutions, you should expect that certain things such as navigating across tools will be difficult and you’ll need to put a decent amount of effort into making them usable. Another big investment is building and maintaining reliable telemetry pipelines.</li>
</ul>
<p>Hopefully, we can avoid most of these issues and onboard a significant part of the system onto our new observability stack.</p>
<p>As we continue onboarding, we should see improvement toward our initial goals and may need to adjust them. During the process, we probably learned a lot about our system and are now dealing with new challenges we could not see before due to a lack of observability.</p>
<p>The journey does<a id="_idIndexMarker699"/> not end here. In the same way that we never stop writing tests, we should incorporate and leverage observability in day-to-day tasks.</p>
<h1 id="_idParaDest-215"><a id="_idTextAnchor214"/>Continuous observability</h1>
<p>Observability should not be added as an afterthought<a id="_idIndexMarker700"/> when service or feature development is over. When implementing a complex feature across several services or just adding a new external dependency, we can’t rely on users telling us when it’s broken. Tests usually don’t cover every aspect and don’t represent user behavior.</p>
<p>If we don’t have a reliable telemetry signal, we can’t say whether the feature works or whether customers use it.</p>
<h2 id="_idParaDest-216"><a id="_idTextAnchor215"/>Incorporating observability into the design process</h2>
<p>Making sure we have telemetry<a id="_idIndexMarker701"/> in place is part of feature<a id="_idIndexMarker702"/> design work. The main questions the telemetry should answer are the following:</p>
<ul>
<li>Who uses this feature and how much?</li>
<li>Does it work? Does it break something else?</li>
<li>Does it work as expected? Does it improve things as expected?</li>
</ul>
<p>If we can rely on the existing telemetry to answer these questions, awesome! </p>
<p>We should design instrumentation in a way that covers multiple things at once. For example, when we switch from one external HTTP dependency to a new one, we can leverage existing auto-collected traces and metrics. A common processor that stamps application context on all spans will take care of traces from the new dependency as well.</p>
<p>If we use feature flags, we should make sure we record them on telemetry for operations that participate in an experiment. We can record them on events or spans, for example, following OpenTelemetry semantic conventions for feature flags available at <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/trace/semantic_conventions/feature-flags.md">https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/trace/semantic_conventions/feature-flags.md</a>.</p>
<p>In some cases, default telemetry is not sufficient and we need to add custom events, traces, metrics, or at least extra attributes. It’s rarely a good idea to limit instrumentation to a new log record unless we write it in a structured and aggregable way.</p>
<p>Once a feature is proven useful and fully rolled<a id="_idIndexMarker703"/> out, we might want to remove<a id="_idIndexMarker704"/> this additional telemetry along with the feature flag. It’s a great approach if we’re sure it’s not necessary anymore. Cleaning up and iterating on instrumentation is another important aspect.</p>
<h2 id="_idParaDest-217"><a id="_idTextAnchor216"/>Housekeeping</h2>
<p>As with any other code, instrumentation code degrades and becomes less useful when neglected.</p>
<p>Similar to the test code, any observability code<a id="_idIndexMarker705"/> we write is less reliable than application code – it’s also hard to notice when it reports something incorrect as there is no functional issue. So, validating it with testing or manual checks and fixing it in a timely manner is important.</p>
<p>This is one of the reasons to use popular<a id="_idIndexMarker706"/> instrumentation libraries – they have been through excessive testing by other people. Keeping your instrumentation libraries up to date and sharing custom ones across the company (or open sourcing them) will result in better instrumentation quality.</p>
<p>Another important part is to make small improvements as you notice issues: add missing events, spans, and attributes (don’t forget to check if there is a common one), structure and optimize logs, and adjust their verbosity.</p>
<p>These changes might be risky. We might remove something that people rely on for alerting or analysis. There could be some additional guards in place that prevent it – code reviews, documentation, and tests, but it is rarely possible to account for everything, so be cautious when removing or renaming things.</p>
<p>Another risk is adding something expensive or verbose that would either impact application availability, overwhelm telemetry pipelines, or significantly increase your observability bill. Paying attention to the dev and test telemetry and knowing what’s on the hot path should prevent obvious mistakes.</p>
<p>Building reliable telemetry pipelines with rate-limiting should decrease the severity of such incidents when they make it to production.</p>
<p>As you can see, observability code<a id="_idIndexMarker707"/> is not very different from any other piece of infrastructure. Implementing it starts with some research and experiments and works best when we tune and improve it along with our application.</p>
<h1 id="_idParaDest-218"><a id="_idTextAnchor217"/>Summary</h1>
<p>In this chapter, we discussed how to implement and roll out observability solutions in your organization. These efforts can be motivated and justified by the current monitoring infrastructure not being efficient in investigating and resolving customer issues.</p>
<p>We discussed how we can rely on existing metrics or data to understand whether there is room for improvement and estimate the cost of inaction. Then we looked into common costs associated with implementing and running a modern observability solution – the easiest way to find out is to run a small experiment and compare different vendors.</p>
<p>We explored how we can approach onboarding by starting with a pilot project on a small part of the system and iterating and validating results before we roll it out to the rest of the system. Finally, we discussed the importance of incorporating observability into daily tasks and evolving it along with the code.</p>
<p>This chapter should help you justify initial observability investments and gradually implement the solution across the system. In the next chapter, we’ll talk more about unifying telemetry collection and introducing your own standards.</p>
<h1 id="_idParaDest-219"><a id="_idTextAnchor218"/>Questions</h1>
<ol>
<li>Should we look for a single backend for all telemetry signals or a combination of them optimized for individual telemetry signals?</li>
<li>How would you approach standardizing baggage propagation and usage in your system?</li>
<li>You’re adding a cache to the service. When would you add instrumentation? How would you approach it?</li>
</ol>
<h1 id="_idParaDest-220"><a id="_idTextAnchor219"/>Further reading</h1>
<ul>
<li><em class="italic">Becoming a Rockstar SRE</em> by Jeremy Proffitt and Rod Anami</li>
</ul>
</div>
</body></html>