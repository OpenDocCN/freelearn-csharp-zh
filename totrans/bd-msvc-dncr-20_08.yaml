- en: Scaling Microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine you are part of a development and support team that is responsible for
    developing the company's flagship product—TaxCloud. TaxCloud helps taxpayers file
    their own taxes and charges them a small fee upon the successful filing of taxes.
    Consider you had developed this application using microservices. Now, say the
    product gets popular and gains traction, and suddenly, on the last day of tax
    filing, you get a rush of consumers wanting to use your product and file their
    taxes. However, the payment service of your system is slow, which has almost brought
    the system down, and all the new customers are moving to your competitor's product.
    This is a lost opportunity for your business.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even though this is a fictitious scenario, it can very well happen to any business.
    In e-commerce, we have always experienced these kinds of things in real life,
    especially on special occasions such as Christmas and Black Friday. All in all,
    they point toward one major significant characteristic—the scalability of the system.
    Scalability is one of the most important non-functional requirements of any mission-critical
    system. Serving a couple of users with hundreds of transactions is not the same
    as serving millions of users with several million transactions. In this chapter,
    we will discuss scalability in general. We''ll also discuss how to scale microservices
    individually, what to consider when we design them, and how to avoid cascading
    failure using different patterns. By the end of this chapter, you will have learned
    about:'
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vertical scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Scale Cube model of scalability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling infrastructure using Azure scale sets and Docker Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling a service design through data model caching and response caching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The circuit breaker pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service discovery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalability overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Design decisions impact the scalability of a single microservice. As with other
    application capabilities, decisions that are made during the design and early
    coding phases largely influence the scalability of services.
  prefs: []
  type: TYPE_NORMAL
- en: Microservice scalability requires a balanced approach between services and their
    supporting infrastructures. Services and their infrastructures also need to to
    be scaled in harmony.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability is one of the most important non-functional characteristics of a system
    as it can handle more payload. It is often felt that scalability is usually a
    concern for large-scale distributed systems. Performance and scalability are two
    different characteristics of a system. Performance deals with the throughput of
    the system, whereas scalability deals with serving the desired throughput for
    a larger number of users or a larger number of transactions.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microservices are modern applications and usually take advantage of the cloud.
    Therefore, when it comes to scalability, the cloud provides certain advantages.
    However, it is also about automation and managing costs. So even in the cloud,
    we need to understand how to provision infrastructure, such as virtual machines
    or containers, to successfully serve our microservices-based application even
    in the case of sudden traffic spikes.
  prefs: []
  type: TYPE_NORMAL
- en: Now we will visit each component of our infrastructure and see how we can scale
    it. The initial scaling up and scaling out methods are applied more to hardware
    scaling. With the Auto Scaling feature, you will understand Azure virtual manager
    scale sets. Finally, you will learn about scaling with containers in Docker Swarm
    mode.
  prefs: []
  type: TYPE_NORMAL
- en: Vertical scaling (scaling up)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Scaling up** is a term used for achieving scalability by adding more resources
    to the same machine. It includes the addition of more memory or processors with
    higher speed or simply the migration of applications to a more powerful macOS.'
  prefs: []
  type: TYPE_NORMAL
- en: With upgrades in hardware, there is a limit as to how you can scale the machine.
    It is more likely that you are just shifting the bottleneck rather than solving
    the real problem of improving scalability. If you add more processors to the machine,
    you might shift the bottleneck to memory. Processing power does not increase the
    performance of your system linearly. At a certain point, the performance of a
    system stabilizes even if you add more processing capacity. Another aspect of
    scaling up is that since only one machine is serving all the requests, it becomes
    a single point of failure as well.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, scaling vertically is easy since it involves no code changes; however,
    it is quite an expensive technique. Stack Overflow is one of those rare examples
    of a .NET-based system that is scaled vertically.
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal scaling (scaling out)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you do not want to scale vertically, you can always scale your system horizontally.
    Often, it is also referred to as **scaling out**. Google has really made this
    approach quite popular. The Google search engine is running out of inexpensive
    hardware boxes. So, despite being a distributed system, scaling out helped Google
    in its early days expand its search process in a short amount of time while being
    inexpensive. Most of the time, common tasks are assigned to worker machines and
    their output is collected by several machines doing the same task. This kind of
    arrangement also survives through failures. To scale out, load balancing techniques
    are useful. In this arrangement, a load balancer is usually added in front of
    all the clusters of the nodes. So, from a consumer perspective, it does not matter
    which machine/box you are hitting. This makes it easy to add capacity by adding
    more servers. Adding servers to clusters improves scalability linearly.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling out is a successful strategy when the application code does not depend
    on the server it is running on. If the request needs to be executed on a specific
    server, that is, if the application code has server affinity, it will be difficult
    to scale out. However, in the case of stateless code, it is easier to get that
    code executed on any server. Hence, scalability is improved when a stateless code
    is run on horizontally scaled machines or clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the nature of horizontal scaling, it is a commonly used approach across
    the industry. You can see many examples of large scalable systems managed this
    way, for example, Google, Amazon, and Microsoft. We recommend that you scale microservices
    in a horizontal fashion as well.
  prefs: []
  type: TYPE_NORMAL
- en: Microservice scalability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will review the scaling strategies available for microservices.
    We will look at the Scale Cube model of scalability, how to scale the infrastructure
    layer for microservices, and embed scalability in microservice design.
  prefs: []
  type: TYPE_NORMAL
- en: Scale Cube model of scalability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One way to look at scalability is by understanding Scale Cube. In the book *The
    Art of Scalability: Scalable Web Architecture, Processes, and Organizations for
    the Modern Enterprise*, Martin L. Abbott and Michael T. Fisher define Scale Cube
    as viewing and understanding system scalability. Scale Cube applies to microservice architectures
    as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2d5760c4-f65e-462d-8133-ddf0ba025fe9.png)'
  prefs: []
  type: TYPE_IMG
- en: In this three-dimensional model of scalability, the origin (0,0,0) represents
    the least scalable system. It assumes that the system is a monolith deployed on
    a single server instance. As shown, a system can be scaled by putting the right
    amount of effort into three dimensions. To move a system towards the right scalable
    direction, we need the right trade-offs. These trade-offs will help you gain the
    highest scalability for your system. This will help your system cater to increasing
    customer demand. This is signified by the **Scale Cube** model. Let's look into
    every axis of this model and discuss what they signify in terms of microservice
    scalability.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling of x axis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scaling over the *x *axis means running multiple instances of an application
    behind a load balancer. This is a very common approach used in monolithic applications.
    One of the drawbacks of this approach is that any instance of an application can
    utilize all the data available for the application. It also fails to address application
    complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices should not share a global state or a kind of data store that can
    be accessed by all the services. This will create a bottleneck and a single point
    of failure. Hence, approaching microservice scaling merely over the *x *axis of
    Scale Cube would not be the right approach.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's look at *z *axis scaling. We have skipped *y *axis scaling for a reason.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling of z axis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The *z *axis scaling is based on a split, which is based on the customer or
    requestor of a transaction. While *z *axis splits may or may not address the monolithic
    nature of instructions, processes, or code, they very often do address the monolithic
    nature of the data necessary to perform these instructions, processes, or code.
    Naturally, in *z *axis scaling, there is one dedicated component responsible for
    applying the bias factor. The bias factor might be a country, request origin,
    customer segment, or any form of subscription plan associated with the requestor
    or request. Note that *z *axis scaling has many benefits, such as improved isolation
    and caching for requests; however, it also suffers from the following drawbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: It has increased application complexity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It needs a partitioning scheme, which can be tricky especially if we ever need
    to repartition data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It doesn't solve the problems of increasing development and application complexity.
    To solve these problems, we need to apply *y *axis scaling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to the preceding nature of *z *axis scaling, it is not suitable for use
    in the case of microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling of y axis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *y *axis scaling is based on a functional decomposition of an application
    into different components. The *y *axis of Scale Cube represents the separation
    of responsibility by the role or type of data, or work performed by a certain
    component in a transaction. To split the responsibility, we need to split the
    components of the system as per their actions or roles performed. These roles
    might be based on large portions of a transaction or a very small one. Based on
    the size of the roles, we can scale these components. This splitting scheme is
    referred to as *service or resource-oriented splits*.
  prefs: []
  type: TYPE_NORMAL
- en: This very much resembles what we see in microservices. We split the entire application
    based on its roles or actions, and we scale individual microservice as per its
    role in the system. This resemblance is not accidental; it is the product of the
    design. So we can fairly say that *y *axis scaling is quite suitable for microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding *y *axis scaling is very significant for scaling a microservice
    architecture-based system. So, effectively, we are saying that microservices can
    be scaled by splitting them as per their roles and actions. Consider an order
    management system that is designed to, say, meet certain initial customer demand;
    for this, splitting the application into services such as customer service, order
    service, and payment service will work fine. However, if demand increases, you
    would need to review the existing system closely. You might discover the sub-components
    of an already existing service, which can very well be separated again since they
    are performing a very specific role in that service and the application as a whole.
    This revisit to design with respect to increased demand/load may trigger the re-splitting
    of the order service into a quote service, order processing service, order fulfillment
    service, and so on. Now, a quote service might need more computing power, so we
    might push more instances (identical copies behind it) when compared to other
    services.
  prefs: []
  type: TYPE_NORMAL
- en: This is a near real-world example of how we should scale microservices on the
    AFK Scale Cube's three-dimensional model. You can observe this kind of three-dimensional
    scalability and *y *axis scaling of services in some well-known microservice architectures
    that belong to the industry, such as Amazon, Netflix, and Spotify.
  prefs: []
  type: TYPE_NORMAL
- en: Characteristics of a scalable microservice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the Scale Cube section, we largely focused on scaling the characteristics
    of an entire system or application. In this section, we will focus on scaling
    the characteristics of an individual microservice. A microservice is said to be
    scalable and performant when it exhibits the following major characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Known growth curve: For example, in the case of an order management system,
    we need to know how many orders are supported by the current services and how
    they are proportionate to the order fulfillment service metric (measured in *requests
    per seconds*). The currently measured metrics are called **baseline figures**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Well-studied usage metrics: The traffic pattern generally reveals customer
    demand, and based on customer demand, many parameters mentioned in the previous
    sections regarding microservices can be calculated. Hence, microservices are instrumented,
    and monitoring tools are the necessary companions of microservices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Effective use of infrastructure resources: Based on qualitative and quantitative
    parameters, the anticipation of resource utilization can be done. This will help
    the team predict the cost of infrastructure and plan for it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ability to measure, monitor, and increase the capacity using an automated infrastructure:
    Based on the operational and growth pattern of the resource consumption of microservices,
    it is very easy to plan for future capacity. Nowadays, with cloud elasticity,
    it is even more important to be able to plan and automate capacity. Essentially,
    cloud-based architecture is cost-driven architecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Known bottlenecks: Resource requirements include the specific resources (compute,
    memory, storage, and I/O) that each microservice needs. Identifying these are
    essential for a smoother operational and scalable service. If we identify resource
    bottlenecks, they can be worked on and eliminated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Has dependency scaling in the same ratio: This is self-explanatory. However,
    you cannot just focus on a microservice, leaving its dependencies as bottlenecks.
    A microservice is as scalable as its least scaling dependency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fault tolerant and highly available: Failure is inevitable in distributed systems.
    If you encounter a microservice instance failure, it should be automatically rerouted
    to a healthy instance of the microservice. Just putting load balancers in front
    of microservice clusters won't be sufficient in this case. Service discovery tools
    are quite helpful for satisfying this characteristic of scalable microservices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Has a scalable data persistence mechanism: Individual data store choices and
    design should be scalable and fault-tolerant for scalable microservices. Caching
    and separating out read and write storage will help in this case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, while we are discussing microservices and scalability, the natural arrangement
    of scaling comes into the picture, which is nothing but the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scaling the infrastructure: Microservices operate well over dynamic and software-defined
    infrastructure. So, scaling the infrastructure is an essential component of scaling
    microservices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scaling around service design: Microservice design comprises of an HTTP-based
    API as well as a data store in which the local state for the services is stored.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling the infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will visit all the layers of the microservice infrastructure
    and see them in relation to each other, that is, how each individual infrastructure
    layer can be scaled. In our microservice implementation, there are two major components.
    One is virtual machines and the other is the container hosted on the virtual or
    physical machine. The following diagram shows a logical view of the microservice
    infrastructure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/55de4676-81c0-41bd-82d4-1c02be78f4f0.png)'
  prefs: []
  type: TYPE_IMG
- en: Scaling virtual machines using scale sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scaling virtual machines is quite simple and easy in Azure Cloud. This is where
    microservices shine through. With scale sets, you can raise the instances of the
    same virtual machine images in a short amount of time, and automatically too,
    based on the ruleset. Scale sets are integrated with Azure Autoscale.
  prefs: []
  type: TYPE_NORMAL
- en: Azure virtual machines can be created in such a way so that as a group, they
    always serve the requests even if the volume of the requests increases. In specific
    situations, they can also be deleted automatically if those virtual machines are
    not needed to perform the workload. This is taken care of by the virtual machine
    scale set.
  prefs: []
  type: TYPE_NORMAL
- en: Scale sets also integrate well with load balancers in Azure. Since they are
    represented as compute resources, they can be used with Azure's Resource Manager.
    Scale sets can be configured so that virtual machines can be created or deleted
    on demand. This helps manage virtual machines with the mindset of `pets vs. cattle`, which
    we saw earlier in the chapter in terms of deployment.
  prefs: []
  type: TYPE_NORMAL
- en: For applications that need to scale compute resources in and out, scale operations
    are implicitly balanced across the fault and update domains.
  prefs: []
  type: TYPE_NORMAL
- en: With scale sets, you don't need to correlate loops of independent resources,
    such as NICs, storage accounts, and virtual machines. Even while scaling out,
    how are we going to take care of the availability of these virtual managers? All
    such concerns and challenges have already been addressed with virtual machine
    scale sets.
  prefs: []
  type: TYPE_NORMAL
- en: A scale set allows you to automatically grow and shrink an application based
    on demand. Let's say there's a threshold of 40% utilization. So, maybe once we
    reach 40% utilization, we'll begin to experience performance degradation. And
    at 40% utilization, new web servers get added. A scale set allows you to set a
    rule, as mentioned in the previous sections. An input to a scale set is a virtual
    machine. The rules on a scale set say that at 40% average CPU, for five minutes,
    Azure will add another virtual machine to the scale set. After doing this, calibrate
    the rule again. If the performance is still above 40%, add a third virtual machine
    until it reaches the acceptable threshold. Once the performance drops below 40%,
    it will start deleting these virtual machines based on traffic inactivity and
    so on to reduce the cost of operation.
  prefs: []
  type: TYPE_NORMAL
- en: So by implementing a scale set, you can construct a rule for the performance
    and make your application bigger to handle the greater load by simply automatically
    adding and removing virtual machines. You, as the administrator, will be left
    with nothing to do once these rules are established.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Autoscale measures performance and determines when to scale up and down.
    It is also integrated with the load balancer and NAT. Now, the reason they're
    integrated with the load balancer and with NAT is because as we add these additional
    virtual machines, we're going to have a load balancer and a NAT device in front.
    As requests keep coming in, in addition to deploying the virtual machine, we've
    got to add a rule that allows traffic to be redirected to the new instances. The
    great thing about scale sets is that they not only add virtual machines but also
    work with all the other components of the infrastructure, including things such
    as network load balancers.
  prefs: []
  type: TYPE_NORMAL
- en: In the Azure Portal, a scale set can be viewed as a single entry, even though
    it has multiple virtual machines included in it. To look at the configuration
    and specification details of virtual machines in a scale set, you will have to
    use the Azure Resource Explorer tool. It's a web-based tool available at [https://resources.azure.com](https://resources.azure.com).
    Here you can view all the objects in your subscription. You can view scale sets
    in the Microsoft.Compute section.
  prefs: []
  type: TYPE_NORMAL
- en: Building a scale set is very easy using the Azure templates repository. Once
    you create your own **Azure Resource Manager** (**ARM**) template, you can also
    create custom templates based on scale sets. Due to scope and space constraints,
    we have omitted a detailed discussion and instructions on how to build a scale
    set. You can follow these instructions by utilizing the ARM templates given at
    [https://github.com/gbowerman/azure-myriad](https://github.com/gbowerman/azure-myriad).
  prefs: []
  type: TYPE_NORMAL
- en: An availability set is an older technology, and this feature has limited support.
    Microsoft recommends that you migrate to virtual machine scale sets for faster
    and more reliable autoscale support.
  prefs: []
  type: TYPE_NORMAL
- en: Auto Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the help of monitoring solutions, we can measure the performance parameters
    of an infrastructure. This is usually in the form of performance SLAs. Auto Scaling
    gives us the ability to increase or decrease the resources available to the system,
    based on performance thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: The Auto Scaling feature adds additional resources to cater to increased load.
    It works in reverse, as well. If the load is reduced, then Auto Scaling reduces
    the number of resources available to perform the task. Auto Scaling does it all
    without pre-provisioning the resources, and does this in an automated way.
  prefs: []
  type: TYPE_NORMAL
- en: Auto Scaling can scale in both ways—vertically (adding more resources to the
    existing resource type) or horizontally (adding resources by creating another
    instance of that type of resource).
  prefs: []
  type: TYPE_NORMAL
- en: The Auto Scaling feature makes a decision regarding adding or removing resources
    based on two strategies. One is based on the available metrics of the resource
    or on meeting some system threshold value. The other type of strategy is based
    on time, for example, between 9 a.m. and 5 p.m. IST, instead of three web servers;
    the system needs 30 web servers.
  prefs: []
  type: TYPE_NORMAL
- en: Azure monitoring instruments every resource; all the metric-related data is
    collected and monitored. Based on the data collected, Auto Scaling makes decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Monitor autoscale applies only to virtual machine scale sets, cloud services,
    and app services (for example, web apps).
  prefs: []
  type: TYPE_NORMAL
- en: Container scaling using Docker Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier, in the chapter on deployment, we looked at how to package a microservice
    into a Docker container. We also discussed in detail why containerization is useful
    in the microservice world. In this section, we will advance our skills with Docker
    and also see how easily we can scale our microservices with Docker Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: Inherently, microservices are distributed systems and need to be distributed
    and isolated resources. Docker Swarm provides container orchestration clustering
    capabilities so that multiple Docker engines can work as single virtual engines.
    This is similar to load balancer capabilities; besides, it also creates new instances
    of containers or deletes containers, if the need arises.
  prefs: []
  type: TYPE_NORMAL
- en: You can use any of the available service discovery mechanisms, such as DNS,
    consul, or zookeeper tools, with Docker Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: A swarm is a cluster of Docker engines or nodes where you can deploy your microservices
    as *services*. Now, do not confuse these services with microservices. Services
    are a different concept in Docker implementation. A **service** is the definition
    of the tasks to execute on the worker nodes. You may want to understand the node
    we are referring to in the last sentence. The node, in Docker Swarm context, is
    used for the Docker engine participating in a cluster. A complete swarm demo is
    possible, and ASP.NET Core images are available in the ASP.NET-Docker project
    on GitHub ([https://github.com/aspnet/aspnet-docker](https://github.com/aspnet/aspnet-docker)).
  prefs: []
  type: TYPE_NORMAL
- en: The Azure Container Service has recently been made available. It is a good solution
    for scaling and orchestrating Linux or Windows containers using DC/OS, Docker
    Swarm, or Google Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have understood how to scale a microservice infrastructure, let's
    revisit the scalability aspects of microservice design in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling service design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In these sections, we will look at the components/concerns that need to be taken
    care of while designing or implementing a microservice. With infrastructure scaling
    taking care of service design, we can truly unleash the power of the microservice's
    architecture and get a lot of business value in terms of making a microservice
    a true success story. So, what are the components in service design? Let's have
    a look.
  prefs: []
  type: TYPE_NORMAL
- en: Data persistence model design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In traditional applications, we have always relied on relational databases to
    persist user data. Relational databases are not new to us. They emerged in the
    70s as a way of storing persistent information in a structured way that would
    allow you to make queries and perform data maintenance.
  prefs: []
  type: TYPE_NORMAL
- en: In today's world of microservices, modern applications need to be scaled at
    the hyperscale stage. We are not recommending here that you abandon the use of
    relational databases in any sense. They still have their valid use cases. However,
    when we mix read and write operations in a single database, complications arise
    where we need to have increased scalability. Relational databases enforce relationships
    and ensure the consistency of data. Relational databases work on the well-known
    ACID model. So, in relational databases, we use the same data model for both read
    and write operations.
  prefs: []
  type: TYPE_NORMAL
- en: However, the needs of read and write operations are quite different. In most
    cases, read operations usually have to be quicker than write operations. Read
    operations can also be done using different filter criteria, returning a single
    row or a result set. In most write operations, there is a single row or column
    involved, and usually, write operations take a bit longer when compared to read
    operations. So, we can either optimize and serve reads or optimize and serve writes
    in the same data model.
  prefs: []
  type: TYPE_NORMAL
- en: 'How about we split the fundamental data model into two halves: one for all
    the read operations and the other for all the write operations? Now things become
    far simpler, and it is easy to optimize both the data models with different strategies.
    The impact of this on our microservices is that they, in turn, become highly scalable
    for both kinds of operations.'
  prefs: []
  type: TYPE_NORMAL
- en: This particular architecture is known as **Common Query Responsibility Segregation**
    (**CQRS**). As a natural consequence, CQRS also gets extended in terms of our
    programming model. Now, the database-object relationship between our programming
    models has become much simpler and more scalable.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this comes the next fundamental element in scaling a microservice implementation:
    the caching of data.'
  prefs: []
  type: TYPE_NORMAL
- en: Caching mechanism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Caching is the simplest way to increase the application's throughput. The principle
    is very easy. Once the data is read from data storage, it is kept as close as
    possible to the processing server. In future requests, the data is served directly
    from the data storage or cache. The essence of caching is to minimize the amount
    of work that a server has to do. HTTP has a built-in cache mechanism embedded
    in the protocol itself. This is the reason it scales so well.
  prefs: []
  type: TYPE_NORMAL
- en: With respect to microservices, we can cache at three levels, namely client side,
    proxy, and server side. Let's look at each of them.
  prefs: []
  type: TYPE_NORMAL
- en: First, we have client-side caching. With client-side caching, clients store
    cached results. So the client is responsible for doing the cache invalidation.
    Usually, the server provides guidance, using mechanisms, such as cache control
    and expiry headers, about how long it can keep the data and when it can request
    fresh data. With browsers supporting HTML5 standards, there are more mechanisms
    available, such as local storage, an application cache, or a web SQL database,
    in which the client can store more data.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we move onto the proxy side. Many reverse proxy solutions, such as Squid,
    HAProxy, and NGINX, can act as cache servers as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s discuss server-side caching in detail. In server-side caching, we
    have the following two types:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Response caching: This is an important kind of caching mechanism for a web
    application UI, and honestly, it is simple and easy to implement as well. In response
    to caching, cache-related headers get added to the responses served from microservices.
    This can drastically improve the performance of your microservice. In ASP.NET Core,
    you can implement response caching using the `Microsoft.AspNetCore.ResponseCaching`
    package.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distributed caching for persisted data: A distributed cache enhances microservice
    throughput due to the fact that the cache will not require an I/O trip to any
    external resource. This has the following advantages:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microservice clients get the exact same results.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The distributed cache is backed up by a persistence store and runs as a different
    remote process. So even if the app server restarts or has any problems, it in
    no way affects the cache.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The source's data store has fewer requests made to it.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use distributed providers, such as CacheCow, Redis (for our book *Azure
    Redis Cache*), or Memcache, in a clustered mode for scaling your microservice
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will provide an overview of CacheCow and Azure
    Redis Cache.
  prefs: []
  type: TYPE_NORMAL
- en: CacheCow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CacheCow comes into the picture when you want to implement HTTP caching on both
    the client and server. This is a lightweight library, and currently, ASP.NET Web
    API support is available. CacheCow is open source and comes with an MIT license
    that is available on GitHub ([https://github.com/aliostad/CacheCow](https://github.com/aliostad/CacheCow))[.](https://github.com/aliostad/CacheCow)
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started with CachCow, you need to get ready for both the server and
    client. The important steps are:'
  prefs: []
  type: TYPE_NORMAL
- en: Install the `Install-Package CacheCow.Server` NuGet package within your ASP.NET
    Web API project; this will be your server.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install the `Install-Package CacheCow.Client`  NuGet package within your client
    project; the client application will be WPF, Windows Form, Console, or any other
    web application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a cache store. You need to create a cache store at the server side that
    requires a database for storing cache metadata ([https://github.com/aliostad/CacheCow/wiki/Getting-started#cache-store](https://github.com/aliostad/CacheCow/wiki/Getting-started#cache-store)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to use memcache, refer to [https://github.com/aliostad/CacheCow/wiki/Getting-started](https://github.com/aliostad/CacheCow/wiki/Getting-started) for
    more information.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Redis Cache
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Azure Redis Cache is built on top of an open source called **Redis** ([https://github.com/antirez/redis](https://github.com/antirez/redis)),
    which is an in-memory database and persists on a disk. As per Microsoft ([https://azure.microsoft.com/en-in/services/cache/](https://azure.microsoft.com/en-in/services/cache/)):'
  prefs: []
  type: TYPE_NORMAL
- en: '"Azure Redis Cache is based on the popular open source Redis cache. It gives
    you access to a secure, dedicated Redis cache, managed by Microsoft and accessible
    from any application within Azure."'
  prefs: []
  type: TYPE_NORMAL
- en: 'Getting started with Azure Redis Cache is very simple with the help of these
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a web API project—refer to our code example in [Chapter 2](047f5d0b-a008-48e2-9c7f-c57c16e671f9.xhtml),
    *Implementing Microservices*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement Redis—for a referral point use [https://github.com/StackExchange/StackExchange.Redis](https://github.com/StackExchange/StackExchange.Redis)
    and install the `Install-Package StackExchange.Redis` NuGet package.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update your config file for `CacheConnection` ([https://docs.microsoft.com/en-us/azure/redis-cache/cache-web-app-howto#configure-the-application-to-use-redis-cache](https://docs.microsoft.com/en-us/azure/redis-cache/cache-web-app-howto#configure-the-application-to-use-redis-cache)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then publish on Azure ([https://docs.microsoft.com/en-us/azure/redis-cache/cache-web-app-howto#publish-the-application-to-azure](https://docs.microsoft.com/en-us/azure/redis-cache/cache-web-app-howto#publish-the-application-to-azure)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can also use this template to create Azure Redis Cache:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/Azure/azure-quickstart-templates/tree/master/201-web-app-redis-cache-sql-database](https://github.com/Azure/azure-quickstart-templates/tree/master/201-web-app-redis-cache-sql-database)
    For complete details on Azure Redis Cache refer to this URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.microsoft.com/en-us/azure/redis-cache/](https://docs.microsoft.com/en-us/azure/redis-cache/)'
  prefs: []
  type: TYPE_NORMAL
- en: Redundancy and fault tolerance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We understand that a system's ability to deal with failure and recover from
    failure is not the same as that offered by scalability. However, we cannot deny
    that they are closely related abilities in terms of the system. Unless we address
    the concerns of availability and fault tolerance, it will be challenging to build
    highly scalable systems. In a general sense, we achieve availability by making
    redundant copies available to different parts/components of the system. So, in
    the upcoming section, we will touch upon two such concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Circuit breakers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A circuit breaker is a safety feature in an electronic device that, in the event
    of a short circuit, breaks the electricity flow and protects the device, or prevents
    any further damage to the surroundings. This exact idea can be applied to software
    design. When a dependent service is not available or not in a healthy state, a
    circuit breaker prevents calls from going to that dependent service and redirects
    the flow to an alternate path for a configured period of time.
  prefs: []
  type: TYPE_NORMAL
- en: 'In his famous book, *Release It! Design and Deploy Production-Ready Software*, Michael
    T. Nygard gives details about the circuit breaker. A typical circuit breaker pattern
    is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/83e7a3c1-1650-48ec-b0a7-93ba4e863052.png)'
  prefs: []
  type: TYPE_IMG
- en: As shown in the diagram, the circuit breaker acts as a state machine with three
    states.
  prefs: []
  type: TYPE_NORMAL
- en: Closed state
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the initial state of the circuit, which depicts a normal flow of control.
    In this state, there is a failure counter. If `OperationFailedException` occurs
    in this flow, the failure counter is increased by `1`. If the failure counter
    keeps increasing, meaning the circuit encounters more exception, and reaches the
    failure threshold set, the circuit breaker transitions to an *Open* state. But
    if the calls succeed without any exception or failure, the failure count is reset.
  prefs: []
  type: TYPE_NORMAL
- en: Open state
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Open* state, a circuit has already tripped and a timeout counter has
    started. If a timeout is reached and a circuit still keeps on failing, the flow
    of code enters into the Half-Open state.
  prefs: []
  type: TYPE_NORMAL
- en: Half-Open state
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the Half-Open state, the state machine/circuit breaker component resets the
    timeout counter and again tries to open the circuit, reinitiating the state change
    to the Open state. However, before doing so, it tries to perform regular operations, say
    a call to the dependency; if it succeeds, then instead of the Open state, the
    circuit breaker component changes the state to Closed. This is so that the normal
    flow of the operation can happen, and the circuit is closed again.
  prefs: []
  type: TYPE_NORMAL
- en: For .NET-based microservices, if you want to implement the circuit breaker and
    a couple of fault-tolerant patterns, there is a good library named *Polly* available
    in the form of a NuGet package. It comes with extensive documentation and code
    samples, and moreover, has a fluent interface. You can add *Polly* from [http://www.thepollyproject.org/](http://www.thepollyproject.org/)
    or by just issuing the `install--Package Polly` command from the package manager
    console in Visual Studio.
  prefs: []
  type: TYPE_NORMAL
- en: Service discovery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For a small implementation, how can you determine the address of a microservice?
    For any .NET developer, the answer is that we simply put the IP address and port
    of service in the configuration file and we are good. However, when you deal with
    hundreds or thousands of them dynamically, configured at runtime, you have a service
    location problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now if you peek a bit deeper, we are trying to solve two parts of the problem:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Service registration: This is the process of registration within the central
    registry of some kind, where all the service-level metadata, host lists, ports,
    and secrets are stored.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Service discovery: Establishing communication at runtime with a dependency
    through a centralized registry component is service discovery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Any service registration and discovery solution needs to have the following
    characteristics to make it considerable as a solution for the microservice services
    discovery problem:'
  prefs: []
  type: TYPE_NORMAL
- en: The centralized registry itself should be highly available
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once a specific microservice is up, it should receive the requests automatically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Intelligent and dynamic load balancing capabilities should exist in the solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The solution should be able to monitor the capability of the service health
    status and the load it is subjected to
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The service discovery mechanism should be capable of diverting the traffic to
    other nodes or services from unhealthy nodes, without any downtime or impact on
    its consumers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there is a change in the service location or metadata, the service discovery
    solution should be able to apply the changes without impacting the existing traffic
    or service instances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some of the service discovery mechanisms are available within the open source
    community. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Zookeeper: Zookeeper ([http://zookeeper.apache.org/](http://zookeeper.apache.org/)) is
    a centralized service for maintaining configuration information and naming, providing
    distributed synchronization, and providing group services. It''s written in Java,
    is strongly consistent (CP), and uses the Zab ([http://www.stanford.edu/class/cs347/reading/zab.pdf](http://www.stanford.edu/class/cs347/reading/zab.pdf)) protocol
    to coordinate changes across the ensemble (cluster).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consul: Consul makes it simple for services to register themselves and discover
    other services via a DNS or HTTP interface. It registers external services, such
    as SaaS providers, as well. It also acts as a centralized configuration store
    in the form of key values. It also has failure detection properties. It is based
    on the peer-to-peer gossip protocol.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Etcd: Etcd is a highly available key-value store for shared configuration and
    service discovery. It was inspired by Zookeeper and Doozer. It''s written in Go,
    uses Raft ([https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf](https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf)) for
    consensus, and has an HTTP-plus JSON-based API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scalability is one of the critical advantages of pursuing the microservice architectural
    style. We looked at the characteristics of microservice scalability. We discussed
    the Scale Cube model of scalability and how microservices can scale on the *y *axis
    via functional decomposition of the system. Then we approached the scaling problem
    with the scaling infrastructure. In the infrastructure segment, we looked at the
    strong capability of Azure Cloud to scale, utilizing the Azure scale sets and
    container orchestration solutions, such as Docker Swarm, DC/OS, and Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: In the later stages of the chapter, we focused on scaling with a service design
    and discussed how our data model should be designed. We also discussed certain
    considerations, such as having a split CQRS style model, while designing the data
    model for high scalability. We also briefly touched on caching, especially distributed
    caching, and how it improves the throughput of the system. In the last section,
    to make our microservices highly scalable, we discussed the circuit breaker pattern
    and service discovery mechanism, which are essential for the scalability of microservice
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at the reactive nature of microservices and
    the characteristics of reactive microservices.
  prefs: []
  type: TYPE_NORMAL
