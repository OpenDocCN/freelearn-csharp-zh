- en: Chapter 10. Parallel Programming Patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will review the common problems that a programmer often
    faces while trying to implement a parallel workflow. You will learn the following
    recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Lazy-evaluated shared states
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Parallel Pipeline with `BlockingCollection`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Parallel Pipeline with TPL DataFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Map/Reduce with PLINQ
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Patterns in programming means a concrete and standard solution to a given problem.
    Usually, programming patterns are the result of people gathering experience, analyzing
    the common problems, and providing solutions to these problems.
  prefs: []
  type: TYPE_NORMAL
- en: Since parallel programming has existed for quite a long time, there are many
    different patterns that are used to program parallel applications. There are even
    special programming languages to make programming of specific parallel algorithms
    easier. However, this is where things start to become increasingly complicated.
    In this chapter, I will provide you with a starting point from where you will
    be able to study parallel programming further. We will review very basic, yet
    very useful, patterns that are quite helpful for many common situations in parallel
    programming.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will be using a **shared-state object** from multiple threads. I would
    like to emphasize that you should avoid it as much as possible. As we discussed
    in previous chapters, a shared state is really bad when you write parallel algorithms,
    but on many occasions, it is inevitable. We will find out how to delay the actual
    computation of an object until it is needed and how to implement different scenarios
    to achieve thread safety.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we will show you how to create a structured parallel data flow. We will
    review a concrete case of a producer/consumer pattern, which is called **Parallel
    Pipeline**. We are going to implement it by just blocking the collection first,
    and then we will see how helpful another library from Microsoft is for parallel
    programming—**TPL DataFlow**.
  prefs: []
  type: TYPE_NORMAL
- en: The last pattern that we will study is the **Map/Reduce** pattern. In the modern
    world, this name could mean very different things. Some people consider Map/Reduce
    not as a common approach to any problem, but as a concrete implementation for
    large, distributed cluster computations. We will find out the meaning behind the
    name of this pattern and review some examples of how it might work in cases of
    small parallel applications.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Lazy-evaluated shared states
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe shows how to program a Lazy-evaluated, thread-safe shared state
    object.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To start this recipe, you will need to run Visual Studio 2015\. There are no
    other prerequisites. The source code for this recipe can be found at `BookSamples\Chapter10\Recipe1`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To implement Lazy-evaluated shared states, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Start Visual Studio 2015\. Create a new C# console application project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the `Program.cs` file, add the following `using` directives:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the following code snippet below the `Main` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the following code snippet inside the `Main` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Run the program.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first example shows why it is not safe to use the `UnsafeState` object with
    multiple accessing threads. We see that the `Construct` method was called several
    times, and different threads use different values, which is obviously not right.
    To fix this, we can use a lock when reading the value, and if it is not initialized,
    create it first. This will work, but using a lock with every read operation is
    not efficient. To avoid using locks every time, we can use a traditional approach
    called the **double-checked locking** pattern. We check the value for the first
    time, and if is not null, we avoid unnecessary locking and just use the shared
    object. However, if it was not constructed, we use the lock and then check the
    value for the second time because it could be initialized between our first check
    and the lock operation. If it is still not initialized, only then do we compute
    the value. We can clearly see that this approach works with the second example—there
    is only one call to the `Construct` method, and the first-called thread defines
    the shared object state.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that if the Lazy-evaluated object implementation is thread-safe, it does
    not automatically mean that all its properties are thread-safe as well.
  prefs: []
  type: TYPE_NORMAL
- en: If you add, for example, an `int` public property to the `ValueToAccess` object,
    it will not be thread-safe; you still have to use interlocked constructs or locking
    to ensure thread safety.
  prefs: []
  type: TYPE_NORMAL
- en: This pattern is very common, and that is why there are several classes in the
    Base Class Library to help us. First, we can use the `LazyInitializer.EnsureInitialized`
    method, which implements the double-checked locking pattern inside. However, the
    most comfortable option is to use the `Lazy<T>` class, which allows us to have
    thread-safe, Lazy-evaluated, shared state, out of the box. The next two examples
    show us that they are equivalent to the second one, and the program behaves in
    the same way. The only difference is that since `LazyInitializer` is a static
    class, we do not have to create a new instance of a class, as we do in the case
    of `Lazy<T>`, and therefore, the performance in the first case can be better in
    some rare scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: The last option is to avoid locking at all if we do not care about the `Construct`
    method. If it is thread-safe and has no side effects/serious performance impacts,
    we can just run it several times but use only the first constructed value. The
    last example shows the described behavior, and we can achieve this result using
    another `LazyInitializer.EnsureInitialized` method overload.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Parallel Pipeline with BlockingCollection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe will describe how to implement a specific scenario of a producer/consumer
    pattern, which is called Parallel Pipeline, using the standard `BlockingCollection`
    data structure.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To begin this recipe, you will need to run Visual Studio 2015\. There are no
    other prerequisites. The source code for this recipe can be found at `BookSamples\Chapter10\Recipe2`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To understand how to implement Parallel Pipeline using `BlockingCollection`,
    perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Start Visual Studio 2015\. Create a new C# console application project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the `Program.cs` file, add the following `using` directives:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the following code snippet below the `Main` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the following code snippet inside the `Main` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Run the program.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the preceding example, we implement one of the most common parallel programming
    scenarios. Imagine that we have some data that has to pass through several computation
    stages, which takes a significant amount of time. The latter computation requires
    the results of the former, so we cannot run them in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: If we had only one item to process, there would not be many possibilities to
    enhance the performance. However, if we run many items through the same set of
    computation stages, we can use a Parallel Pipeline technique. This means that
    we do not have to wait until all items pass through the first computation stage
    to go to the next one. It is enough to have just one item that finishes the stage;
    we move it to the next stage, and meanwhile, the next item is being by the previous
    stage, and so on. As a result, we almost have parallel processing shifted by the
    time required for the first item to pass through the first computation stage.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we use four collections for each processing stage, illustrating that we
    can process every stage in parallel as well. The first step that we do is to provide
    the possibility to cancel the whole process by pressing the *C* key. We create
    a cancelation token and run a separate task to monitor the *C* key. Then, we define
    our pipeline. It consists of three main stages. The first stage is where we put
    the initial numbers on the first four collections that serve as the item source
    to the latter pipeline. This code is inside the `Parallel.For` loop of the `CreateInitialValues`
    method, which in turn is inside the `Parallel.Invoke` statement, as we run all
    the stages in parallel; the initial stage runs in parallel as well.
  prefs: []
  type: TYPE_NORMAL
- en: The next stage is defining our pipeline elements. The logic is defined inside
    the `PipelineWorker` class. We initialize the worker with the input collection,
    provide a transformation function, and then run the worker in parallel with the
    other workers. This way, we define two workers, or filters, because they filter
    the initial sequence. One of them turns an integer into a decimal value, and the
    second one turns a decimal to a string. Finally, the last worker just prints every
    incoming string to the console. In all the places, we provide a running thread
    ID to see how everything works. Besides this, we added artificial delays, so the
    item's processing will be more natural, as we really use heavy computations.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, we see the exact expected behavior. First, some items are created
    on the initial collections. Then, we see that the first filter starts to process
    them, and as they are being processed, the second filter starts to work. Finally,
    the item goes to the last worker that prints it to the console.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Parallel Pipeline with TPL DataFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe shows how to implement a Parallel Pipeline pattern with the help
    of the TPL DataFlow library.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To start this recipe, you will need to run Visual Studio 2015\. There are no
    other prerequisites. The source code for this recipe can be found at `BookSamples\Chapter10\Recipe3`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To understand how to implement Parallel Pipeline with TPL DataFlow, perform
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Start Visual Studio 2015\. Create a new C# console application project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add references to the **Microsoft TPL DataFlow** NuGet package. Follow these
    steps to do so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Right-click on the **References** folder in the project and select the **Manage
    NuGet Packages...** menu option.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, add your preferred references to the **Microsoft TPL DataFlow** NuGet
    package. You can use the search option in the **Manage NuGet Packages** dialog
    as follows:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![How to do it...](img/B05292_10_01.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'In the `Program.cs` file, add the following `using` directives:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the following code snippet below the `Main` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the following code snippet inside the `Main` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Run the program.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous recipe, we implemented a Parallel Pipeline pattern to process
    items through sequential stages. It is quite a common problem, and one of the
    proposed ways to program such algorithms is using a TPL DataFlow library from
    Microsoft. It is distributed via NuGet and is easy to install and use in your
    application.
  prefs: []
  type: TYPE_NORMAL
- en: The TPL DataFlow library contains different types of blocks that can be connected
    with each other in different ways and form complicated processes that can be partially
    parallel and sequential where needed. To see some of the available infrastructure,
    let's implement the previous scenario with the help of the TPL DataFlow library.
  prefs: []
  type: TYPE_NORMAL
- en: First, we define the different blocks that will be processing our data. Note
    that these blocks have different options that can be specified during their construction;
    they can be very important. For example, we pass the cancelation token into every
    block we define, and when we signal the cancelation, all of them stop working.
  prefs: []
  type: TYPE_NORMAL
- en: We start our process with `BufferBlock`, we bound its capacity to 5 items maximum.
    This block holds items to pass them to the next blocks in the flow. We restrict
    it to the five-item capacity, specifying the `BoundedCapacity` option value. This
    means that when there will be five items in this block, it will stop accepting
    new items until one of the existing items passes to the next blocks.
  prefs: []
  type: TYPE_NORMAL
- en: The next block type is `TransformBlock`. This block is intended for a data transformation
    step. Here, we define two transformation blocks; one of them creates decimals
    from integers, and the second one creates a string from a decimal value. We can
    use the `MaxDegreeOfParallelism` option for this block, specifying the maximum
    simultaneous worker threads.
  prefs: []
  type: TYPE_NORMAL
- en: The last block is of the `ActionBlock` type. This block will run a specified
    action on every incoming item. We use this block to print our items to the console.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we link these blocks together with the help of the `LinkTo` methods. Here,
    we have an easy sequential data flow, but it is possible to create schemes that
    are more complicated. Here, we also provide `DataflowLinkOptions` with the `PropagateCompletion`
    property set to `true`. This means that when the step is complete, it will automatically
    propagate its results and exceptions to the next stage. Then, we start adding
    items to the buffer block in parallel, calling the block's `Complete` method,
    when we finish adding new items. Then, we wait for the last block to get completed.
    In the case of a cancelation, we handle `OperationCancelledException` and cancel
    the whole process.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Map/Reduce with PLINQ
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe will describe how to implement the Map/Reduce pattern while using
    PLINQ.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To begin this recipe, you will need to run Visual Studio 2015\. There are no
    other prerequisites. The source code for this recipe can be found at `BookSamples\Chapter10\Recipe4`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To understand how to implement Map/Reduce with PLINQ, perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Start Visual Studio 2015\. Create a new C# console application project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the `Program.cs` file, add the following `using` directives:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Add references to the `Newtonsoft.Json` NuGet package and the `System.Net.Http`
    assembly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following code snippet below the `Main` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the following code snippet inside the `Main` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the following code snippet after the `Program` class definition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Run the program.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `Map`/`Reduce` functions are another important parallel programming pattern.
    They are suitable for a small program and large multiserver computations. The
    meaning of this pattern is that you have two special functions to apply to your
    data. The first of them is the `Map` function. It takes a set of initial data
    in a key/value list form and produces another key/value sequence, transforming
    the data to a comfortable format for further processing. Then, we use another
    function, called `Reduce`. The `Reduce` function takes the result of the `Map`
    function and transforms it to the smallest possible set of data that we actually
    need. To understand how this algorithm works, let's look through the preceding
    recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we are going to analyze four classic books' text. We are going to download
    the books from the project Gutenberg's site ([www.gutenberg.org](http://www.gutenberg.org)),
    which can ask for a captcha if you issue many network requests and thus break
    the program logic of this sample. If you see HTML elements in the program's output,
    open one of the book URLs in the browser and complete the captcha. The next thing
    to do is to load a list of English words that we are going to skip when analyzing
    the text. In this sample, we try to load a JSON-encoded word list from GitHub,
    and in case of failure, we just get an empty list.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's pay attention to our `Map`/`Reduce` implementation as a PLINQ extension
    method in the `PLINQExtensions` class. We use `SelectMany` to transform the initial
    sequence to the sequence we need by applying the `Map` function. This function
    produces several new elements from one sequence element. Then, we choose how we
    group the new sequence with the `keySelector` function, and we use `GroupBy` with
    this key to produce an intermediate key/value sequence. The last thing we do is
    apply `Reduce` to the resulting grouped sequence to get the result.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we run all our books processing in parallel. Each processing worker thread
    outputs the resulting information into a string, and after all workers are complete,
    we print this information to the console. We do this to avoid concurrent console
    output, when each worker text overlaps and makes the resulting information unreadable.
    In each worker process, we split the book text into a text lines sequence, chop
    each line into word sequences, and apply our `MapReduce` function to it. We use
    the `Map` function to transform each word into lowercase and use it as the grouping
    key. Then, we define the `Reduce` function as a transformation of the grouping
    element into a key value pair, which has the `Word` element that contains one
    unique word found in the text and the `Count` element, which has information about
    how many times this word has been used. The final step is our query materialization
    with the `ToList` method call, since we need to process this query twice. Then,
    we use our list of stop words to remove common words from our statistics and create
    a string result with the book's title, top 10 words used in the book, and a unique
    word's frequency in the book.
  prefs: []
  type: TYPE_NORMAL
