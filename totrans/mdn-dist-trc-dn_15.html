<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-234"><a id="_idTextAnchor233"/>15</h1>
<h1 id="_idParaDest-235"><a id="_idTextAnchor234"/>Instrumenting Brownfield Applications</h1>
<p>When building brand-new services and systems, it’s easy to achieve a basic level of observability with distributed traces, metrics, and logs using OpenTelemetry instrumentation libraries.</p>
<p>However, we don’t usually create applications from scratch – instead, we evolve existing systems that include services in different stages of their life, varying from experimental to legacy ones that are too risky to change.</p>
<p>Such systems normally have some monitoring solutions in place, with custom correlation formats, telemetry schemas, logs and metrics management systems, dashboards, alerts, as well as documentation and processes around these tools.</p>
<p>In this chapter, we’ll explore instrumentation options<a id="_idIndexMarker733"/> for such heterogeneous systems, which are frequently referred to as <strong class="bold">brownfield</strong>. First, we’ll discuss instrumentation options for legacy parts of the system and then look deeper into context propagation and interoperating with legacy correlation formats. Finally, we’ll talk about existing monitoring solutions and investigate migration strategies.</p>
<p>You’ll learn to do the following:</p>
<ul>
<li>Pick a reasonable level of instrumentation for legacy services</li>
<li>Leverage legacy correlation formats or propagate context transparently to enable end-to-end tracing</li>
<li>Forward telemetry from legacy services to new observability backends</li>
</ul>
<p>By the end of this chapter, you will be able to implement distributed tracing in your brownfield application, keeping changes to legacy parts of a system to a minimum.</p>
<h1 id="_idParaDest-236"><a id="_idTextAnchor235"/>Technical requirements</h1>
<p>The code for this chapter is available in the book’s repository on GitHub at <a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter15">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter15</a>.</p>
<p>To run samples for this chapter, we’ll need a Windows machine with the following tools:</p>
<ul>
<li>.NET SDK 7.0 or later</li>
<li>.NET SDK 4.6.2</li>
<li>Docker and <code>docker-compose</code></li>
</ul>
<h1 id="_idParaDest-237"><a id="_idTextAnchor236"/>Instrumenting legacy services</h1>
<p>The word <strong class="bold">legacy</strong> has a negative connotation in software <a id="_idIndexMarker734"/>development, implying something out of date and not exciting to work on. In this section, we will focus on a different aspect and define a legacy service as something that mostly successfully does its job but no longer evolves. Such services may still receive security updates or fixes for critical issues, but they don’t get new features, refactoring, or optimizations.</p>
<p>Maintaining such a service requires a different set of skills and fewer people than the evolving one, so the context of a specific system can easily get lost, especially after the team that was developing it moved on and now works on something else.</p>
<p>As a result, changing such components is very risky, even when it comes to updating runtime or dependency versions. Any modification might wake up dormant issues, slightly change performance, causing new race conditions or deadlocks. The main problem here is that with limited resources and a lack of context, nobody might know how a service works, or how to investigate and fix such issues. There also may no longer be appropriate test infrastructure to validate changes.</p>
<p>From an observability standpoint, such components usually have some level of monitoring in place, which is likely to be sufficient for maintenance purposes.</p>
<p>Essentially, when working on the observability of a system, we would touch legacy services only when it’s critical for newer parts of the system.</p>
<p>Let’s look at a couple of examples to better understand when changing legacy service is important and how we can minimize the risks.</p>
<h2 id="_idParaDest-238"><a id="_idTextAnchor237"/>Legacy service as a leaf node</h2>
<p>Let’s assume we’re building new parts<a id="_idIndexMarker735"/> of the system using a few legacy services as a dependency, as shown in <em class="italic">Figure 15</em><em class="italic">.1</em>:</p>
<div><div><img alt="Figure 15.1 – New services depend on legacy ones" src="img/B19423_15_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.1 – New services depend on legacy ones</p>
<p>For the purposes of our new observability solution, we may be able to treat a legacy system as a black box. We can trace client calls to the legacy components and measure client-side latency and other stats. Sometimes, we’ll need to know what happens inside the legacy component – for example, to understand client-side issues or work around legacy system limitations. For this, we can leverage existing logging and monitoring tools available in the legacy services. It could be inconvenient, but if it is rare, it can be a reasonable option.</p>
<p>If legacy components support any correlation headers for incoming requests, we can populate them on the client side to correlate across different parts of a system. We’ll look at this in the <em class="italic">Propagating context</em> section of this chapter.</p>
<p>Another thing we may be able to do without changing a legacy system is forking and forwarding its telemetry to the same observability backend – we’ll take a closer look at this in the <em class="italic">Consolidating telemetry from legacy-monitoring </em><em class="italic">tools</em> section.</p>
<p>Being able to correlate telemetry from new and legacy components and store it in the same place could be enough to debug occasional integration issues.</p>
<p>Things get more interesting if a legacy system is in the middle of our application – let’s see why.</p>
<h2 id="_idParaDest-239"><a id="_idTextAnchor238"/>A legacy service in the middle</h2>
<p>When we refactor<a id="_idIndexMarker736"/> a distributed system, we can update downstream and upstream services around a legacy component, as shown in <em class="italic">Figure 15</em><em class="italic">.2</em>:</p>
<div><div><img alt="Figure 15.2 – Legacy service-b is in between the newer service-a and service-c" src="img/B19423_15_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.2 – Legacy service-b is in between the newer service-a and service-c</p>
<p>From the tracing side, the challenge here is that the legacy component does not propagate W3C Trace Context. Operations that go through <strong class="bold">legacy-service-b</strong> are recorded as two traces – one started by <strong class="bold">service-a</strong> and another started by <strong class="bold">service-c</strong>.</p>
<p>We need to either support legacy context propagation format in newer parts of the system, or update the legacy component itself to enable context propagation.</p>
<p>Before we go into the context propagation details, let’s discuss the appropriate level of changes we should consider applying to a service, depending on the level of its maturity.</p>
<h2 id="_idParaDest-240"><a id="_idTextAnchor239"/>Choosing a reasonable level of instrumentation</h2>
<p>Finding the right level of instrumentation<a id="_idIndexMarker737"/> for mature parts of a system depends on how big of a change is needed and how risky it is. Here are several things to consider:</p>
<ul>
<li>Where do legacy services send telemetry to? Is it the same observability backend that we want to use for the newer parts?</li>
<li>How critical is it for the observability of the overall system to get telemetry from legacy components?</li>
<li>Do legacy services support some context propagation format? Can we interoperate with it from newer services?</li>
<li>Can we change some of our legacy services? How old is the .NET runtime? Do we have an adequate testing infrastructure? How big is the load on this service? How critical is the<a id="_idIndexMarker738"/> component?</li>
</ul>
<p>Let’s go through a few solutions that may apply, depending on your answers.</p>
<h3>Not changing legacy services</h3>
<p>When legacy parts of a system<a id="_idIndexMarker739"/> are instrumented with a vendor-specific SDK or agent and send telemetry to the same observability backend as we want to use for newer parts, we might not need to do anything – correlation might work out of the box or with a little context propagation adapter in newer parts of the system.</p>
<p>Your vendor might have a migration plan and documentation explaining how to make services, using their old SDK and OpenTelemetry-based solution, produce consistent telemetry.</p>
<p>Another case when doing nothing is a good option is when our legacy components are mostly isolated and either work side by side with newer parts or are leaf nodes, as shown in <em class="italic">Figure 15</em><em class="italic">.1</em>. Then, we can usually develop and debug new components without data from legacy services.</p>
<p>We could also be able to tolerate having broken traces, especially if they don’t affect critical flows and we’re going to retire legacy services soon.</p>
<p>Doing nothing is the best, but if it’s problematic for overall observability, the next discreet option is passing context though a legacy system.</p>
<h3>Propagating context only</h3>
<p>If newer parts communicate with legacy services<a id="_idIndexMarker740"/> back and forth and we can’t make trace context propagation work, it can prevent us from tracing critical operations through a system. The least invasive change we can do then is to transparently propagate trace context through a legacy service.</p>
<p>When such a service receives a request, we would read the trace context in W3C (B3, or another format) and then pass it through, without any modification to all downstream services.</p>
<p>This way, legacy services will not appear on traces, but we will have consistent end-to-end traces for the newer parts.</p>
<p>We can possibly go further and stamp trace context on the legacy telemetry to simplify debugging.</p>
<p>If transparent context propagation<a id="_idIndexMarker741"/> is still not enough and we need to have telemetry from all services in one place, the next option to consider is forking legacy telemetry and sending it to the new observability backend.</p>
<h3>Forwarding legacy telemetry to the new observability backend</h3>
<p>Debugging issues across different observability<a id="_idIndexMarker742"/> backends and log management tools can be challenging, even when data is correlated.</p>
<p>To improve it, we may be able to intercept telemetry from the legacy system on the way to its backend or enable continuous export from that backend to the new one used by the rest of the system.</p>
<p>Forwarding may require configuration changes on the legacy system, and even if such changes are small, there is still a risk of slowing down the telemetry pipeline and causing an incident for the legacy service.</p>
<p>The younger and the more flexible the system is, the more changes we can consider, and the most invasive one is onboarding a legacy system onto OpenTelemetry and enabling network instrumentations.</p>
<h3>Adding network-level instrumentation</h3>
<p>It’s likely that legacy telemetry<a id="_idIndexMarker743"/> is not consistent with distributed traces coming from new services. We may be able to transform it, or can sometimes tolerate the difference, but we may as well consider enabling minimalistic distributed tracing in legacy services. This will take care of context propagation and produce consistent telemetry with the rest of the system.</p>
<p>With this approach, we’ll pump new telemetry from legacy services to the new backend and keep all existing instrumentations and pipelines running to avoid breaking existing reports, dashboards, and alerts.</p>
<p>Something to be aware of here is that OpenTelemetry works on .NET 4.6.2 or newer versions of .NET. While instrumentations<a id="_idIndexMarker744"/> for IIS, classic ASP.NET, and OWIN are available in the <strong class="bold">contrib</strong> repository (at <a href="https://github.com/open-telemetry/opentelemetry-dotnet-contrib">https://github.com/open-telemetry/opentelemetry-dotnet-contrib</a>), such instrumentations do not get as much love as newer ones.</p>
<p>You might also hit some edge cases with <code>Activity.Current</code> when using IIS – it can get lost during hopping between managed and native threads.</p>
<p>Onboarding existing services to OpenTelemetry while keeping old tools working can be a first step in a migration project, which eventually sunsets legacy monitoring solutions.</p>
<p>This is a viable solution<a id="_idIndexMarker745"/> for any mature service and should be considered unless the service is on a retirement path already. However, if it’s not an option, we can still combine and evolve other approaches mentioned here. Let’s now look at the practical side and see how we can do it.</p>
<h1 id="_idParaDest-241"><a id="_idTextAnchor240"/>Propagating context</h1>
<p>The first goal for context propagation<a id="_idIndexMarker746"/> is to enable end-to-end distributed tracing for new services, even when they communicate through legacy ones, as shown in <em class="italic">Figure 15</em><em class="italic">.2</em>. As a stretch goal, we can also try to correlate telemetry from new and legacy parts.</p>
<p>The solution that would work in most cases involves enabling context propagation in legacy services. Depending on how legacy services are implemented, this change can be significant and risky. So, before we do it, let’s check whether we can avoid it.</p>
<h2 id="_idParaDest-242"><a id="_idTextAnchor241"/>Leveraging existing correlation formats</h2>
<p>Our legacy services might already propagate<a id="_idIndexMarker747"/> context, just in a different format. One popular approach is to pass a correlation ID that serves the same purpose as a trace ID in the W3C Trace Context standard, identifying a logical end-to-end operation.</p>
<p>While correlation ID is not compatible with trace context out of the box, it may be possible to translate one to another.</p>
<p>In a simple case, correlation ID is just a string, and then we just need to pass it to the legacy service in a header. Then, we can expect it to propagate it as is to downstream calls, as shown in <em class="italic">Figure 15</em><em class="italic">.3</em>:</p>
<div><div><img alt="Figure 15.3 – Passing the W3C Trace ID via a legacy correlation header" src="img/B19423_15_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.3 – Passing the W3C Trace ID via a legacy correlation header</p>
<p>Here, <code>correlation-id</code> header along with <code>traceparent</code>, <code>correlation-id</code> up, ignoring the unknown <code>traceparent</code>, and passes it over to <code>traceparent</code> and <code>correlation-id</code> values. It only has <code>correlation-id</code>, so it uses it to continue the trace started by <strong class="bold">service-a</strong>.</p>
<p>Let’s implement<a id="_idIndexMarker748"/> it with a custom OpenTelemetry context propagator, starting with the injection side, as shown in the following code snippet:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">CorrelationIdPropagator.cs</p>
<pre class="source-code">
public override void Inject&lt;T&gt;(PropagationContext context,
  T carrier, Action&lt;T, string, string&gt; setter)
{
  if (context.ActivityContext.IsValid())
<strong class="bold">    </strong><strong class="bold">setter.Invoke(carrier,</strong>
<strong class="bold">      CorrelationIdHeaderName,</strong>
<strong class="bold">      context.ActivityContext.TraceId.ToString());</strong>
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/Brownfield.OpenTelemetry.Common/CorrelationIdPropagator.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/Brownfield.OpenTelemetry.Common/CorrelationIdPropagator.cs</a></p>
<p>Here, we check whether the activity<a id="_idIndexMarker749"/> context is valid and set <code>TraceId</code> as a string on the <code>correlation-id</code> header. We’re setting this propagator up to run after the <code>TraceContextPropagator</code> implementation available in OpenTelemetry, so there is no need to take care of Trace Context headers here.</p>
<p>And here’s the extraction code:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">CorrelationIdPropagator.cs</p>
<pre class="source-code">
public override PropagationContext Extract&lt;T&gt;(
  PropagationContext context, T carrier,
  Func&lt;T, string, IEnumerable&lt;string&gt;&gt; getter)
{
  if (context.ActivityContext.IsValid()) return context;
  var correlationIds = getter.Invoke(carrier,
   CorrelationIdHeaderName);
  if (TryGetTraceId(correlationIds, out var traceId))
  {
<strong class="bold">    var traceContext = new ActivityContext(</strong>
<strong class="bold">      ActivityTraceId.CreateFromString(traceId),</strong>
<strong class="bold">      ActivitySpanId.CreateRandom(),</strong>
<strong class="bold">      ActivityTraceFlags.Recorded,</strong>
<strong class="bold">      isRemote: true);</strong>
    return new PropagationContext(traceContext,
     context.Baggage);
  }
  ...
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/Brownfield.OpenTelemetry.Common/CorrelationIdPropagator.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/Brownfield.OpenTelemetry.Common/CorrelationIdPropagator.cs</a></p>
<p>The custom extraction we implemented here runs after trace context extraction, so if there was a valid <code>traceparent</code> header in the incoming request, then <code>context.ActivityContext</code> is populated by the time the <code>Extract</code> method is called. Here, we give priority to W3C Trace Context and ignore the <code>correlation-id</code> value.</p>
<p>If <code>context.ActivityContext</code> is not populated, we retrieve the <code>correlation-id</code> value and try to translate it to a trace<a id="_idIndexMarker750"/> ID. If we can do it, then we create a new <code>ActivityContext</code> instance, using <code>correlation-id</code> as a trace ID and a fake parent span ID.</p>
<p>Here’s the implementation of the <code>TryGetTraceId</code> method:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">CorrelationIdPropagator.cs</p>
<pre class="source-code">
traceId = correlationId.Replace("-", "");
if (correlationId.Length &lt; 32)
  traceId = correlationId.PadRight(32, '0');
else if (traceId.Length &gt; 32)
  traceId = correlationId.Substring(0, 32);</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/Brownfield.OpenTelemetry.Common/CorrelationIdPropagator.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/Brownfield.OpenTelemetry.Common/CorrelationIdPropagator.cs</a></p>
<p>In this snippet, we support a variety of possible <code>correlation-id</code> formats – we remove dashes if it’s a GUID, and pad or trim it if the length is not right.</p>
<p class="callout-heading">Note</p>
<p class="callout">In a more complicated case, we may need to do other transformations during context extraction and injection. For example, when a legacy system requires a GUID, we can add dashes. Alternatively, if it wants a <code>base64</code>-encoded string, we can decode and encode the trace ID.</p>
<p>Let’s now check out the traces we get with this approach.</p>
<p>First, run new parts of the system with the <code>$ docker-compose up --build</code> command. It starts with <strong class="bold">service-a</strong>, <strong class="bold">service-c</strong>, and the observability stack.</p>
<p>We also need to start <strong class="bold">legacy-service-b</strong>, which is the .NET Framework 4.6.2 application running<a id="_idIndexMarker751"/> on Windows. You can start it with your IDE or the following command:</p>
<pre class="console">
legacy-service-b$ dotnet run --correlation-mode correlation-id</pre>
<p>Then, hit the following URL in your browser: http://localhost:5051/a?to=c. This will send a request to <strong class="bold">service-a</strong>, which will call <strong class="bold">service-c</strong> through <strong class="bold">legacy-service-b</strong>.</p>
<p>Now, let’s open Jaeger at http://localhost:16686 and find the trace from <strong class="bold">service-a</strong>, which should look like the one shown in <em class="italic">Figure 15</em><em class="italic">.4</em>:</p>
<div><div><img alt="Figure 15.4 – An end-to-end trace covering service-a and service-c" src="img/B19423_15_04.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.4 – An end-to-end trace covering service-a and service-c</p>
<p>As you can see, there is no <code>5050</code>) belongs to <strong class="bold">legacy-service-b</strong>.</p>
<p>There is just one trace, but it still looks broken – spans are correlated, but parent-child relationships between the client span on <strong class="bold">service-a</strong> and the server span on <strong class="bold">service-c</strong> are lost.</p>
<p>Still, it’s an improvement. Let’s now disable the <code>correlation-id</code> support on <code>Compatibility__SupportLegacyCorrelation</code> environment variable in <code>docker-compose.yml</code> to <code>false</code> on both services and restarting the docker compose application. Then, we’ll see two independent traces for <strong class="bold">service-a</strong> and <strong class="bold">service-c</strong>, so even the correlation will be lost.</p>
<p class="callout-heading">Note</p>
<p class="callout">By relying on the existing context propagation format and implementing a custom propagation adapter, we can usually record end-to-end traces for new services without any modification to the legacy ones.</p>
<p>Can we also correlate telemetry from the legacy and new services? Usually, legacy services stamp their version of <code>correlation-id</code> on all logs. If that’s the case, we can search using the trace ID across all telemetry but may need to map the trace ID to the correlation ID and back, in the same way we did with the propagator.</p>
<p>However, what if we didn’t have custom correlation<a id="_idIndexMarker752"/> implemented in a legacy service or were not able to implement an adapter? We’d need to modify the legacy service to enable context propagation – let’s see how it can be done.</p>
<h2 id="_idParaDest-243"><a id="_idTextAnchor242"/>Passing context through a legacy service</h2>
<p>Essentially, if there is no existing context propagation<a id="_idIndexMarker753"/> mechanism, we can implement one. To minimize changes to legacy systems, we can propagate context transparently, without modifying it.</p>
<p>We need to intercept incoming and outgoing requests to extract and inject trace context, and we also need a way to pass the context inside the process.</p>
<p>The implementation of this approach, especially the interception, depends on the technologies, libraries, and patterns used in a specific legacy service.</p>
<p>Incoming request interception can be achieved with some middleware or request filter. If IIS is used, it can be also done in a custom HTTP telemetry module, but then we cannot fully rely on ambient context propagation due to managed-to-native thread hops.</p>
<p>Passing context within a process<a id="_idIndexMarker754"/> can be usually achieved with <code>AsyncLocal</code> on .NET 4.6+ or <code>LogicalCallContext</code> on .NET 4.5 – this way, it will be contained in the new code and won’t require plumbing context explicitly.</p>
<p>In our demo system, <strong class="bold">legacy-service-b</strong> is a self-hosted OWIN application, and we can implement context extraction in the OWIN middleware:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">PassThroughMiddleware.cs</p>
<pre class="source-code">
private static readonly
  AsyncLocal&lt;IDictionary&lt;string, object&gt;&gt; _currentContext =
    new AsyncLocal&lt;IDictionary&lt;string, object&gt;&gt;();
public static IDictionary&lt;string, object&gt; CurrentContext =&gt;
  _currentContext.Value;
public override async Task Invoke(IOwinContext context)
{
  var tc = EmptyContext;
  if (context.Request.Headers.TryGetValue("traceparent",
    out var traceparent))
  {
    tc = new Dictionary&lt;string, object&gt;
      {{ "traceparent", traceparent[0] }};
    ...
  }
  _currentContext.Value = tc;
  ...
  using (var scope = _logger.BeginScope(tc))
  {
    await Next.Invoke(context);
  }
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/legacy-service-b/PassThrough/PassThroughMiddleware.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/legacy-service-b/PassThrough/PassThroughMiddleware.cs</a></p>
<p>First, we declare a static <code>AsyncLocal</code> value that holds trace context, represented with a simple dictionary.</p>
<p>In the middleware <code>Invoke</code> method, we read <code>traceparent</code> along with the <code>tracestate</code> and <code>baggage</code> headers (which are omitted for brevity). We populate them in the trace context dictionary. Depending on your needs, you can always limit supported context fields to <code>traceparent</code> only and optimize the code further.</p>
<p>Then, we populate the context <a id="_idIndexMarker755"/>dictionary on the <code>_currentContext</code> field, which we can then access through the public <code>CurrentContext</code> static property.</p>
<p>The last thing we do here is to invoke the next middleware, which we wrap with a logger scope containing the context dictionary. This allows us to populate trace context on all logs coming from <strong class="bold">legacy-service-b</strong>, thus correlating them with telemetry coming from new services.</p>
<p>In practice, legacy applications rarely use <code>ILogger</code>, but logging libraries usually have some other mechanism to populate ambient context on log records. Depending on the library, you may be able to access and populate <code>CurrentContext</code> with little change to the logging configuration code.</p>
<p>Getting back to context propagation, we now need to inject the <code>CurrentContext</code> value into the outgoing requests.</p>
<p>In the case of HTTP and when .NET <code>HttpClient</code> is used, we can do it with custom <code>DelegatingHandler</code> implementation. It will be more tedious with <code>WebRequest</code> usage spread across the application code when there are no helper methods that create them consistently.</p>
<p>The handler implementation<a id="_idIndexMarker756"/> is shown in the following code snippet:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">PassThroughHandler.cs</p>
<pre class="source-code">
protected override Task&lt;HttpResponseMessage&gt; SendAsync(
  HttpRequestMessage request, CancellationToken token)
{
<strong class="bold">  foreach (var kvp in PassThroughMiddleware.CurrentContext)</strong>
<strong class="bold">    request.Headers.Add(kvp.Key, pair.Value?.ToString());</strong>
  return base.SendAsync(request, token);
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/legacy-service-b/PassThrough/PassThroughMiddleware.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/legacy-service-b/PassThrough/PassThroughMiddleware.cs</a></p>
<p>Here, we just inject all fields from <code>CurrentContext</code> on outgoing request headers and then invoke the next handler. That’s it.</p>
<p class="callout-heading">Note</p>
<p class="callout">Starting with the <code>System.Diagnostics.DiagnosticSource</code> package version 6.0.0, .NET provides a <code>DistributedContextPropagator</code> base class along with several implementations, including W3C trace context and a pass-through propagator. It can be useful if you can add a dependency on a newish <code>DiagnosticSource</code> package, or when configuring propagation for native distributed tracing instrumentations in ASP.NET Core and <code>HttpClient</code>. In the case of our legacy service, extraction and injection alone are trivial, so adding a new dependency is not really justified.</p>
<p>Now, we can run the application<a id="_idIndexMarker757"/> again and check the traces:</p>
<ol>
<li>Start new services with <code>$ docker-compose up --build</code> and then <strong class="bold">legacy-service-b</strong> with the following command:<pre class="console">
<strong class="bold">legacy-service-b$ dotnet run --correlation-mode pass-through</strong></pre></li>
<li>Then call <strong class="bold">service-a</strong> with http://localhost:5051/a?to=c again and open Jaeger. We should see a trace like the one in <em class="italic">Figure 15</em><em class="italic">.5</em>:</li>
</ol>
<div><div><img alt="Figure 15.5 – An end-to-end trace with transparent service-b" src="img/B19423_15_05.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.5 – An end-to-end trace with transparent service-b</p>
<p>Here, we have correlation and causation – the client span on <strong class="bold">service-a</strong> is a direct parent of the server span on <strong class="bold">service-c</strong>. However, <strong class="bold">service-b</strong> is nowhere to be seen, as it does not actively participate in the tracing.</p>
<p>Now, we have a couple of options to pass context through the legacy system, but we can be creative and come up with more options specific to our application – for example, we can stamp legacy correlation or request IDs on the new telemetry, or log them and then post-process telemetry to correlate broken traces.</p>
<p>With these options, we should be able<a id="_idIndexMarker758"/> to achieve at least some level of correlation. Let’s now check how we can forward telemetry from legacy services to the new observability backends.</p>
<h1 id="_idParaDest-244"><a id="_idTextAnchor243"/>Consolidating telemetry from legacy monitoring tools</h1>
<p>One of the biggest benefits<a id="_idIndexMarker759"/> a good observability solution can provide<a id="_idIndexMarker760"/> is low cognitive load when debugging an application and reading through telemetry. Even perfectly correlated and high-quality telemetry is very hard to use if it’s spread across multiple tools and can’t be visualized and analyzed together.</p>
<p>When re-instrumenting legacy services with OpenTelemetry is not an option, we should check whether it’s possible to forward existing data from legacy services to a new observability backend.</p>
<p>As with context propagation, we can be creative and should start by leveraging existing solutions. For example, old .NET systems usually report and consume Windows performance counters and send logs to EventLog, or store them on the hard drive.</p>
<p>The OpenTelemetry Collector provides support for such cases via receivers, available in the contrib repository (at https://github.com/open-telemetry/opentelemetry-collector-contrib).</p>
<p>For example, we can configure a file receiver with the following snippet:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">otel-collector-config.yml</p>
<pre class="source-code">
filelog:
  include: [ /var/log/chapter15*.log ]
  operators:
    - type: json_parser
      timestamp:
        parse_from: attributes.Timestamp
        layout: '%Y-%m-%dT%H:%M:%S.%f'
      severity:
        parse_from: attributes.LogLevel</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/configs/otel-collector-config.yml">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/configs/otel-collector-config.yml</a></p>
<p>Here, we configure the collector<a id="_idIndexMarker761"/> receiver and specify the log file location and name pattern. We also configure<a id="_idIndexMarker762"/> mapping and transformation rules for individual properties in log records. In this example, we only map timestamp and log level, but if log records are structured, it’s possible to parse other properties using similar operators.</p>
<p>We can also rely on our backend to grok unstructured log records or parse records at a query time if we rarely need the data.</p>
<p>Here’s an example of collector output with a parsed log record, which, depending on your collector configuration, can send logs to the new observability backend:</p>
<pre class="source-code">
Timestamp: 2023-05-27 01:00:41.074 +0000 UTC
SeverityText: Information
…
Attributes:
<strong class="bold">     -&gt; Scopes: Slice([{"Message":"System.Collections.Generic.Dictionary`2[System.String,System.Object]","traceparent":"00-78987df9861c2d7e46c10bd084570122-53106042475b3e32-01"}])</strong>
     -&gt; Category: Str(LegacyServiceB.LoggingHandler)
...
<strong class="bold">     -&gt; State: Map({"Message":"Request complete. GET http://localhost:5049/c, OK","method":"GET","status":"OK","url":"http://localhost:5049/c","{OriginalFormat}":"Request complete. {method} {url}, {status}"})</strong>
Trace ID:
Span ID:</pre>
<p>As you can see, we could also configure the receiver to parse the <code>traceparent</code> value populated in the log scopes to record <code>Trace ID</code> and <code>Span ID</code> for the proper correlation.</p>
<p>You can reproduce it by running <strong class="bold">legacy-service-b</strong> with the following command and sending some requests to it directly, or via <strong class="bold">service-a</strong>:</p>
<pre class="console">
legacy-service-b $ dotnet run --correlation-mode pass-through &gt; ../
tmp/logs/chapter15.log</pre>
<p>A collector can be helpful in sidecar mode, forwarding data available on the machine where legacy service instances are running, and collecting performance counters or logs. It can also pretend to be our old backend and receive Zipkin or Jaeger spans, Prometheus metrics, and vendor-specific signals, such as Splunk metrics and logs.</p>
<p>We can write custom receivers and leverage collector transformation processors to produce consistent telemetry whenever possible.</p>
<p>In addition to the endless possibilities<a id="_idIndexMarker763"/> a OpenTelemetry Collector can provide, we should<a id="_idIndexMarker764"/> check whether the observability vendor we use for legacy services allows continuous export for collected telemetry, which would allow us to get the data without changing anything on the legacy system.</p>
<h1 id="_idParaDest-245"><a id="_idTextAnchor244"/>Summary</h1>
<p>In this chapter, we explored tracing in brownfield applications, where some of the services can be hard to change and onboard onto a full-fledged observability solution with OpenTelemetry.</p>
<p>We discussed possible levels of instrumentation for such services and found several cases when we can avoid changing old components altogether. Then, we went through the changes we can apply, starting with minimalistic transparent context propagation and going all the way to onboarding onto OpenTelemetry.</p>
<p>Finally, we applied some of these options in practice, enabling end-to-end correlation through a legacy service and forwarding file logs to the OpenTelemetry Collector.</p>
<p>Now, you should be ready to come up with the strategy for your own legacy components and have the building blocks to implement it.</p>
<p>This chapter concludes our journey into distributed tracing and observability on .NET – I hope you enjoyed it! The observability area is evolving fast, but now you have a foundational knowledge to design and implement your systems with observability in mind, evolve them by relying on relevant telemetry data, and operate them with more confidence, knowing what telemetry represents and how it’s collected. Now, it’s time to apply your knowledge or create something new based on it.</p>
<h1 id="_idParaDest-246"><a id="_idTextAnchor245"/>Questions</h1>
<ol>
<li>How would you approach instrumenting an existing service that is a critical part of most user scenarios in your system? This service is mature and is rarely changed, but there are no plans to retire it any time soon.</li>
<li>What can go wrong when we add OpenTelemetry to a legacy service?</li>
<li>When implementing transparent context propagation, can we leverage the <code>Activity</code> class instead of adding our own context primitive and the <code>AsyncLocal</code> field?</li>
</ol>
</div>


<div><h1 id="_idParaDest-247"><a id="_idTextAnchor246"/>Assessments</h1>
<h1 id="_idParaDest-248"><a id="_idTextAnchor247"/>Chapter 1 – Observability Needs of Modern Applications</h1>
<ol>
<li>You can think about a span as a structured event with a strict but extensible schema, allowing you to track any interesting operation. Spans have trace context that describes the relationships between them. They also have a name, start time, end time, status, and a property bag, with attributes to represent operation details.</li>
</ol>
<p>Complex and distributed operations need multiple spans that describe at least each incoming and outgoing request. A group of such correlated spans that share the same <code>trace-id </code>is called a trace.</p>
<ol>
<li value="2">Spans (also known as Activities in .NET) are created by many libraries and applications. To enable correlation, we need to propagate context within the process and between processes.</li>
</ol>
<p>In .NET, we use <code>Activity.Current</code> to propagate context within the process. This is a current span that flows with an execution context in synchronous or asynchronous calls. Whenever a new activity is started, it uses <code>Activity.Current</code> as its parent and then becomes current itself.</p>
<p>To propagate the trace context between the processes, we pass it over the wire to the next service. W3C Trace Context is a standard propagation format for the HTTP protocol, but some services use the B3 format.</p>
<ol>
<li value="3">There is no single answer to this question, but here’re some general considerations on how you can leverage a combination of signals coming from your service:<ul><li>Check whether the problem is widespread and affects more than this user and request. Is your service healthy overall? Is it specific to the API path the user hits, region, partition, feature flag, or new service version? Your observability backend might be able to assist with it</li><li>If the problem is not widespread, find traces for problematic requests using trace context if it is known, or filtering by known attributes. If you see gaps in traces, retrieve logs for this operation. If that’s not enough, use profiling to investigate further. Consider adding more telemetry.</li><li>For widespread issues, you might find the root cause of the problem by identifying specific attributes correlated with the reported problem.</li><li>Otherwise, narrow down the issue layer by layer. Are dependencies working fine? Is there something new upstream? Any changes in the load?</li><li>If issues are not specific to any combination of attributes, check the dependency health and resource utilization. Check the crash and restart count, CPU load, memory utilization, extensive garbage collection, I/O, and network bottlenecks.</li></ul></li>
</ol>
<h1 id="_idParaDest-249"><a id="_idTextAnchor248"/>Chapter 2 – Native Monitoring in .NET</h1>
<ol>
<li value="1">Use <code>Activity.Current?.Id</code> on the page. For example, like this: <code>&lt;</code><code>p&gt;traceparent: &lt;code&gt;@System.Diagnostics.Activity.Current?.Id&lt;/code&gt;&lt;/p&gt;</code>.</li>
<li>If we have <code>dotnet-monitor </code>running as a sidecar, we can connect to its instance corresponding to the problematic service instance, check the metrics and logs, and create dumps. We could even configure <code>dotnet-monitor </code>to trigger a dump collection based on certain events or resource consumption thresholds.</li>
</ol>
<p>If we don’t have <code>dotnet-monitor</code>, but can access service instances, we can install <code>dotnet-monitor </code>there and get diagnostics information from the running process.</p>
<p>If instances are healthy, but the problem is somewhere inside the telemetry pipeline, troubleshooting steps would depend on the tools we use. For example, with Jaeger we can check logs; the Prometheus UI shows connectivity with targets; the OpenTelemetry collector provides logs and metrics for self-diagnostics.</p>
<ol>
<li value="3">Query:<pre class="console">
sum by (service_name, http_route)</pre><pre class="console">
  (rate(http_server_duration_ms_count[1m]))</pre></li>
</ol>
<p>The query sums up the request rates across all running service instances, grouping it by service name and <code>http_route</code> (which represents the API route).</p>
<p>The rate function <code>(rate(http_server_duration_ms_count)</code> first calculates the rate per second, then averages the rate over one minute.</p>
<ol>
<li value="4">Search the traces with the URL and method filter in Jaeger. For uploads, it would be <code>http. url=http://storage:5050/memes/&lt;name&gt; http.method=PUT</code>. To find downloads, we would use <code>http.url=http://storage:5050/memes/&lt;name&gt; http. method=GET</code>. However, this isn’t convenient and we should consider adding the meme name as an attribute on all spans.</li>
</ol>
<h1 id="_idParaDest-250"><a id="_idTextAnchor249"/>Chapter 3 – The .NET Observability Ecosystem</h1>
<ol>
<li value="1">Check the registry (<a href="https://opentelemetry.io/registry/">https://opentelemetry.io/registry/</a>) and OpenTelemetry .NET repo. If you don’t see your library in any of them, search in issues and PRs. It’s also a good idea to search whether anything is available in the library GitHub repo or documentation.</li>
</ol>
<p>When you find an instrumentation, there are several things to check for:</p>
<ul>
<li><strong class="bold">Version and stability</strong>: Beta instrumentations could still have a high quality and be battle-tested but do not guarantee API or telemetry stability</li>
<li><strong class="bold">Performance and thread safety</strong>: Understanding the mechanism behind instrumentation is important to identify possible limitations and issues in advance</li>
</ul>
<ol>
<li value="2">The most common way to instrument libraries and frameworks is <code>ActivitySource</code>— it’s the .NET analog of OpenTelemetry Tracer, which can start activities. You can configure OpenTelemetry to listen to a source by its name. You might also see instrumentations using <code>DiagnosticSource</code>—it’s an older and less structured mechanism available in .NET.</li>
</ol>
<p>It’s also common to leverage hooks provided by libraries that can be global or applied to specific instances of the client.</p>
<ol>
<li value="3">Service meshes can trace requests to and from service mesh sidecars and provide insights into retries, service discovery, or load balancing. If they handle communication with cloud service, remote database, or queue, they can instrument corresponding communication. Service meshes can propagate the context from one application to another but cannot propagate it within the service from incoming to outgoing calls.</li>
</ol>
<h1 id="_idParaDest-251"><a id="_idTextAnchor250"/>Chapter 4 – Low-Level Performance Analysis with Diagnostic Tools</h1>
<ol>
<li value="1">If your service defines SLIs, check them first and see whether they are within the boundaries defined by your SLOs. In other words, check the key metrics that measure your user experience and see whether they are within healthy limits. For REST API-based services, it is usually the throughput of successful requests and latency grouped by API and other things that are important in your application.</li>
</ol>
<p>Resource consumption metrics could be correlated to user experience, but do not determine it. They (and other metrics that describe the internals of your service) can help you understand why the user experience has degraded and can predict future issues with some level of confidence.</p>
<ol>
<li value="2">First, we should try to find which service is responsible: check upstream and downstream services for whether the load on your service is normal and properly distributed across instances. Check whether dependencies are healthy using their server-side metrics when possible.</li>
</ol>
<p>If we can narrow down the issue to a specific service, we can check whether the issue is specific to a certain instance or group of instances, or whether instances are restarting a lot. For affected instances, we can check their resource utilization patterns for memory, CPU, GC frequency, threads, contentions, or anything that looks unusually high or low. Then, we can capture a dump from the problematic instance(s) to analyze memory and thread stacks.</p>
<ol>
<li value="3">Performance tracing (also known as profiling or just tracing) is a technique that allows us to capture detailed diagnostics about application behavior and code – call stacks, GC, contention, network events, or anything else that .NET or third-party libraries want to expose. Such events are off by default but can be enabled and controlled inside the process and out-of-process. Tools such as <code>dotnet-trace</code>, <code>dotnet-monitor</code>, PerfView, PerfCollect, JetBrains dotTrace, Visual Studio, and continuous profilers can collect and visualize them. Performance tracing can be used to investigate functional and performance issues or optimize your code.</li>
</ol>
<h1 id="_idParaDest-252"><a id="_idTextAnchor251"/>Chapter 5 – Configuration and Control Plane</h1>
<ol>
<li value="1">We’d need tail-based sampling that’s applied after span or trace ends and we know the duration or if there were any failures. Tail-based sampling can’t be done inside the process since we have distributed multi-instance applications, but we can use a tail-based sampling processor in the OpenTelemetry Collector that buffers traces and then samples them based on latency, or status codes.</li>
</ol>
<p>If we only capture suspicious traces, we will not have a baseline anymore – we won’t be able to use traces to observe normal system behavior, build analytics, and so on. So, we should additionally capture a percentage or rate of random traces – if we mark them somehow, we can analyze them separately from problematic traces to create unbiased analytics.</p>
<p>It’s always a good idea to rate-limit all traces, so we don’t overload the telemetry pipeline with traffic bursts.</p>
<p>In addition to sampling configuration on the OpenTelemetry Collector, we should consider configuring probability sampling on individual .NET services – depending on this, we would allocate an appropriate number of resources for Collector and also balance the performance impact of the instrumentation.</p>
<ol>
<li value="2">Let’s record a try number using the OpenTelemetry <code>http.resend_count</code> attribute that should be set on each HTTP span that represents a retry or redirect. We can use the <code>EnrichWithHttpRequestMessage</code> hook on the HTTP client instrumentation to intercept the outgoing request and its activity, but where would we get the retry number from?</li>
</ol>
<p>Well, we can maintain it in our retry handler (if you use Polly, you could use <code>Context </code>instead) and pass it to the hook via <code>HttpRequestMessage.Options</code>. So, the final solution could look like this:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">Program.cs</p>
<pre class="console">
AddHttpClientInstrumentation(options =&gt;
{
options.EnrichWithHttpRequestMessage = (act, req) =&gt;
{
if (req.Options.TryGetValue(
new HttpRequestOptionsKey&lt;int&gt;("try"),
out var tryCount) &amp;&amp; tryCount &gt; 0)
act.SetTag("http.resend_count", tryCount);
...
}
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter5/memes/frontend/Program.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter5/memes/frontend/Program.cs</a></p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">RetryHandler.cs</p>
<pre class="console">
for (int i = 0; i &lt; MaxTryCount; i++)
{
request.Options.Set(new
HttpRequestOptionsKey&lt;int&gt;("try"), i);
try
{
var response = await base.SendAsync(request,
token);
...
}
catch (Exception e) { ... }
await Task.Delay(delays[i]);
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter5/memes/frontend/RetryHandler.cs</p>
<ol>
<li value="3">Let’s check out the OpenTelemetry Collector documentation for tail-based sampling at https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/tailsamplingprocessor/README.md. We need to declare and configure the <code>tail_sampling </code>processor and add it to the pipeline. Here’s a sample configuration:</li>
</ol>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">otel-collector-config.yml</p>
<pre class="console">
processors:
...
tail_sampling:
decision_wait: 2s
expected_new_traces_per_sec: 500
policies:
[{ name: limit-rate,
type: rate_limiting,
rate_limiting: {spans_per_second: 50}}]
service:
pipelines:
traces:
receivers: [otlp]
processors: [tail_sampling, batch]
exporters: [jaeger]</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter5/memes/configs/otel-collector-config.yml">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter5/memes/configs/otel-collector-config.yml</a></p>
<p>You can check your current rate of recorded spans using the <code>rate(otelcol_receiver_ accepted_spans[1m]</code>) query in Prometheus and monitor the exported rate with the <code>rate(otelcol_exporter_sent_spans[1m])</code> query.</p>
<h1 id="_idParaDest-253"><a id="_idTextAnchor252"/>Chapter 6 – Tracing Your Code</h1>
<ol>
<li value="1">When setting up OpenTelemetry, you can enable <code>ActivitySource</code> by calling into the <code>TracerProviderBuilder.AddSource</code> method and passing the source name. OpenTelemetry will then create an <code>ActivityListener</code> – a low-level .NET API that listens to <code>ActivitySource</code> instances. The listener samples activities using the callback provided by OpenTelemetry and notifies OpenTelemetry when activities start or end.</li>
<li>Activity (or span) events can be used to represent something that happens at a point in time or is too short to be a span and does not need individual context. At the same time, events must happen in the scope of some activity and are recorded along with it. Activity events stay in memory until the activity is garbage-collected and their number is limited on the exporter side.</li>
</ol>
<p>Logs are usually a better alternative to <code>Activity</code> events as they are not necessarily tied to specific activity, sampling, or exporter limitations. OpenTelemetry treats events and logs similarly. Events expressed as log records are structured and can follow specific semantic conventions.</p>
<ol>
<li value="3">Links provide another way to correlate spans with cover scenarios when the span has multiple parents or is related in some way to several other spans at once. Without links, spans can only have one parent and multiple children and can’t be related to spans in other traces.</li>
</ol>
<p>Links are used in messaging scenarios to express receiving or processing multiple independent messages at once. When we process multiple messages, we need to extract the trace context and create an <code>ActivityLink</code> from each of them. Then, we can pass a collection of these links to the <code>ActivitySource.StartActivity</code> method. We can’t change these links after the corresponding <code>Activity</code> starts. Observability backends support (or don’t support) links in different ways and we might need to adjust the instrumentation based on the backend capabilities.</p>
<h1 id="_idParaDest-254"><a id="_idTextAnchor253"/>Chapter 7 – Adding Custom Metrics</h1>
<ol>
<li value="1">We should first decide what we need the metric for. For example, if we need it to rank memes in search results or to calculate ad hits, we should separate it from telemetry. Assuming we store the meme download counter in a database for business logic purposes, we could also stamp it on traces or events as an attribute when the counter is updated.</li>
</ol>
<p>From a telemetry-only standpoint, metric per meme would have high cardinality as we probably have millions of memes in the system and thousands active per minute. With some additional logic (for example, if we can ignore rarely accessed memes), we might even be able to introduce a metric with a meme name as an attribute.</p>
<p>I would start with traces and aggregate spans by meme name in a rich query. Even if traces are sampled, I can still calculate the estimated number of downloads, compare it between memes, and see trends.</p>
<ol>
<li value="2">Usually, both, but it depends: we need incoming HTTP request traces to investigate individual failures and delays and know what normal request flow looks like under different conditions. Do we need metrics as well? Probably yes. At a high scale, we sample traces aggressively but likely need more precise data than estimated counts. Another problem is that even if we don’t sample or don’t mind rough estimates, querying over all spans during the time window can be expensive and long – it might need to process millions of records. If we build dashboards and alerts on this data, we want queries to be fast and cheap. Even if they are used for ad hoc analysis during incidents, we still want queries to be fast.</li>
</ol>
<p>So, the answer depends on the observability backend, what it is optimized for, and its pricing model, but collecting both gives us a good starting point.</p>
<ol>
<li value="3">For the number of active instances, we can report <code>ObservableUpDownCounter</code> with resource attributes that include instance information. The counter would always report <code>1</code> so that the sum of values across all instances at any given time will represent the number of active processes. This is how Kubernetes does it with <code>kube_node_info</code> or <code>kube_pod_info</code> metrics (check out <a href="https://github.com/kubernetes/kube-state-metrics">https://github.com/kubernetes/kube-state-metrics</a> for more information).</li>
</ol>
<p>Uptime can be reported in multiple ways – for example, as a gauge containing static start time (see <code>kube_node_created</code> or <code>kube_pod_start_time</code>) or as a resource attribute.</p>
<p>Make sure to check whether your environment already emits anything similar or whether OpenTelemetry semantic conventions define a common way to report the metric you’re interested in.</p>
<h1 id="_idParaDest-255"><a id="_idTextAnchor254"/>Chapter 8 – Writing Structured and Correlated Logs</h1>
<ol>
<li value="1">The code uses string interpolation instead of semantic logging. A log message is formatted right away, so the <code>ILogger.Log</code> method is called underneath with the <code>"hello world: 43, bar"</code> string, without any indication that there are two arguments with specific names and values.</li>
</ol>
<p>If the <code>Information</code> level is disabled, string interpolation happens anyway, serializing all arguments and calculating just the message to be dropped.</p>
<p>This code should be changed to <code>logger.LogInformation("hello world: {foo}, {bar}", 42, "bar")</code>.</p>
<ol>
<li value="2">We need to make sure that the usage report is built using log record properties that don’t change:<ul><li>A log message would change a lot when new arguments are added or code is refactored.</li><li>The logging category is usually based on a namespace, which might change during refactoring. We can consider passing categories explicitly as strings instead of a generic type parameter, but the better choice would be to make sure the report does not rely on logging categories. We can use event names or IDs – they have to be specified explicitly; we just need to make sure they are unique and don’t change. One approach would be to declare them in a separate file and document that the usage reports rely on them.</li></ul></li>
<li>Traces and logs describing HTTP requests contain similar information. Logs are more verbose, since we’d usually have human-readable text and need two records for one request (before and after it), with duplicated trace context and other scopes.</li>
</ol>
<p>If your application records all HTTP traces, there is no need to enable HTTP logging as well. If traces are sampled, there is a trade-off between the cost of capturing all telemetry and your ability to investigate rare issues. Many applications don’t really need to capture all telemetry to efficiently investigate problems. For them, collecting sampled traces without HTTP logs would be the best option. If you have to investigate rare issues, one option would be to increase the sampling rate for traces. Recording HTTP logs instead is another option that comes with an additional cost to collect, store, retrieve, and analyze logs.</p>
<h1 id="_idParaDest-256"><a id="_idTextAnchor255"/>Chapter 9 – Best Practices</h1>
<ol>
<li value="1">HTTP traces, potentially combined with some application-specific attributes, can help answer most questions about tiny RESTful service behavior. We can aggregate metrics from traces using OpenTelemetry Collector or at query time on the backend. We still need metrics for resource utilization though. The right questions to ask here are how much this solution costs us and whether there is the potential to reduce costs with sampling and how much we must spend to keep alerts running based on queries over traces. If it’s a lot, then we should look into adding metrics. So, the answer is – yes, but it can be more cost-efficient to add other signals.</li>
<li>In an application under heavy load, every bug will happen again and again. No matter how small of a sampling rate we choose, we’ll record at least some occurrences of such an issue. A high sampling rate would likely have some performance impact, but more importantly, it’ll be very expensive to store all these traces. So, a small sampling rate should be the first choice.</li>
<li>Socket communication can be very frequent, so instrumenting every request with a span can create a huge overhead. A good starting point would be to identify how long a typical session lasts, and if it’s within seconds or minutes, instrument a session with a span. Small requests can be recorded with metrics on a service side, or sometimes with logs/events.</li>
</ol>
<p>OpenTelemetry general and RPC semantic conventions should cover the necessary network attributes to represent the client and server and describe a request. We can also apply suitable RPC metrics to track duration and throughput.</p>
<h1 id="_idParaDest-257"><a id="_idTextAnchor256"/>Chapter 10 – Tracing Network Calls</h1>
<ol>
<li value="1">Reusing existing instrumentation should be the first choice, especially if you don’t have a lot of experience in both tracing and the gRPC stack. As you saw throughout this chapter, there are multiple details related to retries, the order of execution, and other tiny details that are hard to account for.</li>
</ol>
<p>Custom gRPC instrumentation makes sense if existing instrumentation does not satisfy your needs. For example, in our streaming experiments, we could optimize two layers of instrumentation (individual messages and gRPC calls) by merging them into one. We could also correlate requests, responses, and span events better if we knew the message types in the interceptor.</p>
<p>Note that even custom instrumentations benefit from following semantic conventions and relying on common tooling and documentation.</p>
<ol>
<li value="2">In such an application, we should expect to see a very long span that describes a connection between the client and server. If we sample spans, we should customize the sampler to ensure we capture this span. Alternatively, we can just drop it and instead capture events that describe anything important that happens with the encompassing connection.</li>
</ol>
<p>Then, we should think about how/whether to trace individual messages. If they are very small and fast, tracing them individually could be too expensive because of a couple of concerns:</p>
<ul>
<li>The first concern is message size. Trace context can be propagated frugally with the binary format, but still would require at least 26 bytes. You can be creative and come up with even more frugal format, propagating the message index instead of the span ID over the wire. The easiest solution would be to propagate context only for sampled-in messages and rely on metrics and events to see the overall picture.</li>
<li>The second concern is performance overhead. If your processing is very fast, tracing it might be too expensive. Sampling can help offset some of these costs, but you probably don’t need to trace individual messages. Logs and events might give you the right level of observability, and you can correlate them with a message identifier.</li>
</ul>
<p>If your message processing is complex and involves other network calls, you’d benefit from mitigating these concerns and tracing individual messages.</p>
<h1 id="_idParaDest-258"><a id="_idTextAnchor257"/>Chapter 11 – Instrumenting Messaging Scenarios</h1>
<ol>
<li value="1">The most difficult part is finding operations that are important to measure. In our example, it’s the time between when the meme is uploaded and when it became available for other users.</li>
</ol>
<p>We can emit a couple of events to capture these two timestamps along with the meme identifier and any other context. Then, we can find the delta by joining events on the meme identifier.</p>
<p>Another option is to record the timestamp of when the meme was published along with the meme metadata and pass it around our system. Then, we can report delta as a metric or an event.</p>
<ol>
<li value="2">When using batching, it’s usually interesting to know the number of messages in a batch and the payload size. By tuning these numbers, we can reduce network overhead, so having them readily available in the telemetry can be very useful.</li>
</ol>
<p>The key question is what instrument to use: a counter or histogram (a gauge would not fit here).</p>
<p>We can count messages and batches with two metrics. The ratio between them would give us the average batch size.</p>
<p>We can also record the number of messages in a batch and the payload size as histograms. This would give us a distribution in addition to average numbers.</p>
<p>I was tempted to record the batch size as an attribute on existing metrics but decided against it. In a general case, it’s a high-cardinality attribute, which is also hard to visualize in Prometheus; it would make more sense as a separate metric.</p>
<ol>
<li value="3">Baggage represents application-specific context-propagated services. If you have a need to propagate it across messaging systems, it can be injected into each message with the OpenTelemetry propagator similar to trace context.</li>
</ol>
<p>Baggage usually does not need to flow to the messaging system, but it may be hard to prevent it. Attached to every message, it might create a significant overhead in terms of the payload size, so make sure to account for it and be ready to make trade-offs.</p>
<p>On the consumption side, things get more interesting. If messages are processed independently, make sure to restore baggage from the message when processing it.</p>
<p>For batch processing, there is no single answer. Merging baggage from multiple messages may or may not make sense in your application.</p>
<p>If you want to stamp baggage information on your telemetry, one option could be to record known baggage values on link attributes along with message-specific information.</p>
<h1 id="_idParaDest-259"><a id="_idTextAnchor258"/>Chapter 12 – Instrumenting Database Calls</h1>
<ol>
<li value="1">The concept of a database change feed is similar to messaging, and we can apply the same approach we used in the previous chapter for it. The key question is how to propagate context and correlate operations that change the record and process the notification.</li>
</ol>
<p>One solution would be to add a record identifier attribute and use it to find all the operations related to a specific record. When multiple operations concurrently modify the same record, it will generate multiple notifications and we won’t be able to map producer operations to notification processing with the record ID. There might be additional notification identifiers we can use, such as record ETags. But in general cases, correlating operations that modify data and ones that process corresponding notifications would mean we have to add a trace context to the record and modify it on every operation.</p>
<ol>
<li value="2">The answer depends on how your tracing backend treats events and how mature, robust, and reliable the cache configuration and infrastructure are.</li>
</ol>
<p>Arguments for using events would be the following:</p>
<ul>
<li>Spans/activities have a slightly bigger performance overhead than events. Events also could be smaller in terms of telemetry volume.</li>
<li>We don’t need the precise Redis duration for each operation since we have logical layer activity tracing composite calls and Redis metrics.</li>
<li>The status of individual Redis calls is not very important: a set operation is even done in a fire-and-forget manner. It only matters when the failure rate increases significantly, but we’d see it in the metrics.</li>
</ul>
<ol>
<li value="3">The argument to use spans is that it’s more common and convenient because tracing backends do a much better job at visualizing spans and performing any automated analysis on them.</li>
</ol>
<p>To remove limits, remove the <code>deploy</code> section under the <code>mongo</code> container in <code>docker-compose.yml</code>. If you run the application and kill Redis, you’ll see that MongoDB can easily handle the load and throughput changes, which might mean that Redis is not necessary in an application with such a small load.</p>
<h1 id="_idParaDest-260"><a id="_idTextAnchor259"/>Chapter 13 – Driving Change</h1>
<ol>
<li value="1">Using a single backend for all signals has certain advantages. It should be easier to navigate between signals: for example, get all logs correlated with the trace, query events, and traces together with additional context, and jump from metrics to trace with exemplars. So, using a single backend would reduce cognitive load and minimize duplication in backend-related configuration and tooling.</li>
</ol>
<p>Using multiple backends can help reduce costs. For example, it’s usually possible to store logs in a cheaper log management system, assuming you already have everything up and running for logs and metrics. But these backends don’t always support traces well. Adding a new backend for traces and events only would make total sense.</p>
<p>Tools such as Grafana may be able to provide a common UX on top of different backends to mitigate some of the disadvantages.</p>
<ol>
<li value="2">There are a few things that we need to do:<ul><li><strong class="bold">Lock down the context propagation format</strong>: Using W3C Baggage spec is a good default choice unless you already have something in place. It should be documented and, ideally, implemented and configured in internal common libraries shared by all services in your application.</li><li><strong class="bold">Documenting key naming patterns</strong>: Make sure to use namespaces and define the root one for your system. It’ll help filter everything else out. Document several common properties you want to put there – we want to make sure people use them and don’t come up with something custom. Adding helper methods to populate them would also be great.</li><li><strong class="bold">Use common artifacts</strong>: If you want to stamp baggage on telemetry, customize propagation, or just unify baggage keys, make sure to ship common internal libraries with these features.</li></ul></li>
<li>When adding a cache, we’re probably trying to reduce the load on a database and optimize the service response time. We should already have observability of service and database calls and can see whether the cache would help.</li>
</ol>
<p>If we roll this feature out gradually and conditionally, we need to be able to filter and compare telemetry based on feature flags, so we need to make sure they’re recorded.</p>
<p>Finally, we should make sure we have telemetry around the cache that will help us understand how it works, and why it did not work if it fails. Adding this telemetry along with feature code will have the biggest positive impact during development, testing, and initial iterations.</p>
<h1 id="_idParaDest-261"><a id="_idTextAnchor260"/>Chapter 14 – Creating Your Own Conventions</h1>
<ol>
<li value="1">A possible solution is to define and document the stability level for attributes.</li>
</ol>
<p>For example, new conventions are always added at the alpha stability level. Once it’s fully implemented and deployed, and you’re mostly happy with the outcome, the convention can be graduated to beta.</p>
<p>Conventions should stay in beta until someone tries to use them for alerts, reports, or dashboards. If it works fine, or after feedback is addressed, the convention becomes stable. After that, it cannot be changed in a breaking manner.</p>
<ol>
<li value="2">It should be possible to validate actual telemetry to some extent.</li>
</ol>
<p>For example, it should be possible to write a test processor (an in-process one or a custom collector component) that identifies specific spans, events, or metrics that should follow the convention and checks whether the conventions are applied consistently. This test processor could warn about issues found, flag unknown attributes, notify when expected signals were not received, and so on. It should be possible to run it as a part of integration testing in the CI pipeline.</p>
<p>Another approach is to just do a regular audit on a random subset of production telemetry, which could also be automated.</p>
<h1 id="_idParaDest-262"><a id="_idTextAnchor261"/>Chapter 15 – Instrumenting Brownfield Applications</h1>
<ol>
<li value="1">Such a service is a good candidate for migration to OpenTelemetry – since we still update it, there is probably a reasonable test infrastructure and the context within the team to prevent and mitigate failures. As a first option, we should consider adding OpenTelemetry with network instrumentation and then gradually migrating existing tools and processes onto the new observability solution, while evolving an OpenTelemetry-based approach.</li>
</ol>
<p>We can control the costs of this approach with sampling, enabling and moving only essential pieces onto OpenTelemetry. At some point, when we can rely on the new observability solution, we can remove corresponding legacy reporting.</p>
<ol>
<li value="2">It’s likely that the .NET runtime version that the legacy service runs on is older than .NET 4.6.2, and then it’s impossible to use OpenTelemetry. Even if a newer version of .NET Framework is used, adding new dependencies, such as <code>System.Diagnostics.DiagnosticSource</code> and the different <code>Microsoft.Extensions</code> packages that OpenTelemetry brings transitively, can cause runtime problems due to version conflicts.</li>
</ol>
<p>Other risks come from small changes and shifts in how an application works and its performance, waking up or amplifying dormant issues such as race conditions, deadlocks, or thread pool starvation.</p>
<ol>
<li value="3">If you can add newer versions of <code>System.Diagnostics.DiagnosticSource</code> as a dependency, then using <code>Activity</code> is an option.</li>
</ol>
<p>Note that the <code>Activity</code> class is available in .NET, starting with the <code>DiagnosticSource</code> package version 4.4.0 and .NET Core 3.0; however, it went through a lot of changes. Most of the functionality we covered in this book, including W3C Trace Context, was not available in the initial versions.</p>
<p>With newer <code>DiagnosticSource</code> versions, by using <code>Activity</code>, we would modify trace context – instead of passing <code>traceparent</code> as is, we would create server and client spans and then pass an ancestor of the original <code>traceparent</code> to the downstream service. If the legacy service does not report spans to the common observability backend, we’ll see correlated traces, but with missing parent-child relationships, as we saw in <em class="italic">Figure 15.4</em>.</p>
<p>So, we need to have full-fledged distributed tracing implemented or, if no traces are reported, pass context through as is, without using <code>Activity</code> for it.</p>
</div>
</body></html>