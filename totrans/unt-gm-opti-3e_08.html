<html><head></head><body>
        

                            
                    <h1 class="header-title">Dynamic Graphics</h1>
                
            
            
                
<p>There is no question that the Rendering Pipeline of a modern graphics device is complicated. Even rendering a single triangle to the screen requires a multitude of graphics API calls. This includes tasks such as creating a buffer for the camera view that hooks into the operating system (usually via some kind of windowing system), allocating buffers for vertex data, setting up data channels to transfer vertex and texture data from RAM to VRAM, configuring each of these memory spaces to use a specific set of data formats, determining the objects that are visible to the camera, setting up and initiating a draw call for the triangle, waiting for the Rendering Pipeline to complete its task(s), and finally, presenting the rendered image to the screen. However, there's a simple reason for this seemingly convoluted and over-engineered way of drawing such a simple object—rendering often involves repeating the same tasks over and over again, and all of this initial setup makes future rendering tasks very fast.</p>
<p>CPUs are designed to handle virtually any computational scenario, but can't handle too many tasks simultaneously, whereas GPUs are designed for incredibly large amounts of parallelism, but they are limited in the complexity they can handle without breaking that parallelism. Their parallel nature requires immense amounts of data to be copied around very rapidly. During the setup of the Rendering Pipeline, we configure memory data channels for our graphics data to flow through. So, if these channels are properly configured for the types of data we will be passing, then they will operate more efficiently. However, setting them up poorly will result in the opposite.</p>
<p>Both the CPU and GPU are used during all graphics rendering, making it a high-speed dance of processing and memory management that spans software; hardware; multiple memory spaces, programming languages (each suited to different optimizations), processors, and processor types; and a large number of special-case features that can be thrown into the mix.</p>
<p>To make matters even more complicated, every rendering situation we will come across is different in its own way. Running the same application against two different GPUs often results in an apples-versus-oranges comparison due to the different capabilities and APIs they support.</p>
<p>It can be challenging to determine where a bottleneck resides within such a complex web of hardware and software systems, and it can take a lifetime of industry work in 3D graphics if we want to have a strong and immediate intuition about the source of performance issues in modern Rendering Pipelines.</p>
<p>Thankfully, profiling comes to the rescue once again, which makes becoming a Rendering Pipeline wizard less of a necessity. If we can gather data about each device, use multiple performance metrics for comparison, and tweak our scenes to observe how different rendering features affect their behavior, then we should have sufficient evidence to find the root cause of an issue and make appropriate changes. So, in this chapter, you will learn how to gather the right data, dig just deep enough into the Rendering Pipeline to find the real source of the problem, and explore various solutions and workarounds for a multitude of potential issues.</p>
<p>There are many topics to be covered when it comes to improving rendering performance. So, in this chapter, we will explore the following topics:</p>
<ul>
<li>A brief exploration of the Rendering Pipeline, focusing on the parts where the CPU and GPU come into play</li>
<li>General techniques on how to determine whether our rendering is limited by the CPU or GPU</li>
<li>A series of performance optimization techniques and features, such as:
<ul>
<li>Using GPU instancing</li>
<li>Taking advantage <strong>of Level of Detail</strong> (<strong>LOD</strong>) and other culling groups</li>
<li>Using Occlusion Culling</li>
<li>Optimizing Particle Systems</li>
<li>Improve Unity UI</li>
<li>Optimize your Shaders</li>
<li>Optimize lighting and shadow with lightmaps</li>
<li>Applying mobile-specific rendering enhancements</li>
</ul>
</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Exploring the Rendering Pipeline</h1>
                
            
            
                
<p>Poor rendering performance can manifest itself in several ways, depending on whether the device is limited by CPU activity (we are CPU bound) or by GPU activity (we are GPU bound). Investigating a CPU-bound application can be relatively simple since all of the CPU work is wrapped up in loading data from disk/memory and calling graphics API instructions.</p>
<p>However, a GPU-bound application can be more difficult to analyze since the root cause could originate from one of a large number of potential places within the Rendering Pipeline. We might find that we need to rely on a little guesswork or <em>process of elimination</em> to determine the source of a GPU bottleneck. In either case, once the problem is discovered and resolved, we can expect significant improvements since small fixes tend to reap big rewards when it comes to fixing issues in the Rendering Pipeline.</p>
<p>We briefly touched on the Rendering Pipeline in <a href="">Chapter 3</a>, <em>The Benefits of Batching</em>. To briefly summarize the essential points, we know that the CPU sends rendering instructions through the graphics API that funnels through the hardware driver to the GPU device, which results in a list of rendering instructions being accumulated in a queue known as the command buffer. The GPU processes these commands one by one until the command buffer is empty. So long as the GPU can keep up with the rate and complexity of instructions before the next frame is due to begin, we will maintain our frame rate. However, if the GPU falls behind, or the CPU spends too much time generating commands, the frame rate will start to drop.</p>
<p>The following is a greatly simplified diagram of a typical Rendering Pipeline on a modern GPU (which can also vary based on device, technology support, and custom optimizations), showing a broad view of the steps that take place:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/186d7479-5654-40b1-bf0a-c938668e9bf1.png" style="width:39.83em;height:17.58em;"/></p>
<p>The top row represents the work that takes place in the CPU, which includes both the act of calling into the graphics API through the hardware driver and pushing commands into the GPU. The next two rows represent the steps that take place in the GPU. Owing to the GPU's complexity, its internal processes are often split into two different sections—the <strong>Frontend</strong> and the <strong>Backend</strong>, which require a little added explanation.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The GPU frontend</h1>
                
            
            
                
<p>The frontend refers to the part of the rendering process where the GPU handles vertex data. Let's understand how it works:</p>
<ol>
<li>The frontend will receive mesh data from the CPU (a big bundle of vertex information), and a draw call will be issued.</li>
<li>The GPU then gathers all pieces of vertex information from the mesh data and passes them through vertex shaders, which are allowed to modify them and output them in a <em>1-to-1</em> manner.</li>
<li>From this, the GPU now has a list of primitives to process (triangles—the most primitive shapes in 3D graphics). </li>
<li>Next, the rasterizer takes these primitives and determines which pixels of the final image will need to be drawn on to create the primitive based on the positions of its vertices and the current camera view. The list of pixels generated from this process is known as <strong>fragments</strong>, which will be processed in the backend.</li>
</ol>
<p>Vertex shaders are small C-like programs that determine the input data that they are interested in and the way that they will manipulate it, and then will output a set of information for the rasterizer to generate fragments with. It is also home to the process of tessellation, which is handled by geometry shaders (sometimes called <strong>tessellation shaders</strong>), similar to a vertex shader in that they are small scripts uploaded to the GPU, except that they are allowed to output vertices in a <em>1-to-many</em> manner, hence, generating additional geometry programmatically.</p>
<p>The term <strong>shader</strong> is an anachronism from back when these scripts primarily handled lighting and shading tasks before their role was expanded to include all of the tasks they are used for today.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The GPU backend</h1>
                
            
            
                
<p>The backend represents the part of the Rendering Pipeline where fragments are processed. Let's see how it works:</p>
<ol>
<li>Each fragment will pass through a fragment shader (also known as a <strong>pixel shader</strong>). These shaders tend to involve a lot more complex activity compared to vertex shaders, such as depth testing, alpha testing, colorization, texture sampling, lighting, shadows, and various post-processing effects, to name but a few of the possibilities.</li>
</ol>
<ol start="2">
<li>This data is then drawn onto the frame buffer, which holds the current image that will eventually be sent to the display device (our monitor) once rendering tasks for the current frame are complete. There usually are two Frame Buffers in use by graphics APIs by default (although more could be generated for custom rendering scenarios).</li>
<li>At any given moment, one of the frame buffers contains the data from the frame we rendered to and is being presented to the screen, while the other is actively being drawn to by the GPU while it completes commands from the command buffer.</li>
<li>Once the GPU reaches a <kbd>swap buffers</kbd> command (the final instruction the CPU asks it to complete for the given frame), the frame buffers are flipped around so that the new frame is presented.</li>
<li>The GPU will then use the old frame buffer to draw the next frame.</li>
<li>This process repeats each time a new frame is rendered; hence, the GPU only needs two Frame Buffers to handle this task.</li>
</ol>
<p>This entire process, from making graphics API calls to swapping Frame Buffers, repeats continuously for every mesh, vertex, fragment, and frame, so long as our application is still rendering.</p>
<p>Two metrics tend to be the source of bottlenecks in the backend—Fill Rate and memory bandwidth. Let's explore them a little.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Fill Rate</h1>
                
            
            
                
<p>Fill Rate is an inclusive term referring to the speed at which the GPU can draw fragments. However, this only includes fragments that have survived all of the various conditional tests we might have enabled within the given fragment shader. A fragment is merely a <em>potential pixel</em>, and if it fails any of the enabled tests, then it is immediately discarded. This can be an enormous performance-saver, as the Rendering Pipeline can skip the costly drawing step and begin working on the next fragment instead.</p>
<p>One such example of a test that might cull a fragment is <em>Z-testing</em>, which checks whether the fragment from a closer object has already been drawn to the same fragment location (the <em>Z</em> refers to the depth dimension from the point of view of the camera). If so, the current fragment is discarded. If not, then the fragment is pushed through the fragment shader and drawn over the target pixel, which consumes exactly one fill from our Fill Rate. Now, imagine multiplying this process by thousands of overlapping objects, each of which generates hundreds or thousands of possible fragments (higher screen resolutions require more fragments to be processed). This could easily lead to millions of fragments to process every frame due to all of the possible overlaps of the main camera. On top of this, we're trying to repeat this process dozens of times every second. This is why performing so much initial setup in the Rendering Pipeline is important, and it should be reasonably obvious that skipping as many of these draws as we can results in significant rendering cost savings.</p>
<p>Graphics card manufacturers typically advertise a particular Fill Rate as a feature of the card, usually in the form of gigapixels per second, but this is a bit of a misnomer, as it would be more accurate to call it gigafragments per second; however, this argument is mostly academic. Either way, larger values tell us that the device can potentially push more fragments through the Rendering Pipeline. So, with a budget of 30 gigapixels per second and a target frame rate of 60 Hz, we can afford to process <em>30,000,000,000/60 = 500 million fragments</em> per frame before being bottlenecked on Fill Rate. With a resolution of 2,560 x 1,440 and a best-case scenario where each pixel is drawn over only once, we could theoretically draw the entire scene about 125 times without any noticeable problems.</p>
<p>Sadly, this is not a perfect world. Fill Rate is also consumed by other advanced rendering techniques, such as shadows and post-processing effects, that need to take the same fragment data and perform their passes on the Frame Buffer. Even so, we will always end up with some amount of redraw over the same pixels due to the order in which objects are rendered. This is known as <strong>Overdraw</strong>, and it is a useful metric to measure how efficiently we are making use of our Fill Rate.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Overdraw</h1>
                
            
            
                
<p>How much Overdraw we have can be represented visually by rendering all objects with additive alpha blending and a flat coloration. Areas of high Overdraw will show up more brightly, as the same pixel is drawn over with additive blending multiple times. This is precisely how the Scene window's Overdraw shading mode reveals how much Overdraw our scene is undergoing.</p>
<p>The following screenshot shows a scene with several thousand boxes drawn normally (left) versus the Scene window's Overdraw shading mode (right):</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/fd3557fb-135a-49c9-8f45-1bb112fb42bd.png"/></p>
<p>The more Overdraw we have, the more Fill Rate we are wasting by overwriting fragment data. There are several techniques we can apply to reduce Overdraw, which we will explore later.</p>
<p>Note that there are several different queues used for rendering, which can be separated into two types: <strong>opaque queues</strong> and <strong>transparent queues</strong>.<br/>
<br/> Objects rendered in one of the opaque queues can cull away fragments via <em>Z</em>-testing, as explained previously. However, objects rendered in a transparent queue cannot do so since their transparent nature means we can't assume that they won't need to be drawn no matter how many other objects are in the way, which leads to a lot of overdraws.<br/>
<br/> All Unity UI objects always render in a transparent queue, making them a significant source of Overdraw.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Memory bandwidth</h1>
                
            
            
                
<p>The other potential source of bottlenecks in the backend comes from memory bandwidth. memory bandwidth is consumed whenever a texture must be pulled from a section of the GPU's VRAM down into the lower memory levels. This typically happens when a texture is sampled, where a fragment shader attempts to pick the matching texture pixel (or <em>texel</em>) to draw for a given fragment at a given location. The GPU contains multiple cores that each have access to the same area of VRAM, but they also each contain a much smaller, local texture cache that stores the texture(s) the GPU has been most with most recently. This is similar in design with the multitude of CPU memory cache levels that allow memory transfer up and down the chain. This is a hardware design workaround for the fact that faster memory will, invariably, be more difficult and expensive to produce. So, rather than having a giant, expensive block of VRAM, we have a large, cheap block of VRAM, but use a smaller, very fast, lower-level texture cache to perform sampling with, which gives us the best of both worlds; that is, fast sampling with lower costs.</p>
<p>If a texture is needed that is already within the core's local texture cache, then sampling often becomes lightning fast and is barely perceivable. If not, then the texture must be pulled in from VRAM before it can be sampled. This is effectively a cache miss for the texture cache since it will now take time to find and pull the required texture from VRAM. This transfer consumes a certain amount of our available memory bandwidth, specifically an amount equal to the total size of the texture file stored within VRAM (which may not be the exact size of the original file or the size in RAM, due to GPU-level compression techniques).</p>
<p>If we are bottlenecked in memory bandwidth, the GPU will keep fetching the necessary texture files, but the entire process will be throttled, as the texture cache keeps waiting for data to appear before it can process a given batch of fragments. The GPU won't be able to push data back to the Frame Buffer in time to be rendered onto the screen, blocking the whole process and culminating in a poor frame rate.</p>
<p>Proper usage of memory mandwidth is another budgeting concern. For example, with a memory bandwidth of 96 GBs per second per core and a target frame rate of 60 frames per second, the GPU can afford to pull roughly 1.6 GBs (<em>96/60</em>) worth of texture data every frame before being bottlenecked in memory bandwidth. This is not an exact budget, of course, because of the potential for cache misses, but it does give us a rough value to work with.</p>
<p>Memory bandwidth is normally listed on a per-core basis, but some GPU manufacturers may try to mislead by multiplying memory bandwidth by the number of cores to list a bigger, but less practical number. Due to this, research may be necessary to compare apples with apples.</p>
<p>Note that this value is not the maximum limit on the amount of texture data that our game can contain in the project, nor in CPU RAM, nor even in VRAM. It is a metric that essentially limits how much texture swapping can occur during one frame. The same texture could be pulled back and forth multiple times in a single frame, depending on how many shaders need to use the texture, the order that the objects are rendered, and how often texture sampling must occur. Only a handful of objects can consume whole GBs of memory bandwidth since there is only a finite amount of texture cache space available. Having a shader that needs a lot of large textures is more likely to cause cache misses, hence causing a bottleneck on Memory Bandwidth. This is surprisingly easy to trigger if we consider multiple objects requiring different high-quality textures and multiple secondary texture maps (normal maps, emission maps, and so on), which are not batched together. In this case, the texture cache will be unable to hang on to a single texture file long enough to immediately sample it during the next rendering pass.</p>
<p>We have now covered the GPU front and backend and will now move on to our next section in exploring our Rendering Pipeline: lighting and shadowing. </p>


            

            
        
    

        

                            
                    <h1 class="header-title">Lighting and shadowing</h1>
                
            
            
                
<p>In modern games, a single object rarely finishes rendering completely in a single step, primarily due to lighting and shadowing. These tasks are often handled in multiple <em>passes</em> of a fragment shader, once for each of the several light sources, and the final result is combined so that multiple lights are given a chance to be applied. The result appears much more realistic, or, at least, more visually appealing.</p>
<p>Several passes are required to gather shadowing information. So, let's begin:</p>
<ol>
<li>We will first set up our scene to have shadow casters and shadow receivers, which will create or receive shadows, respectively.</li>
<li>Then, each time a shadow receiver is rendered, the GPU renders any shadow caster objects from the point of view of the light source into a texture to collect distance information for each of their fragments.</li>
<li>It then does the same for the shadow receiver, except now that it knows which fragments the shadow casters would overlap from the light source, it can render those fragments darker since they will be in the shadow created by the light source bearing down on the shadow caster. </li>
<li>This information then becomes an additional texture known as a <strong>shadow map</strong> and is blended with the surface for the Shadow Receiver when it is rendered from the point of view of the main camera. This will make its surface appear darker in certain spots where other objects stand between the light source and the given object.</li>
</ol>
<p>A similar process is used to create lightmaps, which are pre-generated lighting information for the more static parts of our scene.</p>
<p>Lighting and shadowing tend to consume a lot of resources throughout all parts of the Rendering Pipeline. We need each vertex to provide a normal direction (a vector pointing away from the surface) to determine how lighting should reflect off that surface, and we might need additional vertex color attributes to apply some extra coloring. This gives the CPU and frontend more information to pass along. Since multiple passes of fragment shaders are required to complete the final rendering, the backend is kept busy, both in terms of Fill Rate (lots and lots of pixels to draw, redraw, and merge) and in terms of memory bandwidth (extra textures to pull in or out for lightmaps and shadow maps). This is why real-time shadows are exceptionally expensive compared to most other rendering features and will inflate draw call counts dramatically when enabled.</p>
<p>However, lighting and shadowing are perhaps two of the most important parts of game art and design to get right, often making the extra performance requirements worth the cost. Good lighting and shadowing can turn a mundane scene into something spectacular, as there is something magical about professional coloring that makes it visually appealing. Even a low-poly art style (for example, the mobile game, <em>Monument Valley</em>) relies heavily on a good lighting and shadowing profile to allow the player to distinguish one object from another and create a visually pleasing scene.</p>
<p>Unity offers multiple features that affect lighting and shadows, from real-time lighting and shadows (of which there are multiple types of each) to static lighting called <strong>lightmapping</strong>. There are a lot of options to explore and, of course, a lot of things that can cause performance issues if we're not careful.</p>
<p>The Unity documentation covers all of the various lighting features in an excellent amount of detail. Start with the following pages and work through them. Doing so will be well worth the time since these systems affect the entire Rendering Pipeline. Refer to the following:
<ul>
<li><a href="https://docs.unity3d.com/Manual/LightingOverview.html">https://docs.unity3d.com/Manual/LightingOverview.html</a></li>
</ul>
<ul>
<li><a href="https://learn.unity.com/tutorial/introduction-to-lighting-and-rendering">https://learn.unity.com/tutorial/introduction-to-lighting-and-rendering</a></li>
</ul>
</p>
<p>There are two different rendering formats, which can greatly affect our lighting performance, known as <strong>Forward Rendering</strong> and <strong>Deferred Rendering</strong>. The setting for these Rendering options can be found under Edit | Project Settings | Player | Other Settings | Rendering and configured on a per-platform basis.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Forward Rendering</h1>
                
            
            
                
<p>Forward Rendering is the traditional form of rendering lights in our scene, as explored previously. During Forward Rendering, each object will be rendered in multiple passes through the same shader. How many passes are required will be based on the number, distance, and brightness of light sources. Unity will try to prioritize the <kbd>DirectionalLight</kbd> component that is affecting the object the most and render the object in a <em>base</em> pass as a starting point. It will then take several of the most powerful <kbd>PointLight</kbd> components nearby and re-render the same object multiple times through the same fragment shader. Each of these light points will be processed on a per-vertex basis, and all remaining lights will be condensed into an <em>average</em> color using a technique called spherical harmonics.</p>
<p class="mce-root">Some of this behavior can be simplified by setting a light's Render Mode to values such as Not Important and changing the value of Edit | Project Settings | Quality | Pixel Light Count. This value limits the number of lights that will be gathered for Forward Rendering but is overridden by any lights with a Render Mode set to Important. It is, therefore, up to us to use this combination of settings responsibly.</p>
<p>As we might imagine, using Forward Rendering can utterly explode our draw call count very quickly in scenes with a lot of light points present due to the number of Render States being configured and shader passes required.</p>
<p>More information on Forward Rendering can be found in the Unity documentation at <a href="http://docs.unity3d.com/Manual/RenderTech-ForwardRendering.html">http://docs.unity3d.com/Manual/RenderTech-ForwardRendering.html</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Deferred Shading</h1>
                
            
            
                
<p>Deferred Rendering, or Deferred Shading as it is sometimes known, is a technique that has been available on GPUs for about a decade or so, but it has not resulted in a complete replacement of the Forward Rendering method due to the caveats involved and somewhat limited support on mobile devices.</p>
<p>Deferred Shading is named as such because actual shading does not occur until much later in the process, that is, it is deferred until later. It works by creating a geometry buffer (called a <em>G-Buffer</em>), where our scene is initially rendered without any lighting applied. With this information, the Deferred Shading system can generate a lighting profile within a single pass.</p>
<p>From a performance perspective, the results are quite impressive as it can generate very good per-pixel lighting with little draw call effort. One disadvantage is that effects such as anti-aliasing, transparency, and applying shadows to animated characters cannot be managed through Deferred Shading alone. In this case, the Forward Rendering technique is applied as a fallback to cover those tasks, hence requiring extra draw calls to complete it. A bigger issue with Deferred Shading is that it often requires more powerful and more expensive hardware and is not available for all platforms, so fewer users will be able to make use of it.</p>
<p>The Unity documentation contains an excellent source of information on the Deferred Shading technique, along with its benefits and pitfalls, which can be found at <a href="http://docs.unity3d.com/Manual/RenderTech-DeferredShading.html">http://docs.unity3d.com/Manual/RenderTech-DeferredShading.html</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Vertex-Lit shading (legacy)</h1>
                
            
            
                
<p>Technically, there are more than two lighting methods. The remaining two are Vertex-Lit shading and a very primitive, feature-lax version of Deferred Rendering (in the Unity documentation, this is called the Legacy Deferred lighting Rendering Path). Vertex-Lit shading is a massive simplification of lighting, as lighting will only be considered per-vertex and not per-pixel. In other words, entire faces are colored the same based on the incoming light color rather than blending lighting colors across the face through individual pixels.</p>
<p>It is not expected that many, or really any, 3D games will make use of this legacy technique, since a lack of shadows and proper lighting make visualizations of depth very difficult. It is mostly used by simple 2D games that don't need to make use of shadows, normal maps, and various other lighting features.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Global Illumination</h1>
                
            
            
                
<p>Global Illumination (<strong>GI</strong>) is an implementation of baked lightmapping. Lightmapping is similar to the shadow maps created by shadowing techniques in that one or more textures are generated for each object that represents extra lighting information and is later applied to the object during its lighting pass of a fragment shader to simulate static lighting effects.</p>
<p>The main difference between these lightmaps and other forms of lighting is that lightmaps are pre-generated (or baked) in the Editor and packaged into the game build. This ensures that we don't need to keep regenerating this information at runtime, saving numerous draw calls and significant GPU activity. Since we can bake this data, we have the luxury of time to generate very high-quality lightmaps (at the expense of larger generated texture files we need to work with, of course).</p>
<p>Since this information is baked ahead of time, it cannot respond to real-time activity during gameplay, and so, by default, any lightmapping information will only be applied to static objects that were present in the scene when the lightmap was generated and at the exact location they were placed. However, light probes can be added to the scene to generate an additional set of lightmap textures that can be applied to nearby dynamic objects that move, allowing such objects to benefit from pre-generated lighting. This won't have pixel-perfect accuracy and will cost disk space for the extra light probe maps and memory bandwidth at runtime to swap them around, but it does generate a more believable and pleasant lighting profile.</p>
<p>There have been several techniques for generating lightmaps developed throughout the years, and Unity has used a couple of different solutions since its initial release. Global Illumination is simply the latest generation of the mathematical techniques behind lightmapping, which offers very realistic coloring by calculating not only how lighting affects a given object, but also how light reflects off nearby surfaces, allowing an object to affect the lighting profile of those around it. This effect is calculated by an internal system called <strong>enlighten</strong>. This tool is used both to create static lightmaps, as well as create something called <strong>Precomputed Realtime GI</strong>, which is a hybrid of real-time and static shading and allows us to simulate effects such as time-of-day (where the direction of light from the sun changes over time) without relying on expensive real-time lighting effects. </p>
<p>A typical issue with generating lightmaps is the length of time it can take to generate them and get visual feedback on the current settings because the lightmapper is often trying to generate full-detail lightmaps in a single pass. If the user attempts to modify its configuration, then the entire job must be canceled and started over. To solve this problem, Unity Technologies implemented Progressive Lightmapper, which performs lightmapping tasks more gradually over time, but also allows them to be modified while they are being calculated. This makes lightmaps of the scene appear to get progressively more detailed as it works in the background while also allowing us to change certain properties when it is still working and without having to restart the entire job. This provides almost immediate feedback and improves the workflow of generating lightmaps immensely.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Multithreaded Rendering</h1>
                
            
            
                
<p>Multithreaded Rendering is enabled by default on most systems, such as desktop and console platforms whose CPUs provide multiple cores. Other platforms still support many low-end devices to enable this feature by default, so it is a toggleable option for them. For Android, it can be enabled via a checkbox under Edit | Project Settings | Player | Other Settings | Multithreaded Rendering, whereas, for iOS, Multithreaded Rendering can be enabled by configuring the application to make use of Apple's Metal API under Edit | Project Settings | Player| Other Settings | Graphics API. At the time of writing this book, WebGL does not support Multithreaded Rendering.</p>
<p>For each object in our scene, there are three tasks to complete: determine whether the object needs to be rendered (through a technique known as <strong>Frustum Culling</strong>), and if so, generate commands to render the object (since rendering a single object can result in dozens of different commands), and then send the command to the GPU using the relevant graphics API. Without Multithreaded Rendering, all of these tasks must happen on the main thread of the CPU; hence, any activity on the main thread becomes part of the critical path for all rendering. When Multithreaded Rendering is enabled, the task of pushing commands into the GPU is handled by a render thread, whereas other tasks, such as culling and generating commands, get spread across multiple worker threads. This setup can save an enormous number of CPU cycles for the main thread, which is where the overwhelming majority of other CPU tasks take place, such as physics and script code.</p>
<p>Enabling this feature will affect what it means to be CPU bound. Without Multithreaded Rendering, the main thread is performing all of the work necessary to generate instructions for the command buffer, meaning that any performance we can save elsewhere will free up more time for the CPU to generate commands. However, when Multithreaded Rendering is taking place, a good portion of the workload is pushed onto separate threads, meaning that improvements to the main thread will have less of an impact on rendering performance via the CPU. </p>
<p>Note that being GPU bound is the same regardless of whether Multithreaded Rendering is taking place. The GPU always performs its tasks in a multithreaded fashion.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Low-level rendering APIs</h1>
                
            
            
                
<p>Unity exposes a rendering API to us through their <kbd>CommandBuffer</kbd> class. This allows us to control the Rendering Pipeline directly through our C# code by issuing high-level rendering commands, such as <kbd>render this object</kbd>, <kbd>with this Material</kbd>, <kbd>using this Shader</kbd>, or <kbd>draw N instances of this piece of procedural geometry</kbd>. This customization is not as powerful as having direct graphics API access, but it is a step in the right direction for Unity developers to customize unique graphical effects.</p>
<p>Check out the Unity documentation on <kbd>CommandBuffer</kbd> to make use of this feature at <a href="http://docs.unity3d.com/ScriptReference/Rendering.CommandBuffer.html">http://docs.unity3d.com/ScriptReference/Rendering.CommandBuffer.html</a>.</p>
<p>If an even more direct level of rendering control is needed, such as we wish to make direct graphics API calls to OpenGL, DirectX, and Metal, then be aware that it is possible to create a native plugin (a small library written in C++ code that is compiled specifically for the architecture of the target platform) that hooks into the Unity's Rendering Pipeline, setting up callbacks for when particular rendering events happen, similar to how <kbd>MonoBehaviours</kbd> hook into various callbacks of the main Unity Engine. This is certainly an advanced topic for most Unity users, but useful to know for the future as our knowledge of rendering techniques and graphics APIs matures.</p>
<p class="mce-root">Unity provides some good documentation on generating a rendering interface in a native plugin at <a href="https://docs.unity3d.com/Manual/NativePluginInterface.html">https://docs.unity3d.com/Manual/NativePluginInterface.html</a>.</p>
<p>It should be obvious that, due to the number of complex processes involved, there are a lot of different ways in which the GPU can become bottlenecked. Now that we have a thorough understanding of the Rendering Pipeline and how bottlenecks may occur, let's explore how to detect these problems.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Detecting performance issues</h1>
                
            
            
                
<p class="mce-root">When you start looking at issues in your game, lighting is often neglected. That's a novice mistake. In the following sections, we will see how to detect and solve lighting-related performance issues.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Profiling rendering issues</h1>
                
            
            
                
<p>The Profiler can be used to quickly narrow down which of the two devices used in the Rendering Pipeline we are bottlenecked within—whether it is the CPU or GPU. We must examine the problem using both the CPU Usage and GPU Usage Areas of the Profiler window, as this can tell us which device is working the hardest.</p>
<p>The following screenshot shows Profiler data for a CPU-bound application. The test involved creating thousands of simple cube objects, with no batching or shadowing techniques taking place. This resulted in an extremely large draw call count (around 32,000) for the CPU to generate commands for, but giving the GPU relatively little work to do due to the simplicity of the objects being rendered:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/2be38ead-357c-45eb-aa7c-4e8597909433.png"/></p>
<p>This example shows that the CPU's Rendering task is consuming a large number of cycles (around 25 ms per frame), whereas the GPU is processing for less than 4 milliseconds, indicating that the bottleneck resides in the CPU. Note that this profiling test was performed against a standalone app, not within the Editor. We now know that our rendering is CPU bound and can begin to apply some CPU-saving performance improvements (being careful not to introduce rendering bottlenecks elsewhere by doing so).</p>
<p>Meanwhile, profiling a GPU-bound application via the Profiler is a little trickier. This time, the test involves creating a simple object requiring minimal draw calls, but using a very expensive shader that samples a texture thousands of times to create an absurd amount of activity in the backend.</p>
<p>To perform fair GPU-bound profiling tests, you should ensure that you disable vertical sync through Edit | Project Settings | Quality | Other | V Sync Count; otherwise, it is likely to pollute our data.</p>
<p>The following screenshot shows Profiler data for this test when it is run in a standalone application:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/916fe7ca-a952-4f7c-81c2-ab787fc36be4.png"/></p>
<p>As we can see in the preceding screenshot, the rendering task of the CPU Usage Area matches closely with the total rendering costs of the GPU Usage Area. We can also see that the CPU and GPU time costs at the bottom of the image are relatively similar (about 29 milliseconds each). This is somewhat confusing as we seem to be bottlenecked equally in both devices, where we would expect the GPU to be working much harder than the CPU.</p>
<p>In actuality, if we drill down into the Breakdown View of the CPU Usage Area using the Hierarchy Mode, we will note that most of the CPU time is spent on the task labeled Gfx.WaitForPresent. This is the amount of time that the CPU is wasting while it waits for the GPU to finish the current frame. Hence, we are, in fact, bottlenecked by the GPU despite appearing as though we are bound by both. Even if Multithreaded Rendering is enabled, the CPU must still wait for the Rendering Pipeline to finish before it can begin the next frame.</p>
<p>Gfx.WaitForPresent is also used to signal that the CPU is waiting on Vertical Sync to complete, hence the need to disable it for this test.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Brute force testing</h1>
                
            
            
                
<p>If we're poring over our profiling data and still not sure where the source of the problem resides, or we're GPU bound and need to determine where we're bottlenecked in the Rendering Pipeline, we should try the brute-force method, that is, cull a specific activity from the scene and check whether it results in greatly improved performance. If a small change results in a significant speed improvement, then we have a strong clue about where the bottleneck lies. There's no harm in this approach if we eliminate enough unknown variables to ensure that the data is leading us in the right direction.</p>
<p class="NormalPACKT">The obvious brute-force test for CPU bounding will be to reduce draw calls to check whether performance suddenly improves. However, this is often not possible since, presumably, we've already been reducing our draw calls to a minimum through techniques such as static batching, dynamic batching, and atlasing. This would mean that we have a very limited scope for reducing them further.</p>
<p class="NormalPACKT">What we can do, however, is intentionally increase our draw call count by a small number, either by introducing more objects or disabling draw call-saving features, such as static and dynamic batching, and observe whether the situation gets significantly worse than before. If so, then we have evidence that we're either very close to being CPU bound or have already become so. </p>
<p>There are two good brute-force tests we can apply to a GPU-bound application to determine whether we're bound by Fill Rate or by memory bandwidth: reducing screen resolution or reducing texture resolution, respectively.</p>
<p>By reducing screen resolution, we will ask the rasterizer to generate significantly fewer fragments and transpose them over a smaller canvas of pixels for the backend to process. This will reduce the Fill Rate consumption of the application, giving this key part of the Rendering Pipeline some additional breathing room. Ergo, if performance suddenly improves with a screen resolution reduction, then Fill Rate should be our primary concern.</p>
<p>A reduction from a resolution of 2560 x 1440 to 800 x 600 is an improvement factor of about eight, which is often more than enough to reduce Fill Rate costs sufficiently to make the application perform well again.</p>
<p>Similarly, if we're bottlenecked on memory bandwidth, then reducing texture quality is likely to result in significant performance improvement. By doing so, we have shrunk the size of our textures, greatly reducing the memory bandwidth costs of our fragment shaders, allowing the GPU to fetch the necessary textures much more quickly. Globally reducing texture quality can be achieved by going to Edit | Project Settings | Quality | Texture Quality and setting the value to Half Res, Quarter Res, or Eighth Res.</p>
<p>An application bound by the CPU has ample opportunities for performance enhancements through practically every performance-enhancing tip in this book. If we free up CPU cycles from other activities, then we can afford to render more objects through more draw calls, keeping in mind, of course, that each will cost us more activity in the GPU. There are, however, additional opportunities to make some indirect improvements in draw call count while we try to improve other parts of the Rendering Pipeline. This includes Occlusion Culling, tweaking our lighting and shadowing behavior, and modifying our shaders. These will be explained in the following sections as we investigate various performance enhancements.</p>
<p>Meanwhile, we will probably need to apply a little brute-force testing and guesswork to determine how a GPU-bound application is bottlenecked. Most applications are bottlenecked by Fill Rate or memory bandwidth, so we should start there. It is rare to find performance bottlenecks in the frontend, at least on desktop applications, so it is worth checking only after we've verified that the other sources are not the problem. Vertex shaders are often trivial compared to fragment shaders, and so the only real opportunity to cause problems with frontend processing is either to push too much geometry or to have overly complex geometry shaders.</p>
<p>Ultimately, this investigation should help us to determine whether we are CPU bound or GPU bound, and, in the latter case, whether we are bound by the frontend or backend, and again in the latter case, whether we are bound by Fill Rate or memory bandwidth. With this knowledge, there are several techniques we can apply to improve performance.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Rendering performance enhancements</h1>
                
            
            
                
<p>We should now have all of the information we need to make sense of performance bottlenecks so that we can start to apply fixes. For the remainder of this chapter, we will cover a series of techniques to improve Rendering Pipeline performance for CPU-bound and GPU-bound applications.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Enabling/disabling GPU skinning</h1>
                
            
            
                
<p>The first tip involves a setting that eases the burden on the CPU or GPU frontend at the expense of the other, that is, GPU Skinning. Skinning is the process where mesh vertices are transformed based on the current location of their animated bones. The animation system, working on the CPU, transforms the object's bones that are used to determine its current pose, but the next important step in the animation process is wrapping the mesh vertices around those bones to place the mesh in the final pose. This is achieved by iterating over each vertex and performing a weighted average against the bones connected to those vertices.</p>
<p>This vertex processing task can either take place on the CPU or within the frontend of the GPU, depending on whether the GPU Skinning option is enabled. This feature can be toggled under Edit | Project Settings | Player Settings | Other Settings | GPU Skinning. Enabling this option pushes skinning activity to the GPU, although bear in mind that the CPU must still transfer the data to the GPU and will generate instructions on the command buffer for the task, so it doesn't remove the CPU's workload entirely. Disabling this option eases the burden on the GPU by making the CPU resolve the mesh's pose before transferring mesh data across and simply asking the GPU to draw it as is. Obviously, this feature is useful if we have lots of animated meshes in our scenes and can be used to help either bounding case by pushing the work onto the least busy device.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Reducing geometric complexity</h1>
                
            
            
                
<p>This tip concerns the GPU frontend. We have already covered some techniques on mesh optimization in <a href="">Chapter 4</a>, <em>Optimizing Your Art Assets</em>, which can help to reduce our mesh's vertex attributes. As a quick reminder, it is not uncommon to use a mesh that contains a lot of unnecessary UV and normal vector data, so our meshes should be double-checked for this kind of superfluous fluff. We should also let Unity optimize the structure for us, which minimizes cache misses as vertex data is read within the frontend.</p>
<p>The goal is simply to reduce actual vertex counts. There are three solutions to this:</p>
<ul>
<li>First, we can simplify the mesh by either having the art team manually tweak and generate meshes with lower polycounts or using a mesh decimation tool to do this for us.</li>
<li>Second, we could simply remove meshes from the scene, but this should be a last resort.</li>
<li>The third option is to implement automatic culling through features such as LOD, which will be explained later in this chapter.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Reducing tessellation</h1>
                
            
            
                
<p>Tessellation through geometry shaders can be a lot of fun, as it is a relatively underused technique that can really make our graphical effects stand out from among the crowd of games that use only the most common effects. However, it can contribute enormously to the amount of processing work taking place in the frontend.</p>
<p>There aren't really any simple tricks we can exploit to improve tessellation, besides improving our tessellation algorithms or easing the burden caused by other frontend tasks to give our tessellation tasks more room to breathe. Either way, if we have a bottleneck in the frontend and are making use of tessellation techniques, we should double-check that they are not consuming the lion's share of the frontend's budget.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Employing GPU instancing</h1>
                
            
            
                
<p>GPU Instancing is a means to render multiple copies of the same mesh quickly by exploiting the fact that they will have identical Render States, hence require minimal draw calls. This is practically identical to dynamic batching, except that it is not an automatic process. In fact, we can think of dynamic batching as a poor man's <em>GPU instancing</em> since GPU instancing can enable even better savings and allows for more customization by allowing parameterized variations. </p>
<p>GPU Instancing is applied at the Material level with the Enable Instancing checkbox, and variations can be introduced by modifying shader code. This way, we can give different instances of different rotations, scales, colors, and so on. This is useful for rendering scenes such as forests and rocky areas where we want to render hundreds or thousands of different copies of a mesh with some slight variation.</p>
<p>Note that Skinned Mesh Renderers cannot be instanced for similar reasons that they cannot be dynamically batched, and not all platforms and APIs support GPU Instancing.</p>
<p>The following screenshot shows the benefits of GPU Instancing on a group of 512 cube objects (with some extra lighting and shadowing applied to increase the total draw call count):</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/00c67826-051c-41b4-ab2e-9035543c2cd2.png" style="width:38.17em;height:20.33em;"/></p>
<p>This system is much more versatile than dynamic batching since we have more control over how objects are batched together. Of course, there are more opportunities for mistakes if we batch things in inefficient ways, so we should be careful to use them wisely.</p>
<p>Check out the Unity documentation for more information on GPU Instancing at <a href="https://docs.unity3d.com/Manual/GPUInstancing.html">https://docs.unity3d.com/Manual/GPUInstancing.html</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using mesh-based LOD</h1>
                
            
            
                
<p>LOD is a broad term referring to the dynamic replacement of features based on their distance from the camera and/or how much space they take up in the camera's view. Since it can be difficult to tell the difference between a low- and high-quality object at great distances, there is very little reason to render the high-quality version, and so we may as well dynamically replace distant objects with something more simplified. The most common implementation of LOD is mesh-based LOD, where meshes are dynamically replaced with lower detailed versions as the camera gets farther and farther away. </p>
<p>Making use of mesh-based LOD can be achieved by placing multiple objects in the scene and making them children of <kbd>GameObject</kbd> with an attached <kbd>LODGroup</kbd> component. The LOD group's purpose is to generate a bounding box from these objects and decide which object should be rendered based on the size of the bounding box within the camera's field of view. If the object's bounding box consumes a large area of the current view, then it will enable the mesh(es) assigned to lower LOD groups, and if the bounding box is very small, it will replace the mesh(es) with those from higher LOD groups. If the mesh is too far away, it can be configured to hide all child objects. So, with the proper setup, we can have Unity replace meshes with simpler alternatives, or cull them entirely, which eases the burden on the rendering process.</p>
<p>Check out the Unity documentation for more detailed information on the mesh-based LOD feature at <a href="http://docs.unity3d.com/Manual/LevelOfDetail.html">http://docs.unity3d.com/Manual/LevelOfDetail.html</a>.</p>
<p>This feature can cost us a large amount of development time to fully implement; artists must generate lower polygon count versions of the same object, and level designers must generate LOD groups, configure them, and test them to ensure that they don't cause jarring transitions as the camera moves closer or farther away.</p>
<p>Note that some game development middleware companies offer third-party tools for automated LOD mesh generation. These might be worth investigating to compare their ease of use versus quality loss versus cost-effectiveness.</p>
<p>Mesh-based LOD will also cost us in disk footprint as well as RAM and CPU; the alternative meshes need to be bundled, loaded into RAM, and the <kbd>LODGroup</kbd> component must routinely test whether the camera has moved to a new position that warrants a change in LOD level. The benefits on the Rendering Pipeline are rather impressive, however. Dynamically rendering simpler meshes reduces the amount of vertex data we need to pass and potentially reduces the number of draw calls, Fill Rate, and memory bandwidth needed to render the object.</p>
<p>Due to the number of sacrifices needed for mesh-based LOD to function, developers should avoid preoptimizing by automatically assuming that mesh-based LOD will help them. Excessive use of the feature will lead to burdening other parts of our application's performance and chew up precious development time, all for the sake of paranoia. It should only be used if we start to observe problems in the Rendering Pipeline, and we've got CPU, RAM, and development time to spare.</p>
<p>Having said that, scenes that feature large, expansive views of the world and have lots of camera movement might want to consider implementing this technique very early, as the added distance and a massive number of visible objects will likely exacerbate the vertex count enormously. As a counterexample, scenes that are always indoors or feature a camera with a viewpoint looking down at the world will find little benefit in this technique since objects will tend to be at a similar distance from the camera at all times. Examples include <strong>R</strong><strong>eal-Time Strategy</strong> (<strong>RTS</strong>) and <strong>Multiplayer Online Battle Arena</strong> (<strong>MOBA</strong>) games.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Culling groups</h1>
                
            
            
                
<p>Culling groups are a part of the Unity API that effectively allows us to create our own custom LOD system as a means of coming up with our own ways of dynamically replacing certain gameplay or rendering behaviors. Some examples of things we might want to apply LOD to include replacing animated characters with a version with fewer bones, applying simpler shaders, skipping Particle System generation at great distances, and simplifying AI behavior.</p>
<p>Since the culling group system at its most basic level simply tells us whether objects are visible to the camera and how big they are, it also has other uses in the realm of gameplay, such as determining whether certain enemy spawn points are currently visible to the player or whether a player is approaching certain areas. There is a wide range of possibilities available with the culling group system that makes it worth considering. Of course, the time spent to implement, test, and redesign scenes to exploit can be significant.</p>
<p>Check out the Unity documentation for more information on culling groups at <a href="https://docs.unity3d.com/Manual/CullingGroupAPI.html">https://docs.unity3d.com/Manual/CullingGroupAPI.html</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Making use of Occlusion Culling</h1>
                
            
            
                
<p>One of the best ways to reduce both Fill Rate consumption and Overdraw is to make use of Unity's Occlusion Culling system. The system works by partitioning the world into a series of small cells and flying a virtual camera through the scene, making a note of which cells are invisible from other cells (are <em>occluded</em>) based on the size and position of the objects present.</p>
<p>Note that this is different from the technique of Frustum Culling, which culls objects outside the current camera view. Frustum Culling is always active and automatic. Objects culled by this process are, therefore, automatically ignored by the Occlusion Culling system.</p>
<p>Occlusion Culling data can only be generated for objects properly labeled Occluder Static and/or Occludee Static under the StaticFlags drop-down menu. Occluder Static is the general setting for static objects we expect to be so large that they will both occlude and be occluded by other objects, such as skyscrapers or mountains, which can hide other objects behind them, as well as be hidden behind each other, and so on. Occludee Static is a special case for things, such as transparent objects that always require other objects behind them to be rendered, but they themselves need to be hidden if something large blocks their visibility.</p>
<p>Naturally, because Static flags must be enabled for Occlusion Culling, this feature will not work for dynamic objects.</p>
<p>The following screenshot shows how effective Occlusion Culling can be at reducing the number of rendered objects from our scene from an external point of view for the sake of demonstration. From the point of view of the main camera, the two situations appear identical.</p>
<p>The Rendering Pipeline is not wasting time rendering objects that are obscured by closer ones:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/a9858734-38ee-4ee6-a23b-3c2ab7d506bf.png" style="width:41.75em;height:27.75em;"/></p>
<p>Enabling the Occlusion Culling feature will cost additional disk space, RAM, and CPU time. Extra disk space is required to store the occlusion data, extra RAM is needed to keep the data structure in memory, and there will be a CPU processing cost to determine which objects are being occluded in each frame. The Occlusion Culling data structure must be properly configured to create cells of the appropriate size for our scene, and the smaller the cells, the longer it takes to generate the data structure. However, if it is configured correctly for the scene, Occlusion Culling can provide both fill rate savings through reduced Overdraw and draw call savings by culling nonvisible objects.</p>
<p>Note that even though an object may be culled by occlusion, its shadows must still be calculated, so we won't save any draw calls or fill rate from those tasks.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Optimizing Particle Systems</h1>
                
            
            
                
<p>Particle Systems are useful for a huge number of different visual effects, and usually, the more particles they generate, the better the effect looks. However, we will need to be responsible about the number of particles generated and the complexity of shaders used since they can touch on all parts of the Rendering Pipeline; they generate a lot of vertices for the frontend (each particle is a quad) and could use multiple textures, which consume Fill Rate and memory bandwidth in the backend, so they can potentially cause an application to be bound anywhere if used irresponsibly.</p>
<p>Reducing Particle System density and complexity is fairly straightforward—use fewer Particle Systems, generate fewer particles, and/or use fewer special effects. Atlasing is also another common technique for reducing Particle System performance costs. However, there is an important performance consideration behind Particle Systems that is not too well known and happens behind the Scenes, and that is the process of automatic Particle System culling.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Making use of Particle System culling</h1>
                
            
            
                
<p>The basic idea is that all Particle Systems are either predictable or not (deterministic versus non-deterministic), depending on various settings. When a Particle System is predictable and not visible to the main view, then the entire Particle System can be automatically culled away to save performance. As soon as a predictable Particle System comes back into view, Unity can figure out exactly how the Particle System is meant to look at that moment as if it had been generating particles the entire time it wasn't visible.</p>
<p>So long as the Particle System generates particles in a very procedural way, then the state is immediately solvable mathematically.</p>
<p>However, if any setting forces the Particle System to become unpredictable or nonprocedural, then it would have no idea what the current state of the Particle System needs to be, had it been hidden previously, and will hence need to render it fully every frame regardless of whether or not it is visible. Settings that break a Particle System's predictability include, but are not limited to, making the Particle System render in world-space; applying external forces, collisions, and trails; or using complex animation curves. Check out the blog post mentioned previously for a rigorous list of nonprocedural conditions.</p>
<p>Note that Unity provides a useful warning on Particle Systems when something would cause it to break automatic culling, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/3f6ba5ab-ee42-45e7-8c6e-0a361636a0cf.png" style="width:34.08em;height:8.92em;"/></p>
<p>Unity Technologies has released an excellent blog post covering this topic, which can be found at <a href="https://blogs.unity3d.com/2016/12/20/unitytips-particlesystem-performance-culling/">https://blogs.unity3d.com/2016/12/20/unitytips-particlesystem-performance-culling/</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Avoiding recursive Particle System calls</h1>
                
            
            
                
<p>Many methods available to a <kbd>ParticleSystem</kbd> component are recursive calls. Calling them will iterate through each child of the Particle System, which then calls <kbd>GetComponent&lt;ParticleSystem&gt;()</kbd> on each child, and, if the component exists, it will call the appropriate method. This then repeats for each child <kbd>ParticleSystem</kbd> beneath the original parent, its grandchildren, and so on. This can be a huge problem with deep hierarchies of Particle Systems, which is sometimes the case with complex effects.</p>
<p>There are several <kbd>ParticleSystem</kbd> API calls affected by this behavior, such as <kbd>Start()</kbd>, <kbd>Stop()</kbd>, <kbd>Pause()</kbd>, <kbd>Clear()</kbd>, <kbd>Simulate()</kbd>, and <kbd>isAlive()</kbd>. We obviously cannot avoid these methods entirely since they represent the most common methods we would want to call on a Particle System. However, each of these methods has a <kbd>withChildren</kbd> parameter that defaults to <kbd>true</kbd>. Bypassing <kbd>false</kbd> in place of this parameter (for example, by calling <kbd>Clear(false)</kbd>, it disables the recursive behavior and will not call into its children. Hence, the method call will only affect the given Particle System, reducing the overhead cost of the call.</p>
<p>This is not always ideal since we do often want all children of the Particle System to be affected by the method call. Another approach, therefore is to, cache the <kbd>ParticleSystem</kbd> components in the same way we learned in <a href="">Chapter 2</a>, <em>Scripting Strategies</em>, and iterate through them manually ourselves (making sure that we pass <kbd>false</kbd> for the <kbd>withChildren</kbd> parameter each time).</p>
<p>Note that there is a bug in Unity 2017.1 and older, where additional memory is allocated each time <kbd>Stop()</kbd> and <kbd>Simulate()</kbd> are called (even if the Particle System has already been stopped). This bug was fixed in Unity 2017.2.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Optimizing Unity UI</h1>
                
            
            
                
<p>Unity's first few attempts at built-in UI systems were not particularly successful; it is often quickly supplanted by products on the Asset Store. However, the latest generation of their solution (simply called Unity UI) has become a much more popular solution, so many developers are starting to rely on it for their UI needs, so much so, in fact, that Unity Technologies bought the company behind the Text Mesh Pro asset in early 2017 and merged it into the Unity UI as a built-in feature.</p>
<p>Let's explore a few techniques we can use to improve the performance of Unity's built-in UI.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using more Canvases</h1>
                
            
            
                
<p>A <kbd>Canvas</kbd> component's primary task is to manage the meshes that are used to draw the UI elements beneath them in the Hierarchy window and issue the draw calls necessary to render those elements. An important task of the Canvas is to batch these meshes together (which can only happen if they share the same Material) to reduce draw calls. However, when changes are made to a Canvas, or any of its children, this is known as <em>dirtying</em> the Canvas.</p>
<p>When a Canvas is <em>dirty</em>, it needs to regenerate meshes for all of the UI elements beneath it before it can issue a draw call. This regeneration process is not a simple task and is a common source of performance problems in Unity projects because, unfortunately, many things can cause the Canvas to be made dirty. Even changing a single UI element within a Canvas can cause this to occur. There are so many things that cause dirtying, and so few that don't (and usually only in certain circumstances) that it's best to simply err on the side of caution and assume that any change will cause this effect.</p>
<p>Perhaps the only notable action that doesn't cause dirtying is changing a <kbd>Color</kbd> property of a UI element.</p>
<p>If we find that our UI causes a large spike in CPU usage any time something changes (or sometimes literally every frame if they're being changed every frame), one solution we can apply is to simply use more Canvases. A common mistake is to build the entire game's UI in a single Canvas and keep it this way as the game code, and its UI continues to become more complex.</p>
<p>This means that it will need to check every element every time anything changes in the UI, which can become more and more disastrous on performance as more elements are crammed into a single Canvas. However, each Canvas is independent and does not need to interact with other Canvases in the UI, and so by splitting up the UI into multiple Canvases, we can separate the workload and simplify the tasks required by any single Canvas.</p>
<p>Ensure that you add a <kbd>GraphicsRaycaster</kbd> component to the same <kbd>GameObject</kbd> as the child Canvas so that its own child elements can still be interacted with. Conversely, if none of the Canvas' child elements are interactable, then we can safely remove any <kbd>GraphicsRaycaster</kbd> components from it to reduce performance costs.</p>
<p>In this case, even though an element still changes, fewer other elements will need to be regenerated in response, reducing the performance cost. The downside of this approach is that elements across different Canvases will not be batched together, so we should try to keep similar elements with the same Material grouped together within the same Canvas, if possible.</p>
<p>It's also possible to make a Canvas a child of another Canvas, for the sake of organization, and the same rules apply. If an element changes in one Canvas, the other will be unaffected.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Separating objects between static and dynamic Canvases</h1>
                
            
            
                
<p>We should strive to try and generate our Canvases in a way that groups elements based on when they get updated. We should think of our elements as fitting within one of three groups: </p>
<ul>
<li><strong>Static</strong>: Static UI elements are those that never change; good examples of these are background images, labels, and so on</li>
<li><strong>Incidental Dynamic</strong>: Dynamic elements are those that can change, where Incidental Dynamic objects are those UI elements that only change in response to something, such as a UI button press or a hover action</li>
<li><strong>Continuous Dynamic</strong>: Continuous Dynamic objects are those UI elements that update regularly, such as animated elements</li>
</ul>
<p>We should try to split UI elements from these three groups into three different Canvases for any given section of our UI, as this will minimize the amount of wasted effort during regeneration.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Disabling Raycast Target for non-interactive elements</h1>
                
            
            
                
<p>UI elements have a Raycast Target option, which enables them to be interacted with by clicks, taps, and other user behavior. Each time one of these events takes place, the <kbd>GraphicsRaycaster</kbd> component will perform pixel-to-bounding box checks to figure out which element has been interacted with and is a simple iterative <kbd>for</kbd> loop. By disabling this option for non-interactive elements, we're reducing the number of elements that <kbd>GraphicsRaycaster</kbd> needs to iterate through, thereby saving performance.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Hiding UI elements by disabling the parent Canvas component</h1>
                
            
            
                
<p>The UI uses a separate layout system to handle the regeneration of certain element types, which operates in a similar way as dirtying a Canvas. <kbd>UIImage</kbd>, <kbd>UIText</kbd>, and <kbd>LayoutGroup</kbd> are all examples of components that fall under this system. Many things can cause a layout system to become dirty, the most obvious of which are enabling and disabling such elements. However, if we want to disable a portion of the UI, we can avoid these expensive regeneration calls from the layout system by simply disabling the <kbd>Canvas</kbd> component they are children of. This can be done by setting the <kbd>Canvas</kbd> component's <kbd>enabled</kbd> property to <kbd>false</kbd>.</p>
<p>The drawback of this approach is that, if any child objects that have some <kbd>Update()</kbd>, <kbd>FixedUpdate()</kbd>, <kbd>LateUpdate()</kbd>, or coroutine code, then we would also need to disable them manually, otherwise they will continue to run. By disabling the <kbd>Canvas</kbd> component, we're only stopping the UI from being rendered and interacted with, and we should expect various update calls to continue to happen as normal.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Avoiding Animator components</h1>
                
            
            
                
<p>Unity's <kbd>Animator</kbd> components were never intended to be used with the latest version of its UI System, and their interaction with it is a naive implementation. Each frame, the animator will change properties on UI elements that cause their layouts to be dirtied and cause regeneration of a lot of internal UI information. We should avoid using Animators entirely, and instead perform tweening ourselves or use a utility asset intended for such operations.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Explicitly defining the event camera for World Space Canvases</h1>
                
            
            
                
<p>Canvases can be used for UI interactions in both 2D and 3D. This is determined by whether the Canvas has its Render Mode setting configured to Screen Space (2D) or World Space (3D). Any time a UI interaction takes place, the <kbd>Canvas</kbd> Component will check its <kbd>eventCamera</kbd> property (exposed as Event Camera in the Inspector window) to figure out which camera to use. By default, a 2D Canvas will set this property to the main camera, but a 3D Canvas leaves it set to <kbd>null</kbd>. This is unfortunate because, each time the event camera is needed, it will still use the main camera, but will do so by calling <kbd>FindObjectWithTag()</kbd>. Finding objects by tag isn't as bad of a performance cost as using the other variations of <kbd>Find()</kbd>, but its performance cost scales linearly with the more tags we use in a given project. To make matters worse, the event camera is accessed fairly often during a given frame for a World Space Canvas, which means that leaving this property <kbd>null</kbd> will cause a huge performance hit for no real benefit. We should manually set this property to the main camera for all of our World Space Canvases.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Don't use alpha to hide UI elements</h1>
                
            
            
                
<p>Rendering a UI element with an alpha value of <kbd>0</kbd> in its <kbd>color</kbd> property will still cause a draw call to be issued. We should favor changing the <kbd>IsActive</kbd> property of a UI element to hide it when necessary. Another alternative is to use Canvas Groups via <kbd>CanvasGroup</kbd> Components, which can be used to control the alpha transparency of all child elements beneath them. Setting the <kbd>alpha</kbd> value of a Canvas Group to <kbd>0</kbd> will cull away its child objects, and, therefore, no draw calls will be issued.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Optimizing ScrollRects</h1>
                
            
            
                
<p><kbd>ScrollRect</kbd> Components are UI elements that are used to scroll through a list of other UI elements and are fairly common in mobile applications. Unfortunately, the performance of these elements scales very poorly with size since the Canvas needs to regenerate them regularly. There are several things we can do to improve the performance of our <kbd>ScrollRect</kbd> Components. Let's take a look at some of them in the following.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Make sure to use a RectMask2D</h1>
                
            
            
                
<p>It's possible to create scrolling UI behavior by simply placing other UI elements with a lower <kbd>depth</kbd> value than the <kbd>ScrollRect</kbd> elements. However, this is bad practice since there will be no culling taking place in <kbd>ScrollRect</kbd>, and every element will need to be regenerated for each frame that <kbd>ScrollRect</kbd> is moving. If we haven't already, we should use a <kbd>RectMask2D</kbd> Component to clip and cull child objects that are not visible. This Component creates a region of space, whereby any child UI elements within it will be culled away if they are outside the bounds of the <kbd>RectMask2D</kbd> Component. The cost of determining whether to cull an object compared to the savings of rendering too many invisible ones is typically worth it.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Disable Pixel Perfect for ScrollRects</h1>
                
            
            
                
<p>Pixel Perfect is a setting on a <kbd>Canvas</kbd> component that forces its child UI elements to be drawn with direct alignment to the pixels on the screen. This is often a requirement for art and design, as the UI elements will appear much sharper than if it was disabled. While this alignment behavior is a relatively expensive operation, it is effectively mandatory that it will be enabled for the majority of our UI to keep things crisp and clear. However, for animating and fast-moving objects, it can be somewhat pointless due to the motion involved.</p>
<p>Disabling Pixel Perfect for <kbd>ScrollRect</kbd> elements is a good way to make some impressive savings. However, since the Pixel Perfect setting affects the entire Canvas, we should make sure to enable the <kbd>ScrollRect</kbd> element as a child object beneath a separate Canvas so that other elements will maintain their pixel-aligned behavior.</p>
<p>Different kinds of animated UI elements actually look better with Pixel Perfect disabled. Be sure to do some testing, as this can save quite a bit of performance.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Manually stop ScrollRect motion</h1>
                
            
            
                
<p>The Canvas will always need to regenerate the entire <kbd>ScrollRect</kbd> element even if the velocity is moving by a fraction of a pixel each frame. We can manually freeze its motion once we detect that its velocity is below a certain threshold using <kbd>ScrollRect.velocity</kbd> and <kbd>ScrollRect.StopMovement()</kbd>. This can help to reduce the frequency of regeneration a great deal.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using empty UIText elements for full-screen interaction</h1>
                
            
            
                
<p>A common implementation in most UIs is to activate a large, transparent interactable element that covers the entire screen, forcing the player to handle a popup before proceeding, while still allowing the player to see what's going on behind it (as a means of not ripping the player out of the game experience entirely). This is often done with a <kbd>UIImage</kbd> element, but unfortunately, this can break batching operations, and transparency can be a problem on mobile devices.</p>
<p>A hacky way around this problem is to use a <kbd>UIText</kbd> element with no Font or Text defined. This creates an element that doesn't need to generate any renderable information and only handles bounding box checks for interaction.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Checking the Unity UI source code</h1>
                
            
            
                
<p>If we're having significant problems with the performance of our UI, it is possible to look into the source code to figure out exactly what might be going on and hopefully discover ways to get around the problem.</p>
<p>A more drastic measure, but a potential option, could be to actually modify the UI code, compile it, and add it to our project manually.</p>
<p class="mce-root"/>
<p>Unity provides the code for its UI system in a Bitbucket repository, found at <a href="https://bitbucket.org/Unity-Technologies/ui">https://bitbucket.org/Unity-Technologies/ui</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Checking the documentation</h1>
                
            
            
                
<p>The tips mentioned previously are some of the more obscure, undocumented, or critical performance optimization tips for the UI system. There are several great resources on the Unity website that explain how the UI system works and how best to optimize it, which is far too large to fit in this book verbatim.</p>
<p>Start with the following page and work your way through them for many more helpful UI optimization tips: <a href="https://unity3d.com/learn/tutorials/temas/best-practices/guide-optimizing-unity-ui">https://unity3d.com/learn/tutorials/temas/best-practices/guide-optimizing-unity-ui</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Shader optimization</h1>
                
            
            
                
<p>Fragment shaders are the primary consumers of Fill Rate and memory bandwidth. The costs depend on their complexity—how much texture sampling takes place, how many mathematical functions are used, and many more factors. The GPU's parallel nature (sharing small pieces of the overall job between hundreds of threads) means that any bottleneck in a thread will limit how many fragments can be pushed through that thread during a frame.</p>
<p>The classic analogy is a vehicle assembly line. A complete vehicle requires multiple stages of manufacture to complete. The critical path to completion might involve stamping, welding, painting, assembly, and inspection, where each step is completed by a single team. For any given vehicle, no stage can begin before the previous one is finished, but whatever team handled the stamping for the last vehicle can begin stamping for the next vehicle as soon as it has finished. This organization allows each team to become masters of their particular domain rather than trying to spread their knowledge too thin, which would likely result in less consistent quality in the batch of vehicles.</p>
<p>We can double the overall output by doubling the number of teams, but if any team gets blocked, then precious time is lost for any given vehicle, as well as all future vehicles that would pass through the same team. If these delays are rare, then they can be negligible in the grand scheme, but if not, and even if one stage takes several minutes longer than normal each and every time it must complete the task, then it can become a bottleneck that threatens the release of the entire batch.</p>
<p>The GPU parallel processors work similarly: each processor thread is an assembly line, each processing stage is a team, and each fragment is the thing that needs to be built. If the thread spends a long time processing a single stage, then time is lost on each fragment. This delay will multiply such that all future fragments coming through the same thread will be delayed. This is a bit of an oversimplification, but it often helps to paint a picture of how quickly some poorly optimized shader code can chew up our Fill Rate, and how small improvements in shader optimization provide big benefits in backend performance.</p>
<p>Shader programming and optimization is a very niche area of game development. Their abstract and highly specialized nature requires a very different kind of thinking to generate high-quality shader code compared to typical gameplay or Engine code. They often feature mathematical tricks and back-door mechanisms for pulling data into the shader, such as pre-computing values and putting them in texture files. Because of this, and the importance of optimization, shaders tend to be very difficult to read and reverse-engineer.</p>
<p>Consequently, many developers rely on prewritten shaders, visual shader creation tools from the Asset Store, such as Shader Forge or Amplify Shader Editor. This simplifies the act of initial shader code generation, but might not result in the most efficient form of shaders. Whether we're writing our own shaders, or we're relying on pre-written/pre-generated shaders, we might find it worthwhile to perform some optimization passes over them using some tried-and-true techniques, which we are going to see in the following sections.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Consider using shaders intended for mobile platforms</h1>
                
            
            
                
<p>The built-in mobile shaders in Unity do not have any specific restrictions that force them only to be used on mobile devices. They are simply optimized for minimum resource usage (and tend to feature some of the other optimizations listed in this section).</p>
<p>Desktop applications are perfectly capable of using these shaders, but they tend to feature a loss of graphical quality. It only becomes a question of whether the loss of graphical quality is acceptable. So, consider doing some testing with the mobile equivalents of common shaders to check whether they are a good fit for your game.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using small data types</h1>
                
            
            
                
<p>GPUs can calculate with smaller data types more quickly than larger types (particularly on mobile platforms), so the first tweak we can attempt is replacing our <kbd>float</kbd> data types (32-bit, floating-point) with smaller versions such as <kbd>half</kbd> (16-bit, floating-point) or even <kbd>fixed</kbd> (12-bit, fixed point). The size of the data types listed previously will vary depending on what floating-point formats the target platform prefers. The sizes listed are the most common.</p>
<p>Optimization stems from the relative size between formats since there are fewer bits to process.</p>
<p>Color values are good candidates for precision reduction, as we can often get away with fewer precise color values without a noticeable loss in coloration. However, the effects of reducing precision can be very unpredictable for graphical calculations. So, changes such as these can require some testing to verify that the reduced precision is costing too much graphical fidelity.</p>
<p>Note that the effects of these tweaks can vary enormously between one GPU architecture and another (for example, AMD versus Nvidia versus Intel) and even GPU brands from the same manufacturer. In some cases, we can make some decent performance gains for a trivial amount of effort. In other cases, we might see no benefit at all.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Avoiding changing precision while swizzling</h1>
                
            
            
                
<p>Swizzling is the shader programming technique of creating a new vector (an array of values) from an existing vector by listing the components in the order in which we wish to copy them into the new structure.</p>
<p>Here are some examples of swizzling:</p>
<pre>float4 input = float4(1.0, 2.0, 3.0, 4.0);  // initial test value (x, y, z, w)<br/><br/>// swizzle two components<br/>float2 val1 = input.yz; // val1 = (2.0, 3.0)<br/><br/>// swizzle three components in a different order<br/>float3 val2 = input.zyx; // val2 = (3.0, 2.0, 1.0)<br/><br/>// swizzle the same component multiple times<br/>float4 val3 = input.yyy; // val3 = (2.0, 2.0, 2.0)<br/><br/>// swizzle a scalar multiple times<br/>float sclr = input.w; // sclr = (4.0)<br/>float3 val4 = sclr.xxx; // val4 = (4.0, 4.0, 4.0)</pre>
<p>We can use both the <kbd>xyzw</kbd> and <kbd>rgba</kbd> representations to refer to the same components, sequentially. It does not matter whether it is a color or a vector; they just make the shader code easier to read. We can also list components in any order we like to fill in the desired data, repeating them if necessary.</p>
<p>Converting from one precision type to another in a shader can be a costly operation, but converting the precision type while simultaneously swizzling can be particularly painful. If we have mathematical operations that use swizzling, ensure that they don't also convert the precision type. In these cases, it would be wiser to simply use the high-precision data type from the very beginning or reduce precision across the board to avoid the need for changes in precision.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using GPU-optimized helper functions</h1>
                
            
            
                
<p>The shader compiler often performs a good job of reducing mathematical calculations down to an optimized version for the GPU, but compiled custom code is unlikely to be as effective as both the <strong>Cg</strong> library's built-in helper functions and the additional helpers provided by the Unity <strong>Cg</strong> included files. If we are using shaders that include custom function code, perhaps we can find an equivalent helper function within the <strong>Cg</strong> or Unity libraries that can do a better job than our custom code can.</p>
<p>These extra <kbd>include</kbd> files can be added to our shader within the <kbd>CGPROGRAM</kbd> block, as follows:</p>
<pre>CGPROGRAM<br/>// other includes<br/>#include "UnityCG.cginc"<br/>// Shader code here<br/>ENDCG</pre>
<p>Example <strong>Cg</strong> library functions to use are <kbd>abs()</kbd> for absolute values, <kbd>lerp()</kbd> for linear interpolation, <kbd>mul()</kbd> for multiplying matrices, and <kbd>step()</kbd> for step functionality. Useful <kbd>UnityCG.cginc</kbd> functions include <kbd>WorldSpaceViewDir()</kbd> for calculating the direction toward the camera and <kbd>Luminance()</kbd> for converting color into grayscale.</p>
<p>Check out <a href="http://http.developer.nvidia.com/CgTutorial/cg_tutorial_appendix_e.html">http://http.developer.nvidia.com/CgTutorial/cg_tutorial_appendix_e.html</a> for a full list of Cg standard library functions.<br/>Check out the Unity documentation for a complete and up-to-date list of possible <kbd>include</kbd> files and their accompanying helper functions at <a href="http://docs.unity3d.com/Manual/SL-BuiltinIncludes.html">http://docs.unity3d.com/Manual/SL-BuiltinIncludes.html</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Disabling unnecessary features</h1>
                
            
            
                
<p>Perhaps we can make savings by simply disabling shader features that aren't vital. Does the shader really need transparency, <em>Z</em>-writing, alpha testing, and/or alpha blending? Will tweaking these settings or removing these features give us a good approximation of our desired effect without losing too much graphical fidelity? Making such changes is a good way of making Fill Rate cost savings.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Removing unnecessary input data</h1>
                
            
            
                
<p>Sometimes, the process of writing a shader involves a lot of back and forth experimentation in editing code and viewing it in the Scene. The typical outcome of this process is that input data that was needed when the shader was going through early development is now surplus fluff once the desired effect has been obtained, and it's easy to forget what changes were made when/if the process drags on for an extended period. However, these redundant data values can cost the GPU valuable time, as they must be fetched from memory even if they are not explicitly used by the shader. So, we should double-check our shaders to ensure that all of their input geometry, vertices, and fragment data is actually being used.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Exposing only necessary variables</h1>
                
            
            
                
<p>Exposing unnecessary variables from our shader to the accompanying Material can be costly, as the GPU can't assume these values are constant, which means the compiler cannot compile away these values. This data must be pushed from the CPU with every pass since they can be modified at any time through a Material object's methods such as <kbd>SetColor()</kbd> and <kbd>SetFloat()</kbd>. If we find that, toward the end of the project, we always use the same value for these variables, then they should be replaced with a constant in the shader to remove such excess runtime workload. The only cost is obfuscating what could be critical graphical effect parameters, so this should be done very late in the process.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Reducing mathematical complexity</h1>
                
            
            
                
<p>Complicated mathematics can severely bottleneck the rendering process, so we should do whatever we can to limit the damage. It is entirely possible to store a map of complex mathematical function outputs by precalculating them and placing them as floating-point data in a texture file. A texture file is, after all, just a huge blob of floating-point values that can be indexed quickly with three dimensions: <kbd>x</kbd>, <kbd>y</kbd>, and color (<kbd>rgba</kbd>).</p>
<p>We can feed this texture into the shader and sample the pre-generated table in the shader at runtime instead of completing a complex calculation at runtime.</p>
<p>We may not see any improvement with functions such as <kbd>sin()</kbd> and <kbd>cos()</kbd> since they've been heavily optimized to make use of GPU architecture, but complex methods such as <kbd>pow()</kbd>, <kbd>exp()</kbd>, <kbd>log()</kbd>, and our own custom mathematical calculations can only be optimized so much and would be good candidates for simplification. This is assuming that we can easily index the result from the texture with <em>x</em> and <em>y</em> coordinates. If complex calculations are required to generate those coordinates, then it may not be worth the effort.</p>
<p>This technique will cost us additional graphics memory to store the texture at runtime and some memory bandwidth, but if the shader has already been receiving a texture (which they are, in most cases), but the alpha channel is not being used, then we could sneak the data in through the texture's alpha channel, costing us literally no performance since that data has already been passed through anyway. This will involve hand-editing our art assets to include such data in any unused color channel(s), possibly requiring coordination between programmers and artists, but is a very good way of saving shader processing costs with no runtime sacrifices.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Reducing texture sampling</h1>
                
            
            
                
<p>Texture sampling is at the core of all memory bandwidth costs. The fewer textures we use, and the smaller we make them, the better. The more we use, the more cache misses we are likely to invoke, and the larger they are, the more memory bandwidth is consumed transferring them to the texture cache. Such situations should be simplified as much as possible to avoid severe GPU bottlenecks.</p>
<p>Even worse, sampling textures in a non-sequential order would likely result in some very costly cache misses for the GPU to suffer through. So, if this is being done, then the texture should be reordered so that it can be sampled in a more sequential order. For example, if we're sampling by inverting the <kbd>x</kbd> and <kbd>y</kbd> coordinates (for example, <kbd>tex2D(y, x)</kbd> instead of <kbd>tex2D(x, y)</kbd>), the texture lookup would iterate through the texture vertically, then horizontally, inflicting a cache-miss almost every iteration. A lot of performance could be saved by simply rotating the texture file data and performing a sample in the correct order (<kbd>tex2D(x,y)</kbd>).</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Avoiding conditional statements</h1>
                
            
            
                
<p>When conditional statements are run through a modern-day CPU, they undergo a lot of clever predictive techniques to make use of <em>instruction-level parallelism</em>. This is a feature where the CPU attempts to predict which direction a conditional statement will go in before it has actually been resolved and speculatively begins processing the most likely result of the conditional using any free cores that aren't being used to resolve the conditional (fetching some data from memory, copying some floating-point values into unused registers, and so on). If it turns out that the decision is wrong, then the current result is discarded, and the proper path is taken instead. So long as the cost of speculative processing and discarding false results is less than the time spent waiting to decide the correct path, and it is right more often than it is wrong, then this is a net gain for the CPU's speed.</p>
<p>However, this feature is less beneficial for GPU architecture because of its parallel nature. The GPU's cores are typically managed by some higher-level construct that instructs all cores under its command to perform the same machine code-level instruction simultaneously, such as a huge stamping machine that stamps sheets of metal in groups simultaneously. So, if the fragment shader requires <kbd>float</kbd> to be multiplied by <kbd>2</kbd>, then the process will begin by having all cores copy data into the appropriate registers in one coordinated step. Only when all cores are finished copying to the registers will the cores be instructed to begin the second step: multiplying all registers by <kbd>2</kbd> all in a second simultaneous action.</p>
<p>Hence, when this system stumbles onto a conditional statement, it cannot resolve the two statements independently. It must determine how many of its child cores will go down each path of the conditional, grab the list of required machine code instructions for one path, resolve them for all cores taking that path, and repeat these steps for each path until all possible paths have been processed. So, for an <kbd>if-else</kbd> statement (two possibilities), it will tell one group of cores to process the <kbd>true</kbd> path, and then ask the remaining cores to process the <kbd>false</kbd> path. Unless every core takes the same path, it must process both paths every time.</p>
<p>So, we should avoid branching and conditional statements in our shader code. Of course, this depends on how essential the conditional is to achieving the graphical effect we desire. However, if the conditional is not dependent on per-pixel behavior, then we would often be better off absorbing the cost of unnecessary mathematics than inflicting a branching cost on the GPU. </p>


            

            
        
    

        

                            
                    <h1 class="header-title">Reducing data dependencies</h1>
                
            
            
                
<p>The compiler will try its best to optimize our shader code into the more GPU-friendly low-level language so that it is not waiting on data to be fetched when it could be processing some other task. For example, the following poorly optimized code could be written in our shader:</p>
<pre>float sum = input.color1.r;<br/>sum = sum + input.color2.g;<br/>sum = sum + input.color3.b;<br/>sum = sum + input.color4.a;<br/>float result = calculateSomething(sum);</pre>
<p>This code has a data dependency such that each calculation cannot begin until the last finishes due to the dependency on the <kbd>sum</kbd> variable. However, such situations are often detected by the shader compiler and optimized into a version that uses instruction-level parallelism. The following code is the high-level code equivalent of the resulting machine code after the previous code is compiled:</p>
<pre>float sum1, sum2, sum3, sum4;<br/>sum1 = input.color1.r;<br/>sum2 = input.color2.g;<br/>sum3 = input.color3.b;<br/>sum4 = input.color4.a;<br/>float sum = sum1 + sum2 + sum3 + sum4;<br/>float result = CalculateSomething(sum);</pre>
<p>In this case, the compiler would recognize that it can fetch the four values from memory in parallel and complete the summation once all four have been fetched independently via thread-level parallelism. This can save a lot of time relative to performing the four fetches one after another.</p>
<p>However, long chains of data dependency that cannot be compiled away can absolutely murder shader performance. If we create a strong data dependency in our shader's source code, then it has no freedom to make any optimizations. For example, the following data dependency would be painful on performance, as one step literally cannot be completed without waiting on another to fetch data, since sampling each texture requires sampling another texture beforehand, and the compiler cannot assume that the data hasn't changed in the meantime.</p>
<p>The following code represents a very strong data dependency between instructions since each relies on texture data being sampled from the previous instruction:</p>
<pre>float4 val1 = tex2D(_tex1, input.texcoord.xy);<br/>float4 val2 = tex2D(_tex2, val1.yz); // requires data from _tex1<br/>float4 val3 = tex2D(_tex3, val2.zw); // requires data from _tex2</pre>
<p>Strong data dependencies such as these should be avoided whenever possible.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Surface Shaders</h1>
                
            
            
                
<p>Unity's Surface Shaders are a simplified form of fragment shaders, allowing Unity developers to get to grips with shader programming in a more simplified fashion. The Unity Engine takes care of converting our Surface Shader code for us, abstracting away some of the optimization opportunities we have just covered. However, it does provide some miscellaneous values that can be used as replacements, which reduce accuracy but simplify the mathematics in the resulting code. Surface Shaders are designed to handle the general case fairly efficiently, but optimization is best achieved with a personal touch by writing our own shaders.</p>
<p>The <kbd>approxview</kbd> attribute will approximate the view direction, saving costly operations. The <kbd>halfasview</kbd> attribute will reduce the precision of the view vector, but beware of its effect on mathematical operations involving multiple-precision types. The <kbd>noforwardadd</kbd> attribute will limit the shader to only considering a single directional light, reducing draw calls, since the shader will render in only a single pass, and lighting complexity. Finally, the <kbd>noambient</kbd> attribute will disable ambient lighting in the shader, removing some extra mathematical operations that we may not need.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Use shader-based LOD</h1>
                
            
            
                
<p>We can force Unity to render distant objects using simpler shaders, which can be an effective way of saving Fill Rate, particularly if we're deploying our game onto multiple platforms or supporting a wide range of hardware capability. The <kbd>LOD</kbd> keyword can be used in the shader to set the onscreen size factor that the shader supports. If the current LOD level does not match this value, it will drop to the next fallback shader and so on until it finds the shader that supports the given size factor. We can also change a given shader object's LOD value at runtime using the <kbd>maximumLOD</kbd> property.</p>
<p>This feature is similar to the mesh-based LOD covered earlier and uses the same LOD values for determining object form factor, so it should be configured as such.</p>
<p>Check out <a href="https://docs.unity3d.com/Manual/SL-ShaderLOD.html">https://docs.unity3d.com/Manual/SL-ShaderLOD.html</a> in the Unity documentation for more information on shader-based LOD.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using less texture data</h1>
                
            
            
                
<p>This approach is simple, straightforward, and always a good idea to consider. Reducing texture quality, either through resolution or bit rate, is not ideal for graphical quality, but we can sometimes get away with using 16-bit textures without any noticeable degradation.</p>
<p>Mipmaps (explored in <a href="">Chapter 4</a>, <em> Optimizing</em> <em> Your Art Assets</em>) are another excellent way of reducing the amount of texture data being pushed back and forth between VRAM and the Texture Cache. Note that the Scene window has a Mipmaps shading mode, which will highlight textures in our scene blue or red, depending on whether the current texture scale is appropriate for the current Scene window's camera position and orientation. This will help identify what textures are good candidates for further optimization.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Testing different GPU texture compression formats</h1>
                
            
            
                
<p>As you learned in <a href="">Chapter 4</a>, <em>Optimizing Your Art Assets</em>, there are different texture compression formats, which can reduce our application's disk footprint (executable file size), runtime CPU, and RAM usage. These compression formats are designed to support GPU architecture for the given platform. There are many different formats, such as DXT, PVRTC, ETC, and ASTC, but only a handful of these are available on a given platform.</p>
<p>By default, Unity will pick the best compression format determined by the Compression setting for a texture file. If we drill down into platform-specific options for a given texture file, then different compression type options will be available, listing the different texture formats the given platform supports. We may be able to find some space or performance savings by overriding the default choices for compression.</p>
<p>Although beware that if we're at the point where individually tweaking texture compression techniques is necessary, then hopefully we have already exhausted all other options for reducing memory bandwidth. By going down this road, we would be committing ourselves to support many different devices each in their own specific way. Many developers would prefer to keep things simple with a general solution instead of personal customization and time-consuming handiwork for small performance gains.</p>
<p>Check out the Unity documentation for an overview of all of the different texture formats available and which formats Unity prefers by default at <a href="https://docs.unity3d.com/Manual/class-TextureImporterOverride.html">https://docs.unity3d.com/Manual/class-TextureImporterOverride.html</a>.</p>
<p>In older versions of Unity, all formats were exposed for Advanced texture types, but if the platform did not support the given type, it would be handled at the software level. In other words, the CPU would need to stop and recompress the texture to the desired format the GPU wants, as opposed to the GPU taking care of it with a specialized hardware chip. Unity Technologies decided to remove this capability in more recent versions so that we can't accidentally cause these problems.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Minimizing texture swapping</h1>
                
            
            
                
<p>This one is fairly straightforward. If memory bandwidth is a problem, then we need to reduce the amount of texture sampling we're doing. There aren't really any special tricks to exploit here since memory bandwidth is all about throughput, so the primary metric under consideration is the volume of data we're pushing.</p>
<p>One way to reduce volume is to simply lower texture resolution and, hence, quality. This is obviously not ideal, so another approach is to find clever ways to reuse textures on different meshes, but using different Material and shader properties. For instance, a properly darkened brick texture may appear to look like a stone wall instead. Of course, this will require different Render States, and hence, we won't save on draw calls, but it could reduce memory bandwidth consumption.</p>
<p>Did you ever notice how clouds and bushes looked exactly the same in Super Mario Bros but with different colors? This is the same concept.</p>
<p>There could also be ways to combine textures into atlases to reduce the number of swaps needed. If there are a group of textures that are always used together at similar times, then they could potentially be merged together. This could save the GPU from having to pull in separate texture files over and over again during the same frame.</p>
<p>Finally, removing textures from the application entirely is always the last resort option we could employ.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">VRAM limits</h1>
                
            
            
                
<p>One last consideration related to textures is how much VRAM we have available. Most texture transfer from CPU to GPU occurs during initialization, but can also occur when a non-existent texture is first required by the current view. This process is normally asynchronous and will result in a blank texture being used until the full texture is ready for rendering (refer to <a href="">Chapter 4</a>, <em>Optimizing Your Art Assets</em>, to note that this assumes read/write access is disabled for the texture). As such, we should avoid introducing new textures at runtime too frequently.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Preloading textures with hidden GameObjects</h1>
                
            
            
                
<p>The blank texture that is used during asynchronous texture loading can be jarring when it comes to game quality. We would like a way to control and force the texture to be loaded from disk to RAM and then to VRAM before it is actually needed.</p>
<p>A common workaround is to create a hidden <kbd>GameObject</kbd> that uses the texture and place it somewhere in the scene on the route that the player will take toward the area where it is actually needed. As soon as the player looks at that object, the texture is needed by the Rendering Pipeline (even if it's technically hidden), it will begin the process of copying the data from RAM to VRAM. This is a little clunky but easy to implement and works sufficiently well in most cases.</p>
<p>We can also control such behavior via Script code by changing a Material's <kbd>texture</kbd> property:</p>
<pre>GetComponent&lt;Renderer&gt;().material.texture = textureToPreload;</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Avoid texture thrashing</h1>
                
            
            
                
<p>In the rare event that too much texture data is loaded into VRAM, and the required texture is not present, the GPU will need to request it from RAM and overwrite one or more existing textures to make room for it. This is likely to worsen over time as the memory becomes fragmented, and it introduces a risk that the texture just flushed from VRAM needs to be pulled again within the same frame. This will result in a serious case of memory thrashing and should be avoided at all costs.</p>
<p>This is less of a concern on modern consoles such as the PS4, Xbox One, and Wii U since they share a common memory space for both CPU and GPU. This design is a hardware-level optimization, given the fact that the device is always running a single application, and almost always rendering 3D graphics. However, most other platforms must share time and space with multiple applications, where a GPU is merely an optional device and is not always present. They, therefore, feature separate memory spaces for the CPU and GPU, and we must ensure that the total texture usage at any given moment remains below the available VRAM of the target hardware.</p>
<p>Note that this thrashing is not precisely the same as hard disk thrashing, where memory is copied back and forth between the main memory and the virtual memory (the swap file), but it is analogous. In either case, data is being unnecessarily copied back and forth between two regions of memory because too much data is being requested in too short a time period for the smaller of the two memory regions to hold it all.</p>
<p>Thrashing such as this can be a common cause of dreadful rendering performance when games are ported from modern consoles to desktop platforms and should be treated with care.</p>
<p>Avoiding this behavior may require customizing texture quality and file sizes on a per-platform and per-device basis. Be warned that some players are likely to notice these inconsistencies if we're dealing with hardware from the same console or desktop GPU generation. As many of us will know, even small differences in hardware can lead to a lot of apples-versus-oranges comparisons, but hardcore gamers tend to expect a similar level of quality across the board.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Lighting optimization</h1>
                
            
            
                
<p>We covered the theory of lighting behavior earlier in this chapter, so let's run through some techniques we can use to improve lighting costs.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using real-time shadows responsibly</h1>
                
            
            
                
<p>As mentioned previously, shadowing can easily become one of the largest consumers of draw calls and Fill Rate, so we should spend the time to tweak these settings until we get the performance and/or graphical quality we need. </p>
<p>There are multiple important settings for shadowing that can be found under Edit | Project Settings | Quality | Shadows. As far as the Shadows option is concerned, Soft Shadows are expensive, Hard Shadows are cheap, and No Shadows are free. Shadow Resolution, Shadow Projection, Shadow Distance, and Shadow Cascades are also important settings that affect the performance of our shadows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/0a9db04b-a94b-4c06-bf69-2a5fe8d7e102.png"/></p>
<p>Shadow Distance is a global multiplier for runtime shadow rendering. There is little point in rendering shadows at a great distance from the camera, so this setting should be configured specific to our game and how much shadowing we expect to witness during gameplay. It is also a common setting that is exposed to the user in an options screen, so they can choose how far to render shadows to get the game's performance to match their hardware (at least on desktop machines).</p>
<p>Higher values of Shadow Resolution and Shadow Cascades will increase our memory bandwidth and Fill Rate consumption. Both of these settings can help to curb the effects of artifacts generated by shadow rendering, but at the cost of much larger shadow map texture sizes, costing increased memory bandwidth and VRAM.</p>
<p>The Unity documentation contains an excellent summary of the topic of the aliasing effect of shadow maps and how the Shadow Cascades feature helps to solve the problem at <a href="http://docs.unity3d.com/Manual/DirLightShadows.html">http://docs.unity3d.com/Manual/DirLightShadows.html</a>.</p>
<p>It's worth noting that Soft Shadows do not consume any more memory or CPU overhead relative to Hard Shadows, as the only difference is a more complex shader. This means that applications with enough Fill Rate to spare can enjoy the improved graphical fidelity of Soft Shadows.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using culling masks</h1>
                
            
            
                
<p>A <kbd>Light</kbd> component's Culling Mask property is a layer-based mask that can be used to limit the objects that will be affected by the given light. This is an effective way of reducing lighting overhead, assuming that the layer interactions also make sense with how we are using layers for physics optimization. Objects can only be a part of a single layer, and reducing physics overhead probably trumps lighting overhead in most cases; hence, if there is a conflict, then this may not be the ideal approach.</p>
<p>Note that there is limited support for Culling Masks when using Deferred Shading. Due to the way it treats lighting in a very global fashion, only four Layers can be disabled from the mask, limiting our ability to optimize its behavior.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using baked lightmaps</h1>
                
            
            
                
<p>Baking lighting and shadowing into a scene is significantly less processor-intensive than generating them at runtime. The downside is the added application disk footprint, memory consumption, and potential for memory bandwidth abuse. Ultimately, unless a game's lighting effects are being handled exclusively through the legacy Vertex-Lit Shading format or through a single <kbd>DirectionalLight</kbd> instance, then it should probably include lightmapping somewhere to make some huge budget savings on lighting calculations. Relying entirely on real-time lighting and shadows is a recipe for disaster due to the performance costs they are likely to inflict.</p>
<p>Several metrics can affect the cost of lightmapping, however, such as their resolution, compression, whether we are using pre-computed real-time GI, and, of course, the number of objects in our scene. The lightmapper generates textures that span all of the objects marked Lightmap Static in the scene, and, hence, the more we have, the more texture data must be generated for them.</p>
<p>This would be an opportunity to make use of additive or subtractive scene loading to minimize how many objects need to be processed in each frame. This, of course, pulls in even more lightmap data while more than one scene is loaded, so we should expect a big bump in memory consumption each time this happens, only to have it freed once the old scene is unloaded.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Optimizing rendering performance for mobile devices</h1>
                
            
            
                
<p>Unity's ability to deploy to mobile devices has contributed greatly to its popularity among hobbyist, small, and mid-size development teams. As such, it would be prudent to cover some approaches that are more beneficial for mobile platforms than for desktop and other devices. Let's take a look at a few of these approaches. </p>
<p>Note that any, or all, of the following approaches may eventually become obsolete, at least for newer devices. The capabilities of mobile devices have advanced blazingly fast, and the following techniques as they apply to mobile devices merely reflect conventional wisdom from the last half-decade or so. We should test the assumptions behind these approaches to check whether the limitations of mobile devices still fit the mobile marketplace.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Avoiding alpha testing</h1>
                
            
            
                
<p>Mobile GPUs haven't quite reached the same levels of chip optimization as desktop GPUs, and alpha testing remains a particularly costly task on mobile devices. In most cases, it should simply be avoided in favor of alpha blending.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Minimizing draw calls</h1>
                
            
            
                
<p>Mobile applications are more often bottlenecked on draw calls than on Fill Rate. Not that Fill Rate concerns should be ignored (nothing should, ever!), but this makes it almost necessary for any mobile application of reasonable quality to implement mesh combining, batching, and atlasing techniques from the very beginning. Deferred Rendering is also the preferred technique, as it fits well with other mobile-specific concerns, such as avoiding transparency and having too many animated characters, but of course, not all mobile devices and graphics APIs support it.</p>
<p>Check out the Unity documentation for more information on which platforms/APIs support Deferred Shading at <a href="https://docs.unity3d.com/Manual/RenderingPaths.html">https://docs.unity3d.com/Manual/RenderingPaths.html</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Minimizing Material count</h1>
                
            
            
                
<p>This concern goes hand in hand with the concepts of batching and atlasing. The fewer Materials we use, the fewer draw calls required. This strategy will also help with concerns relating to VRAM and memory bandwidth, which tend to be very limited on mobile devices.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Minimizing texture size</h1>
                
            
            
                
<p>Most mobile devices feature a very small texture cache relative to desktop GPUs. There are very few devices on the market still supporting OpenGL ES 1.1 or lower, such as the iPhone 3G, but these devices could only support a maximum texture size of 1024 x 1024. Devices supporting OpenGLES 2.0, such as everything from the iPhone 3GS to the iPhone 6S, can support textures up to 2048 x 2048. Finally, devices supporting OpenGLES 3.0 or greater, such as devices running iOS 7, can support textures up to 4096 x 4096.</p>
<p>There are way too many Android devices to list here, but the Android developer portal gives a handy breakdown of OpenGLES device support. This information is updated regularly to help developers to determine supported APIs in the Android market at <a href="https://developer.android.com/about/dashboards/index.html">https://developer.android.com/about/dashboards/index.html</a>.</p>
<p>Double-check the device hardware we are targeting to be sure that it supports the texture file sizes we wish to use.  However, later-generation devices are never the most common devices in the mobile marketplace. If we wish our game to reach a wide audience (increasing its chances of success), then we must be willing to support weaker hardware.</p>
<p>Note that textures that are too large for the GPU will be downscaled by the CPU during initialization. This wastes valuable loading time and is going to leave us with unintended loss of quality due to an uncontrolled reduction in resolution. This makes texture reuse of paramount importance for mobile devices due to the limited VRAM and texture cache sizes available.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Making textures square and the power-of-two</h1>
                
            
            
                
<p>We have already covered this topic in <a href="">Chapter 4</a>, <em>Optimizing Your Art Assets</em>, but it is worth revisiting the subject of GPU-level Texture Compression. The GPU will find it difficult, or simply be unable, to compress the texture if it is not in a square format, so make sure that you stick to the common development convention and keep things square and sized to a power-of-two.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using the lowest possible precision formats in shaders</h1>
                
            
            
                
<p>Mobile GPUs are particularly sensitive to precision formats in its shaders, so the smallest formats should be used, such as <kbd>half</kbd>. On a related note, precision format conversion should be avoided at all costs for the same reason.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>If you've made it this far without skipping ahead, then congratulations are in order. That was a lot of information to absorb for just one subsystem of the Unity Engine, but then it is clearly the most complicated of them all, requiring a matching depth of explanation. Hopefully, you've learned a lot of approaches to help you to improve your rendering performance and enough about the Rendering Pipeline to know how to use them responsibly.</p>
<p>By now, we should be used to the idea that, except for algorithm improvements, every performance enhancement we implement will come with some related cost that we must be willing to bear for the sake of removing one bottleneck. We should always be ready to implement multiple techniques until we've squashed them all, and potentially spend a lot of additional development time to implement and test some performance-enhancing features.</p>
<p>In the next chapter, let's bring performance optimization into the modern era by exploring some performance improvements we can apply to VR and AR projects.</p>


            

            
        
    </body></html>