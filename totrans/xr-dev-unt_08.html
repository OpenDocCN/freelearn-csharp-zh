<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-150"><a id="_idTextAnchor026"/>8</h1>
<h1 id="_idParaDest-151">Building Advanced XR Techniques</h1>
<p>Up until now, you have explored various parts of XR development, crafting immersive and interactive XR applications suited for a wide array of use cases. As you venture further into mastering XR development, it’s essential to not only be proficient at creating basic to intermediate XR applications but also master advanced techniques that elevate the commercial viability and influence of your XR offerings.</p>
<p>This chapter is designed to familiarize you with crucial, high-level XR methods, all through a hands-on approach. You will dive into hand-tracking integration, use eye- and head-tracking for complex interactions, and discover how to establish a multiplayer server to create an engaging multiplayer XR experience.</p>
<p>The content of this chapter might sound intimidating but don’t worry – we’re here to guide you every step of the way as you incorporate these sophisticated XR strategies into various scenes. Leveraging the solid XR foundation you’ve built from earlier chapters, you’ll find these advanced techniques more intuitive than anticipated.</p>
<p>Regardless of the XR equipment you possess, this chapter promises a wealth of knowledge, with all techniques being adaptable to different setups. We’ll delve deeper into this in the <em class="italic">Technical </em><em class="italic">requirements</em> section.</p>
<p>This chapter includes the following sections:</p>
<ul>
<li>Adding hand-tracking to XR experiences</li>
<li>Interacting with objects in XR experiences via eye- or head-tracking</li>
<li>Building a VR multiplayer application</li>
</ul>
<h1 id="_idParaDest-152">Technical requirements</h1>
<p>Navigating the hand-tracking, eye-tracking, and multiplayer aspects of this chapter requires understanding both shared and unique technical prerequisites. For a seamless development experience, we recommend acquainting yourself with all listed requirements. Though this chapter delves into advanced topics and may initially seem daunting compared to earlier sections, you can be sure that all tutorials in this chapter are designed for straightforward execution – even if you don’t have a VR headset at hand.</p>
<p>First, let’s address the overarching technical requirements. To follow along with the tutorials in this chapter, you’ll need Unity <em class="italic">2021.3 LTS</em> or a newer version. Validate your hardware’s suitability by comparing it to the system requirements described on Unity’s website: <a href="https://docs.unity3d.com/Manual/system-requirements.html">https://docs.unity3d.com/Manual/system-requirements.html</a>. Depending on your VR headset’s specifications, ensure that your setup supports <em class="italic">Windows</em>/<em class="italic">Linux</em>/<em class="italic">Mac</em> or <em class="italic">Android </em><em class="italic">Build Support</em>.</p>
<p>Most contemporary VR headsets, particularly those with inside-out camera tracking, incorporate hand-tracking capabilities. Examples include the Meta Quest series, HTC Vive Cosmos Elite, PlayStation VR2, Pico Neo 2 Eye, the Lynx-R1, Valve Index, HP Reverb G2, Varjo VR-3, and the soon-to-be-released Apple Vision Pro. Always refer to the technical specifications of your VR headset to find out whether it supports hand-tracking.</p>
<p>If you don’t have access to a headset that supports hand-tracking, you can still follow this chapter as the XR Device Simulator can replicate the hand-tracking features of a VR headset flawlessly.</p>
<p>Eye-tracking in VR is an evolving field, and interestingly, you can delve into the eye-tracking tutorials of this chapter’s eye- and head-gaze-tracking section regardless of your VR headset’s specifications. Even if you’re working solely with the XR Device Simulator without a physical VR headset, you’ll find our tutorial accessible. While we won’t spoil all the details now, it’s worth noting that the XR Interaction Toolkit provides innovative solutions for situations where a VR headset lacks standard eye-tracking capabilities. As of the time of this book’s publication, VR headsets that offer eye-tracking include the PlayStation VR2, HP Reverb G2 Omnicept Edition, Pico Neo 3 Pro Eye, HTC Vive Pro Eye, Pico Neo 2 Eye, and Meta Quest Pro. Furthermore, the upcoming Apple Vision Pro is anticipated to feature eye-tracking. This list might not cover all available options by the time you read this book, so always check the specifications of your VR headset to confirm eye-tracking support.</p>
<p>Now that your hardware is all set up, let’s start exploring hand-tracking using the XR Interaction Toolkit.</p>
<h1 id="_idParaDest-153">Adding hand-tracking to XR experiences</h1>
<p>In this section, you<a id="_idIndexMarker633"/> will learn how to add a hand-tracking functionality to your XR experiences. Before creating a new Unity project, however, you<a id="_idIndexMarker634"/> must understand the technical concept of hand-tracking and how far it can enrich an XR experience compared to regular controllers.</p>
<h2 id="_idParaDest-154">Understanding hand-tracking and potential use cases</h2>
<p>At its core, <strong class="bold">hand-tracking</strong> in VR <a id="_idIndexMarker635"/>refers to the technological capability of directly detecting, capturing, and interpreting the nuanced movements and positioning of a user’s bare hands and fingers within a virtual environment.</p>
<p>Unlike controller-based tracking, which relies on external devices to mediate and translate user inputs into VR actions such as an Xbox controller, hand-tracking operates without intermediary hardware, offering a direct mapping of real-world hand gestures and movements into the virtual realm. This approach leverages sophisticated sensors, cameras, and algorithms to construct a real-time, dynamic model of the user’s hand. From grabbing objects to casting spells with finger gestures, hand-tracking provides a vast array of potential interactions. It facilitates complex and subtle interactions that are hard to replicate with traditional controllers, permitting more organic and intuitive interactions within VR.</p>
<p>For a VR headset to harness the potential of hand-tracking, it should be equipped with high-resolution sensors and cameras capable of capturing detailed movements, down to the subtle motions of individual fingers. These cameras often need to be oriented in a manner that provides a wide field of view to consistently track hands as they move.</p>
<p>Hand-tracking requires real-time interpretation of complex hand and finger movements, necessitating robust processing power. The VR headset should have an onboard processor or be connected to a machine that can handle these computations without latency.</p>
<p>Beyond hardware, the headset’s software must be designed or adaptable to recognize and interpret hand movements effectively. This includes having algorithms capable of differentiating between intentional gestures and inadvertent hand motions.</p>
<p>Building on these foundational requirements, the next section will guide you through setting up our Unity project to effectively utilize and enable hand-tracking, unlocking a richer, more immersive experience for users of your XR experiences.</p>
<h2 id="_idParaDest-155">Implementing hand-tracking with the XR Interaction Toolkit</h2>
<p>To kick off <a id="_idIndexMarker636"/>our <a id="_idIndexMarker637"/>process of adding hand-tracking capabilities to an XR project, we must perform the following steps:</p>
<ol>
<li>Go to Unity Hub and create a new project by navigating to the <code>AdvancedXRTechniques</code>, and clicking the <strong class="bold">Create </strong><strong class="bold">project</strong> button.</li>
<li>In the <strong class="bold">Scene Hierarchy</strong> window, you will see that your scene only contains <strong class="bold">Main Camera</strong> and <strong class="bold">Directional Light</strong>. You can delete <strong class="bold">Main Camera</strong> as it is not needed.</li>
<li>Import <code>com.unity.xr.interaction.toolkit</code>, and hitting <em class="italic">Enter</em>. The toolkit should now be automatically added to your project. Staying in the <strong class="bold">Package Manager</strong> window, navigate to the <strong class="bold">Samples</strong> tab of the newly added <strong class="bold">XR Interaction Toolkit</strong> package and import <strong class="bold">Starter Assets</strong>, <strong class="bold">XR Device Simulator</strong>, and <strong class="bold">Hands Interaction Demo</strong> by clicking the <strong class="bold">Import</strong> button next to each of them.</li>
<li>To enable hand-tracking in our scene, we don’t only need <code>com.unity.xr.hands</code>, and hitting <em class="italic">Enter</em>. Once the package has been added to your project, navigate to the <strong class="bold">Samples</strong> tab of the <strong class="bold">XR Hands</strong> package in the <strong class="bold">Package Manager</strong> window and click the <strong class="bold">Import</strong> button next to the <strong class="bold">HandVisualizer</strong> sample.</li>
<li>Now, we would typically drag and drop the <code>XR Interaction Hands Setup</code> into the search bar of the <code>0</code>,<code>0</code>,<code>0</code>).</li>
<li>It is <a id="_idIndexMarker638"/>time to set up <strong class="bold">XR Plug-in Management</strong> correctly to enable hand-tracking. Navigate to <strong class="bold">Edit</strong> | <strong class="bold">Project Settings</strong> | <strong class="bold">XR Plug-in Management</strong> and select the <strong class="bold">OpenXR</strong> checkbox on either the <strong class="bold">Windows/Mac/Linux</strong> tab or <strong class="bold">Android</strong>, depending on the needs of your VR headset.</li>
<li>Once <a id="_idIndexMarker639"/>the <strong class="bold">OpenXR</strong> plugin has been installed, navigate to its subtab underneath the <strong class="bold">XR Plug-in Management</strong> tab on the left-hand side. Here, go to either the <strong class="bold">Windows/Mac/Linux</strong> or <strong class="bold">Android</strong> tab. Select the <strong class="bold">+</strong> button to add an <strong class="bold">Interaction Profile</strong> item to your project. Besides selecting the controllers of your VR headset in the newly opened menu, you should also add another <strong class="bold">Interaction Profile</strong> called <strong class="bold">Hands Interaction Profile</strong>. At its core, <strong class="bold">Hand Interaction Profile</strong> in <strong class="bold">OpenXR</strong> provides a standardized way to interpret hand gestures and movements across different VR headsets. Different VR headsets might have their own technology and methods for tracking hands. Without a standardized system, developers would need to write unique code for each headset’s hand-tracking system, which can be time-consuming and impractical.</li>
<li>Staying in the <strong class="bold">OpenXR</strong> subtab, select the <strong class="bold">Hand Interaction Poses</strong> and <strong class="bold">Hand Tracking Subsystem</strong> checkboxes, regardless of which VR headset you have.<p class="list-inset">When you check <strong class="bold">Hand Interaction Poses</strong>, you are telling Unity to use a standard set of poses or gestures defined by the <strong class="bold">OpenXR</strong> standard. These include grabbing (grip), pointing (aim), pinching, and poking. So, instead of manually coding the detection of these gestures, Unity does it for you based on this standard. By selecting the <strong class="bold">Hand Tracking Subsystem</strong> checkbox, Unity uses the <strong class="bold">OpenXR</strong> standard to keep track of where the hands are and how they move. This subsystem is like the engine under the hood that keeps an eye on hand movements.</p><p class="list-inset">If you have a Meta Quest device, you must also select the <strong class="bold">Meta Hand Tracking Aim</strong> checkbox. This feature enhances the existing hand-tracking by also understanding the direction or aim of the hands, giving your VR app a better <a id="_idIndexMarker640"/>sense of where users are pointing or what they might be trying to interact with. While our <strong class="bold">Hand Interaction Profile</strong> provides a base layer of understanding hand movements, these checkboxes dive deeper into gesture specifics, actual tracking, and device-specific features.</p></li>
<li>Now, let’s <a id="_idIndexMarker641"/>add a simple plane to the scene functioning as a ground floor and a cube to interact with testing out hand-tracking. You can achieve this by right-clicking in the hierarchy and selecting <code>Ground Floor</code> and position it at the origin (<code>0</code><code>0</code><code>0</code>).</li>
</ol>
<p>Repeat the previous step but instead of selecting <code>Hand Tracking Cube</code>. Position it at (<code>0</code>, <code>1.25</code>, <code>1</code>) and scale it to (<code>0.1</code>, <code>0.1</code>, <code>0.1</code>). In the <code>Hand Tracking Cube</code>, click the <code>XR Grab Interactable</code> in the search bar. Select the <strong class="bold">XR Grab Interactable</strong> script by double-clicking on it. You should see that a <strong class="bold">Rigidbody</strong> component has been automatically added to <strong class="bold">InteractableCube</strong>, alongside the <strong class="bold">XR Grab Interactable</strong> script. Inside the <strong class="bold">Rigidbody</strong> component, make sure that the <strong class="bold">Use Gravity</strong> and <strong class="bold">Is Kinematic</strong> checkboxes are selected so that our interaction with the cube is possible while obeying the laws of gravity.</p>
<p class="callout-heading">Important note</p>
<p class="callout">If you don’t have access to a VR headset with hand-tracking capabilities, you can simply test this feature by using the XR Device Simulator, as detailed in the <em class="italic">Installing the XR Device Simulator</em> and <em class="italic">Using the XR Device Simulator</em> sections of <a href="B20869_03.xhtml#_idTextAnchor009"><em class="italic">Chapter 3</em></a>. To switch to hand-tracking mode, simply press the <em class="italic">H</em> key.</p>
<p>It’s time to test out the scene using your VR headset. Start the scene as you typically would. You don’t even need to have the VR headset’s controllers on hand. Once the scene is running, adjust your head so that the external cameras on the VR headset are directed at your hands.</p>
<p>You <a id="_idIndexMarker642"/>should notice<a id="_idIndexMarker643"/> that the controller visuals have been replaced by hand visuals, mirroring the exact position and movements of your real hands, as shown in <em class="italic">Figure 8</em><em class="italic">.1</em>.</p>
<div><div><img alt="Figure 8.1 – The hand visuals you will see once you put away your controllers and direct your gaze toward your hands" src="img/B20869_08_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – The hand visuals you will see once you put away your controllers and direct your gaze toward your hands</p>
<p>Try moving each finger separately, shift your hands around, and even hide one hand behind your back. If everything was set up correctly and your VR headset fully supports hand-tracking, the virtual hands should mimic your real hand movements accurately. If you hide one hand behind your back, its corresponding virtual hand should vanish too.</p>
<p>Finally, let’s test the interaction with the cube in your scene using only hand-tracking. Aim your right hand’s laser pointer at the cube. Touch the tips of your right thumb and right index finger together, forming a near triangle shape, as shown in <em class="italic">Figure 8</em><em class="italic">.2</em>.</p>
<div><div><img alt="Figure 8.2 – The overlayed hand visuals as you touch your right thumb and right index finger together in real life" src="img/B20869_08_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – The overlayed hand visuals as you touch your right thumb and right index finger together in real life</p>
<p>You <a id="_idIndexMarker644"/>are now grabbing the cube. While maintaining this position, point in various directions and observe the<a id="_idIndexMarker645"/> cube moving correspondingly.</p>
<p>Congratulations! You’ve now achieved a similar interactive experience in your VR scene with hand-tracking as you would have using standard VR controllers.</p>
<p>To explore how the hand-tracking features of the XR Interaction Toolkit work with other types of objects such as UI elements or buttons, delve into <strong class="bold">Hands Interaction Demo</strong>, which is available within the toolkit’s <strong class="bold">Samples</strong> area that we imported at the beginning. To access it, follow these steps:</p>
<ol>
<li>Head to <strong class="bold">Assets</strong> in the <strong class="bold">Project</strong> window. From there, navigate to <strong class="bold">Samples</strong> | <strong class="bold">XR Interaction Toolkit</strong> | <strong class="bold">Version Number</strong> | <strong class="bold">Hands Interaction Demo</strong> | <strong class="bold">Runtime</strong>.</li>
<li>Then, double-click on the <strong class="bold">HandsDemoScene</strong> Unity scene. If you prefer, you can quickly locate <strong class="bold">HandsDemoScene</strong> using the search bar in the <strong class="bold">Project</strong> window.</li>
<li>Once<a id="_idIndexMarker646"/> you’ve <a id="_idIndexMarker647"/>opened the scene in the Unity Editor, hit the <strong class="bold">Play</strong> button. This will let you experience firsthand how to engage with UI elements, press buttons, and even manipulate 3D objects using just your hands.</li>
</ol>
<p>By now, you should feel like a hand-tracking expert. The next section will introduce you to another advanced concept of XR development: eye-tracking.</p>
<h1 id="_idParaDest-156">Interacting with objects in XR experiences via eye- or head-tracking</h1>
<p>In this section, you will <a id="_idIndexMarker648"/>not only learn about eye-tracking itself and how it can enrich your XR experience, but you will also implement eye-tracking functionalities to your XR experiences, regardless of whether your VR headset supports eye-tracking or not.</p>
<h2 id="_idParaDest-157">Understanding eye-tracking and potential use cases</h2>
<p>From reading <a id="_idIndexMarker649"/>a person’s intent to enhancing digital interactions, eye-tracking technology is revolutionizing how technologies of all kinds perceive and interpret human behavior.</p>
<p>The eyes are often dubbed the “windows to the soul” due to their ability to express and convey emotions, intent, and attention. Biologically speaking, several key aspects of the eyes play into this:</p>
<ul>
<li><strong class="bold">Pupil dilation</strong>: Often a <a id="_idIndexMarker650"/>subconscious response, the pupils can dilate or contract based on emotional states, levels of attention, or reactions to stimuli. For instance, someone’s pupils might dilate upon seeing someone they’re attracted to or contract when exposed to bright light.</li>
<li><strong class="bold">Saccades</strong>: These <a id="_idIndexMarker651"/>are rapid, jerky movements of the eyes when they change focus from one point to another. Often unnoticed by us, saccades play a pivotal role in how we gather visual information from our surroundings.</li>
<li><strong class="bold">Blinking and micro-expressions</strong>: The rate of blinking can indicate various states, from<a id="_idIndexMarker652"/> relaxation to stress. Furthermore, subtle movements around the eyes can give away fleeting emotions – these are known as micro-expressions.</li>
</ul>
<p>Eye-tracking <a id="_idIndexMarker653"/>technology revolves around monitoring and recording the eye’s movement and gaze points. Here’s how it<a id="_idIndexMarker654"/> typically works:</p>
<ul>
<li><strong class="bold">Light source</strong>: Infrared light is directed toward the eyes. This light reflects off the cornea and retina.</li>
<li><strong class="bold">Sensors and cameras</strong>: These detect the reflected light off the eyes. Advanced systems might use multiple cameras from different angles to capture a three-dimensional view of the eye’s movement.</li>
<li><strong class="bold">Data processing</strong>: The raw data captured by the sensors is processed using algorithms to deduce the direction of the gaze and the point of focus.</li>
<li><strong class="bold">Representation</strong>: The gaze data is usually represented as a heatmap or gaze plot on the observed medium, be it a computer screen or a physical environment.</li>
</ul>
<p>Eye-tracking bridges the gap between the digital and real worlds by making virtual interactions more human-like. When social avatars in XR mimic real eye movements by blinking, gazing, and showing emotions, it deepens the sense of presence and immersion for the user.</p>
<p>Likewise, eye-tracking can drastically improve the understanding of user intent in the XR space. This means that XR environments can adapt in real time to the user’s focus. For example, a horror game could trigger a scare only when the user is looking in the right direction, maximizing the emotional impact. By analyzing where users look, how often, and for how long, developers can glean valuable insights. This can guide design choices, ensure important elements capture attention, and refine user interfaces.</p>
<p>Eye-tracking paves the way for more intuitive user interfaces. For instance, instead of navigating<a id="_idIndexMarker655"/> through menus using clunky hand controllers, users can simply gaze at a menu option to select it.</p>
<p>Fortunately, you can enrich your XR scene with eye-tracking, regardless of whether your VR headset supports it or not. If this sparks your curiosity, follow along with the tutorial in the next section.</p>
<h2 id="_idParaDest-158">Setting up an XR scene to support eye- and head-tracking</h2>
<p>The XR Interaction<a id="_idIndexMarker656"/> Toolkit supports eye- and head-tracking, enhancing user engagement in XR apps. While eye-tracking specifically captures where the user’s eyes are focused, head-tracking determines the direction the user’s head is pointing or gazing. Let’s try out these tracking techniques ourselves by revisiting the basic VR scene we created in our last session.</p>
<p>Some key components of the XR Interaction Toolkit’s eye- and head-tracking functionalities are already inside of our scene. To observe them, click on the arrow button next to the <strong class="bold">XR Interaction Hands Setup</strong> prefab in the <strong class="bold">Scene Hierarchy</strong> window of your project to see its children. Navigate to <strong class="bold">XR Origin (XR Rig)</strong> | <strong class="bold">Camera Offset</strong> and enable the <strong class="bold">XR Gaze Interactor</strong> and <strong class="bold">Gaze Stabilized</strong> prefabs. These prefabs are not only part of the <strong class="bold">XR Interaction Hands Setup</strong> prefab, but also the <strong class="bold">XR Interaction Setup</strong> prefab, which you would use in your scene if you don’t include hand-tracking.</p>
<p>These prefabs work with all kinds of VR headsets, regardless of whether they support eye-tracking or not. If your headset doesn’t support eye-tracking, it will use the built-in head-tracking feature of the VR headset to estimate the head gaze.</p>
<p>While incorporating eye- and head-tracking into our scene is a significant step forward, it’s only part of the equation for crafting intuitive gaze-based interactions in XR. If it were that simple, there would be no need for this chapter, especially considering we’ve utilized this prefab in every VR scene throughout this book. To truly harness the capabilities of eye-tracking, we must also populate our scene with objects that can interact <a id="_idIndexMarker657"/>with the <strong class="bold">XR Gaze Interactor</strong> prefab. Let’s create these objects:</p>
<ol>
<li>To keep our scene organized, right-click on <code>Hand Tracking Cube</code> in the <code>Eye Tracking and Hand Tracking Interactables</code>. This GameObject will store two more cubes enabling different kinds of eye-tracking interactions alongside the cube we already created.</li>
<li>Let’s place all of the cubes on a very simple table. Create the table by clicking the <code>Table</code>, position it at (<code>0</code>, <code>0.5</code>, <code>0</code>), and scale it to (<code>3</code>, <code>1</code>, <code>1</code>).</li>
<li>Scale <code>Hand Tracking Cube</code> to (<code>0.5</code>, <code>0.5</code>, <code>0.5</code>) and position it on <code>Table</code> by changing the values to (<code>-1</code>, <code>0</code>, <code>0</code>). Also, make sure you uncheck the <code>Hand Tracking Cube</code> and select the <code>Hand Tracking Cube</code>. Rename the two cubes <code>Eye Tracking Cube 1</code> and <code>Eye Tracking Cube 2</code>. Change the position of <code>Eye Tracking Cube 1</code> to (<code>0</code>, <code>0</code>, <code>0</code>) and the position of <code>Eye Tracking Cube 2</code> to (<code>1</code>, <code>0</code>, <code>0</code>).</p></li>
<li>Next, let’s add the function that when the user’s gaze is pointing at <code>Eye Tracking Cube 1</code>, a sphere will appear on top of it. We can add a sphere to our scene by selecting <code>Eye Tracking Cube 1</code> in the <code>0</code>, <code>1</code>, <code>0</code>) and scale it to (<code>0.5</code>, <code>0.5</code>, <code>0.5</code>).</li>
<li>Similar to <code>Eye Tracking Cube 1</code>, some text should appear when the user of our VR scene looks at <code>Eye Tracking Cube 2</code>. To make the interaction more complex, the text should change, depending on whether the user looks at the cube <a id="_idIndexMarker658"/>and whether the cube is selected or deselected via a controller button press. To create the needed UI elements, right-click on <code>Eye Tracking Cube 2</code> in the <code>0</code>, <code>-1</code>, <code>-1.01</code>) and its <code>1</code>. Now, right-click on <code>No state detected!</code> in the <code>Interactable Text</code>, scale it to <code>0.005</code> in all directions, and position it at (<code>0.1</code>, <code>0.1</code>,<code> 0</code>).</li>
<li>Let’s add some colorful materials to our scene to make it more visually appealing. Specifically, we want to add two materials for our cubes – one for their default appearance and one if they are hovered over. So, we will create a material for <code>Table</code> and one for <code>Materials</code> and double-click on it to open it. Create a new material inside the <code>Materials</code> folder by right-clicking, selecting <code>HighlightedCube</code>. In the <code>HighlightedCube</code>, click on the colored cell next to the <code>240</code>, G: <code>240</code>, B: <code>140</code>) with an <code>70</code>.</li>
<li>Right-click inside the <code>Materials</code> folder and select <code>CubeMaterial</code>, <code>TableMaterial</code>, and <code>SphereMaterial</code> materials. For <code>CubeMaterial</code>, we chose a sophisticated red color (R: <code>186</code>, G: <code>6</code>, B: <code>6</code>); <code>TableMaterial</code> has been assigned a dark brown color (R: <code>58</code>, G: <code>40</code>, B: <code>3</code>); and for <code>SphereMaterial</code>, we selected a blue color (R: <code>29</code>, G: <code>120</code>, B: <code>241</code>) and made it appear metallic by setting the <code>1</code>. All of these materials have an <code>255</code>. Apply <code>CubeMaterial</code> to all three cubes in the<a id="_idIndexMarker659"/> scene by simply dragging and dropping the material from the <code>TableMaterial</code> and <code>SphereMaterial</code>.</li>
</ol>
<p><em class="italic">Figure 8</em><em class="italic">.3</em> shows what your VR scene should currently look like.</p>
<div><div><img alt="Figure 8.3 – The current status of the VR scene for eye-tracking" src="img/B20869_08_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – The current status of the VR scene for eye-tracking</p>
<p>In the next section, you<a id="_idIndexMarker660"/> will learn how you can interact with these cubes via eye or head gaze.</p>
<h2 id="_idParaDest-159">Interacting with objects via eye and head gaze using the XR Interaction Toolkit</h2>
<p>When a <a id="_idIndexMarker661"/>user looks at one of the two cubes we created for gaze-tracking, they should be able to interact with them as described in the previous section. To accomplish this, we need to adjust some components of our scene.</p>
<p>The <strong class="bold">XR Gaze Interactor</strong> prefab will only interact with objects in a scene that have either an <strong class="bold">XR Simple Interactable</strong> or <strong class="bold">XR Grab Interactable</strong> script attached to them and that have the <strong class="bold">Allow Gaze Interaction</strong> checkbox of the respective script enabled. This means we must add either one of these two scripts to our two new cubes.</p>
<p>In our case, we will add the <strong class="bold">XR Simple Interactable</strong> script to each of the two eye-tracking cubes. We need to perform the following three steps to achieve this:</p>
<ol>
<li>Press <em class="italic">Ctrl</em>/<em class="italic">Cmd</em> and select both cubes in the <strong class="bold">Scene </strong><strong class="bold">Hierarchy</strong> window.</li>
<li>Click the <code>XR Simple Interactable</code> into the search bar, and double-click on the script to add it to both cubes.</li>
<li>Click the arrow next to the <strong class="bold">Gaze Configuration</strong> property of the <strong class="bold">XR Simple Interactable</strong> script component to open the gaze-related properties. Select the <strong class="bold">Allow Gaze </strong><strong class="bold">Interaction</strong> checkbox.</li>
</ol>
<p>Now, we need to specify how we can interact with our two cubes via gaze. Let’s start with the first cube. Remember that our goal is to make the blue sphere appear on top of the first cube only when our eyes, head, or controllers are directed toward it. We can add this logic to the cube by following these steps:</p>
<ol>
<li>In the <code>Eye Tracking Cube 1</code>, navigate to the end of the <strong class="bold">XR Simple Interactable</strong> script component. Click on the arrow next to <strong class="bold">Interactable Events</strong> to open it. Add two new events to the <strong class="bold">First Hover Entered</strong> function of <strong class="bold">First/Last </strong><strong class="bold">Hover</strong> by clicking on the <strong class="bold">+</strong> button two times.</li>
<li>Let’s fill in the missing information of our newly created interactable events. As indicated by the <code>Eye Tracking Cube 1</code> from the <strong class="bold">Scene Hierarchy</strong> window into the first <strong class="bold">None (Object)</strong> cell. Repeat this for the second event by assigning it to <strong class="bold">Sphere</strong>.</li>
<li>To <a id="_idIndexMarker662"/>change the material of <code>Eye Tracking Cube 1</code> when it is hovered over via controllers, the eyes, or the head, we must assign the necessary functions to each interactable event and provide them with the needed parameters. Select <code>HighlightedCube</code> material via the search bar and select it. For the <code>Eye Tracking Cube 1</code> with your controllers, eyes, or head, you will see the blue sphere appearing on top of it.</li>
<li>Once your controllers, eyes, or head are no longer directed toward <code>Eye Tracking Cube 1</code>, the cube’s material should change back to its default color and the sphere should disappear. To accomplish this, go to the <code>Eye Tracking Cube 1</code> and <code>CubeMaterial</code>, and select it. Repeat this process for <strong class="bold">Sphere</strong> by assigning it to the <strong class="bold">GameObject</strong> | <strong class="bold">SetActive (bool)</strong> function again and ensuring the newly appeared checkbox is not checked this time.</li>
<li>To<a id="_idIndexMarker663"/> evaluate the logic we’ve put in place so far, it’s necessary to either disable or hide the blue <strong class="bold">Sphere</strong> object in our scene. This ensures that upon initial entry into the scene, the user doesn’t see it. You can achieve this by selecting <strong class="bold">Sphere</strong> within the <strong class="bold">Scene Hierarchy</strong> window and then deselecting the checkbox next to its name at the top of the <strong class="bold">Inspector</strong> window.</li>
</ol>
<p>Voilà – we’ve completed all the required steps to add a powerful eye-tracking capability to our first cube. <em class="italic">Figure 8</em><em class="italic">.4</em> shows what the cube should look like once your eyes or head are directed toward it.</p>
<div><div><img alt="Figure 8.4 – The highlighted cube and blue sphere appear because of your eyes or head hovering over the cube" src="img/B20869_08_04.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – The highlighted cube and blue sphere appear because of your eyes or head hovering over the cube</p>
<p>If you are working with head gaze, you will only see the blue sphere when the cube is positioned at the center of your current field of view when you are wearing a VR headset or using the XR Device Simulator. You can observe the same effect when your controllers are pointing at the cube.</p>
<p>We will add an even more powerful interaction to our second cube. Like the first cube, a text element will appear underneath the second cube when the eyes, head, or controllers are directed toward it. This time, however, the displayed text itself will change <a id="_idIndexMarker664"/>based on whether the cube is hovered over, selected, or deselected using a combination of eye, head, and controller interactions. Follow these steps to accomplish this goal:</p>
<ol>
<li>Repeat all the steps you performed for <code>Eye Tracking Cube 1</code> with the following modification: drag and drop <code>Eye Tracking Cube 2</code> and <strong class="bold">Canvas</strong> into the two events of the <strong class="bold">First Hover Entered</strong> function and the <strong class="bold">Last Hover </strong><strong class="bold">Exited</strong> function.</li>
<li>So far, the cube’s color changes and the text element becomes visible once the cube is hovered over. However, the text itself should also change to <code>Interactable Text</code> from the <code>Hovered</code> into the newly appeared empty text cell.</li>
<li>When the cube is hovered over and selected by pressing the controller button, which is typically reserved for moving objects around, the text displayed underneath the cube should change. To implement this logic, scroll down to <code>Interactable Text</code> element from the <code>Selected</code> into the empty text field.</li>
<li>Once the cube is no longer selected via controllers, the text should change again. This can be accomplished by adding a new event to the <code>Interactable Text</code> to the <code>Deselected</code> into the newly created text cell.</li>
<li>Before testing out the logic of the second cube, hide it in the scene by deselecting it in its <strong class="bold">Inspector</strong> window, as you did with the first cube.</li>
</ol>
<p>It is time to <a id="_idIndexMarker665"/>try out interacting with this cube. <em class="italic">Figure 8</em><em class="italic">.5</em> shows the cube in its three interactable states – being hovered over, being hovered over plus selected, and being hovered over but deselected.</p>
<div><div><img alt="Figure 8.5 – The cube in its three interactable states" src="img/B20869_08_05.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – The cube in its three interactable states</p>
<p>Hurray – you <a id="_idIndexMarker666"/>have successfully added different eye-tracking interactions to your scene! In the next section, you will learn how to set up a multiplayer XR game in Unity.</p>
<h1 id="_idParaDest-160">Building a VR multiplayer application</h1>
<p>In this section, you’ll <a id="_idIndexMarker667"/>craft your first VR multiplayer application. Though it involves a fair amount of C# coding, you’ve gained enough knowledge to smoothly navigate this tutorial. But before we dive in, let’s pause and delve into the nuances of constructing multiplayer applications, the necessary components, and the appeal of multiplayer experiences within the XR context.</p>
<h2 id="_idParaDest-161">Understanding multiplayer applications and multiplayer networking systems</h2>
<p>At its core, a <strong class="bold">multiplayer</strong> experience allows multiple users to interact within a shared digital <a id="_idIndexMarker668"/>environment simultaneously. This environment can range from simple text-based interfaces to complex virtual realities. The key components of a multiplayer experience typically include servers that host the game environment, networking systems that handle data synchronization and communication, player avatars, and game logic that governs interaction rules.</p>
<p>Adding a multiplayer mode to a game can make it more unpredictable due to human behaviors and decision-making. By contrast, <strong class="bold">single-player</strong> modes are typically more controlled and can be designed around a predefined narrative.</p>
<p>By adding<a id="_idIndexMarker669"/> multiplayer capabilities to XR, users are encouraged to jointly engage in tasks, challenges, or experiences, making the environment feel more alive and dynamic. Examples include cooperative puzzle-solving, virtual team-building exercises, and joint exploration.</p>
<p class="callout-heading">Important note</p>
<p class="callout">If you are serious about becoming an XR developer, you should feel comfortable creating multiplayer XR experiences. Virtual meetups in forums such as VR Chat are among the most popular XR applications to date. These meetups underline the desire of people to hang out with other real humans in virtual worlds, instead of being only surrounded by virtual assets.</p>
<p>The most crucial part of any multiplayer <a id="_idIndexMarker670"/>game is the <strong class="bold">multiplayer networking system</strong>. At its essence, a multiplayer networking system is the digital backbone that enables various players to interact seamlessly within a shared environment. This system ensures that actions performed by one player are reflected accurately and consistently for all other players, creating a harmonized virtual experience.</p>
<p>A key component <a id="_idIndexMarker671"/>of every multiplayer networking system is <strong class="bold">servers</strong>. These are powerful computers that host the game’s digital environment. They are the central point that players connect to, and they maintain the authoritative state of the game. In some configurations, one of the players might act as a<a id="_idIndexMarker672"/> server, termed <strong class="bold">peer-to-peer</strong>, but dedicated servers are more common in larger games due to stability and scalability.</p>
<p>The individual devices or computers used by players are<a id="_idIndexMarker673"/> called <strong class="bold">client systems</strong>. They send data such as player movements or actions to the server and receive updates about the game world and other players.</p>
<p>In real-time games, even slight delays can affect gameplay. Multiplayer networking systems use techniques such as <strong class="bold">lag compensation</strong> to make sure players have smooth experiences. This involves predicting movements or actions and then reconciling differences once actual data arrives.</p>
<p>To avoid multiplayer games becoming targets for cheating or hacking, networking systems employ measures such as data encryption, authoritative servers, and cheat detection tools.</p>
<p>Beyond <a id="_idIndexMarker674"/>gameplay, players often wish to communicate, be it through text, voice, or other mediums. Networking systems provide the infrastructure for these interactions, ensuring real-time and clear communication.</p>
<p>Here is an overview of the three main multiplayer network providers that are interesting for Unity developers:</p>
<ul>
<li><strong class="bold">Photon Unity Networking</strong> (<strong class="bold">PUN</strong>): PUN is a solution tailored for Unity’s multiplayer <a id="_idIndexMarker675"/>games. Its cloud-based approach means that developers don’t need to worry about server creation or maintenance. PUN offers a free tier. Although it comes with certain restrictions, the free version is heavily used by indie developers and hobbyists who are starting their journey into multiplayer game development.</li>
<li><strong class="bold">Mirror</strong>: Mirror<a id="_idIndexMarker676"/> offers a community-powered, open source networking tool tailored for Unity. It’s an evolution of the deprecated Unity networking system. Being open source, it doesn’t impose licensing fees, making it an economical choice. Mirror is renowned for its flexibility and provides developers with a greater degree of control over their multiplayer logic. However, its customizable nature means that it presents a slightly more challenging learning curve for beginners.</li>
<li><strong class="bold">Netcode</strong>: Netcode<a id="_idIndexMarker677"/> is Unity’s proprietary solution for game networking, formerly known as <em class="italic">MLAPI</em>. This <a id="_idIndexMarker678"/>evolution and rebranding signifies Unity’s commitment to continuous improvement and its dedication to providing developers with top-tier tools for multiplayer game development. Being an intrinsic Unity solution, Netcode promises seamless integration with Unity’s ecosystem.</li>
</ul>
<p>In the following sections, we’ll create a multiplayer game of our own. For this, we’ll be leveraging Photon PUN 2 using the free tier. Its zero-cost barrier, combined with an intuitive setup process for beginners, makes it an ideal candidate for rapid prototyping and smaller-scale projects.</p>
<p>The next section will teach you how to set up PUN for the VR multiplayer game we are about to create.</p>
<h2 id="_idParaDest-162">Setting up PUN for our VR multiplayer game</h2>
<p>Our <a id="_idIndexMarker679"/>objective for this VR multiplayer game is both straightforward and robust, encompassing all essential elements needed for a multiplayer experience. We aim to design a VR scene where multiple users can join concurrently. Users should see themselves and each other through avatars that consist of a head and two controller components. They should witness the movements of others in this shared space in real time, observing how they maneuver or rotate their controllers. Moreover, through hand animations, they should be able to discern when others are pressing their hands.</p>
<p>Before we start importing Photon PUN 2 into our project, let’s create a new scene for this part of this chapter. Follow these steps to get started:</p>
<ol>
<li>Open the <code>AdvancedXRTechniques</code> project, which we created in the previous sections of this chapter. Save the current scene using an expressive name such as <code>HandAndEyeTrackingScene</code> by navigating to <strong class="bold">File</strong> | <strong class="bold">Save As</strong> and typing in the scene name of your choice.</li>
<li>Go to <code>MultiplayerScene</code>.</li>
<li>In the Unity Editor of your new scene, delete <code>XR Interaction Setup</code> via the search bar of the <code>0</code>,<code>0</code>,<code>0</code>).</li>
</ol>
<p>Now that we’ve set up our scene, follow these steps to install and set up our networking system, PUN:</p>
<ol>
<li>We<a id="_idIndexMarker680"/> can install PUN like any other package via the Unity Asset Store. Visit <a href="https://assetstore.unity.com/packages/tools/network/pun-2-free-119922">https://assetstore.unity.com/packages/tools/network/pun-2-free-119922</a> or search for <code>PUN 2 – FREE</code> via the Unity Asset Store (<strong class="bold">Window</strong> | <strong class="bold">Asset Store</strong>). Click the <strong class="bold">Add to my Assets</strong> button to add the package to your assets. Once the package has been added, a new button called <strong class="bold">Open in Unity</strong> will appear on the Asset Store’s website. Click it to open the package in the <strong class="bold">Package Manager</strong> window of your project. Press the <strong class="bold">Download</strong> and <strong class="bold">Import</strong> buttons to import the asset into your project.</li>
<li>The <strong class="bold">PUN Wizard</strong> window shown in <em class="italic">Figure 8</em><em class="italic">.6</em> will pop up in the Unity Editor.</li>
</ol>
<div><div><img alt="Figure 8.6 – The PUN Wizard pop-up window" src="img/B20869_08_06.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – The PUN Wizard pop-up window</p>
<p class="list-inset">If you already have a Photon account, you can simply type in your <strong class="bold">AppId</strong> here. Otherwise, input your email and click on the <strong class="bold">Setup Project</strong> button once you are done. This will create an account and forward you to a web page where you can set your password.</p>
<p class="callout-heading">Important note</p>
<p class="callout">If you accidentally close this pop-up window, simply head to <a href="https://www.photonengine.com/">https://www.photonengine.com/</a> and sign up there.</p>
<ol>
<li value="3">Once<a id="_idIndexMarker681"/> you have signed in to your account, head to <a href="https://dashboard.photonengine.com/">https://dashboard.photonengine.com/</a> and click on the <strong class="bold">Create A New App</strong> button. You will be forwarded to the page shown in <em class="italic">Figure 8</em><em class="italic">.7</em>.</li>
</ol>
<div><div><img alt="Figure 8.7 – The web page you will be forwarded to once you create a new application on the Photon website" src="img/B20869_08_07.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – The web page you will be forwarded to once you create a new application on the Photon website</p>
<ol>
<li value="4">Choose the <code>My first Multiplayer Game</code>. Click the <strong class="bold">CREATE</strong> button – notice your newly created application in the dashboard. As shown in <em class="italic">Figure 8</em><em class="italic">.8</em>, you can find your Photon <strong class="bold">App ID</strong> in one of the layout elements<a id="_idIndexMarker682"/> of the dashboard. Copy it to connect to the server later.</li>
</ol>
<div><div><img alt="Figure 8.8 – The dashboard element containing your App ID" src="img/B20869_08_08.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.8 – The dashboard element containing your App ID</p>
<ol>
<li value="5">Head back to your Unity project. If you still see the <strong class="bold">PUN Wizard</strong> window, you can directly paste your <strong class="bold">App Id</strong> inside of it and click the <strong class="bold">Setup Project</strong> button. Alternatively, you can type your <strong class="bold">App Id</strong> directly into <strong class="bold">Photon Server Settings</strong> by navigating to <strong class="bold">Window</strong> | <strong class="bold">Photon Unity Networking</strong> | <strong class="bold">Highlight Server Settings</strong>, as shown in <em class="italic">Figure 8</em><em class="italic">.9</em>.</li>
</ol>
<div><div><img alt="Figure 8.9 – How to add the App Id to Photon Server Settings in Unity" src="img/B20869_08_09.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.9 – How to add the App Id to Photon Server Settings in Unity</p>
<p>Now, it’s<a id="_idIndexMarker683"/> time to connect our Unity project to the server. The next section will show you how to do this.</p>
<h2 id="_idParaDest-163">Connecting to the server via Network Manager</h2>
<p>To<a id="_idIndexMarker684"/> connect our Unity project to the PUN server, we must add a new GameObject with an associated C# script to our scene. Let’s go through this process step by step:</p>
<ol>
<li>Create a new empty GameObject by right-clicking in the <code>Network Manager</code>.</li>
<li>Now, we want to add a script to this GameObject that we can use to connect to the PUN server and check whether someone else joined the server. To create this script, navigate to the <code>Network Manager</code> and click the <strong class="bold">Add </strong><strong class="bold">Component</strong> button.</li>
<li>Search for <code>NetworkManager</code>, select the <strong class="bold">New Script</strong> option, and press the <strong class="bold">Create and Add</strong> button to create a C# script called <strong class="bold">NetworkManager.cs</strong> that is automatically added to <strong class="bold">NetworkManager</strong> as a component. Double-click on the script to open it in your preferred IDE.</li>
<li>The <strong class="bold">NetworkManager</strong> script serves as a foundational element for initiating and managing network interactions in a VR multiplayer application using the PUN framework. Delete everything that is currently inside of the <strong class="bold">NetworkManager</strong> script so that you can start on a clean foundation.</li>
</ol>
<p>Let’s start adding our code logic by importing the following libraries:</p>
<pre class="source-code">
using UnityEngine;
using Photon.Pun;
using Photon.Realtime;</pre>
<p>The <code>Photon.Pun</code> and <code>Photon.Realtime</code> libraries are vital for any PUN application as they<a id="_idIndexMarker685"/> grant access to core multiplayer networking functionalities.</p>
<p>Next, let’s define the <code>NetworkManager</code> class and some important constants:</p>
<pre class="source-code">
public class NetworkManager : MonoBehaviourPunCallbacks
{
    private const string ROOM_NAME = "Multiplayer Room";
    private const byte MAX_PLAYERS = 5;
}</pre>
<p>The <code>NetworkManager</code> class inherits from <code>MonoBehaviourPunCallbacks</code>. This inheritance means that it’s not just a standard Unity script (<code>MonoBehaviour</code>), but that it also has special callback functions provided by PUN that notify our script of various networking events.</p>
<p>While <code>ROOM_NAME</code> represents the name of the room players will join or create, <code>MAX_PLAYERS</code> defines the maximum number of players allowed in a room, which in this case is <code>5</code> players. We will need both constants later in the script.</p>
<p>Let’s continue by connecting our application to Photon servers:</p>
<pre class="source-code">
private void Awake()
{
    InitiateServerConnection();
}
private void InitiateServerConnection()
{
     if (!PhotonNetwork.IsConnected)
    {
        PhotonNetwork.ConnectUsingSettings();
        Debug.Log("Attempting server connection...");
    }
}</pre>
<p>Once<a id="_idIndexMarker686"/> the <code>Awake()</code> method is invoked, it initiates the connection to the PUN server by calling the <code>InitiateServerConnection()</code> method. If the client isn’t already connected to Photon, the <code>InitiateServerConnection()</code> method will attempt to connect to the server space using the <strong class="bold">App Id</strong> value that we inserted before. Refer to <em class="italic">Figure 8</em><em class="italic">.9</em> in case you missed this step. The second line of this method logs our attempt via a debug message.</p>
<p>If the connection attempt is successful, the following method will be executed:</p>
<pre class="source-code">
public override void OnConnectedToMaster()
{
    base.OnConnectedToMaster();
    Debug.Log("Connected to Master Server.");
    JoinOrCreateGameRoom();
}</pre>
<p>This method is an override of the <code>OnConnectedToMaster</code> callback from Photon. It is triggered once the application successfully connects to the Photon Master Server. This method is an integral part of the PUN framework, allowing us to execute specific logic after a successful connection. Inside this method, we log the successful connection to the Master Server. We also call the <code>JoinOrCreateGameRoom()</code> method, which uses the constants we defined at the beginning of our script:</p>
<pre class="source-code">
private void JoinOrCreateGameRoom()
    {
        RoomOptions options = new RoomOptions
        {
            MaxPlayers = MAX_PLAYERS,
            IsVisible = true,
            IsOpen = true
        };
        PhotonNetwork.JoinOrCreateRoom(ROOM_NAME, options, TypedLobby.Default);
}</pre>
<p>In the<a id="_idIndexMarker687"/> last line of this method, the client attempts to join or create a room, called <strong class="bold">Multiplayer Room</strong>, with five being the maximum allowed number of players in the room. If the room doesn’t exist, it creates one with the provided options.</p>
<p>The next method in our code is an override:</p>
<pre class="source-code">
public override void OnJoinedRoom()
{
    base.OnJoinedRoom();
    Debug.Log("Successfully joined a room.");
}</pre>
<p>This method is an override of the <code>OnJoinedRoom</code> callback from Photon. It’s triggered when the client successfully joins a room. In the last line, a debug message is printed to confirm successful room entry.</p>
<p>To manage new players, we can use the following method:</p>
<pre class="source-code">
public override void OnPlayerEnteredRoom(Player newParticipant)
{
    base.OnPlayerEnteredRoom(newParticipant);
    Debug.Log("Another player has joined the room.");
}</pre>
<p>This <code>OnPlayerEnteredRoom()</code> method is also an override of the <code>OnPlayerEnteredRoom</code> callback from Photon. It’s called whenever a new player enters the room. Once again, a debug message is printed in the last line to notify that another player has joined.</p>
<p>Hurray, you<a id="_idIndexMarker688"/> have implemented all the necessary components into the <code>NetworkManager</code> script! As you can see, it provides a foundational structure to connect to Photon’s servers, manage multiplayer rooms, and handle player interactions in a VR environment. Let’s see whether our code logic works. In the next section, you will find out how you can test a multiplayer application on a single device.</p>
<h2 id="_idParaDest-164">Testing the multiplayer scene from one device</h2>
<p>To test a <a id="_idIndexMarker689"/>multiplayer scene from a single device, we need to run the scene twice. This can be accomplished by first building and running the scene on the computer and subsequently launching it from within the Editor. Here’s a step-by-step breakdown:</p>
<ol>
<li>In the Unity Editor, head to <strong class="bold">Files</strong> | <strong class="bold">Build Settings</strong> and select the <strong class="bold">Windows/ Mac/ Linux</strong> tab. Add the open scene you want to test, click the <strong class="bold">Build</strong> button, and choose a folder for the executable file you are about to build.</li>
<li>Once the file has been built, click on the <strong class="bold">Play</strong> button in the Unity Editor to start the scene.</li>
<li>Now, open the executable file that you just built.</li>
<li>Head to the <strong class="bold">Console</strong> window in the Unity Editor and check the <strong class="bold">Debug</strong> statements. If you did everything correctly, you will see the debug lines we defined in the scripts previously, as shown in <em class="italic">Figure 8</em><em class="italic">.10</em>.</li>
</ol>
<div><div><img alt="Figure 8.10 – The debug lines you should see if everything works as expected" src="img/B20869_08_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.10 – The debug lines you should see if everything works as expected</p>
<p>To ensure <a id="_idIndexMarker690"/>that our scene functions optimally, we must test its VR features thoroughly. From our assessments, navigating the scene is smooth, and both the headset and controllers are accurately tracked and positioned. Currently, our scene establishes a server connection as soon as the initial player activates it, and we have set up methods to detect the entry of new players.</p>
<p>However, our <strong class="bold">XR Interaction Setup</strong> isn’t entirely primed for multiplayer operations. It excels at managing locomotion and interactions, but it doesn’t feature animated models for different body parts. In simpler terms, if someone were to enter our multiplayer scene at this moment, they might only see the controller. Worse, they might not detect any part of our avatar because the controller models aren’t network-conscious. This means that the objects present aren’t synchronized across all players in the session. But don’t worry, we’ll tackle this issue in the upcoming section.</p>
<h2 id="_idParaDest-165">Using scripts to display our avatar’s hands and face</h2>
<p>Our <a id="_idIndexMarker691"/>next <a id="_idIndexMarker692"/>objective is to display an avatar’s hands and face when it connects to a server. To achieve this, let’s append a new script to <code>Network Manager</code> by selecting it in the <code>NetworkPlayerPlacer</code> after clicking the <code>NetworkPlayerPlacer</code>. Open the script by double-clicking on it.</p>
<h3>Managing player presence</h3>
<p>The <code>NetworkPlayerPlacer</code> script <a id="_idIndexMarker693"/>we are about to dive into is tailored for overseeing player presence, a core element in multiplayer VR applications. Specifically, this means to create and delete player avatars or their in-game representations. The <code>NetworkPlayerPlacer</code> script is tailored for this exact role. It begins with the following declarations:</p>
<pre class="source-code">
using UnityEngine;
using Photon.Pun;
public class NetworkPlayerPlacer : MonoBehaviourPunCallbacks
{
    private GameObject playerInstance;
    private const string PLAYER_PREFAB_NAME = "Network Player";
}</pre>
<p>The import statements and class declarations bear a close resemblance to those in the <code>NetworkManager</code> script. Both classes are derived from <code>MonoBehaviourPunCallbacks</code>. <code>playerInstance</code> serves as a reference to the instantiated player object within the scene. Meanwhile, <code>PLAYER_PREFAB_NAME</code> is a constant string that holds the name of the player prefab set for instantiation. It’s anticipated that this prefab is registered and accessible in the Photon resources directory.</p>
<p>Let’s create the first method of the <code>NetworkPlayerPlacer</code> script:</p>
<pre class="source-code">
public override void OnJoinedRoom()
{
    base.OnJoinedRoom();
    SpawnPlayer();
}</pre>
<p>The <code>OnJoinedRoom()</code> method overrides the <code>OnJoinedRoom</code> callback from Photon, which gets triggered when the local player successfully joins a room.</p>
<p>The base<a id="_idIndexMarker694"/> class implementation of <code>OnJoinedRoom</code> is called with <code>base.OnJoinedRoom()</code>. In the last line, the <code>SpawnPlayer()</code> method is called, which looks like this:</p>
<pre class="source-code">
private void SpawnPlayer()
{
     playerInstance = PhotonNetwork.Instantiate(PLAYER_PREFAB_NAME, transform.position, transform.rotation);
}</pre>
<p>This method instantiates a new player object using Photon’s networked instantiation method. The new player will be spawned at the position and rotation of the <code>NetworkPlayerPlacer</code> object. This ensures that the player object is networked and synchronized across all clients in the room.</p>
<p>The next method in our script overrides the <code>OnLeftRoom</code> callback from Photon, which is called when the local player leaves a room:</p>
<pre class="source-code">
public override void OnLeftRoom()
{
    base.OnLeftRoom();
    DespawnPlayer();
}</pre>
<p>The <code>OnLeftRoom()</code> method consists of two calls: one to the base class implementation of <code>OnLeftRoom</code> and one to <code>DespawnPlayer()</code>, a method to despawn the player. Let’s have a look at the <code>DespawnPlayer()</code> method next:</p>
<pre class="source-code">
private void DespawnPlayer()
{
    if (playerInstance)
    {
        PhotonNetwork.Destroy(playerInstance);
    }
}</pre>
<p>This <a id="_idIndexMarker695"/>method handles the despawning of the player object.</p>
<p>The <code>if</code> statement checks whether the <code>playerInstance</code> reference is not null. If this is true, a player object exists. The <code>PhotonNetwork.Destroy(playerInstance);</code> line destroys the networked player object. This will ensure that the object is removed not just locally but across all clients.</p>
<p>In essence, the <code>NetworkPlayerSpawner</code> script, with its two key callback functions, <code>OnJoinedRoom()</code> and <code>OnLeftRoom()</code>, seamlessly handles the appearance and disappearance of player avatars in our VR multiplayer space, complementing the functionalities offered by the <code>NetworkManager</code> script.</p>
<h3>Creating a face and hands</h3>
<p>Earlier, we <a id="_idIndexMarker696"/>noted that the script anticipates a prefab called <code>Resources</code> folder. Unity uses this default naming convention when referencing objects by their name. To create the needed folder, click on the <code>Resources</code>.</p>
<p>Next, we need to create the <strong class="bold">Network Player</strong> prefab, which will represent our avatar via the network. Follow these steps to achieve this:</p>
<ol>
<li>Right-click in the <code>Network Player</code>.</li>
<li>Add the <strong class="bold">Photon View</strong> component to it in the <strong class="bold">Inspector</strong> window by pressing the <strong class="bold">Add Component</strong> button and searching for it. Photon requires this component to instantiate the player on the server. Without it, the system wouldn’t recognize ownership or synchronize the player’s body parts accurately.</li>
<li>Add three other empty GameObjects as children to <code>Network Player</code> by selecting <code>Network Player</code> in the <code>Head</code>, <code>Left Hand</code>, and <code>Right Hand</code>.</li>
<li>Now, we need to create 3D objects to represent the three components of our avatar. For <code>Head</code>, we can use a simple <code>Head</code> in the <code>Head</code> and scale it to (<code>0.1</code>, <code>0.1</code>, <code>0.1</code>)</li>
<li>We could also use <a id="_idIndexMarker697"/>primitives for the hands of our avatar. However, since there are a lot of animated hand models available on the internet, we can simply use one of them. For this project, we used the <strong class="bold">Oculus Hands</strong> sample from the <strong class="bold">Oculus Integration</strong> package on Unity’s Asset Store (<a href="https://assetstore.unity.com/packages/tools/integration/oculus-integration-82022">https://assetstore.unity.com/packages/tools/integration/oculus-integration-82022</a>). However, we suggest cloning the hands directly from this book’s GitHub repository. This avoids unnecessary additional content and missing assets due to frequent updates in the <strong class="bold">Oculus </strong><strong class="bold">Integration</strong> package.</li>
<li>Now, select <code>Network Player</code> in the <code>Resources</code> folder in the <code>Network Player</code> in our <strong class="bold">Scene Hierarchy</strong> window anymore and can delete it.</li>
</ol>
<p>We must add another script to <code>Network Manager</code> to make sure the <code>Head</code>, <code>Left Hand</code>, and <code>Right Hand</code> components follow the position of the user’s headset and controllers. To do this, select <code>Network Manager</code> in the <code>NetworkPlayer</code> into the search bar, and create a new script with this name.</p>
<p>Now that our script has been created, let’s learn how we can use it to track the player’s position and movement.</p>
<h3>Tracking player position and movements</h3>
<p>The <code>NetworkPlayer</code> script controls how a player’s movements are tracked and represented<a id="_idIndexMarker698"/> within the multiplayer VR environment. The script encapsulates functionalities that ensure the position and rotation of the avatar’s head, left hand, and right hand are accurately tracked and updated.</p>
<p>Let’s have a look at the beginning of this script:</p>
<pre class="source-code">
using UnityEngine;
using UnityEngine.XR;
using Photon.Pun;
using UnityEngine.InputSystem;
public class NetworkPlayer : MonoBehaviour
{
    public Transform head;
    public Transform leftHand;
    public Transform rightHand;
    private PhotonView photonView;
    public InputActionAsset xriInputActions;
    private InputActionMap headActionMap;
    private InputActionMap leftHandActionMap;
    private InputActionMap rightHandActionMap;
    private InputAction headPositionAction;
    private InputAction headRotationAction;
    private InputAction leftHandPositionAction;
    private InputAction leftHandRotationAction;
    private InputAction rightHandPositionAction;
    private InputAction rightHandRotationAction;
}</pre>
<p>Besides<a id="_idIndexMarker699"/> importing the regular Unity and PUN namespaces, a class named <code>NetworkPlayer</code> inheriting from <code>MonoBehaviour</code> is declared. The <code>Transform</code> variables, <code>head</code>, <code>leftHand</code>, and <code>rightHand</code>, represent the 3D position, rotation, and scale of a player’s VR avatar components in the game. The <code>PhotonView</code> component is fundamental for PUN, determining ownership and synchronization of objects in multiplayer. <code>InputActionAsset</code>, called <code>xriInputActions</code>, is a Unity-configured set of input definitions tailored for VR headsets. The <code>InputActionMap</code> variables, such as <code>headActionMap</code>, group related input actions, allowing for organized handling of inputs such as head or hand movements. Finally, the <code>InputAction</code> variables such as <code>headPositionAction</code> and <code>headRotationAction</code> detect individual input movements from the VR headset. To synchronize the player’s real-world VR movements with their in-game avatar, we need methods in our script. Let’s start with the first one:</p>
<pre class="source-code">
void Start()
    {
        photonView = GetComponent&lt;PhotonView&gt;();
        // Get the Action Maps
        headActionMap = xriInputActions.FindActionMap("XRI Head");
        leftHandActionMap = xriInputActions.FindActionMap("XRI LeftHand");
        rightHandActionMap = xriInputActions.FindActionMap("XRI RightHand");
        // Get the Position and Rotation actions for each action map
        headPositionAction = headActionMap.FindAction("Position");
        headRotationAction = headActionMap.FindAction("Rotation");
        leftHandPositionAction = leftHandActionMap.FindAction("Position");
        leftHandRotationAction = leftHandActionMap.FindAction("Rotation");
        rightHandPositionAction = rightHandActionMap.FindAction("Position");
        rightHandRotationAction = rightHandActionMap.FindAction("Rotation");
        // Enable actions
        headPositionAction.Enable();
        headRotationAction.Enable();
        leftHandPositionAction.Enable();
        leftHandRotationAction.Enable();
        rightHandPositionAction.Enable();
        rightHandRotationAction.Enable();
    }</pre>
<p>In <a id="_idIndexMarker700"/>the <code>Start()</code> method, <code>photonView</code> is initialized. Then, action maps of the head, left hand, and right hand are retrieved using their names. These names (<code>XRI Head</code>, <code>XRI LeftHand</code>, and <code>XRI RightHand</code>) are predefined in <code>Position</code> and <code>Rotation</code> actions for the head and each hand are retrieved. Lastly, all these actions are enabled so that they begin reading input values.</p>
<p>The next <a id="_idIndexMarker701"/>method in our script is the <code>Update()</code> method:</p>
<pre class="source-code">
void Update()
    {
        if (photonView.IsMine)
        {
            rightHand.gameObject.SetActive(false);
            leftHand.gameObject.SetActive(false);
            head.gameObject.SetActive(false);
            MapPosition(head, XRNode.Head);
            MapPosition(leftHand, XRNode.LeftHand);
            MapPosition(rightHand, XRNode.RightHand);
        }
    }</pre>
<p>In a Photon-powered multiplayer environment, numerous <code>NetworkPlayer</code> instances will emerge, representing each player in the game. However, on every player’s device, only a single instance genuinely represents that specific player, while the rest signify remote players. The <code>photonView.IsMine</code> property, when checked within the <code>Update()</code> method’s <code>if</code> statement, determines whether the <code>NetworkPlayer</code> instance on a given machine corresponds to the player using that machine. This differentiation is pivotal in multiplayer scenarios to identify who has control over certain activities or decisions. If it returns true, then the <code>NetworkPlayer</code> instance pertains to the local player of that device. Consequently, the visual representation of that player’s head and hands is deactivated, ensuring a seamless user experience.</p>
<p>Within our multiplayer scene, two primary players exist: our local player (via <code>Network Player</code>). This remote player is visible to all other participants in the multiplayer environment. As the local player already has<a id="_idIndexMarker702"/> hand models, introducing an additional hand model from the remote player can lead to a disorienting experience, particularly given network update latencies, as you can see in <em class="italic">Figure 8</em><em class="italic">.11</em>.</p>
<div><div><img alt="Figure 8.11 – What happens when the SetActive(false) statements are not implemented" src="img/B20869_08_11.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.11 – What happens when the SetActive(false) statements are not implemented</p>
<p>Thus, we ensure the elements of <code>Network Player</code> remain invisible to the user. Conversely, if the property returns <code>false</code>, it means the instance represents another participant as viewed from that device. Finally, the <code>if</code> statement also updates the position and rotation of the hands and head based on VR headset or VR controller movements using the <code>MapPosition()</code> method.</p>
<p>Now, let’s have a look at the <code>MapPosition()</code> method:</p>
<pre class="source-code">
void MapPosition(Transform target, XRNode node)
    {
        Vector3 position = Vector3.zero;
        Quaternion rotation = Quaternion.identity;
        if (node == XRNode.Head)
        {
            position = headPositionAction.ReadValue&lt;Vector3&gt;();
            rotation = headRotationAction.ReadValue&lt;Quaternion&gt;();
        }
        else if (node == XRNode.LeftHand)
        {
            position = leftHandPositionAction.ReadValue&lt;Vector3&gt;();
            rotation = leftHandRotationAction.ReadValue&lt;Quaternion&gt;();
        }
        else if (node == XRNode.RightHand)
        {
            position = rightHandPositionAction.ReadValue&lt;Vector3&gt;();
            rotation = rightHandRotationAction.ReadValue&lt;Quaternion&gt;();
        }
        target.position = position;
        target.rotation = rotation;
    }</pre>
<p>The <a id="_idIndexMarker703"/>purpose of the <code>MapPosition()</code> method is to assign the correct position and rotation to the passed-in <code>if</code> statement and reads the corresponding input values. For example, if the node is <code>XRNode.Head</code>, it reads the position and rotation values from <code>headPositionAction</code> and <code>headRotationAction</code>, respectively. These <code>InputAction</code> variables fetch the latest position and rotation values from the VR headset at every frame. Finally, the <code>MapPosition()</code> method assigns the fetched position and rotation to the passed-in target transform.</p>
<p>In essence, this method ensures that the in-game representation of the player moves in sync with their real-world movements in VR.</p>
<p>The last method in this script is the <code>OnDestroy()</code> method:</p>
<pre class="source-code">
void OnDestroy()
    {
        headPositionAction.Disable();
        headRotationAction.Disable();
        leftHandPositionAction.Disable();
        leftHandRotationAction.Disable();
        rightHandPositionAction.Disable();
        rightHandRotationAction.Disable();
    }</pre>
<p>The <code>OnDestroy()</code> method simply disables all the input actions when the script is destroyed, ensuring no unwanted input readings are occurring in the background.</p>
<p>Overall, the <code>NetworkPlayer</code> script serves as a bridge, translating a player’s real-world VR movements into the virtual multiplayer space. By adding this script to our scene, players can experience a synchronous and immersive VR multiplayer environment.</p>
<p>When we test the application now, we can see the head and both of the hands somewhere in the scene aligned with our controller position. Next, we’ll animate our hands when we press the trigger and grip button.</p>
<h2 id="_idParaDest-166">Animating the hand models</h2>
<p>Your<a id="_idIndexMarker705"/> multiplayer application has come a long way. Now, let’s take the immersion one step further with hand animations. By mapping controller inputs to these animations, users will experience a heightened realism within the virtual world. If animations sound foreign to you, we recommend a quick detour to the <em class="italic">Understanding animations and animator systems</em> section of <a href="B20869_05.xhtml#_idTextAnchor016"><em class="italic">Chapter 5</em></a>.</p>
<p>Within the <code>Prefabs</code> folder and then follow these steps:</p>
<ol>
<li>Attach an <code>Animator</code>.<p class="list-inset">In <a href="B20869_05.xhtml#_idTextAnchor016"><em class="italic">Chapter 5</em></a>, we touched on <strong class="bold">Animator</strong> in Unity, which is essential for controlling character animations. <strong class="bold">Animator</strong> needs <strong class="bold">Animator Controller</strong> to manage how animations transition and interact. It also requires <strong class="bold">Avatar</strong>, which is a map of the character’s skeletal structure, ensuring animations fit and move realistically on the character.</p></li>
</ol>
<p>2.	For our avatars, we can select the <strong class="bold">L_hand_skeletal_lowres</strong> and <strong class="bold">R_hand_skeletal_lowres</strong> options from <strong class="bold">Multiplayer Scene</strong> | <strong class="bold">Oculus</strong> <strong class="bold">Hands</strong> | <strong class="bold">Models</strong>.</p>
<p>3.	However, we’re yet to acquire <code>Left Hand Animator</code> and <code>Right </code><code>Hand Animator</code>.</p>
<p>Double-click on <code>Left Hand Animator</code> to reveal an <strong class="bold">Animator</strong> window showcasing <strong class="bold">Layers</strong> and a <strong class="bold">Parameters</strong> tab. Start with the <strong class="bold">Parameters</strong> tab and introduce <strong class="bold">Grip</strong> and <strong class="bold">Trigger</strong> parameters, both of the <em class="italic">float</em> type. They indicate the pressure intensity of the trigger and grip actions:</p>
<ul>
<li><strong class="bold">Trigger</strong> determines the degree of the hand’s pinch</li>
<li><strong class="bold">Grip</strong> governs the degree of the fist’s bend</li>
</ul>
<p>However, simply having these parameters won’t magically animate the fingers. To bring them to life, we need to bind these parameters to specific animation states or integrate them within blend trees. These concepts have different approaches to connecting animation states, so let’s compare them to see which approach fits our needs the best:</p>
<ul>
<li><code>0.5</code>, the hand might transition to <strong class="bold">HandClosed</strong>; otherwise, it will remain in the <strong class="bold">HandOpen</strong> state.</li>
<li><strong class="bold">Blend trees</strong> enable seamless transitioning between multiple animations depending on one or more parameters. It’s analogous to using a music mixer to blend songs.</li>
</ul>
<p>For<a id="_idIndexMarker706"/> hand animations, it is useful to have three states: <strong class="bold">HandOpen</strong>, <strong class="bold">HandHalfClosed</strong>, and <strong class="bold">HandFullyClosed</strong>. Instead of basic transitions, which can feel abrupt, a blend tree can be used to fluidly move between these states, all based on the <strong class="bold">Grip</strong> value. This is why it is the best choice for our needs.</p>
<p>To set up a blend tree, right-click in the <strong class="bold">Base Layer</strong> window’s center and opt for <strong class="bold">Create State</strong> | <strong class="bold">From New Blend Tree</strong>. Your <strong class="bold">Baser Layer</strong> window should be similar to what’s shown in <em class="italic">Figure 8</em><em class="italic">.12</em>.</p>
<div><div><img alt="Figure 8.12 – Base Layer in the Animator window" src="img/B20869_08_12.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.12 – Base Layer in the Animator window</p>
<p>By double-clicking on <strong class="bold">Blend Tree</strong> and selecting it afterward, you’ll be prompted to select a <strong class="bold">Blend Type</strong> property, which includes the following:</p>
<ul>
<li><strong class="bold">1D</strong>: This pivots around a single parameter, such as pacing, to transition between animations such as walking and running</li>
<li><strong class="bold">2D Freeform Cartesian</strong>: This employs two separate parameters – for instance, determining animation intensity based on a character’s mood and energy levels</li>
<li><strong class="bold">2D Freeform Directional</strong>: Here, two parameters act as directional vectors, such as aiming a weapon in horizontal and vertical directions</li>
<li><strong class="bold">2D Simple Directional</strong>: Typically, this uses two parameters, often <em class="italic">X</em> and <em class="italic">Y</em> inputs, resembling a joystick’s movements, to determine character direction</li>
</ul>
<p>For <a id="_idIndexMarker707"/>nuanced hand animations using both <strong class="bold">Grip</strong> and <strong class="bold">Trigger</strong>, <strong class="bold">2D Freeform Cartesian</strong> is ideal. Each parameter operates autonomously: <strong class="bold">Grip</strong> dictates the intensity of the fist, while <strong class="bold">Trigger</strong> oversees the pinching gesture. Adjust <strong class="bold">Blend Type</strong> to <strong class="bold">2D Freeform Cartesian</strong>, then designate <strong class="bold">Grip</strong> for the <em class="italic">X</em>-axis and <strong class="bold">Trigger</strong> for the <em class="italic">Y</em>-axis in the <strong class="bold">Inspector</strong> window. In the same window, we need to define some key motions.</p>
<p>For a solid setup, you should consider the following four key motions depicting the extremities of the hand movements:</p>
<ul>
<li><code>0</code>. Fortunately, we don’t need to create such an animation clip ourselves as the <code>Multiplayer Scene</code> | <code>Oculus Hands</code> | <code>Animations</code><strong class="bold"> </strong>folder.</li>
<li><code>0</code> and the <code>1</code>. This would be the <strong class="bold">l_hand_pinch_anim</strong> animation clip, which is in the same folder as <strong class="bold">Take 001</strong>.</li>
<li><code>1</code> and a <code>0</code>. That would be the <code>Animations </code>folder as before.</li>
<li><code>1</code> for both cases. This would also be the <code>1</code>.</li>
</ul>
<p>If you did<a id="_idIndexMarker712"/> everything correctly, your <strong class="bold">Blend Tree</strong> should look as shown in <em class="italic">Figure 8</em><em class="italic">.13</em>.</p>
<div><div><img alt="Figure 8.13 – The Blend Tree settings in Unity" src="img/B20869_08_13.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.13 – The Blend Tree settings in Unity</p>
<p>Repeat this process for the <strong class="bold">Right Hand</strong> animator controller. Once we’ve done this, we can drag the just-created <strong class="bold">Left Hand</strong> and <strong class="bold">Right Hand</strong> animator controllers into the corresponding fields of the <strong class="bold">Left Hand Model</strong> and <strong class="bold">Right Hand </strong><strong class="bold">Model</strong> prefabs.</p>
<p>Next, we just need to create a script that will link the <strong class="bold">Grip</strong> and <strong class="bold">Trigger</strong> variables’ float values to the button inputs of our controller.</p>
<h2 id="_idParaDest-167">Enabling the hand animations at runtime</h2>
<p>In<a id="_idIndexMarker713"/> this section, we’ll be delving into the process of enabling hand animations in real time within our multiplayer VR application. This means that during the actual gameplay, as users interact with the virtual environment and other players, they can witness and experience hand gestures such as pinching or making a fist. This real-time interaction enhances the immersion and interactivity of our multiplayer VR game. By integrating these animations at runtime, users can seamlessly respond to game mechanics or communicate non-verbally with other players.</p>
<p>We are<a id="_idIndexMarker714"/> going to do this by creating a script to hold our <code>HandControllerPresence</code> into the search bar, create a new script with the same name, and then open it. We’ll start by defining the necessary variables:</p>
<pre class="source-code">
    public GameObject handVisualizationPrefab;
    [SerializeField] private InputActionProperty triggerAction;
    [SerializeField] private InputActionProperty gripAction;
    private GameObject instantiatedHandVisual;
    private Animator handMotionController;</pre>
<p><code>handVisualizationPrefab</code> is a public GameObject variable that refers to the hand model prefab that will be animated based on the user’s VR controller inputs. In our case, this will be the <code>triggerAction</code> and <code>gripAction</code> are serialized fields, meaning they can be assigned in the Unity Editor but remain private in the script. They are designed to fetch the current values of the trigger and grip inputs, respectively.</p>
<p><code>instantiatedHandVisual</code> and <code>handMotionController</code> are private variables that are used internally. The former stores the instantiated hand model in the scene, while the latter is a reference to the <code>Awake()</code> and <code>InitializeHandController()</code> methods:</p>
<pre class="source-code">
void Awake()
    {
        InitializeHandController();
    }
    void InitializeHandController()
    {
        instantiatedHandVisual = Instantiate(handVisualizationPrefab, transform);
        handMotionController = instantiatedHandVisual.GetComponent&lt;Animator&gt;();
    }</pre>
<p>The <code>Awake()</code> method <a id="_idIndexMarker715"/>is called when the script instance is loaded. Here, it calls the <code>InitializeHandController()</code> method to set up the hand visuals and animations. This method instantiates the hand model prefab in the scene and attaches it to the object to which this script is attached. Then, it fetches and stores the <code>handMotionController</code> variable. This <code>AdjustHandMotion()</code> method:</p>
<pre class="source-code">
   void AdjustHandMotion()
    {
        float triggerIntensity = triggerAction.action.ReadValue&lt;float&gt;();
        float gripIntensity = gripAction.action.ReadValue&lt;float&gt;();
        handMotionController.SetFloat("Trigger", triggerIntensity);
        handMotionController.SetFloat("Grip", gripIntensity);
    }</pre>
<p>This method fetches the current values of the trigger and grip inputs using the <code>triggerAction</code> and <code>gripAction</code> variables, respectively. These values are in the range of 0 (not pressed) to 1 (fully pressed).</p>
<p>The<a id="_idIndexMarker716"/> fetched values (<code>triggerIntensity</code> and <code>gripIntensity</code>) are then passed to the hand’s <code>SetFloat()</code> method. This effectively adjusts the hand’s animation based on the real-time inputs from the VR controller. Finally, we just need to call this method once per frame to ensure that the hand’s animations are updated in real time to match the user’s VR controller inputs. This is done in the <code>Update()</code> method:</p>
<pre class="source-code">
  void Update()
    {
        AdjustHandMotion();
    }</pre>
<p>Now, we just need to assign the corresponding <code>handVisualiationPrefab</code> and the controller actions to the <strong class="bold">Trigger Action</strong> and <strong class="bold">Grip Action</strong> fields for both controllers, as shown in <em class="italic">Figure 8</em><em class="italic">.14</em> for the left controller.</p>
<div><div><img alt="Figure 8.14 – ﻿The HandControllerPresence script and its references﻿" src="img/B20869_08_14.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.14 – The HandControllerPresence script and its references</p>
<p>You might be wondering why exactly are we using the <strong class="bold">XRI LeftHand</strong> references shown in <em class="italic">Figure 8</em><em class="italic">.14</em>. In our scene, we are using <strong class="bold">XR Interaction Setup</strong>, which comes with the XR Interaction Toolkit’s <strong class="bold">Starter Assets</strong>. This <strong class="bold">XR Interaction Setup</strong> uses <strong class="bold">XRI Default Input Actions</strong> as controller input handling.</p>
<p>Let’s have a look at them by heading to <strong class="bold">Samples</strong> | <strong class="bold">XR Interaction Toolkits</strong> | <strong class="bold">Your</strong> <strong class="bold">Version</strong> | <strong class="bold">Starter Assets</strong> and double-clicking on <strong class="bold">XRI Default Input Actions</strong>. This will open the window shown in <em class="italic">Figure 8</em><em class="italic">.15</em>.</p>
<div><div><img alt="Figure 8.15 – The XRI Default Input Actions of the XR Interaction Toolkit" src="img/B20869_08_15.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.15 – The XRI Default Input Actions of the XR Interaction Toolkit</p>
<p>The<a id="_idIndexMarker717"/> XR Interaction Toolkit action-based input system in Unity allows for device-agnostic control setups. It uses <strong class="bold">Action Maps</strong> to group related actions, such as <strong class="bold">XRI LeftHand Interaction</strong>, to handle specific actions such as <strong class="bold">Select Value</strong> or <strong class="bold">Activate Value</strong>. These actions are then tied to specific inputs, called <strong class="bold">bindings</strong>, such as the <strong class="bold">Grip</strong> button or the <strong class="bold">Trigger</strong> button of an XR controller, allowing <a id="_idIndexMarker718"/>for flexible and intuitive control configurations across various XR devices. You can add additional action maps, actions, and bindings or change existing ones. You can even build a complete Input Actions setup of your own under <strong class="bold">Assets</strong> | <strong class="bold">Create</strong> | <strong class="bold">Input Actions</strong>. However, you’ll need to link this new Input Actions setup in the <strong class="bold">Inspector</strong> window of <strong class="bold">Input Action Manager</strong> under <strong class="bold">Action Assets</strong>. With that, we have implemented the animation of the hand models. To begin testing the application with a friend, follow these simple steps:</p>
<ol>
<li>Open your Unity project and navigate to <strong class="bold">File</strong>, then <strong class="bold">Build Settings</strong>.</li>
<li>Add the currently open scene to <strong class="bold">Build Settings</strong>.</li>
<li>Click on the <strong class="bold">Build</strong> option. When prompted, choose a directory that you can easily access and remember.</li>
<li>This action will generate an executable file along with other vital scene-related files in the chosen directory. For your friend to join in, share the entire folder with them.</li>
<li>Initiate your connection to the server by pressing the <strong class="bold">Play</strong> button. As you await your connection, instruct your friend to launch the executable file.</li>
</ol>
<p>And voilà! You’ve successfully set up your first multiplayer environment. Now, both you and your friend can interact, as depicted in <em class="italic">Figure 8</em><em class="italic">.16</em>.</p>
<div><div><img alt="Figure 8.16 – Me and my friend in our multiplayer environment" src="img/B20869_08_16.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.16 – Me and my friend in our multiplayer environment</p>
<p class="callout-heading">Note</p>
<p class="callout">You might have noticed a stunning deep-space background with a picturesque planet. This aesthetic touch is provided by the Skybox Volume 2 package, which we imported from the Unity Asset Store (<a href="https://assetstore.unity.com/packages/2d/textures-materials/sky/skybox-volume-2-nebula-3392">https://assetstore.unity.com/packages/2d/textures-materials/sky/skybox-volume-2-nebula-3392</a>). It significantly enhances the ambiance of our environment.</p>
<p>As we <a id="_idIndexMarker719"/>approach the end of this chapter, you should now feel capable and confident in enhancing your XR experiences using advanced techniques. However, this doesn’t imply that every feature, such as eye-tracking or hand-tracking, should be mindlessly applied to all your projects. Nor does it imply that all your subsequent XR endeavors must be multiplayer applications.</p>
<p>What it does mean is that you’ve broadened your horizons. You now possess a richer repertoire of interaction techniques to create your XR applications. Instead of indiscriminately applying eye-tracking to every scene element, for instance, you can selectively use it for components that would truly benefit from it. Imagine a virtual exhibition of firefighter vehicles: rather than overloading the user with information, you could <a id="_idIndexMarker720"/>seamlessly display technical specifications when the user’s gaze settles on a particular vehicle. Interactive components such as animated firefighters or explorable vehicle interiors can be activated based on the user’s visual focus, providing an engaging and dynamic user experience. Similarly, in a treasure-hunting game, a hint might reveal itself when a player’s gaze meets a mystical mirror. The possibilities are endless!</p>
<h1 id="_idParaDest-168">Summary</h1>
<p>In this chapter, you learned how to implement hand-tracking, gaze-tracking, and multiplayer capabilities into your XR scenes. Incorporating these high-level XR methods into your future XR projects will enable you to create much more intuitive, immersive, and fun experiences for users. Combined with the knowledge you gained in the previous chapters on how to create and deploy interactive XR experiences, you should feel comfortable developing a wide range of XR projects yourself, no matter if they involve coding, animations, particles, audio, or multiplayer support.</p>
<p>Now that you have a firm grip on the technical and programming aspects of XR, the next chapter will shift the spotlight to the business realm of XR. There, you’ll explore powerful XR plugins and toolkits beyond the XR Interaction Toolkit, ARKit, ARCore, and AR Foundation that could be valuable additions to your XR development skills moving forward. The subsequent chapter won’t just keep you up to date on the latest trends in XR but will also equip you with industry best practices to ensure the success of your projects.</p>
</div>
</body></html>