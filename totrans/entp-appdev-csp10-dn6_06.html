<html><head></head><body>
		<div><h1 id="_idParaDest-69"><em class="italic"><a id="_idTextAnchor205"/>Chapter 4</em>: Threading and Asynchronous Operations</h1>
			<p>So far, we have looked at various design principles, patterns, what is new in .NET 6, and architecture guidelines that we are going to use during this book. In this chapter, we will see how we can take advantage of asynchronous programming while building enterprise applications. </p>
			<p>One of the key measures for any web application is <em class="italic">scalability</em> – that is, scaling to reduce the time taken to serve a request, increase the number of requests that a server can process, and increase the number of users an application can simultaneously serve without increasing the load time. For mobile/desktop apps, scaling can improve the responsiveness of the app, allowing users to perform various actions without freezing the screen. </p>
			<p>The proper use of asynchronous programming techniques and parallel constructs can do wonders in improving these metrics, and the best thing for this in C# is the simplified syntax of the <strong class="bold">Task Parallel Library</strong> (<strong class="bold">TPL</strong>), async-await, with which we can write clean asynchronous code.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Understanding the jargon</li>
				<li>Demystifying threads, lazy initialization, and <code>ThreadPool</code></li>
				<li>Understanding locks, semaphores, and <code>SemaphoreSlim</code></li>
				<li>Introducing tasks and parallels</li>
				<li>Introducing async-await</li>
				<li>Using concurrent collections for parallelism</li>
			</ul>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor206"/><a id="_idTextAnchor207"/>Technical requirements</h1>
			<p>You will need a basic understanding of .NET Core, C#, and the basics of LINQ. The code examples for this chapter can be found here: <a href="https://github.com/PacktPublishing/Enterprise-Application-Development-with-C-10-and-.NET-6-Second-Edition/tree/main/Chapter04">https://github.com/PacktPublishing/Enterprise-Application-Development-with-C-10-and-.NET-6-Second-Edition/tree/main/Chapter04</a>.</p>
			<p>A few instructions for the code can be found here: <a href="https://github.com/PacktPublishing/Enterprise-Application-Development-with-C-10-and-.NET-6-Second-Edition/tree/main/Enterprise%20Application">https://github.com/PacktPublishing/Enterprise-Application-Development-with-C-10-and-.NET-6-Second-Edition/tree/main/Enterprise%20Application</a>.</p>
			<h1 id="_idParaDest-71"><a id="_idTextAnchor208"/><a id="_idTextAnchor209"/>Understanding the jargon</h1>
			<p>Before we dive<a id="_idIndexMarker191"/> into the technicalities of threading and asynchronous operations, let's take a real-world example and build an analogy between multitasking in real life and parallel programming. Imagine that you are waiting in a queue in a restaurant to order food, and while waiting in the queue, you reply to an email. Then, having ordered the food and while<a id="_idTextAnchor210"/> waiting for it to arrive, you answered a phone call. In the restaurant, there are multiple counters where orders are being taken, and food is prepared by the chef while orders are being placed.</p>
			<p>While you were waiting in line, you concurrently replied to an email. Similarly, while you were ordering, the restaurant was parallelly taking orders at many other counters. The chef is cooking parallelly while orders are being placed. Also, you were given a token to pick up your food from the pickup counter; however, depending upon the preparation time of your food, an order placed after yours may arrive at the pickup counter before yours.</p>
			<p>When talking about parallel programming, some key terms will appear multiple times. This jargon is represented in the following figure:</p>
			<p class="figure-caption"><a id="_idTextAnchor211"/></p>
			<div><div><img src="img/Figure_4.1_B18507.jpg" alt="Figure 4.1 – Concurrency versus parallelism versus asynchronous&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.1 – Concurrency versus parallelism versus asynchronous</p>
			<p>Let's <a id="_idIndexMarker192"/>cover each term:</p>
			<ul>
				<li><strong class="bold">Parallelism</strong>: This entails<a id="_idTextAnchor212"/><a id="_idIndexMarker193"/> multiple tasks being performed independently at the  same time, as in the example of multiple restaurant orders being placed from different counters. In terms of enterprise applications, parallelism would be multiple threads/tasks being executed at the same time in a multicore CPU. However, a single-core CPU also supports parallelism through hyper-threading, which usually involves the logical division of a single core into more than one core, such as a hyper-threading-enabled dual-core CPU, which acts like a quad-core – that is, four cores.</li>
				<li><strong class="bold">Concurrency</strong>: This <a id="_idIndexMarker194"/>entails doing many tasks at the same time, such as in our previous examp<a id="_idTextAnchor213"/>le of replying to an email while queuing for a restaurant counter, or the chef seasoning one dish and heating the pan for a second dish. In terms of enterprise applications, concurrency involves multiple threads sharing a core and, based on their time slicing, executing tasks and performing <a id="_idIndexMarker195"/>context switching.</li>
				<li><strong class="bold">Asynchronous</strong>: Asynchronous<a id="_idIndexMarker196"/> programming is a technique that relies on executing t<a id="_idTextAnchor214"/>asks asynchronously instead of blocking the current thread while it is waiting. In our example, asynchronicity is waiting for your token to be called for you to go to the pickup counter while the chef is working on preparing your food. But while you're waiting, you have moved away from the ordering counter, thereby allowing other orders to be placed. This is like a task that executes asynchronously and frees up resources while waiting on an I/O task (for instance, while waiting on data from a database call). The beauty of asynchronicity is that tasks are executed either parallelly or concurrently, which is completely abstracted from developers by the framework. This lets the developer focus their development efforts on the business logic of the application rather than on managing tasks. We will see this in the <em class="italic">Tasks and parallels</em> section.</li>
				<li><code>CLR ThreadPool</code>. In a multicore/multiprocessor system, multithreading helps to achieve parallelism by executing newly created threads in different cores.</li>
			</ul>
			<p>Now that we<a id="_idIndexMarker198"/> understand the key terms in parallel programming, let's move on to look at how to create threads and the role of <code>ThreadPool</code> in .NET<a id="_idTextAnchor216"/><a id="_idTextAnchor217"/> Core.</p>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor218"/>Demystifying threads, lazy initialization, and ThreadPool</h1>
			<p>A thread <a id="_idTextAnchor219"/>is <a id="_idIndexMarker199"/>the<a id="_idIndexMarker200"/> smallest unit in <a id="_idIndexMarker201"/>an operating system, and it executes instructions in the processor. A process is a bigger executing container, and the thread inside th<a id="_idTextAnchor220"/>e process is the smallest unit to use processor time and execute instructions. The key thing to remember is that whenever your code needs to be<a id="_idTextAnchor221"/> executed in a process, it should be assigned to a thread. Each processor can only execute one instruction at a time; that's why, in a single-core syste<a id="_idTextAnchor222"/>m, at any point time, only one thread is being executed. There are scheduling algorithms that are used to allocate processor time to a thread. A thread typically has a stack (which keeps track of execution history), registers in which to store various variables, and counters to hold instructions that need to be executed.</p>
			<p>A quick look at <strong class="bold">Task Manager</strong> will give us details regarding the number of physical and logical cores, and navigating to <strong class="bold">Resource Monitor</strong> will tell us about the CPU usage in each core. The<a id="_idIndexMarker202"/> following figure shows the details of a hyper-threading-enabled <a id="_idIndexMarker203"/>quad-core CPU that can execute eight<a id="_idIndexMarker204"/> threads in parallel at any<a id="_idTextAnchor223"/> poi<a id="_idTextAnchor224"/>nt in time:</p>
			<div><div><img src="img/Figure_4.2_B18507.jpg" alt="Figure 4.2 – Task Manager and Resource Monitor&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.2 – Task Manager and Resource Monitor</p>
			<p>A <a id="_idIndexMarker205"/>typical <a id="_idIndexMarker206"/>application in .NET Core has one single<a id="_idIndexMarker207"/> thread when it is started and can ad<a id="_idTextAnchor225"/>d more threads by manually creating them. A quick re<a id="_idTextAnchor226"/>fresher on ho<a id="_idTextAnchor227"/>w this is done will be covered in the follo<a id="_idTextAnchor228"/><a id="_idTextAnchor229"/>wing sections.</p>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor230"/>Working with System.Threading.Thread</h2>
			<p>We can create<a id="_idIndexMarker208"/> new threads by creating an instance of <code>System.Threading.Threa<a id="_idTextAnchor231"/>d</code> and passing a method delegate. Here is a simple example that simulates retrieving data from an API and loading a file from a disk:</p>
			<pre class="source-code">Thread loadFileFromDisk = new Thread(LoadFileFromDisk);</pre>
			<pre class="source-code">void LoadFileFromDisk(object? obj)</pre>
			<pre class="source-code">{</pre>
			<pre class="source-code">    Thread.Sleep(2000);</pre>
			<pre class="source-code">    Console.WriteLine("data returned from API");</pre>
			<pre class="source-code">}</pre>
			<pre class="source-code">loadFileFromDisk.Start();</pre>
			<pre class="source-code">Thread fetchDataFromAPI = new Thread(FetchDataFromAPI);</pre>
			<pre class="source-code">void FetchDataFromAPI(object? obj)</pre>
			<pre class="source-code">{</pre>
			<pre class="source-code">    Thread.Sleep(2000);</pre>
			<pre class="source-code">    Console.WriteLine("File loaded from disk");</pre>
			<pre class="source-code">}</pre>
			<pre class="source-code">fetchDataFromAPI.Start("https://dummy/v1/api"); //Parameterized method</pre>
			<pre class="source-code">Console.Rea<a id="_idTextAnchor232"/>dLine();</pre>
			<p>In the previous code, <code>FetchDataFromAPI</code> and <code>LoadFileFromDisk</code> are the methods that would run on the new thread.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">At any point in time, only one thread will be executing on each core – that is, only one thread is allotted CPU time. So, to achieve<a id="_idIndexMarker209"/> concurrency, the <strong class="bold">Operating Sy<a id="_idTextAnchor233"/>stem</strong> (<strong class="bold">OS</strong>) does a context switch when a thread that's been allocated CPU time is idle or if a high-priority thread arrives in the queue (there may be other reasons too, such as if a thread is waiting on a synchronization object or the allotted CPU time is reached).</p>
			<p class="callout">Since a thread that is switched out won't have completed its work, at some point, it will be assigned CPU time again. As such, the OS needs to save the state of the thread (its stack, its registers, and so on) and retrieve it again when the thread is allotted CPU time. Context switching is usually very expensive and one of the key areas of performance impr<a id="_idTextAnchor234"/>ovement.</p>
			<p>All the properties and methods of the <code>Thread</code> class can be further reviewed at <a href="https://docs.microsoft.com/en-us/dotnet/api/system.threading.thread?view=net-6.0">https://docs.microsoft.com/en-us/dotnet/api/system.threading.thread?view=net-6.0</a>.</p>
			<p>Although managing threads come with the advantage of having more control over how they are executed, it also comes with overheads in the form of the following:</p>
			<ul>
				<li>Managing the life cycle of threads, such as creating threads, recycling them, and context switching.</li>
				<li>Implementing concepts such as progress tracking/reporting for thread execution. Also, cancellation is quite complex and has limited support.</li>
				<li>Exceptions on threads need to be handled appropriately; otherwise, they may lead to the application crashing.</li>
				<li>Debugging, testing, and code maintenance can become a bit complex an<a id="_idTextAnchor235"/>d, at times, can lead to performance issues if not handled correctly.</li>
			</ul>
			<p>This is where <a id="_idIndexMarker210"/>the <code>ThreadPool</code> comes into<a id="_idIndexMarker211"/> play, which is discussed in<a id="_idTextAnchor236"/><a id="_idTextAnchor237"/> the next section.</p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor238"/>ThreadPool</h2>
			<p>Threads can be <a id="_idIndexMarker212"/>created by making use of pools of threads managed by .NET Core, more commonly known as the CLR <code>ThreadPool</code>. The CLR <code>ThreadPool</code> is a set of worker threads t<a id="_idTextAnchor239"/>hat are loaded into your application along with the CLR and take care of the thread life cycle, including recycling threads, creating threads, and supporting better context switching. The CLR <code>ThreadPool</code> can be consumed by various APIs available in the <code>System.Threading.ThreadPool</code> class. Specifically, for scheduling an operation on a thread, there is the <code>QueueUserWorkItem</code> method, which takes a delegate of the method that needs to be scheduled. In the previous code, let's replace the code for creating a new thread with the following code, meaning the application will use <code>ThreadPool</code>:</p>
			<pre class="source-code">ThreadPool.QueueUserWorkItem(FetchDataFromAPI);</pre>
			<p>As the name suggests, <code>QueueUserWorkItem</code> of the <code>ThreadPool</code> class does make use of queues, whereby any code that is supposed to be executed on th<a id="_idTextAnchor240"/>e <code>ThreadPool</code> thread would be <a id="_idIndexMarker213"/>queued and then dequeued – that is, assigned to a worker thread in a <strong class="bold">First-In, First-Out</strong> (<strong class="bold">FIFO</strong>) manner.</p>
			<p>The way <code>ThreadPool</code> is designed is that it has a global queue, and items are queued in it when we do the following:</p>
			<ul>
				<li>Call <code>QueueUserWorkItem</code> or a similar method of the <code>ThreadPool</code> class using a thread that is not part of the <code>ThreadPool</code> threads</li>
				<li>Call through the TPL</li>
			</ul>
			<p>When a new thread is created in <code>ThreadPool</code>, it maintains its own local queue that checks the global queue and dequeues the work item in a FIFO manner; however, if the code executing on this thread creates another thread, such as a child thread, then that gets queued in the local queue as opposed to the global queue. </p>
			<p>The order of<a id="_idIndexMarker214"/> execution for operations in the local queue of the worker thread is always <code>ThreadPool</code>, where <em class="italic">n</em> is the number of threads in <code>ThreadPool</code> – that is, <em class="italic">n</em> local queues – and <em class="italic">1</em> refers to the global queue.</p>
			<p>A high-level representation of <code>ThreadPool</code> is shown in t<a id="_idTextAnchor241"/>he following figure:</p>
			<div><div><img src="img/Figure_4.3_B18507.jpg" alt="Figure 4.3 – ThreadPool high-level representation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.3 – ThreadPool high-level representation</p>
			<p>Apart from <code>QueueUserWorkItem</code>, there are a lot of other properti<a id="_idTextAnchor242"/>es/methods available for the <code>ThreadPool</code> class, such as these:</p>
			<ul>
				<li><code>SetMinThreads</code>: Used to set the minimum worker and asynchronous I/O threads that <code>ThreadPool</code> will have when the program is started</li>
				<li><code>SetMaxThreads</code>: Used to set the maximum worker and asynchronous I/O threads that <code>ThreadPool</code> will have, after which, new request<a id="_idTextAnchor243"/>s are queued</li>
			</ul>
			<p>All the properties<a id="_idIndexMarker215"/> and methods of the <code>ThreadPool</code> class can be further reviewed at <a href="https://docs.microsoft.com/en-us/dotnet/api/system.threading.threadpool?view=net-6.0">https://docs.microsoft.com/en-us/dotnet/api/system.threading.threadpool?view=net-6.0</a>.</p>
			<p>Although writing multithreaded code via <code>QueueUserWorkItem</code> of the <code>ThreadPool</code> thread simplifies life cycle management for threads, it has its own limitations:</p>
			<ul>
				<li>We cannot get a response from the work that is scheduled on the <code>ThreadPool</code> thread, hence the return type of the delegate is void.</li>
				<li>It is not easy to track the progress of the work that is scheduled on the <code>ThreadPool</code> thread, so something such as progress reporting isn't easy to achieve.</li>
				<li>It's not meant for long-running requests.</li>
				<li><code>ThreadPool</code> threads are always background threads; so, unlike foreground threads, if a process is shut down, it will not wait for the <code>ThreadPool</code> threads to complete their work.</li>
			</ul>
			<p>As there are limitations with <code>QueueUserWorkItem</code>, the <code>ThreadPool</code> threads<a id="_idTextAnchor244"/> can also be consumed through the TPL, which we will use in our enterprise application and is covered later in this chapter. In .NET Core, the TPL is the preferred approach to achieve concurrency/parallelism, as it overcomes all the limitations we have seen so far and eventually <a id="_idIndexMarker216"/>helps to achieve the goal of allowing your application to sc<a id="_idTextAnchor245"/><a id="_idTextAnchor246"/>ale and be responsive.</p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor247"/>Lazy initialization</h2>
			<p>The lazy initialization <a id="_idIndexMarker217"/>of a class is a pattern where the creation of an object is deferred until it is used for the first time. This pattern is based on the premise that as long as a class's properties are not being used, there is no advantage to initializing an obj<a id="_idTextAnchor248"/>ect. Hence, this delays object creation and ultimately reduces the memory footprint of the application, improving performance. An example of this would be creating a database connection object only when you are about to retrieve data from a database. Lazy initialization is a good fit for classes that hold a lot of data and are potentially expensive to create. For instance, a class for loading all the products in an e-commerce application can be lazily initialized only when there is a need to list the products.</p>
			<p>A typical implementation of such a class, as presented next, restricts the initialization of properties in constructors and has one or more methods that populate the properties of the class:</p>
			<pre class="source-code">        public class ImageFile</pre>
			<pre class="source-code">        {</pre>
			<pre class="source-code">            string fileName;</pre>
			<pre class="source-code">            object loadImage;</pre>
			<pre class="source-code">            public ImageFile(string fileName)</pre>
			<pre class="source-code">            {</pre>
			<pre class="source-code">                this.fileName = fileName;</pre>
			<pre class="source-code">            }</pre>
			<pre class="source-code">            public object GetImage()</pre>
			<pre class="source-code">            {</pre>
			<pre class="source-code">                if (loadImage == null)</pre>
			<pre class="source-code">                {</pre>
			<pre class="source-code">                    loadImage = File.ReadAllText(fileName);</pre>
			<pre class="source-code">                }</pre>
			<pre class="source-code">                return loadImage;</pre>
			<pre class="source-code">            }</pre>
			<pre class="source-code">        }</pre>
			<p>Assuming this is a class used to load an image from a disk, there is no use in l<a id="_idTextAnchor249"/>oading the image in the constructor <a id="_idIndexMarker218"/>because it cannot be consumed until the <code>GetImage</code> method is called. So, the lazy initialization pattern suggests that instead of initializing the <code>loadImage</code> object in the constructor, it should be initialized in <code>GetImage</code>, which means that the image is loaded into memory only when it is needed. This can also be achieved through properties, as shown here:</p>
			<pre class="source-code">        object loadImage;</pre>
			<pre class="source-code">        public object LoadImage</pre>
			<pre class="source-code">        {</pre>
			<pre class="source-code">            get</pre>
			<pre class="source-code">            {</pre>
			<pre class="source-code">                if (loadImage == null)</pre>
			<pre class="source-code">                {</pre>
			<pre class="source-code">                    loadImage = File.ReadAllText(fileName);</pre>
			<pre class="source-code">                }</pre>
			<pre class="source-code">                return loadImage;</pre>
			<pre class="source-code">            }</pre>
			<pre class="source-code">        }</pre>
			<p>As you can see, this is something that's typically done with cache objects and is also known as <a id="_idIndexMarker219"/>the <code>LoadImage</code> method or property, it will lead to making a call to disk multiple times. So, there is a need for synchronizat<a id="_idTextAnchor250"/>ion here through locks or some other mechanism, which obviously will add to the maintenance overhead, and the class implementation might become even more complex.</p>
			<p>So, even though we can implement our own lazy load pattern, in C#, we have the <code>System.Lazy</code> class to handle such an implementation. One of the key advantages of using the <code>System.Lazy</code> class is that it is thread-safe.</p>
			<p>The <code>S<a id="_idTextAnchor251"/>ystem.Lazy</code> class provides multiple constructors to implement lazy initialization. Here<a id="_idIndexMarker220"/> are the two most common ways that we can make use of:</p>
			<ul>
				<li>Wrapping the class around <code>Lazy</code> and using the <code>Value</code> method of that object to retrieve data. This is typically used for classes that have initialization logic in constructors. Some sample code follows:<pre>        public class ImageFile
        {
            string fileName;
            public object LoadImage { get; set; }
            public ImageFile(string fileName)
            {
                this.fileName = fileName;
                this.LoadImage = $"File {fileName}
                 loaded from disk";
            }
        }</pre></li>
			</ul>
			<p>While initializing this class, we will use the generic type of the <code>System.Lazy</code> class and pass the <code>ImageFile</code> class as its type and the object of <code>ImageFile</code> as a delegate:</p>
			<pre>        Lazy&lt;ImageFile&gt; imageFile = new
         Lazy&lt;ImageFile&gt;(() =&gt; new ImageFile("test"));
        var image = imageFile.Value.LoadImage;</pre>
			<p>Here, if you put a breakpoint in the <code>ImageFile</code> class's constructor, it would be hit only when the <code>Value</code> method of the <code>System.Lazy</code> cl<a id="_idTextAnchor252"/>ass is called.</p>
			<ul>
				<li>For classes that have a method to load various parameters, we can pass the method to <a id="_idIndexMarker221"/>the <code>Lazy</code> class as a delegate. Taking the previous sample code and moving the file-retrieving logic to a separate method is shown here:<pre>        public class ImageFile
        {
            string fileName;
            public object LoadImage { get; set; }
            public ImageFile(string fileName)
            {
                this.fileName = fileName;
            }
            public object LoadImageFromDisk()
            {
                this.LoadImage = $"File
                 {this.fileName} loaded from disk";
                return LoadImage;
            }
        }</pre></li>
			</ul>
			<p>And while initializing this class, we pass a Lambda to the generic<a id="_idTextAnchor253"/> delegate, and that generic delegate is passed to initialize an object of the <code>System.Lazy</code> class, as shown in the following code:</p>
			<pre>        Func&lt;object&gt; imageFile = new Func&lt;object&gt;(()
         =&gt; { var obj = new ImageFile("test");
        return obj.LoadImageFromDisk(); });
        Lazy&lt;object&gt; lazyImage = new
         Lazy&lt;object&gt;(imageFile);
        var image = lazyImage.Value;</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">A func in C# is a type of delegate that takes zero or more parameters and returns a value. More details can be found here: <a href="https://docs.microsoft.com/en-us/dotnet/api/system.func-1?view=net-6.0">https://docs.microsoft.com/en-us/dotnet/api/system.func-1?view=net-6.0</a>.</p>
			<p>Both ways will <a id="_idIndexMarker222"/>delay the initializing of the object until the call to the <code>Value</code> method is made. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">One important thing that we need to note is that although <code>Lazy</code> objects are thread-safe, objects created through values aren't thread-safe. So, in this case, <code>lazyImage</code> is thread-safe, but <code>image</code> isn't. Hence, it needs to be synchronized in a multithreaded environment.</p>
			<p>In general, lazy initialization is a good fit for caching classes and singleton classes and can be further extended for objects that are expensive to in<a id="_idTextAnchor254"/>itialize.</p>
			<p>All the properties of the <code>Lazy</code> class can be further reviewed at <a href="https://docs.microsoft.com/en-us/dotnet/api/system.lazy-1?view=net-6.0">https://docs.microsoft.com/en-us/dotnet/api/system.lazy-1?view=net-6.0</a>.</p>
			<p>Although lazy initialization can be achieved by wrapping the underlying object with the <code>System.Lazy</code> class, there is also the <code>LazyInitializer</code> static class available in .NET that can be used for lazy initialization through its <code>EnsureInitial<a id="_idTextAnchor255"/>ized</code> method.</p>
			<p>It has a couple of constructors as mentioned in the MSDN documentation at <a href="https://docs.microsoft.com/en-us/dotnet/api/system.threading.lazyinitializer.ensureinitialized?view=net-6.0">https://docs.microsoft.com/en-us/dotnet/api/system.threading.lazyinitializer.ensureinitialized?view=net-6.0</a>.</p>
			<p>However, the idea is the same, in that it expects an object and a function to populate the object. Taking the previous example, if we had to use <code>LazyInitializer.EnsureInitialized</code> for lazy initialization, we would need to pass the instance of the object and the Lambda that creates the actual object to <code>LazyInitializer.EnsureInitialized</code>, as shown in the following code:</p>
			<pre class="source-code">        object image = null;</pre>
			<pre class="source-code">        LazyInitializer.EnsureInitialized(ref image, () =&gt;</pre>
			<pre class="source-code">            {</pre>
			<pre class="source-code">                var obj = new ImageFile("test");</pre>
			<pre class="source-code">                return obj.LoadImageFromDisk();</pre>
			<pre class="source-code"><a id="_idTextAnchor256"/>            });</pre>
			<p>Here, we are <a id="_idIndexMarker223"/>passing two parameters – one is the object that holds the value of the property of the <code>image</code> class, and the other is the function that creates an object of the <code>image</code> class and returns the image. So, this is as simple as calling the <code>Value</code> property of the <code>System.Lazy</code> property without having the overhead of initializing the object.</p>
			<p>Clearly, a small added advantage of lazy initializing using <code>LazyInitializer</code> is that there aren't additional objects that aren't created, meaning a smaller memory footprint. On the other hand, <code>System.Lazy</code> provides much more readable code. So, if there are clear <em class="italic">space optimizations</em>, go with <code>LazyInitializer</code>; otherwise, use <code>System.Lazy</code> for much cl<a id="_idTextAnchor257"/><a id="_idTextAnchor258"/>eaner and more readable code.</p>
			<h1 id="_idParaDest-76"><a id="_idTextAnchor259"/>Understanding locks, semaphores, and SemaphoreSlim</h1>
			<p>In the previous <a id="_idIndexMarker224"/>sections, we <a id="_idIndexMarker225"/>saw<a id="_idIndexMarker226"/> how we can use various APIs in .NET to achieve parallelism. However, when we are doing that, we need to take additional care with shared variables. Let's take the enterprise e-commerce application that we are building in this book. Think about the workflow of purchasing an item. Say th<a id="_idTextAnchor260"/>at two users are planning to buy a product and only one item i<a id="_idTextAnchor261"/>s available. Let's say that both users add the item to the cart and the first user p<a id="_idTextAnchor262"/>laces their order, and while the order is being processed through the payment gateway, the second user also tries to place their order.</p>
			<p>In such cases, the second order should fail (assuming that the first order succeeded) because the quantity for the book is now zero; that would happen only if there was proper synchronization being applied to the quantity across threads. Also, if the first order fails in the payment gateway or the first user cancels their transaction, the second order should go through. So, what we are saying here is that the quantity should be locked while the first order is being processed and should be released only when the order is completed (ending in success or failure). Before we get into the handling mechanism, let's quickly rec<a id="_idTextAnchor263"/><a id="_idTextAnchor264"/>ap what the critical section is.</p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor265"/>The critical section and thread safety</h2>
			<p>The critical <a id="_idIndexMarker227"/>section is the part of<a id="_idIndexMarker228"/> an application that reads/writes variables that are used by multiple threads. We can think of these as the global variables th<a id="_idTextAnchor266"/>at are used across the application and are modified in different places at different times or at the same time. In a multithreaded scenario, at any poi<a id="_idTextAnchor267"/>nt in time, only one thread should be allowed to modify such variables, and only one thread should be allowed to enter the critical section. </p>
			<p>If there are no such variables/sections in your application, it can be considered thread-safe. So, it's always advisable to identify variables in the application that are not thread-safe and handle them accordingly. To protect access to the critical section fro<a id="_idTextAnchor268"/>m non-thread-safe variables, there<a id="_idIndexMarker229"/> are <a id="_idIndexMarker230"/>va<a id="_idTextAnchor269"/>rious constructs available, known as <strong class="bold">synchronization primitives</strong> or <strong class="bold">synchronization constructs</strong>, which primarily fall into two categories:</p>
			<ul>
				<li><strong class="bold">Locking constructs</strong>: These allow a thread to e<a id="_idTextAnchor270"/>nter the critical section to protect access to the shared resources, and all other threads wait until the lock is freed by the acquired thread.</li>
				<li><strong class="bold">Signaling constructs</strong>: These allow a thread to enter t<a id="_idTextAnchor271"/>he critical section by signaling the availability of resources, as in a producer-consumer model, where a producer<a id="_idIndexMarker231"/> locks a resource and the consumer waits for a signal rather <a id="_idIndexMarker232"/>than polling.</li>
			</ul>
			<p>Let's discuss a few synchroni<a id="_idTextAnchor272"/><a id="_idTextAnchor273"/>zation primitives in the next section.</p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor274"/>Introducing locks</h2>
			<p>A <strong class="bold">lock</strong> is a basic<a id="_idIndexMarker233"/> class that allows you to achieve synchron<a id="_idTextAnchor275"/>ization in multithreaded code where any variable inside the lock block can be accessed by only one thread. In locks, the thread acquiring the lock needs to release the lock, and until then, any other thread trying to enter the lock goes into a wait state. A simple lock can be created, as shown in the following code:</p>
			<pre class="source-code">            object locker = new object();</pre>
			<pre class="source-code">            lock (locker)</pre>
			<pre class="source-code">            {</pre>
			<pre class="source-code">                quantity--;</pre>
			<pre class="source-code">            }</pre>
			<p>The thread that is the first to execute this code will acquire the lock and release it after the completion of the code block. Locks can also be acquired using <code>Monitor.Enter</code> and <code>Monitor.Exit</code>, and in fact, using a lock compiler internally converts the thread t<a id="_idTextAnchor276"/>o <code>Monitor.Enter</code> and <code>Monitor.Exit</code>. A few important points about locks follow:</p>
			<ul>
				<li>They should always be used on the reference type due to their thread affinity.</li>
				<li>They are very expensive in terms of performance, as they pause the threads that want to enter the critical section before allowing them to resume, which adds some lag.</li>
				<li>Double-checking the acquiring lock is also a good practice, similar to how it is done in<a id="_idTextAnchor277"/> the singleton implementation.</li>
			</ul>
			<p>Locks do have some problems:</p>
			<ul>
				<li>You need to lock the shared data/object wherever it's being modified or enumerated. It's easy to miss critical sections in the application as <em class="italic">critical section</em> is more of a logical term. Compilers will not flag it if there aren't any locks around a critical section.</li>
				<li>If not handled correctly, you mig<a id="_idTextAnchor278"/>ht end up in a deadlock.</li>
				<li>Scalability is a problem, as only one thread can access a lock at a time, while all other threads must wait.<p class="callout-heading">Note</p><p class="callout">There is another important<a id="_idIndexMarker234"/> concept known as <strong class="bold">atomicity</strong>. An operation is atomic only if there isn't any way to read the intermediate state of a variable or to write the intermediate state to a variable. For example, if an integer's value is being modified from two to six, any thread reading this integer value will only see two or six; none of the threads will see the thread's intermediate state where the integer was only partially updated. Any code that is thread-safe automatically guarantees atomicity.</p><p class="callout">Use concurrent <a id="_idIndexMarker235"/>collections, described in a later section, instead of locks, as concurrent collections int<a id="_idTextAnchor279"/><a id="_idTextAnchor280"/>ernally handle locking critical secti<a id="_idTextAnchor281"/>ons.</p></li>
			</ul>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor282"/>Mutex (Windows only)</h2>
			<p>A <code>System.Threading.Mutex</code> class, and any thread that wants to enter the critical section needs to call the <code>WaitOne</code> method. Releasing a mutex happens through the <code>ReleaseMutex</code> method; so, we basically create an instance of the <code>System.Threading.Mutex</code> class and call <code>WaitOne</code>/<code>ReleaseMutex</code> to enter/exit the critical section, respectively. A couple of important<a id="_idTextAnchor283"/> points about mutexes follow:</p>
			<ul>
				<li>Mutexes have thread affinity, so a thread that calls <code>WaitOne</code> needs to call <code>ReleaseMutex</code>.</li>
				<li>A constructor of the <code>System.Threading.Mutex</code> class is available that accepts the name of a mutex, which allows sharing across processes<a id="_idTextAnchor284"/><a id="_idTextAnchor285"/> using the name<a id="_idIndexMarker237"/> passed to the constructor.</li>
			</ul>
			<h2 id="_idParaDest-80"><a id="_idTextAnchor286"/>Introducing semaphores and SemaphoreSlim</h2>
			<p>A <strong class="bold">semaphore</strong> is a<a id="_idIndexMarker238"/> non-exclusive<a id="_idIndexMarker239"/> lock that supports synchronization by allowing multiple threads to enter a critical section. However, unlike exclusive <a id="_idTextAnchor287"/>locks, a semaphore is used in scenarios where there is a need to restrict access to a pool of resources – for example, a database <a id="_idTextAnchor288"/>connection pool that allows a fixed number of connections between an application and a database. </p>
			<p>Going back to our example of shopping for a product in an e-commerce application, if  the available quantity of a product is 10, that means that 10 people can add this item to their shopping carts and place orders. If 11 orders are placed concurrently, 10 users should be allowed to place orders, and the 11th should be put on hold until the first 10 orders are completed.</p>
			<p>In .NET, a semaphore can be created by creating an instance of the <code>System.Threading.Semaphore</code> class and passing two parameters:</p>
			<ul>
				<li>The initial number of active requests</li>
				<li>The total number of concurrently allowed requests</li>
			</ul>
			<p>Here is a simple code snippet that creates a semaphore:</p>
			<pre class="source-code">Semaphore quantity = new Semaphore(0, 10);</pre>
			<p>In this case, <code>0</code> means none of the requests has acquired the shared resource and a maximum of 10 concurrent requests are allowed. To acquire a shared resource, we need to call <code>WaitOne()</code>, and to release a resource, we need to call the <code>Release()</code> method.</p>
			<p>To create semaphores, there is another lightweight class available in .NET, and that is <code>SemaphoreSlim</code>, the slim version, which usually relies on a<a id="_idIndexMarker240"/> concept called <code>SemaphoreSlim</code> uses a small loop that runs for a few microseconds so that it doesn't have to go through the costly process of blocking, context switching, and internal kernel transition (semaphores use Windows kernel semaphores to lock a resource). Eventually, <code>SemaphoreSlim</code> falls back to locking if the shared resource still needs to be locked.</p>
			<p>Creating a <code>SemaphoreSlim</code> instance is almost the same as for semaphores; the only difference is that for locking, it has <code>WaitAsync</code> instead of <code>WaitOne</code>. There is also <code>CurrentCount</code> available, which tells u<a id="_idTextAnchor289"/>s the number of locks acquired.</p>
			<p>Some key facts about semap<a id="_idTextAnchor290"/>hores and <code>SemaphoreSlim</code> follow:</p>
			<ul>
				<li>As a semaphore is used to access a pool of resources, semaphores and <code>SemaphoreSlim</code> don't have thread affinity, and any thread can release a resource.</li>
				<li>The <code>Semaphore</code> class in .NET Core supports named semaphores. Named semaphores can be used to lock resources across processes; however, the <code>SemaphoreSlim</code> class does not support named semaphores.</li>
				<li>The <code>SemaphoreSlim</code> class, unlike <code>Semaphore</code>, supports asynchronous methods and cancellation, which means it can be used well with async-await methods. The <a id="_idIndexMarker241"/>async-await keyword helps in writing non-blocking <a id="_idIndexMarker242"/>asynchronous methods and is covered in the <a id="_idTextAnchor291"/><a id="_idTextAnchor292"/><em class="italic">Introducing async-await</em> section in this chapter.</li>
			</ul>
			<h2 id="_idParaDest-81">Choosing the <a id="_idTextAnchor293"/>right synchronization constructs</h2>
			<p>There are <a id="_idIndexMarker243"/>other signaling constructs to cover; the following table gives you a high-level view of their usage and real-life examples of them:</p>
			<div><div><img src="img/Table_4.1_a.jpg" alt="Table 4.1 – A synchronization constructs comparison&#13;&#10;"/>
				</div>
			</div>
			<div><div><img src="img/Table_4.1_b.jpg" alt="Table 4.1 – A synchronization constructs comparison&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Table 4.1 – A sy<a id="_idTextAnchor294"/>nchronization constructs comparison</p>
			<p>So far, we <a id="_idIndexMarker244"/>have covered the following:</p>
			<ul>
				<li>Various ways of multithreading using the <code>Thread</code> and <code>ThreadPool</code> classes and their limitations</li>
				<li>The importance of lazy initialization and how it helps in multithreaded environments</li>
				<li>The various synchronization co<a id="_idTextAnchor295"/>nstructs that are available in .NET</li>
			</ul>
			<p>We will use these concepts in later chapters when we create some cross-cutting components.</p>
			<p>In the next section, we will see how to overcome the limitations of <code>Thread</code> and <code>T<a id="_idTextAnchor296"/><a id="_idTextAnchor297"/>hreadPool</code> through<a id="_idIndexMarker245"/> tasks and the use of the TPL.</p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor298"/>Introducing tasks and parallels</h1>
			<p>We know <a id="_idIndexMarker246"/>that<a id="_idIndexMarker247"/> asynchronous programming<a id="_idIndexMarker248"/> helps our applic<a id="_idTextAnchor299"/>ations to scale and respond better, so implementing asynchronous applications should not be overhead for developers. <code>Thread</code> and <code>ThreadPool</code>, while helping to achieve asynchronicity, add a lot of overhead and come with limitations. </p>
			<p>Hence, Microsoft came up with tasks that make it easier to deve<a id="_idTextAnchor300"/>lop asynchronous applications. In fact, most of the newer APIs in .NET 6 only support the asynchronous way of programming – for<a id="_idIndexMarker249"/> example, the <strong class="bold">Universal Windows Platform</strong> (<strong class="bold">UWP</strong>) doesn't even expose APIs to create threads without tasks. As such, understanding tasks and the TPL is fundamental to being able to write asynchronous programs using C#. </p>
			<p>We will dive deep into these topics in this section, and later, we will see how the C# async-await keywords comb<a id="_idTextAnchor301"/><a id="_idTextAnchor302"/>ined with the TPL simplify asynchronous programming.</p>
			<h2 id="_idParaDest-83"><a id="_idTextAnchor303"/>Introduction to Task and the TPL</h2>
			<p>The idea behind <a id="_idIndexMarker250"/>asynchronous<a id="_idIndexMarker251"/> progra<a id="_idTextAnchor304"/>mming is that none of the threads should be waiting on an operation <a id="_idTextAnchor305"/>– that is, the framework should have the capability to wrap an operation into some abstraction and then resume once the operation is completed without blocking any threads. This abstraction is nothing but the <code>Task</code> class, which is exposed through <code>System.Threading.Tasks</code> and helps in writing asynchronous code in .NET.</p>
			<p>The <code>Task</code> class simplifies wrapping any wait operation, whether it is data retrieved from a database, a file being loaded into memory<a id="_idTextAnchor306"/> from disk, or any highly CPU-intensive operat<a id="_idTextAnchor307"/>ion, and simplifies running it on a separate thread if needs be. It has the following important features:</p>
			<ul>
				<li><code>Task</code> supports returning values from an operation once it is completed through its generic type, <code>Task&lt;T&gt;</code>.</li>
				<li><code>Task</code> takes care of scheduling threads on <code>ThreadPool</code>, partitioning operations, and scheduling more than one thread from <code>ThreadPool</code> accordingly, all while abstracting the complexity of doing it.</li>
				<li>Reports completion supports cancellation through <code>CancellationToken</code> and progress reporting through <code>IProgress</code>.</li>
				<li><code>Task</code> supports creating child tasks and manages relationships between child and parent tasks.</li>
				<li>Exceptions are propagated to the calling application, even for multi-hierarchical parent/child tasks.</li>
				<li>Most importantly, <code>Task</code> supports async-await, which helps in resuming the processing in a calling application/method once the operation in the task is completed.</li>
			</ul>
			<p>The TPL is a group of APIs provided by .NET in <code>System.Threading.Tasks</code> and <code>System.Threading</code>, and it provides wa<a id="_idTextAnchor308"/>ys to create and manage tasks. Tasks can be created by creating an object of the <code>System.Threading.Tasks.Task</code> class and passing a<a id="_idIndexMarker252"/> block <a id="_idIndexMarker253"/>of code that needs to be executed on the task. We can create a task in multiple ways:</p>
			<ul>
				<li>You can create an object of the <code>Task</code> class and pass a Lambda expression. In this method, it needs to be started explicitly, as shown in the following code:<pre>            Task dataTask = new Task(() =&gt;
             FetchDataFromAPI("https://foo.com/api"));
            dataTask.Start();</pre></li>
				<li>A task can also be created using <code>Task.Run</code>, as shown in the following code, which supports creating and starting the task without explicitly calling <code>Start()</code>:<pre>Task dataTask = Task.Run(() =&gt; FetchDataFromAPI ("https://foo.com/api"));</pre></li>
				<li>Another way to create a task is by using <code>Task.Factory.StartNew</code>:<pre>Task dataTask = Task.Factory.StartNew(() =&gt; FetchDataFromAPI("https://foo.com/api"));</pre></li>
			</ul>
			<p>In all these methods, a <code>ThreadPo<a id="_idTextAnchor309"/>ol</code> thread is used to run the <code>FetchDataFromAPI</code> method and is refe<a id="_idTextAnchor310"/>renced via the <code>dataTask</code> object, which is returned to the caller to track the completion of the operation/exception. </p>
			<p>As this task would asynchronously execute on a <code>ThreadPool</code> thread, and as all <code>ThreadPool</code> threads are background threads, the application wouldn't wait for the <code>FetchDataFromAPI</code> method to complete. The TPL exposes a <code>Wait</code> method to wait on the completion of the task, such as <code>dataTask.Wait()</code>. Here is a code snippet from a <a id="_idIndexMarker254"/>small <a id="_idIndexMarker255"/>console application that uses a task:</p>
			<pre class="source-code">Task t = Task.Factory.StartNew(() =&gt;</pre>
			<pre class="source-code">             FetchDataFromAPI("https://foo.com"));</pre>
			<pre class="source-code">t.Wait();</pre>
			<pre class="source-code">void FetchDataFromAPI(string apiURL)</pre>
			<pre class="source-code">{</pre>
			<pre class="source-code">     Thread.Sleep(2000);</pre>
			<pre class="source-code">     Console.WriteLine("data returned from API");</pre>
			<pre class="source-code">}</pre>
			<p>In this snippet, we used a Lambda expression. However, it could be a delegate or action delegate (in the case of a parameter-less method), so something such as the following can also be used to create a task:</p>
			<pre class="source-code">Task t = Task.Factory.StartNew(delegate { FetchDataFromAPI("https://foo.com");});</pre>
			<p>Either way, you receive a reference to<a id="_idTextAnchor311"/> the <code>Task</code> object and handle it accordingly. If a meth<a id="_idTextAnchor312"/>od is returning a value, then we can use a generic version of the <code>Task</code> class and use the <code>Result</code> method to retrieve data from <code>Task</code>. For example, if <code>FetchDataFromAPI</code> returns a string, we can use <code>Task&lt;String&gt;</code>, as shown in the following snippet:</p>
			<pre class="source-code">            Task&lt;string&gt; t =</pre>
			<pre class="source-code">             Task.Factory.StartNew&lt;string&gt;(()</pre>
			<pre class="source-code">             =&gt; FetchDataFromAPI(""));</pre>
			<pre class="source-code">            t.Wait();</pre>
			<pre class="source-code">            Console.WriteLine(t.Result);</pre>
			<p>There are various additional parameters that each of these methods accepts, and a few important ones are as follows:</p>
			<ul>
				<li>Cancellation using an object of the <code>CancellationToken</code> class, generated using the <code>CancellationTokenSource</code> class.</li>
				<li>Control the behavior of task creation and execution through the <code>TaskCreationOptions</code> enum.</li>
				<li>Custom implementation of <code>TaskScheduler</code> to control how tasks are queued.</li>
			</ul>
			<p><code>TaskCreationOptions</code> is an enum in the TPL that tells <code>TaskScheduler</code> what kind of task we are creating. For example, we can create a long-running task, as follows:</p>
			<pre class="source-code">Task&lt;string&gt; t = Task.Factory.StartNew&lt;string&gt;(() =&gt; FetchDataFromAPI(""), TaskCreationOptions.LongRunning);</pre>
			<p>Although this<a id="_idIndexMarker256"/> doesn't<a id="_idIndexMarker257"/> guarantee any faster output, it acts more like a hint to the scheduler to optimize itself. For example, the scheduler can spin up more threads if it sees a long-running task being scheduled. All the options for this enum can be found at <a href="https://docs.microsoft.com/en-us/dotnet/api/system.threading.tasks.taskcreationoptions?view=net-6.0">https://docs.microsoft.com/en-us/dotnet/api/system.threading.tasks.taskcreationoptions?view=net-6.0</a>.</p>
			<p><code>Task</code> also supports waiting on multiple tasks at the same time by creating and passing all the tasks as parameters to the following methods:</p>
			<ul>
				<li><code>WaitAll</code>: Wait for the completion of all tasks and block the current thread. Not recommended for application development.</li>
				<li><code>WhenAll</code>: Wait for the completion of all tasks without blocking the current thread. Usually used with async-await. Recommended for application development.</li>
				<li><code>WaitAny</code>: Wait for the completion of one of the tasks and block the current thread until then. Not recommended for application development.</li>
				<li><code>WhenAny</code>: Wait for the completion of one of the tasks without blocking the current thread. Usually<a id="_idIndexMarker258"/> used with async-awai<a id="_idTextAnchor314"/>t. Not recommended for application <a id="_idIndexMarker259"/>development<a id="_idTextAnchor315"/>.</li>
			</ul>
			<p>Tasks, unlike threads, have comprehensive e<a id="_idTextAnchor316"/><a id="_idTextAnchor317"/>xception handling support. Let's see that in the next section.</p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor318"/>Handling task exceptions</h2>
			<p>Exception <a id="_idIndexMarker260"/>handling in tasks is as simp<a id="_idTextAnchor319"/>le as writing a <code>try</code> block around the task and then catching the exceptions, which are usually wrapped in <code>AggregateException</code>, as shown in the following code snippet:</p>
			<pre class="source-code">            try</pre>
			<pre class="source-code">            {</pre>
			<pre class="source-code">                Task&lt;string&gt; t =</pre>
			<pre class="source-code">                 Task.Factory.StartNew&lt;string&gt;(()</pre>
			<pre class="source-code">                 =&gt; FetchDataFromAPI(""));</pre>
			<pre class="source-code">                t.Wait();</pre>
			<pre class="source-code">            }</pre>
			<pre class="source-code">            catch (AggregateException agex)</pre>
			<pre class="source-code">            {</pre>
			<pre class="source-code">                //Handle exception</pre>
			<pre class="source-code">                Console.WriteLine(</pre>
			<pre class="source-code">                  agex.InnerException.Message);</pre>
			<pre class="source-code">            }</pre>
			<p>In the preceding code, <code>agex.InnerException</code> will give you the actual exception, as we are waiting on a single task. However, if we are waiting on multiple tasks, it would be the <code>InnerExceptions</code> collection that we could loop through. Also, it comes with a <code>Handle</code> callback method, which can be subscribed in a <code>catch</code> block, and the callback once triggered will have information about the exception.</p>
			<p>As shown in the preceding code, for a tas<a id="_idTextAnchor320"/>k to propagate an exception, we need to call the <code>Wait</code> method or some other blocking construct such as <code>WhenAll</code> to trigger the <code>catch</code> block. However, under the hood, any exception to <code>Task</code> is actually held in the <code>Exception</code> property of the <code>Task</code> class, which is of the <code>AggregateException</code> type and can be observed for any underlying exceptions in the task.</p>
			<p>Also, if a task is the parent of attached child tasks or nested tasks, or if you are waiting on multiple tasks, multiple exceptions can be thrown. To propagate all the exceptions back to the calling<a id="_idIndexMarker261"/> thread, the <code>Task</code> infrastru<a id="_idTextAnchor321"/>cture wraps them in an <code>AggregateException</code> instance.</p>
			<p>More details about handling exceptions can be found at <a href="https://docs.microsoft.com/en-us/dotnet/standard/parallel-programming/exception-handling-task-parallel-library">https://docs.microsoft.com/en-us/dotnet/standard/parallel-programming/exception-handling-task-parallel-library</a>.</p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor325"/>Implementing task cancellation</h2>
			<p>.NET provides <a id="_idIndexMarker262"/>two primary classes to support the cancellation of a task:</p>
			<ul>
				<li><code>CancellationTokenSource</code>: A class that creates a cancellation token and supports the cancellation of a token through the <code>Cancel</code> method</li>
				<li><code>CancellationToken</code>: A structure that listens to cancellation and triggers a notification if a task is canceled</li>
			</ul>
			<p>For canceling a task, there are two types of cancellation: </p>
			<ul>
				<li>One where a task is executed by mistake and needs to be canceled immediately </li>
				<li>Another where a task has started and needs to be stopped (aborted) midway </li>
			</ul>
			<p>For the former, we can create a task that supports cancellation. We use the TPL APIs and pass the cancellation token to the constructor and call the <code>Cancel</code> method of the <code>CancellationTokenSource</code> class if the task needs to be canceled, as shown in the following code snippet:</p>
			<pre class="source-code">            cts = new CancellationTokenSource();</pre>
			<pre class="source-code">            CancellationToken token = cts.Token;</pre>
			<pre class="source-code">            Task dataFromAPI = Task.Factory.StartNew(()</pre>
			<pre class="source-code">             =&gt; FetchDataFromAPI(new List&lt;string&gt; {</pre>
			<pre class="source-code">                "https://foo.com",</pre>
			<pre class="source-code">                "https://foo1.com",}), token);</pre>
			<pre class="source-code">            cts.Cancel();</pre>
			<p>All the .NET Core APIs that support asynchronous calling, such as <code>GetAsync</code> and <code>PostAsync</code> of the <code>HttpClient</code> class, have overloads to accept cancellation tokens. For the latter case (aborting a task), the decision<a id="_idTextAnchor326"/> is based on whether the operation that would be running supports cancellation or not. Assuming it supports cancellation, we can pass the cancellation token to the method and, inside the method call, check the <code>IsCancellationRequested</code> property of the cancellation token and handle it accordingly.</p>
			<p>Let's create a simple console application that creates a task that does support cancellation. Here, we are creating a <code>FetchDataFromAPI</code> method that accepts a list of URLs and retrieves data from those URLs. This method also supports cancellation using <code>CancellationToken</code>. In the implementation, we loop through the list of URLs and continue until <a id="_idIndexMarker263"/>cancellation is requested or the loop completes all iterations:</p>
			<pre class="source-code">        static string FetchDataFromAPI(List&lt;string&gt;</pre>
			<pre class="source-code">         apiURL, CancellationToken token)</pre>
			<pre class="source-code">        {</pre>
			<pre class="source-code">            Console.WriteLine("Task started");</pre>
			<pre class="source-code">            int counter = 0;</pre>
			<pre class="source-code">            foreach (string url in apiURL)</pre>
			<pre class="source-code">            {</pre>
			<pre class="source-code">                if (token.IsCancellationRequested)</pre>
			<pre class="source-code">                {</pre>
			<pre class="source-code">                    throw new TaskCanceledException($"data</pre>
			<pre class="source-code">                     from API returned up to iteration</pre>
			<pre class="source-code">                       {counter}");</pre>
			<pre class="source-code">                    //throw new </pre>
			<pre class="source-code">                    //OperationCanceledException($"data </pre>
			<pre class="source-code">                    //from API returned up to iteration </pre>
			<pre class="source-code">                    //{counter}"); </pre>
			<pre class="source-code">                    // Alternate exception with same result</pre>
			<pre class="source-code">                    //break; // To handle manually</pre>
			<pre class="source-code">                }</pre>
			<pre class="source-code">                Thread.Sleep(1000);</pre>
			<pre class="source-code">                Console.WriteLine($"data retrieved from</pre>
			<pre class="source-code">                 {url} for iteration {counter}");</pre>
			<pre class="source-code">                counter++;</pre>
			<pre class="source-code">            }</pre>
			<pre class="source-code">            return $"data from API returned up to iteration</pre>
			<pre class="source-code">             {counter}";</pre>
			<pre class="source-code">        }</pre>
			<p>Now, call <code>FetchDataFrom<a id="_idTextAnchor327"/>API</code> with<a id="_idIndexMarker264"/> a list of four URLs from the main method, as shown in the following code. Here, we are creating <code>CancellationToken</code> using the <code>Token</code> property of the <code>CancellationTokenSource</code> class and passing it to the <code>FetchDataFromAPI</code> method. We are simulating a cancellation after 3 seconds so that <code>FetchDataFromAPI</code> will be canceled before the fourth URL is retrieved:</p>
			<pre class="source-code">CancellationTokenSource cts = new CancellationTokenSource();</pre>
			<pre class="source-code">CancellationToken token = cts.Token;</pre>
			<pre class="source-code">Task&lt;string&gt; dataFromAPI;</pre>
			<pre class="source-code">try</pre>
			<pre class="source-code">{</pre>
			<pre class="source-code">    dataFromAPI = Task.Factory.StartNew&lt;string&gt;(() =&gt;</pre>
			<pre class="source-code">     FetchDataFromAPI(new List&lt;string&gt; {</pre>
			<pre class="source-code">    "https://foo.com","https://foo1.com","https://foo2.com"</pre>
			<pre class="source-code">      ,"https://foo3.com", "https://foo4.com", }, token));</pre>
			<pre class="source-code">    Thread.Sleep(3000);</pre>
			<pre class="source-code">    cts.Cancel(); //Trigger cancel notification to </pre>
			<pre class="source-code">                  //cancellation token</pre>
			<pre class="source-code">    dataFromAPI.Wait(); // Wait for task completion</pre>
			<pre class="source-code">    Console.WriteLine(dataFromAPI.Result); // If task is </pre>
			<pre class="source-code">      //completed display message accordingly</pre>
			<pre class="source-code">}</pre>
			<pre class="source-code">catch (AggregateException agex)</pre>
			<pre class="source-code">{// Handle exception}</pre>
			<p>Once we run this <a id="_idIndexMarker265"/>code, we can see output for three URLs and then an exception/break (based on whichever line is commented out in the <code>FetchDataFromAPI</code> method).</p>
			<p>In the preceding sample, we have simulated a long-running code block using a <code>for</code> loop and <code>Thread.Sleep</code>, canceled the task, and handled the code accordingly. However, there could be a scenario where the long-running code block may not support cancellation. </p>
			<p>In those cases, we must write a wrapper method that accepts a cancellation token and have the wrapper internally call the long-running operation; then, in the main method, we call the wrapper code. The following snippet shows a wrapper method that makes u<a id="_idTextAnchor328"/>se of <code>TaskCompletionSource</code>, which is another class in the TPL. It is used to convert non-task-based asynchronous methods (including even the ones based on asynchronous methods) to tasks through the <code>Task</code> property available in the class. In this case, we will pass the cancellation token to <code>TaskCompletionSource</code> so that its <code>Task</code> is updated accordingly:</p>
			<pre class="source-code">        static Task&lt;string&gt;</pre>
			<pre class="source-code">         FetchDataFromAPIWithCancellation(List&lt;string&gt;</pre>
			<pre class="source-code">         apiURL, CancellationToken cancellationToken)</pre>
			<pre class="source-code">        {</pre>
			<pre class="source-code">            var tcs = new TaskCompletionSource&lt;string&gt;();</pre>
			<pre class="source-code">            tcs.TrySetCanceled(cancellationToken);</pre>
			<pre class="source-code">            // calling overload of long running operation </pre>
			<pre class="source-code">            // that doesn't support cancellation token</pre>
			<pre class="source-code">            var dataFromAPI = Task.Factory.StartNew(() =&gt;</pre>
			<pre class="source-code">             FetchDataFromAPI(apiURL));</pre>
			<pre class="source-code">            // Wait for the first task to complete</pre>
			<pre class="source-code">            var outputTask = Task.WhenAny(dataFromAPI,</pre>
			<pre class="source-code">             tcs.Task);</pre>
			<pre class="source-code">            return outputTask.Result;</pre>
			<pre class="source-code">        }</pre>
			<p>In this<a id="_idIndexMarker266"/> case, <code>CancellationToken</code> is tracked through the <code>Task</code> property of <code>TaskCompletionSource</code>, and we created another task to call our long-running operation (the one without cancellation token support), and whichever<a id="_idTextAnchor329"/> task finishes first is the one we return.</p>
			<p>Of course, the <code>Main</code> method needs to be updated to call the wrapper, as shown here (the rest of the code remains the same):</p>
			<pre class="source-code">            dataFromAPI = Task.Factory.StartNew(() =&gt;</pre>
			<pre class="source-code">             FetchDataFromAPIWithCancellation(new</pre>
			<pre class="source-code">             List&lt;string&gt;</pre>
			<pre class="source-code">                {</pre>
			<pre class="source-code">                        "https://foo.com",</pre>
			<pre class="source-code">                        "https://foo1.com",</pre>
			<pre class="source-code">                        "https://foo2.com",</pre>
			<pre class="source-code">                        "https://foo3.com",</pre>
			<pre class="source-code">                        "https://foo4.com",</pre>
			<pre class="source-code">                    }, token)).Result;</pre>
			<p>This doesn't cancel the underlying method but still allows the application to exit before the underlying operation is completed.</p>
			<p>Task cancellation is a very useful mechanism that helps in reducing unwanted processing, either<a id="_idIndexMarker267"/> in tasks that haven't started yet or ones that have started but need to be stopped/abor<a id="_idTextAnchor330"/><a id="_idTextAnchor331"/>ted. Hence, all the asynchronous APIs in .NET do support cancellation.</p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor332"/>Implementing continuations</h2>
			<p>In enterprise <a id="_idIndexMarker268"/>applications, m<a id="_idTextAnchor333"/>ost of the time, there will be a need to create multiple tasks, build a hierarchy of tasks, create dependent tasks, or create child/parent relationships between tasks. Task continuation can be used to define such child tasks/sub-tasks. It works like JavaScript promises and supports chaining tasks up to multiple levels. Just like promises, the subsequent task in a hierarchy executes after the first task, and this can be further chained to multiple levels.</p>
			<p>There are various ways to achieve task continuation, but the most common way is to use the <code>ContinueWith</code> method of the <code>Task</code> class, as shown in the following example:</p>
			<pre class="source-code">Task.Factory.StartNew(() =&gt; Task1(1)) // 1+2 = 3</pre>
			<pre class="source-code">                .ContinueWith(a =&gt; Task2(a.Result)) // 3*2 = 6</pre>
			<pre class="source-code">                    .ContinueWith(b =&gt; Task3(b.Result))// 6-2=4</pre>
			<pre class="source-code">                        .ContinueWith(c =&gt; Console.WriteLine(c.Result));</pre>
			<pre class="source-code">Console.ReadLine();</pre>
			<pre class="source-code">static int Task1(int a) =&gt; a + 2;</pre>
			<pre class="source-code">static in<a id="_idTextAnchor334"/>t Task2(int a) =&gt; a * 2;</pre>
			<pre class="source-code">static int Task3(int a) =&gt; a - 2;</pre>
			<p>As you might have guessed, here the output would be <code>4</code>, and each task executes once the preceding task's execution is completed.</p>
			<p><code>ContinueWith</code> accepts one important enum called <code>TaskContinuationOptions</code>, which supports continuation for different conditions. For example, we can pass <code>TaskContinuationOptions.OnlyOnFaulted</code> as a parameter to create a continuation task that executes when there is an exception in the preceding task or pass <code>TaskContinuationOptions.AttachedToParent</code> to create a continuation task that enforces a parent-child relationship and forces a parent task to complete execution only after the child task.</p>
			<p>As with <code>WhenAll</code> and <code>WhenAny</code>, <code>ContinueWith</code> also comes with similar siblings, as follows:</p>
			<ul>
				<li><code>Task.Factory.ContinueWhenAll</code>: This accepts multiple task references as parameters and creates a continuation when all the tasks are completed.</li>
				<li><code>Task.Factory.ContinueWhenAny</code>: This accepts multiple task references as parameters and creates a co<a id="_idTextAnchor335"/>ntinuation when one of the referenced tasks is completed.</li>
			</ul>
			<p>Grasping task <a id="_idIndexMarker269"/>continuation is critical to understanding the under-the-ho<a id="_idTextAnchor336"/><a id="_idTextAnchor337"/>od workings of async-await, which we will discuss later in this chapter.</p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor338"/>SynchronizationContext</h2>
			<p><code>SynchronizationContext</code> is an <a id="_idIndexMarker270"/>abstract class available in <code>System.Threading</code> that helps in communication between <a id="_idTextAnchor339"/>threads. For example, updating a UI element from a parallel task requires the thread to rejoin the UI thread and resume execution. <code>SynchronizationContext</code> provides this abstraction primarily through the <code>Post</code> method of this class, which accepts a delegate to execute at a later stage. So, in the preceding example, if I need to update a UI element, I need to take <code>SynchronizationContext</code> of the UI thread, call its <code>Post</code> method, and pass the necessary data to update the UI element.</p>
			<p>As <code>SynchronizationContext</code> is an abstract class, there are various derived types of it – for instance, Windows Forms has <code>WindowsFormsSynchronizationContext</code> and WPF has <code>DispatcherSynchronizationContext</code>.</p>
			<p>The primary advantage of <code>SynchronizationContext</code> being an abstraction is that it can be helpful to <a id="_idIndexMarker271"/>queue a d<a id="_idTextAnchor340"/><a id="_idTextAnchor341"/>elegate, irrespective <a id="_idTextAnchor342"/>of the overridden implementation of the <code>Post</code> method.</p>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor343"/>TaskScheduler</h2>
			<p>When we created tasks using<a id="_idIndexMarker272"/> the various methods described earlier, we saw that a task gets <em class="italic">scheduled</em> on a <code>ThreadPool</code> thread, but the question arises of who or what does that. <code>System.Threading.Tasks.TaskScheduler</code> is the class available in the TPL that takes care of queueing and executing task delegates on a <code>ThreadPool</code> thread.</p>
			<p>Of course, this is an abstract class, and the framework comes with two derived classes:</p>
			<ul>
				<li><code>ThreadPoolTaskScheduler</code></li>
				<li><code>SynchronizationContextScheduler</code></li>
			</ul>
			<p><code>TaskScheduler</code> exposes a <code>Default</code> property, which is by default set to <code>ThreadPoolTaskScheduler</code>. Hence, by default, all tasks are scheduled to <code>ThreadPool</code> threads; however, a GUI application typically uses <code>SynchronizationContextScheduler</code> so <a id="_idTextAnchor344"/>that tasks can successfully go back and update UI elements.</p>
			<p>.NET Core comes with sophisticated derived types of the <code>TaskScheduler</code> and <code>SynchronizationContext</code> classes. However, they play a major role in async-await, and they help in debugging any deadlock-related issues quickly.</p>
			<p>Note that looking at the internal workings of <code>TaskScheduler</code> and <code>SynchronizationContext</code> is <a id="_idTextAnchor345"/><a id="_idTextAnchor346"/>beyond the scope of this book and is left to you to explore as an exercise.</p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor347"/>Implementing data parallelism</h2>
			<p>Data parallelism <a id="_idIndexMarker273"/>is <a id="_idTextAnchor348"/>all about partitioning a source collection into multiple parallel executable tasks that perform the same operation parallelly. With the TPL, this is available in the <code>Parallel</code> static class, which exposes methods such as <code>For</code> and <code>ForEach</code> with multiple overloads to handle such execution.</p>
			<p>Say you have a collection of a million numbers and you need to find the prime numbers. Data parallelism can come in handy here, as the collection can be split into ranges and evaluated for prime numbers. A typically parallel <code>for</code> loop is written, as shown in the following snippet:</p>
			<pre class="source-code">            List&lt;int&gt; numbers = Enumerable.Range(1,</pre>
			<pre class="source-code">             100000).ToList();</pre>
			<pre class="source-code">            Parallel.For(numbers.First(), numbers.Last(), x</pre>
			<pre class="source-code">             =&gt; CalculatePrime(x));</pre>
			<p>However, a more realistic example would be something like an image processing application that needs to process each pixel in an image and reduce the brightness of each pixel by five points. Such operations can be hugely benefited by data parallelism, as each pixel is <a id="_idIndexMarker274"/>inde<a id="_idTextAnchor349"/>pendent of the others and hence can be processed parallelly.</p>
			<p>Similarly, there is a <code>ForEach</code> method in the <code>Parallel</code> static class, which can be us<a id="_idTextAnchor350"/>ed as follows:</p>
			<pre class="source-code">Parallel.ForEach(numbers, x =&gt; CalculatePrime(x));</pre>
			<p>Some of the key advantages of data parallelism using <code>Parallel.For</code> and <code>Parallel.ForEach</code> are listed here:</p>
			<ul>
				<li>Good for canceling loops; they work similarly to <code>break</code> in a regular <code>for</code> loop. In <code>Parallel.For</code>, this is supported by passing <code>ParallelStateOptions</code> to the delegate and then calling <code>ParallelStateOptions.Break</code>. When <code>Break</code> is encountered by one of the tasks, the <code>LowestBreakIteration</code> property of the <code>ParallelStateOptions</code> class is set, and all the parallel tasks will iterate until this number is reached. <code>ParallelLoopResult</code>, which is the return type of <code>Parallel.For</code> and <code>Parallel.ForEach</code>, has the <code>IsCompleted</code> property, which states whether the loop executed prematurely.</li>
				<li>They also support stopping the loop immediately through <code>ParallelStateOptions.Stop</code>. Also, some of the constructors of <code>Parallel.For</code> and <code>Parallel.ForEach</code> accept cancellation tokens, which can also be used to simulate <code>ParallelStateOptions.Stop</code>; however, a loop should be wrapped within a <code>try…catch</code> block, as <code>OperationCanceledException</code> would be thrown.</li>
				<li>If one of the tasks throws an exception, all the tasks will complete their current iteration and then stop processing. As with tasks, <code>AggregateException</code> is thrown back.</li>
				<li>Degrees of parallelism are supported by passing <code>ParallelOptions</code> and setting <code>MaxDegreeOfParallelism</code>, which will control the number of cores that tasks can parallelly execute on.</li>
				<li>The custom partitioning of a source collection is supported through range partitioning or chunk partitioning.</li>
				<li>Supports thread-safe local variables that are scoped to a thread or partition.</li>
				<li>Nested <code>Parallel.For</code> loops are supported, and their synchronization is automatically <a id="_idIndexMarker275"/>handled without introducing any manual synchronization.</li>
				<li>If <a id="_idTextAnchor351"/>each iteration uses a shared variable, synchronization needs to be implemented explicitly. So, to get the most out of data parallelism, use it for operations that can execute independently for each iteration without depending on shared resources.<p class="callout-heading">Tip</p><p class="callout">Data parallelism should be used carefully, as at times it is misused. It's like splitting 40 tasks among 4 people. If organizing this work (splitting and consolidating it) among 4 people represe<a id="_idTextAnchor352"/>nts much more work than just performing the overall work of the 40 tasks, then data parallelism isn't the right choice. For further reading, refer to <a href="https://docs.microsoft.com/en-us/dotnet/standard/parallel-programming/data-parallelism-task-parallel-library">https://docs.microsoft.com/en-us/dotnet/standard/parallel-programming/data-parallelism-task-parallel-library</a>.</p></li>
			</ul>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor355"/>Using Parallel LINQ (PLINQ)</h2>
			<p>PLINQ is a <a id="_idIndexMarker276"/>parallel implementation of LINQ; this is a set of APIs available in the <code>ParallelEnumera<a id="_idTextAnchor356"/>ble</code> class that enables the parallel execution of LINQ queries. The simplest way of making a LINQ query run parallelly is to embed the <code>AsParallel</code> method in the LINQ query. See the following code snippet, which calls a method that calculates the prime numbers between 1 and 1,000:</p>
			<pre class="source-code">List&lt;int&gt; numbers = Enumerable.Range(1, 1000).ToList();</pre>
			<pre class="source-code">var resultList = numbers.AsParallel().Where(I =&gt; CalculatePrime</pre>
			<pre class="source-code">(i)).ToList();</pre>
			<p>Using LINQ query syntax, this would be as follows:</p>
			<pre class="source-code">var primeNumbers = (from i in numbers.AsParallel()where CalculatePrime(i) select i).ToList();</pre>
			<p>Internally, this query is split into multiple smaller queries that are parallelly executed on each processor, hence speeding up the query. The partitioned source needs to be merged back on the main thread so that the result (output c<a id="_idTextAnchor357"/>ollection) can be looped through for further processing/display.</p>
			<p>Let's create a console application that prints all prime numbers between a given range, using PLINQ combined with <code>Parallel.For</code>. Add the following method, which takes a number and returns <code>true</code> if it's a prime number and <code>false</code> otherwise:</p>
			<pre class="source-code">bool CalculatePrime(int num)</pre>
			<pre class="source-code">{</pre>
			<pre class="source-code">    bool isDivisible = false;</pre>
			<pre class="source-code">    for (int i = 2; i &lt;= num / 2; i++)</pre>
			<pre class="source-code">    {</pre>
			<pre class="source-code">        if (num % i == 0)</pre>
			<pre class="source-code">        {</pre>
			<pre class="source-code">            isDivisible = true;</pre>
			<pre class="source-code">            break;</pre>
			<pre class="source-code">        }</pre>
			<pre class="source-code">    }</pre>
			<pre class="source-code">    if (!isDivisible &amp;&amp; num != 1)</pre>
			<pre class="source-code">        return true;</pre>
			<pre class="source-code">    else</pre>
			<pre class="source-code">        return false;</pre>
			<pre class="source-code">}</pre>
			<p>Now, in the main met<a id="_idTextAnchor358"/>hod, add the following code, which creates a list of the first 100 numbers that we will loop through using PLINQ before passing it to the <code>CalculatePrime</code> method; then, we'll <a id="_idIndexMarker277"/>finally display the list of prime numbers using <code>Parallel.ForEach</code>:</p>
			<pre class="source-code">List&lt;int&gt; numbers = Enumerable.Range(1, 100).ToList();</pre>
			<pre class="source-code">try</pre>
			<pre class="source-code">{</pre>
			<pre class="source-code">       var primeNumbers = (from number in </pre>
			<pre class="source-code">       numbers.AsParallel() where CalculatePrime(number) == </pre>
			<pre class="source-code">       true select number).ToList();</pre>
			<pre class="source-code">  Parallel.ForEach(primeNumbers, (primeNumber) =&gt;</pre>
			<pre class="source-code">  {</pre>
			<pre class="source-code">    Console.WriteLine(primeNumber);</pre>
			<pre class="source-code">  });</pre>
			<pre class="source-code">}</pre>
			<pre class="source-code">catch (AggregateException ex)</pre>
			<pre class="source-code">{</pre>
			<pre class="source-code">  Console.WriteLine(ex.InnerException.Message);</pre>
			<pre class="source-code">}</pre>
			<p>The output for this sample would be a list of prime numbers; however, you can see that the output will not be prime numbers in ascending order but in a random order, as the <code>CalculatePrime</code> method is called with multiple numbe<a id="_idTextAnchor359"/>rs parallelly.</p>
			<p>A diagram of the internal working of the preceding code follows:</p>
			<div><div><img src="img/Figure_4.4_B18507.jpg" alt="Figure 4.4 – PLINQ and Parallel.ForEach&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.4 – PLINQ and Parallel.ForEach</p>
			<p>PLINQ further <a id="_idIndexMarker278"/>provi<a id="_idTextAnchor360"/>des a method to process the result of each partition/thread without the overhead of merging the result into a calling thread using <code>ForAll</code>, and the preceding code can be further optimized as follows:</p>
			<pre class="source-code">            (from i in numbers.AsParallel()</pre>
			<pre class="source-code">             where CalculatePrime(i) == true</pre>
			<pre class="source-code">             select i).ForAll((primeNumber) =&gt;</pre>
			<pre class="source-code">               Console.WriteLine(primeNumber));</pre>
			<p class="callout-heading">Tip</p>
			<p class="callout">One of the best tools for playing around with LINQ/PLINQ is LINQPad; I recommend that y<a id="_idTextAnchor361"/>ou download it from <a href="https://www.linqpad.net/Download.aspx">https://www.linqpad.net/Download.aspx</a>.</p>
			<p>Some of the important things to remember for PLINQ are as follows:</p>
			<ul>
				<li>Merging results to the main thread can be configured by using the <code>WithMergeOption</code> method and passing the appropriate value through the <code>ParallelMergeOperation</code> enum.</li>
				<li>As with other parallel extensions, any exception is returned as <code>AggregateException</code>, and the execution of all the iterations stops immediately. Of course, if exceptions are swallowed within the delegate instead of them being thrown back, the execution can continue.</li>
				<li>There are various other extension methods, such as <code>AsSequential</code> and <code>AsOrdered</code>, and these can be combined in one single LINQ query. For example, based on that, <code>AsSequential</code> can be combined with <code>AsParallel</code> so that some partitions can be run sequentially and other partitions can be executed <a id="_idIndexMarker279"/>parallelly.</li>
				<li>Supports cancellation using the <code>WithCancellation</code> method.</li>
				<li>Degre<a id="_idTextAnchor362"/>es of parallelism are supported through <code>WithDegreeOfParallelism</code>.</li>
			</ul>
			<p>Data parallelism and PLINQ provide a lot of APIs that can be used to quickly enable the parallel execution of code without adding any additional overhead to the application logic. However, there is a subtle difference between them, as explained in the preceding section, and they should be used differently accordingly.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">PLINQ and the TPL together comprise parallel extensions.</p>
			<p>In this section, we have used <code>Thread.Sleep</code> in many places, but that has primarily been to simulate long-running operations; however, it is never recommended that you use this in <a id="_idIndexMarker280"/>production.</p>
			<p>In the next section, we will see how <a id="_idTextAnchor363"/><a id="_idTextAnchor364"/>we can club tasks with async-await and use async-await in enterprise applications.</p>
			<h1 id="_idParaDest-91"><a id="_idTextAnchor365"/>Introducing async-await</h1>
			<p>So far, we have<a id="_idIndexMarker281"/> discussed writing asynchronous code using tasks and how the TPL simplifies creating and managing tas<a id="_idTextAnchor366"/>ks. However, tasks primarily rely on continuation, callbacks, or events to continue execution after the completion of a task. </p>
			<p>In enterprise applications, managing such code would be difficult; any runtime exceptions would be difficult to debug if too many tasks were chained. That's where C# comes in with async-await, a language feature introduced in C# 5.0 that simplifies the writing of asynchronous code, makes it more readable and maintainable, improves exception handling, and makes things easy to debug. So, let's dive into async-await.</p>
			<p><code>async</code> is a keyword in C# that is used as a modifier and, when prefixed to any method (or Lambda), converts a method into a state machine, enabling the method to use the <code>await</code> keyword in its body.</p>
			<p><code>await</code> is a keyword in C# that is used as an operator and is followed by an expression that returns an awaitable object (usually a task). <code>await</code> can be used only inside a method that has an <code>async</code> modifier, and as soon as a caller encounters an <code>await</code> statement, control is retur<a id="_idTextAnchor367"/><a id="_idTextAnchor368"/>ned and things are resumed; after <code>await</code>, the task is completed using continuations.</p>
			<h2 id="_idParaDest-92"><a id="_idTextAnchor369"/>The task-based asynchronous pattern</h2>
			<p>Th<a id="_idTextAnchor370"/>e <code>async</code> modifier and then use <code>await</code> on an asynchronous operation that is wrapped in a task (or any custom awaitable type that exposes <code>GetAwaiter()</code>). To put it simply, this pattern involves representing an asynchronous operation using a single method that has an <code>async</code> modifier and returns a task; any asynchronous operation is further awaited using <code>await</code>. The following is a sample co<a id="_idTextAnchor371"/>de snippet that downloads a file asynchronously, which is implemented<a id="_idTextAnchor372"/> using the TAP:</p>
			<div><div><img src="img/Figure_4.5_B18507.jpg" alt="Figure 4.5 – A sample asynchronous method using async-await&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.5 – A sample asynchronous method using async-await</p>
			<p>In the<a id="_idIndexMarker284"/> preceding<a id="_idIndexMarker285"/> figure, control flows as follows (using the number labels in the figure):</p>
			<ol>
				<li>The application starts execution with the <code>Main</code> method. Since <code>Main</code> is prefixed with the <code>async</code> method, it gets transformed into a type that implements a state machine. Execution continues until <code>await</code> is encountered at <code>await</code> <code>DownloadFileAsync</code>, and the thread is returned to the caller.</li>
				<li>Before returning to the caller, a call to the <code>DownloadFileAsync</code> method is stored in a <code>Task</code> object, and a reference to the <code>Task</code> object is also preserved. The remaining code of the <code>Main</code> method is wrapped inside the continuation of this task.</li>
				<li>A <code>ThreadPool</code> thread will start executing a <code>DownloadFileAsync</code> method, and it repeats the same steps – that is, it converts a method into a type that implements a state machine, continues execution until <code>await</code> is encountered, and then the task that is referenced is passed back; the remaining code is moved to the continuation of this task.</li>
				<li>At some point, when the <code>DownloadDataTaskAsync</code> method is completed, the task continuation gets triggered and will execute the remaining code.</li>
				<li>The process repeats until the task that has the reference of <code>DownloadFileAsync</code> completes and its continuation is executed, which is <code>Console.WriteLine("Fi<a id="_idTextAnchor373"/>le downloaded!!")</code> in this case, and then the applicat<a id="_idTextAnchor374"/>ion exits.</li>
			</ol>
			<p>At an approximate<a id="_idIndexMarker286"/> high <a id="_idIndexMarker287"/>level, the code would be transformed as shown here:</p>
			<div><div><img src="img/Figure_4.6_B18507.jpg" alt="Figure 4.6 – A transformed sample asynchronous method&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.6 – A transformed sample asynchronous method</p>
			<p>Although this is an oversimplification of the under-the-hood workings of async-await, we can see the compiler doing a lot of heavy lifting, including generating a type that implements a state machine and continuing the execution using the state of the callback.</p>
			<p>We have seen how simple it is to write async methods, and we will be writing many such methods in our enterprise application throughout the course of the book. However, async-await is<a id="_idTextAnchor375"/> not a silver bullet; it is not an answer to every application issue. We need to verify certain factors to make use of async-await. Let's see what the principles are for using async-await.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The preceding code would change slightly if there was <code>SynchronizationContext</code>. For instance, in Windows Forms or WPF apps, continuation is posted on the current <code>SynchronizationContext</code> using the <code>Post</code> method of <code>SynchronizationContext</code> or <code>TaskScheduler.FromCurrentSynchronizationContext</code>. As per the standard naming convention, asynchronous methods are suff<a id="_idTextAnchor376"/><a id="_idTextAnchor377"/>ixed with the word <code>async</code> for <a id="_idIndexMarker288"/>readability<a id="_idIndexMarker289"/> purposes, but syntactically, it is not needed.</p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor378"/>Principles of using async-await</h2>
			<p>As we s<a id="_idTextAnchor379"/>tart using <a id="_idIndexMarker290"/>async-await, there are certain practices that are recommended that will enable an application to take advantage of asynchronous principles. For example, for nested calls, we should use async-await all the way; do not use <code>.Result</code> and so on. Here are a few guidelines to help you use async-await effectively.</p>
			<h3>Chain async-await all the way</h3>
			<p>An asynchronous<a id="_idIndexMarker291"/> method implemented using async-await should be triggered from an <a id="_idTextAnchor380"/>async-await method so that it is properly awaited. If we try to call an asynchronous method from a synchronous method using the <code>Result</code> method or the <code>Wait</code> method of a task, it could lead to a deadlock. </p>
			<p>Let's look at the following code snippet from a WPF application that downloads files from the network upon a button click. However, instead of awaiting a call to the asynchronous method, we are using the <code>Result</code> method of <code>Task</code>:</p>
			<pre class="source-code">        private void Button_Click(object sender,</pre>
			<pre class="source-code">         RoutedEventArgs e)</pre>
			<pre class="source-code">        {</pre>
			<pre class="source-code">            var task = </pre>
			<pre class="source-code">            DownloadFileAsync("https://github.com/Ravindra-</pre>
			<pre class="source-code">            a/largefile/blob/master/README.md", @$"{System.IO.Directory.GetCurrentDirectory()}\download.txt");</pre>
			<pre class="source-code">            bool fileDownload = task.Result; // Or </pre>
			<pre class="source-code">                            //task.GetAwaiter().GetResult()</pre>
			<pre class="source-code">            if (fileDownload)</pre>
			<pre class="source-code">            {</pre>
			<pre class="source-code">                MessageBox.Show("file downloaded");</pre>
			<pre class="source-code">            }</pre>
			<pre class="source-code">        }</pre>
			<pre class="source-code">        private async Task&lt;bool&gt; DownloadFileAsync(string</pre>
			<pre class="source-code">         url, string path)</pre>
			<pre class="source-code">        {</pre>
			<pre class="source-code">            // Create a new web client object</pre>
			<pre class="source-code">            using WebClient webClient = new WebClient();</pre>
			<pre class="source-code">            // Add user-agent header to avoid forbidden </pre>
			<pre class="source-code">            // errors.</pre>
			<pre class="source-code">            webClient.Headers.Add("user-agent",</pre>
			<pre class="source-code">              "Mozilla/5.0 (Windows NT 10.0; WOW64)");</pre>
			<pre class="source-code">            byte[] data = await</pre>
			<pre class="source-code">              webClient.DownloadDataTaskAsync(url);</pre>
			<pre class="source-code">            // Write data in file.</pre>
			<pre class="source-code">            Using var fileStream = File.OpenWrite(path);</pre>
			<pre class="source-code">            {</pre>
			<pre class="source-code">                await fileStream.WriteAsync(data, 0,</pre>
			<pre class="source-code">                 data.Length);</pre>
			<pre class="source-code">            }</pre>
			<pre class="source-code">            return true;</pre>
			<pre class="source-code">        }</pre>
			<p>In this method, the code after <code>await</code> <code>webClient.DownloadDataTaskAsync(url);</code> will never execute, for the following reasons:</p>
			<ul>
				<li>As soon as await is encountered, the <code>Task</code> reference object captures <code>SynchronizationContext</code> in <code>TaskAwaitable</code> through the <code>GetAwaiter</code> method.</li>
				<li>Once the <code>async</code> operation is completed, the continuation of that <code>await</code> needs to execute on <code>SynchronizationContext</code> (through <code>SynchronizationContext.Post</code>).</li>
				<li>However, <code>SynchronizationContext</code> is already blocked because the call to <code>task.Result</code> on the click of a button is on the same <code>SynchronizationContext</code> and <a id="_idTextAnchor381"/>is waiting for <code>DownloadDataTaskAsync</code> to complete, hence it is causing a deadlock.</li>
			</ul>
			<p>So, never <a id="_idIndexMarker292"/>block <code>async</code> methods; the best way to do <code>async</code> is all the way. So, in the preceding code, you would change the call to <code>await</code> <code>DownloadFileAsync</code> (and <code>async void</code> for button click – <code>await</code> needs a method to have an <code>async</code> modifier).</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The same code works fine in ASP.NET Core 6 applications without causing a deadlock because ASP.NET Core 6 doesn't have <code>SynchronizationContext</code>, and continuation executes on a <code>ThreadPool</code> thread without any involvement of a request context; however, blocking asynchronous calls is still not recommended, even in ASP.NET Core 6.</p>
			<h3>ConfigureAwait</h3>
			<p>In the p<a id="_idTextAnchor382"/>receding <a id="_idIndexMarker293"/>discussion, since we had the end-to-end<a id="_idIndexMarker294"/> application code, it was easier to find the cause of the deadlock. However, if we are developing a library with asynchronous methods that can be used in WPF, ASP.NET Core 6, or .NET Framework applications, we need to ensure that the asynchronous code within the library does not cause a deadlock, even though the caller may be consuming library methods through synchronous methods (<code>GetAwaiter().GetResult()</code>).</p>
			<p>In such cases, <code>Task</code> provides a method called <code>ConfigureAwait</code> that accepts a Boolean value, which, when <code>true</code>, will use the original context of the caller and, when <code>false</code>, will resume operation after <code>await</code> without depending on the original context. In layman's terms, any code after <code>await</code> will execute independently, irrespective of the state of the context that initiated the request.</p>
			<p>Us<a id="_idTextAnchor383"/>e <code>ConfigureAwait(false)</code>, especially if you are implementing a library method, as it will avoid running a continuation on the original context. For library methods, it is a must to use <code>ConfigureAwait(false)</code>, as they should never depend on the calling/original context for the continuation. For example, the following code won't cause a deadlock:</p>
			<pre class="source-code">        private void Button_Click(object sender, RoutedEventArgs e)</pre>
			<pre class="source-code">        {</pre>
			<pre class="source-code">            string output = GetAsync().Result; //Blocking </pre>
			<pre class="source-code">              //code, ideally should cause deadlock.</pre>
			<pre class="source-code">            MessageBox.Show(output);</pre>
			<pre class="source-code">        }</pre>
			<pre class="source-code">        //  Library code        </pre>
			<pre class="source-code">        public async Task&lt;string&gt; GetAsync()</pre>
			<pre class="source-code">        {</pre>
			<pre class="source-code">            var uri = new Uri("http://www.google.com");</pre>
			<pre class="source-code">            return await new HttpClient().</pre>
			<pre class="source-code">             GetStringAsync(uri).ConfigureAwait(false);</pre>
			<pre class="source-code">        }</pre>
			<p>By default, every <code>await</code> expression has <code>ConfigureAwait(true)</code>, so it's recommended to call <code>ConfigureAwait(false)</code> explicitly as much as possible. Apart from avoiding deadlocks, <code>ConfigureAwait(false)</code> also improves performance, as there is no marshaling of the original context.</p>
			<p>This brings us to the question of whether there is a scenario that needs to use <code>ConfigureAwait(true)</code>. The answer is that there are scenarios where a cu<a id="_idTextAnchor384"/>stom <code>SynchronizationContext</code> is being<a id="_idIndexMarker295"/> built that needs to be used by a <a id="_idIndexMarker296"/>callback, and it is then recommended to use <code>ConfigureAwait(true)</code>, or at least not use <code>ConfigureAwait(false)</code>, as the default behavior of any task is the same as <code>ConfigureAwait(true)</code>.</p>
			<h3>CPU-bound versus I/O-bound</h3>
			<p>Always use <a id="_idIndexMarker297"/>async-await for I/O-bound<a id="_idIndexMarker298"/> work <a id="_idIndexMarker299"/>and the TPL for CPU-bound work to achieve as<a id="_idTextAnchor385"/>ynchrony. I/O operations such as database calls, network calls, and filesystem calls can be wrapped in async-await asynchronous methods. However, a CPU-intensive operation such as calculating pi is better handled using the TPL.</p>
			<p>Going back to our earlier discussion, the idea of asynchronous programming is to release <code>ThreadPool</code> threads instead of waiting on the completion of an operation. This can very easily be achieved when we represent outbound calls as tasks and use async-await.</p>
			<p>However, for a CPU-intensive operation, a <code>ThreadPool</code> thread will continue to execute instructions on the worker thread (as it is a CPU-intensive operation and needs CPU time) and obviously cannot release that thread. This means that wrapping a CPU-intensive operation in async-await is not going to yield any benefit and is the same as running it synchronously. So, a better way to handle CPU-intensive operations is by using the TPL.</p>
			<p>This does not mean we will stop using async-await the moment we encounter a CPU-intensive method. The recommended way is to still use async-await to manage CPU-bound operations along with the TPL and not break our first principle of using async-await all the way.</p>
			<p>Here is a simple<a id="_idIndexMarker300"/> code snippet<a id="_idIndexMarker301"/> using <a id="_idIndexMarker302"/>async-await to manage CPU-bound work:</p>
			<pre class="source-code">        private async Task CPUIOResult()</pre>
			<pre class="source-code">        {</pre>
			<pre class="source-code">            var doExpensiveCalculationTask = Task.Run(() =&gt; </pre>
			<pre class="source-code">              DoExpensiveCalculation()); //Call a method </pre>
			<pre class="source-code">              //that does CPU intense operation        </pre>
			<pre class="source-code">           var downloadFileAsyncTask = DownloadFileAsync();</pre>
			<pre class="source-code">            await Task.WhenAll(doExpensiveCalculationTask,</pre>
			<pre class="source-code">             downloadFileAsyncTask);</pre>
			<pre class="source-code">        }</pre>
			<pre class="source-code">private async Task DownloadFileAsync(string url, string path)</pre>
			<pre class="source-code">        {</pre>
			<pre class="source-code">            // Implementation</pre>
			<pre class="source-code">        }</pre>
			<pre class="source-code">        private float DoExpensiveCalculation()</pre>
			<pre class="source-code">        {</pre>
			<pre class="source-code">            //Implementation</pre>
			<pre class="source-code">        }</pre>
			<p>As seen in the preceding code, it's still possible to manage CPU-bound work with a mix of async-await and the TPL; it's up to the developer to assess all the possible options and write their code accordingly.</p>
			<h3>A<a id="_idTextAnchor386"/>void async void</h3>
			<p>Always make <a id="_idIndexMarker303"/>sure to have <code>Task</code> or <code>Task&lt;T&gt;</code> as the return<a id="_idIndexMarker304"/> type for an asynchronous method implemented using async-await instead of <code>void</code> if a method is not expected to return anything. The reason for this is that <code>Task</code> is a complex abstraction that handles many things for us, such as exception handling and task completion status. However, if an asynchronous method has an <code>async</code> <code>void</code> return type, it is like a fire-and-forget method, and any caller to this method won't be able to know the status of the operation, even if there is an exception. </p>
			<p>That is because inside an <code>async</code> <code>void</code> method, as soon as an <code>await</code> expression is encountered, the call is returned to the caller without any reference to <code>Task</code>, so there is no reference to raise an exception for. For a UI application such as WPF, any exceptions on the <code>async</code> <code>void</code> method will crash the application; however, an exception for this is <code>async</code> <code>void</code> event handlers.</p>
			<p>Another disadvantage with <code>async</code> <code>void</code> methods is the inability to write unit tests and assert them correctly. So, it's always recommended to use async <code>Task</code> exceptions as top-level event handlers (top-level is key here) because top-level events such as a button click or a mouse click are more of a one-way signal an<a id="_idTextAnchor387"/>d are not used any differently in asynchronous code compared to their synchronous counterparts.</p>
			<p>The same consideration needs to be taken in the case of <code>async</code> Lambdas, where we need to avoid passing them as an argument to a method that takes the <code>Action</code> type as its parameters. See the following example:</p>
			<pre class="source-code">long elapsedTime = AsyncLambda(async() =&gt;</pre>
			<pre class="source-code">{</pre>
			<pre class="source-code">    await Task.Delay(1000);</pre>
			<pre class="source-code">});</pre>
			<pre class="source-code">Console.WriteLine(elapsedTime);</pre>
			<pre class="source-code">Console.ReadLine();</pre>
			<pre class="source-code">static long AsyncLambda(Action a)</pre>
			<pre class="source-code">{</pre>
			<pre class="source-code">    Stopwatch sw = new Stopwatch();</pre>
			<pre class="source-code">    sw.Start();</pre>
			<pre class="source-code">    for (int i = 0; i &lt; 10; i++)</pre>
			<pre class="source-code">    {</pre>
			<pre class="source-code">        a();</pre>
			<pre class="source-code">    }</pre>
			<pre class="source-code">    return sw.ElapsedMilliseconds;</pre>
			<pre class="source-code">}</pre>
			<p>Here, it's expected that the value of <code>elapsedTime</code> will be somewhe<a id="_idTextAnchor388"/>re around 10,000. However, it's close to 100 for the same reason – that is, with <code>Action</code> being a delegate of the <code>void</code> return type, the call to <code>AsyncLambda</code> is returned immediately to the <code>Main</code> method (as with any <code>async</code> <code>void</code> method). This can be fixed by changing <code>AsyncLambda</code> as follows (or just by changing the parameter to <code>Func&lt;Task&gt;</code> and handling the <a id="_idIndexMarker305"/>wait on <code>a()</code> accordingly) and then forcing the<a id="_idIndexMarker306"/> caller to use <code>async</code> all the way:</p>
			<pre class="source-code">        async static Task&lt;long&gt; AsyncLambda(Func&lt;Task&gt; a)</pre>
			<pre class="source-code">        {</pre>
			<pre class="source-code">            Stopwatch sw = new Stopwatch();</pre>
			<pre class="source-code">            sw.Start();</pre>
			<pre class="source-code">            for (int i = 0; i &lt; 10; i++)</pre>
			<pre class="source-code">            {</pre>
			<pre class="source-code">                await a();</pre>
			<pre class="source-code">            }</pre>
			<pre class="source-code">            return sw.ElapsedMilliseconds;</pre>
			<pre class="source-code">        }</pre>
			<p>A word of caution – if there are methods in your application that accept the <code>Action</code> type parameters, it's recommended that you have an overload that accepts <code>Func&lt;Task&gt;</code> or <code>Func&lt;Task&lt;T&gt;&gt;</code>. Fortunately, the C# compiler automatically handles this and always calls the overload with <code>Func&lt;Task&gt;</code> as a parameter.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Use the <a id="_idTextAnchor389"/><a id="_idTextAnchor390"/>Visual <a id="_idIndexMarker307"/>Studio 2022 Exception Helper feature to <a id="_idIndexMarker308"/>debug <code>async</code> exceptions that are rethrown by framework code.</p>
			<h2 id="_idParaDest-94">Async streams with<a id="_idTextAnchor391"/> IAsyncEnumerable</h2>
			<p>We all<a id="_idIndexMarker309"/> know that <code>foreach</code> <a id="_idTextAnchor392"/>is used to loop<a id="_idIndexMarker310"/> over <code>IEnumerable&lt;T&gt;</code> or <code>IEnumerator&lt;T&gt;</code>. Let's look at the following code, in which we retrieve all employee IDs from a database and loop through each employee to print their ID:</p>
			<pre class="source-code">        static async Task Main(string[] args)</pre>
			<pre class="source-code">        {</pre>
			<pre class="source-code">            var employeeTotal = await</pre>
			<pre class="source-code">             GetEmployeeIDAsync(5);</pre>
			<pre class="source-code">            foreach (int i in employeeTotal)</pre>
			<pre class="source-code">            {</pre>
			<pre class="source-code">                Console.WriteLine(i);</pre>
			<pre class="source-code">            }</pre>
			<pre class="source-code">        }</pre>
			<p>The <code>GetEmployeeIDAsync</code> implementation is as follows:</p>
			<pre class="source-code">        static async Task&lt;IEnumerable&lt;int&gt;&gt;</pre>
			<pre class="source-code">         GetEmployeeIDAsync(int input)</pre>
			<pre class="source-code">        {</pre>
			<pre class="source-code">            int id = 0;</pre>
			<pre class="source-code">            List&lt;int&gt; tempID = new List&lt;int&gt;();</pre>
			<pre class="source-code">            for (int i = 0; i &lt; input; i++) //Some async DB </pre>
			<pre class="source-code">              //iterator method like ReadNextAsync</pre>
			<pre class="source-code">            {</pre>
			<pre class="source-code">                await Task.Delay(1000); // simulate async</pre>
			<pre class="source-code">                id += i; // Hypothetically calculation</pre>
			<pre class="source-code">                tempID.Add(id);</pre>
			<pre class="source-code">            }</pre>
			<pre class="source-code">            return tempID;</pre>
			<pre class="source-code">        }</pre>
			<p>Here, you can see that we must use a temporary list until we have receiv<a id="_idTextAnchor393"/>ed all the records from the database, and finally, we return the list. However, if there is an iterator in our method, <code>yield</code> in C# is an obvious choice, as that helps in returning the results immediately<a id="_idIndexMarker311"/> and avoiding temporary<a id="_idIndexMarker312"/> variables. Now, say you used <code>yield</code>, as shown in the following code:</p>
			<pre class="source-code">yield return id;</pre>
			<p>You would receive the following error upon compilation:</p>
			<pre class="source-code">The body of 'Program.GetEmployeeIDAsync(int)' cannot be an iterator block because 'Task&lt;IEnumerable&lt;int&gt;&gt;' is <a id="_idTextAnchor394"/>not an iterator interface type</pre>
			<p>Hence, there is a need to be able to use <code>yield</code> with an <code>async</code> method and also loop through a collection to call an application asynchronously. That's where C# 8.0 came up with asynchronous streams through <code>IAsyncEnumerable</code>, which primarily enables you to return data immediately and asynchronously consume a collection. So, the preceding code can be changed as follows:</p>
			<pre class="source-code">await foreach (int i in GetEmployeeIDAsync(5))</pre>
			<pre class="source-code">    {</pre>
			<pre class="source-code">        Console.WriteLine(i);</pre>
			<pre class="source-code">    }       </pre>
			<pre class="source-code">static async IAsyncEnumerable&lt;int&gt;</pre>
			<pre class="source-code"> GetEmployeeIDAsync(int input)</pre>
			<pre class="source-code">{</pre>
			<pre class="source-code">    int id = 0;</pre>
			<pre class="source-code">    List&lt;int&gt; tempID = new List&lt;int&gt;();</pre>
			<pre class="source-code">    for (int i = 0; i &lt; input; i++)</pre>
			<pre class="source-code">    {</pre>
			<pre class="source-code">        <a id="_idTextAnchor395"/>await Task.Delay(1000);</pre>
			<pre class="source-code">        id += <a id="_idTextAnchor396"/>i; // Hypothetically calculation</pre>
			<pre class="source-code">        yield return id;</pre>
			<pre class="source-code">    }</pre>
			<pre class="source-code">}</pre>
			<p>So, here you can see that once a method starts returning, <code>IAsy<a id="_idTextAnchor397"/><a id="_idTextAnchor398"/><a id="_idTextAnchor399"/>ncEnumerable</code> loops can be iterated <a id="_idTextAnchor400"/><a id="_idIndexMarker313"/>asynchronously, and this is helpful <a id="_idIndexMarker314"/>in many situations to write cleaner code.</p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor401"/>ThreadPool starvation</h2>
			<p>Say you have <a id="_idIndexMarker315"/>an application with asynchronous <a id="_idIndexMarker316"/>code. However, you have noticed that periodically, during high loads, the response time for requests drastically increases. You research it further, but neither is the CPU of your server fully utilized nor is the memory of your process high, and it isn't a case of your database becoming a bottleneck either. In this case, your application is possibly causing what is known as <code>ThreadPool</code> starvation.</p>
			<p><code>ThreadPool</code> starvation is a state in which new threads keep being added to serve concurrent requests, and eventually, a point is reached where <code>ThreadPool</code> is unable to add more threads, and requests start seeing delayed response times or even start failing in the worst-case scenario. Even if <code>ThreadPool</code> can add threads at a rate of one or two per second, new requests may be coming at a higher rate (as in a burst load on a web application during the holiday season). Hence, there is a significant increase in the response time. There are multiple reasons why this can happen; a few of them are listed here:</p>
			<ul>
				<li>The consumption of more threads to speed up long-running CPU-bound work</li>
				<li>The calling of an <code>async</code> method in a <code>sync</code> method using <code>GetAwaiter().GetResult()</code></li>
				<li>The incorrect use of synchronization primitives, such as a thread holding a lock for a long time and other threads waiting to acquire it</li>
			</ul>
			<p>In all the preceding points, the common thing is blocking code; so, the use of blocking code such as <code>Thread.Sleep</code> even for a short duration, something such as <code>GetAwaiter().GetResult()</code>, or trying to allocate more threads for a CPU-bo<a id="_idTextAnchor402"/>und item increases the number of threads in <code>ThreadPool</code> and eventually leads to starvation.</p>
			<p><code>ThreadPool</code> starvation can be furth<a id="_idTextAnchor403"/>er diagnosed <a id="_idIndexMarker317"/>using tools such as <strong class="bold">PerfView</strong>, where you capture a trace for, say, 200 seconds, and verify the growth of threads in your process. If you see that your threads are growing at a rapid pace during peak load, then there is a possibility of starvation.</p>
			<p>The best way to prevent <code>ThreadPool</code> starvation is to use async-await throughout the application and never block any <code>async</code> calls. Also, the throttling of newly created operations can help, as it restricts the number of items that can be queued at a time.</p>
			<p>In this section, we discussed two important constructs, async-await and the TPL, which when<a id="_idIndexMarker318"/> combined <a id="_idIndexMarker319"/>make writing asynchronous code simpler. In the next section, we will learn about various dat<a id="_idTextAnchor404"/><a id="_idTextAnchor405"/>a structures that are available in .NET 6 to support synchronization/thread safety without writing any additional code.</p>
			<h1 id="_idParaDest-96">Using concurr<a id="_idTextAnchor406"/>ent collections for parallelism</h1>
			<p>Collections <a id="_idIndexMarker320"/>classes are one of the most used types to encapsulate, retrieve, a<a id="_idTextAnchor407"/>nd modify enumerated sets of related data. <code>Dictionary</code>, <code>list</code>, <code>queue</code>, and <code>array</code> are some of the frequently used collection types, but they are not thread-safe. These collections are good if you access them from just one thread at a time. </p>
			<p>A real-world environment would be multithreaded, and to make it thread-safe, you will have to implement various synchronization constructs, as described in an earlier section. To solve this problem, Microsoft came up with concurrent collection classes, such as <code>ConcurrentQueue</code>, <code>ConcurrentBag</code>, <code>ConcurrentDictionary</code>, and <code>ConcurrentStack</code>, whic<a id="_idTextAnchor408"/><a id="_idTextAnchor409"/>h are thread-safe, as they internally implement synchronization. Let's <a id="_idIndexMarker321"/>look at them in detail in the following sections.</p>
			<h2 id="_idParaDest-97">C<a id="_idTextAnchor410"/>oncurrentDictionary</h2>
			<p>Let's stimulate a <a id="_idIndexMarker322"/>multithreaded <a id="_idIndexMarker323"/>environment using a dictionary. Consider the <code>t1</code> task as one operation from a client who is adding to the dictionary and the <code>t2</code> task as a second operation from another client who is reading from the dictionary.</p>
			<p>We add <code>Thread.Sleep</code> in each task to mimic a real-world scenario to ensure that one task doesn't complete before the other in this example. Let's consider an example console application with the following code snippet:</p>
			<pre class="source-code">// Task t1 as one operation from a client who is adding to the dictionary.</pre>
			<pre class="source-code">Dictionary&lt;int, string&gt; employeeDictionary = new Dictionary&lt;int, string&gt;();            </pre>
			<pre class="source-code">            Task t1 = Task.Factory.StartNew(() =&gt;</pre>
			<pre class="source-code">            {</pre>
			<pre class="source-code">                for (int i = 0; i &lt; 100; ++i)</pre>
			<pre class="source-code">                {</pre>
			<pre class="source-code">                    employeeDictionary.TryAdd(i, "Employee"</pre>
			<pre class="source-code">                     + i.ToString());</pre>
			<pre class="source-code">                    Thread.Sleep(100);</pre>
			<pre class="source-code">                    </pre>
			<pre class="source-code">                }</pre>
			<pre class="source-code">            });</pre>
			<p>This is <code>Task</code> <code>t2</code> as a second operation from another client who is reading from the dictionary:</p>
			<pre class="source-code">            Task t2 = Task.Factory.StartNew(() =&gt;</pre>
			<pre class="source-code">            {</pre>
			<pre class="source-code">                Thread.Sleep(500);</pre>
			<pre class="source-code">                foreach (var item in employeeDictionary)</pre>
			<pre class="source-code">                {</pre>
			<pre class="source-code">                    Console.WriteLine(item.Key + <a id="_idTextAnchor411"/>"-" +</pre>
			<pre class="source-code">                      item.Value);</pre>
			<pre class="source-code">                    Thread.Sleep(100);</pre>
			<pre class="source-code">                }</pre>
			<pre class="source-code">            });</pre>
			<p>Now, both <a id="_idIndexMarker324"/>tasks are executed at the <a id="_idIndexMarker325"/>same time, as shown in the following:</p>
			<pre class="source-code">try</pre>
			<pre class="source-code">            {</pre>
			<pre class="source-code">                Task.WaitAll(t1, t2); // Not recommended to </pre>
			<pre class="source-code">                  //use in production application.</pre>
			<pre class="source-code">            }</pre>
			<pre class="source-code">            catch (AggregateException ex)</pre>
			<pre class="source-code">            {</pre>
			<pre class="source-code">                Console.WriteLine(ex.Flatten().Message);</pre>
			<pre class="source-code">            }</pre>
			<pre class="source-code">            Console.ReadLine();</pre>
			<p>When you run this progr<a id="_idTextAnchor412"/>am, you will get the following exception, which states that you cannot modify and enumerate the collection at the same time:</p>
			<div><div><img src="img/Table_4.2.jpg" alt="Table 4.2 – The ConcurrentDictionary sample output&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Table 4.2 – The ConcurrentDictionary sample output</p>
			<p>You may think now that we can add a lock to manage thread sy<a id="_idTextAnchor413"/>nchronization and avoid this exception in multithreaded scenarios for thread safety. I added a lock to the code wherever the <a id="_idIndexMarker326"/>dictionary is modified<a id="_idIndexMarker327"/> and enumerated to synchronize the threads. Here are the updated code snippets:</p>
			<ol>
				<li value="1">First, we have <code>Task</code> <code>t1</code> as one operation from a client who is adding to the dictionary:<pre>Dictionary&lt;int, string&gt; employeeDictionary = new Dictionary&lt;int, string&gt;();            
            Task t1 = Task.Factory.StartNew(() =&gt;
            {
                for (int i = 0; i &lt; 100; ++i)
                {
                    //Lock the shared data
                    lock (syncObject)
                    {
                        employeeDictionary.TryAdd(i,
                          "Employee" + i.ToString())<a id="_idTextAnchor414"/>;
                    }
                    Thread.Sleep(100);
                    
                }
            });</pre></li>
				<li>Then, we have <code>Task</code> <code>t2</code> as a second operation from another client who is reading<a id="_idIndexMarker328"/> from the<a id="_idIndexMarker329"/> dictionary:<pre>            Task t2 = Task.Factory.StartNew(() =&gt;
            {
                Thread.Sleep(500);
                //Lock the shared data
                lock (syncObject)
                {
                    foreach (var item in
                     employeeDictionary)
                    {
                        Console.WriteLine(item.Key + 
                          "-" + item.Value);
                        Thread.Sleep(100);
                    }
                }
            });</pre></li>
				<li>Now, we have both tasks executed at the same time:<pre>try
            {
                Task.WaitAll(t1, t2); // Not 
                  //recommended to use in production 
                  //application.
            }
            catch (AggregateException ex)
            {
                Console.WriteLine(ex.Flatten()
                  .Message);
            }
            Console.ReadLine();</pre></li>
			</ol>
			<p>When you run this code, you will not see any exceptions. Ho<a id="_idTextAnchor415"/>wever, locks have some issues, as mentioned earlier, so this code can be rewritten using concurrent<a id="_idIndexMarker330"/> collections. They <a id="_idIndexMarker331"/>internally use a multiple-thread synchronization technique that helps to scale well, prevent data corruption, and avoid all the problems with locks.</p>
			<p>We can rewrite our code using <code>ConcurrentDictionary</code>, which is available in the <code>System.Collections.Concurrent</code> namespace. Replace <code>Dictionary</code> with <code>ConcurrentDictionary</code> in the sample code. You can also remove the reference to the <code>System.Collections.Generic</code> namespace, as <code>Dictionary</code> is not used now. Also, remove all the locks. The updated code is as follows, where we replace <code>Dictionary</code> with <code>ConcurrentDictionary</code> and remove the lock:</p>
			<p>We have <code>Task t1</code> as one operation from a client who is adding to the dictionary, and an explicit lock is not needed with concurrent collections:</p>
			<pre>ConcurrentDictionary&lt;int, string&gt; employeeDictionary = new ConcurrentDictionary&lt;int, string&gt;();
            Task t1 = Task.Factory.StartNew(() =&gt;
            {
                for (int i = 0; i &lt; 100; ++i)
                {
                    employeeDictionary.TryAdd(i, 
                      "Employee"
                      + i.ToString());
                    Thread.Sleep(100);
                    
                }
            });</pre>
			<ol>
				<li value="4">Then, we have <code>Task t2</code> as a second operation from another client who is reading from<a id="_idIndexMarker332"/> the dictionary, and<a id="_idIndexMarker333"/> an explicit lock is not needed with concurrent collections:<pre>            Task t2 = Task.Factory.StartNew(() =&gt;
            {
                Thread.Sleep(500);
                foreach (var item in 
                  employeeDictionary)
                {
                    Console.WriteLine(item.Key + "-" +
                       item.Value);
                    Thread.Sleep(100);
                }
            });</pre></li>
				<li>Now, both tasks are executed at the same time:<pre>try
            {
                Task.WaitAll(t1, t2);
            }
            catch (AggregateException ex) // You will 
              //not get Exception
            {
                Console.WriteLine(ex.Flatten()
                  .Message);
            }
            Console.ReadLine();</pre></li>
			</ol>
			<p>When you run the program now, you will not<a id="_idTextAnchor416"/> get any exceptions, as all operations are thread-safe and atomic in <code>ConcurrentDictionary</code>. There is no overhead for the developer in implementing the locks and maintaining them as the project grows bigger. Here are<a id="_idIndexMarker334"/> some caveats with<a id="_idIndexMarker335"/> concurrent collections such as <code>ConcurrentDictionary</code> that you need to bear in mind:</p>
			<ul>
				<li>If two threads call <code>AddOrUpdate</code>, there's no guarantee which of the factory delegates will be called and even no guarantee that if a factory delegate produces an item, the item will be stored in the dictionary.</li>
				<li>The enumerator obtained by the <code>GetEnumerator</code> call is not a snapshot and may be modified during enumeration (which doesn't cause any exceptions).</li>
				<li>Key and value properties are snapshots of corresponding collections and may not correspond to the ac<a id="_idTextAnchor417"/><a id="_idTextAnchor418"/>tual dictionary state.</li>
			</ul>
			<p>We've looked at <code>ConcurrentDictionary</code> in detail; let's look at other concurrent collections in the next section.</p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor419"/>Producer-consumer concurrent collections</h2>
			<p>In producer<a id="_idIndexMarker336"/>-consumer<a id="_idIndexMarker337"/> concurrent collections, one <a id="_idTextAnchor420"/>or more threads can produce tasks (adding to a queue, stack, or bag, for instance), and one or more other threads can consume tasks from the same collection (the queue, stack, or bag).</p>
			<p><code>ConcurrentDictionary</code>, which we saw in the previous section, is a general-purpose collection class where you add an item that you want and specify which item you want to read. Other concurrent collections are designed for specific pr<a id="_idTextAnchor421"/>oblems:</p>
			<ul>
				<li><code>ConcurrentQueue</code> is for scenarios where you want FIFO.</li>
				<li><code>ConcurrentStack</code> is for scenarios where you want LIFO.</li>
				<li><code>ConcurrentBag</code> is for scenarios where you want the same thread producing and consuming data stored in the bag and the order doesn't matter.</li>
			</ul>
			<p>These three collections are also known as <strong class="bold">producer-consumer<a id="_idTextAnchor422"/> collections</strong>, where one or more threads can produce tasks and consu<a id="_idTextAnchor423"/>me tasks from the same collection, as shown in the following figure:</p>
			<div><div><img src="img/Figure_4.7_B18507.jpg" alt="Figure 4.7 – A producer-consumer concurrent collection&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.7 – A producer-consumer concurrent collection</p>
			<p>All these three collections implement the <code>IProducerConsumerCollection&lt;T&gt;</code> interface, and the most important methods are <code>TryAdd</code> and <code>TryTake</code>, as shown here:</p>
			<pre class="source-code">// Returns: true if the object was added successfully; otherwise, false.        </pre>
			<pre class="source-code">bool TryAdd(T item);</pre>
			<pre class="source-code">// Returns true if an object was removed an<a id="_idTextAnchor424"/>d returned successfully; otherwise, false.</pre>
			<pre class="source-code">bool TryTake([MaybeNullWhen(false)] out T item);</pre>
			<p>Let's take an <a id="_idIndexMarker338"/>example <a id="_idIndexMarker339"/>of a producer-consumer and simulate it using <code>ConcurrentQueue</code>:</p>
			<ul>
				<li><strong class="bold">Producer</strong>: A client <a id="_idIndexMarker340"/>sending a request to a <a id="_idTextAnchor425"/>web service and the server storing a request in a queue</li>
				<li><strong class="bold">Consumer</strong>: A worker<a id="_idIndexMarker341"/> thread pulling the request from the queue and processing it</li>
			</ul>
			<p>The implementation is shown in the following code:</p>
			<pre class="source-code">//Producer: Client sending request to web service and server storing the request in queue.</pre>
			<pre class="source-code">ConcurrentQueue&lt;string&gt; concurrentQueue = new ConcurrentQueue&lt;string&gt;();            </pre>
			<pre class="source-code">            Task t1 = Task.Factory.StartNew(() =&gt;</pre>
			<pre class="source-code">            {</pre>
			<pre class="source-code">                for (int i = 0; i &lt; 10; ++i)</pre>
			<pre class="source-code">                {</pre>
			<pre class="source-code">                    concurrentQueue.Enqueue("Web request " </pre>
			<pre class="source-code">                      + i);</pre>
			<pre class="source-code">                    Console.WriteLine("Sending "+ "Web </pre>
			<pre class="source-code">                      request " + i);</pre>
			<pre class="source-code">                    Thread.Sleep(100);</pre>
			<pre class="source-code">                }</pre>
			<pre class="source-code">            });</pre>
			<p>Now, we <a id="_idIndexMarker342"/>have <code>Consumer</code>, where <a id="_idIndexMarker343"/>a <code>Worker</code> thread pulls the request from the queue and processes it:</p>
			<pre class="source-code">            Task t2 = Task.Factory.StartNew(() =&gt;</pre>
			<pre class="source-code">            {</pre>
			<pre class="source-code">                while (true)</pre>
			<pre class="source-code">                {</pre>
			<pre class="source-code">                    if (concurrentQueue.TryDequeue(out</pre>
			<pre class="source-code">                     string request))</pre>
			<pre class="source-code">                    {</pre>
			<pre class="source-code">                        Console.WriteLine("Processing "+</pre>
			<pre class="source-code">                         request);</pre>
			<pre class="source-code">                    }</pre>
			<pre class="source-code">                    else</pre>
			<pre class="source-code">                    {</pre>
			<pre class="source-code">                        Console.WriteLine("No request");</pre>
			<pre class="source-code">                    }</pre>
			<pre class="source-code">                }</pre>
			<pre class="source-code">            });</pre>
			<p>Both producer and consumer tasks are executed at the same time successfully. Wait for all provided tasks to complete execution within the specified number of milliseconds. Refer to the following code snippet:</p>
			<pre class="source-code">try</pre>
			<pre class="source-code">            {                </pre>
			<pre class="source-code">                Task.WaitAll(new Task[] { t1, t2 }, 1000);</pre>
			<pre class="source-code">            }</pre>
			<pre class="source-code">            catch (AggregateException ex) // No exception</pre>
			<pre class="source-code">            {</pre>
			<pre class="source-code">                Console.WriteLine(ex.Flatten().Message);</pre>
			<pre class="source-code">            }</pre>
			<p>This is according to the method definition from Microsoft:</p>
			<ul>
				<li><code>concurrentQueue.Enqueue</code>: This adds an object to the end of <code>ConcurrentQueue&lt;T&gt;</code>.</li>
				<li><code>concurrentQueue.TryDequeue</code>: This tries to remove and return <a id="_idTextAnchor426"/>the object at the beginning of <code>ConcurrentQueue</code>.</li>
			</ul>
			<p>When you run the program, you can see <code>task</code> <code>t1</code> producing requests and <code>task</code> <code>t2</code> polling and <a id="_idIndexMarker344"/>then consuming<a id="_idIndexMarker345"/> requests. We'll get into the details in a short while. We also said that these classes implement <code>IProducerConsumerCollection&lt;T&gt;</code>, so we are going to make three changes to the previous code:</p>
			<ul>
				<li>Replace <code>ConcurrentQueue&lt;string&gt;</code> with <code>IProducerConsumerCollection&lt;string&gt;</code>.</li>
				<li>Replace <code>concurrentQueue.Enqueue</code> with <code>concurrentQueue.TryAdd</code>.</li>
				<li>Replace <code>concurrentQueue.TryDequeue</code> with <code>concurrentQueue.TryTake</code>.</li>
			</ul>
			<p>This is how the code looks now:</p>
			<pre class="source-code">IProducerConsumerCollection&lt;string&gt; concurrentQueue = new ConcurrentQueue&lt;string&gt;();</pre>
			<pre class="source-code">//Removed code for brevity.</pre>
			<pre class="source-code">Task t1 = Task.Factory.StartNew(() =&gt;</pre>
			<pre class="source-code">            {</pre>
			<pre class="source-code">                for (int i = 0; i &lt; 10; ++i)</pre>
			<pre class="source-code">                {</pre>
			<pre class="source-code">                    <code>concurrentQueue.TryAdd</code>("Web request " + </pre>
			<pre class="source-code">                      i);</pre>
			<pre class="source-code">//Removed code for brevity.</pre>
			<pre class="source-code">Task t2 = Task.Factory.StartNew(() =&gt;</pre>
			<pre class="source-code">            {</pre>
			<pre class="source-code">                while (true)</pre>
			<pre class="source-code">                {</pre>
			<pre class="source-code">                    if (<code>concurrentQueue.TryTake</code>(out string</pre>
			<pre class="source-code">                     request))</pre>
			<pre class="source-code">//Removed code for brevity.</pre>
			<p>Now, go ahead <a id="_idIndexMarker346"/>and run <a id="_idIndexMarker347"/>the program. You can see <code>task</code> <code>t1</code> producing requests and <code>task</code> <code>t2</code> polling and then consuming requests. You can see all 10 requests produced by <code>task</code> <code>t1</code> and consumed by <code>task</code> <code>t2</code>. But there are two problems:</p>
			<ul>
				<li>The producer is producing at its own rate, the consumer is consuming at its own rate, and there is no synchronization.</li>
				<li>There is continuous indefinite polling from the consumer in <code>task</code> <code>t2</code>, which is not good for performance and CPU usage, as we can see by <code>concurrentQueue.TryTake</code>.</li>
			</ul>
			<p>This is where <code>BlockingCol<a id="_idTextAnchor429"/>lection&lt;T&gt;</code> comes in handy.</p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor430"/>The BlockingCollection&lt;T&gt; class</h2>
			<p><code>BlockingCollection&lt;T&gt;</code> supports<a id="_idIndexMarker348"/> bounding<a id="_idIndexMarker349"/> and blocking. Bounding allows you to specify a maximum capacity for a collection. Controlling the maximum size of a collection helps to prevent producing threads from moving too far ahead of consuming threads. Multiple producing threads can add items to <code>BlockingCollection&lt;T&gt;</code> concurrently until the collection reaches its maximum size, after which they will be blocked until an item is removed by consumers.</p>
			<p>Similarly, multiple consuming threads can remove items from a blocking collection concurrently till the collection becomes empty, after which they will be blocked until an item is added by producers.<a id="_idTextAnchor431"/> A producing thread can invoke the <code>CompleteAdding</code> method when no more items will be added and indicate that it has completed adding. This will help consumers to monitor the <code>IsCompleted</code> property to know that no more items will be added when the collection is empty. </p>
			<p>When you create a <code>BlockingCollection&lt;T&gt;</code> class, along with the bounding capacity, you can also specify the type of concurrent collection to use depending upon the scenario. By default, the collection type is <code>ConcurrentQueue&lt;T&gt;</code> for <code>BlockingCollection&lt;T&gt;</code> when you don't specify the type.</p>
			<p>Here is a sample code snippet:</p>
			<pre class="source-code">BlockingCollection&lt;string&gt; blockingCollection = new BlockingCollection&lt;string&gt;(new ConcurrentQueue&lt;string&gt;(),5);    </pre>
			<pre class="source-code">            Task t1 = Task.Factory.StartNew(() =&gt;</pre>
			<pre class="source-code">            {</pre>
			<pre class="source-code">                for (int i = 0; i &lt; 10; ++i)</pre>
			<pre class="source-code">                {</pre>
			<pre class="source-code">                    blockingCollection.TryAdd("Web request</pre>
			<pre class="source-code">                     " + i);</pre>
			<pre class="source-code">                    Console.WriteLine("Sending " + "Web</pre>
			<pre class="source-code">                      request " + i);</pre>
			<pre class="source-code">                    Thread.Sleep(100);</pre>
			<pre class="source-code">                }</pre>
			<pre class="source-code">                blockingCollection.CompleteAdding();</pre>
			<pre class="source-code">            });</pre>
			<p>Then, the consumer <a id="_idIndexMarker350"/>with<a id="_idIndexMarker351"/> the <code>Worker</code> thread pulls the item from the queue and processes it:</p>
			<pre class="source-code">            Task t2 = Task.Factory.StartNew(() =&gt;</pre>
			<pre class="source-code">            {</pre>
			<pre class="source-code">                while (!blockingCollection.IsCompleted)</pre>
			<pre class="source-code">                {</pre>
			<pre class="source-code">                    if (blockingCollection.TryTake(out</pre>
			<pre class="source-code">                     string request,100))</pre>
			<pre class="source-code">                    {</pre>
			<pre class="source-code">                        Console.WriteLine("Processing " +</pre>
			<pre class="source-code">                         request);</pre>
			<pre class="source-code">                    }</pre>
			<pre class="source-code">                    else</pre>
			<pre class="source-code">                    {</pre>
			<pre class="source-code">                        Console.WriteLine("No request");</pre>
			<pre class="source-code">                    }</pre>
			<pre class="source-code">                }</pre>
			<pre class="source-code">            });</pre>
			<p>Now, the producer and consumer thread are accessed concurrently.</p>
			<p>There are a few points to consider in the code:</p>
			<ul>
				<li>The specified bounding of <code>5</code>: <code>BlockingCollection&lt;string&gt; blockingCollection = new BlockingCollection&lt;string&gt;(new ConcurrentQueue&lt;string&gt;(),5);</code>.</li>
				<li>The producing thread invokes the <code>CompleteAdding</code> method when no more items will be added to indicate that it has completed adding: <code>blockingCollection.CompleteAdding();</code>.</li>
				<li>Consumers monitor the <code>IsCompleted</code> property to find out that no more items will be added when the collection is empty: <code>while (!blockingCollection.IsCompleted)</code>.</li>
				<li>Try to remove an item from <code>BlockingCollection&lt;T&gt;</code> in the specified time – for example, I have gone with 100 milliseconds: <code>if (blockingCollection.TryTake(out string request, 100))</code>.</li>
			</ul>
			<p>This is the power of a block<a id="_idTextAnchor432"/>ing collection. Both the producer and consumer are decoupled, they <a id="_idIndexMarker352"/>can be <a id="_idIndexMarker353"/>coded independently by different teams, and at runtime, they use a blocking concurrent collection to share data with each other. Plus, at the same time, flow is controlled with the bounding capacity so that <a id="_idTextAnchor433"/>the producer doesn't move too far ahead of consumers.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">In addition to the <code>TryTake</code> method that we've seen, you can also use a <code>foreach</code> loop to remove items from a blocking collection. You can read about it here: </p>
			<p class="callout"><a href="https://docs.microsoft.com/en-us/dotnet/standard/collections/thread-safe/how-to-use-foreach-to-remove">https://docs.microsoft.com/en-us/dotnet/standard/collections/thread-safe/how-to-use-foreach-to-remove</a></p>
			<p class="callout">With blocking collections, there will be scenarios where the consum<a id="_idTextAnchor435"/>er will have to work with multiple collections and take or add items. The <code>TakeFromAny</code> and <code>AddToAny</code> methods will help you in this scenario. You can read further about these two methods here:</p>
			<p class="callout"><a href="https://docs.microsoft.com/en-us/dotnet/api/system.collections.concurrent.blockingcollection-1.takefromany?view=net-6.0">https://docs.microsoft.com/en-us/dotnet/api/system.collections.concurrent.blockingcollection-1.takefromany?view=net-6.0</a></p>
			<p class="callout"><a href="https://docs.microsoft.com/en-us/dotnet/api/system.collections.concurrent.blockingcollection-1.addtoany?view=net-6.0">https://docs.microsoft.com/en-us/dotnet/api/system.collections.concurrent.blockingcollection-1.addtoany?view=net-6.0</a></p>
			<h1 id="_idParaDest-100"><a id="_idTextAnchor438"/>Summary</h1>
			<p>Wrapping up, writing, and maintaining clean asynchronous code is difficult. However, with the various constructs available in .NET and C#, developers can now write asynchronous code with less framework overhead and focus more on the business requirements. </p>
			<p>In this chapter, we covered various ways to write scalable asynchronous code using the TPL, async-await, and concurrent collections, and we also covered the fundamentals of threads and <code>ThreadPool</code> in .NET to understand the framework internals and write cleaner code for enterprise applications. Now, we have a deeper understanding of multithreading and how to protect shared data in a multithreaded environment. We learned about creating tasks and implementing asynchronous functions using async-await, and finally, we learned about the concurrent collections available in .NET Core and their implementation in various concurrent scenarios.</p>
			<p>In the next chapter, we will<a id="_idTextAnchor439"/><a id="_idTextAnchor440"/> look into dependency injection in .NET 6 and how it plays a significant role in loosely coupling various low-level classes in enterprise applications.</p>
			<h1 id="_idParaDest-101"><a id="_idTextAnchor441"/>Questions</h1>
			<ol>
				<li value="1">In a multithreaded environment, which of the following data structures should you use to protect data from getting overwritten/corrupted?</li>
			</ol>
			<p>a. <code>async</code>-<code>await</code>.</p>
			<p>b. Tasks.</p>
			<p>c. Synchronization constructs such as locks.</p>
			<p>d. Data never gets corrupted.</p>
			<p><strong class="bold">Answer : a</strong></p>
			<ol>
				<li value="2">If you have a WPF application that retrieves data from a REST API, which of the following should you implement for better responsiveness?</li>
			</ol>
			<p>a. A concurrent collection</p>
			<p>b. <code>Parallel.For</code></p>
			<p>c. <code>async</code>-<code>await</code> for the REST API calls</p>
			<p><strong class="bold">Answer: c</strong></p>
			<ol>
				<li value="3">Which of the following should be passed to cancel a task?</li>
			</ol>
			<p>a. <code>CancellationToken</code></p>
			<p>b. <code>ConcurrentDictionary</code></p>
			<p>c. <code>SemaphoreSlim</code></p>
			<p><strong class="bold">Answer: a</strong></p>
			<ol>
				<li value="4">Which of the following is the recommended return type <a id="_idTextAnchor442"/><a id="_idTextAnchor443"/>for an asynchronous method that uses async-await and does not return anything?</li>
			</ol>
			<p>a. <code>async void</code></p>
			<p>b. <code>async Task</code></p>
			<p>c. <code>async book</code></p>
			<p>d. <code>async Task&lt;bool&gt;</code></p>
			<p><strong class="bold">Answer: b</strong></p>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor444"/>Further reading</h1>
			<ul>
				<li><a href="https://www.packtpub.com/product/hands-on-parallel-programming-with-c-8-and-net-core-3/9781789132410">https://www.packtpub.com/product/hands-on-parallel-programming-with-c-8-and-net-core-3/9781789132410</a></li>
				<li><a href="https://devblogs.microsoft.com/dotnet/configureawait-faq/">https://devblogs.microsoft.com/dotnet/configureawait-faq/</a></li>
				<li><a href="http://www.albahari.com/threading/">http://www.albahari.com/threading/</a></li>
				<li><em class="italic">Dataflow (Task Parallel Library) | Microsoft Docs</em>: <a href="https://docs.microsoft.com/en-us/dotnet/standard/parallel-programming/dataflow-task-parallel-library">https://docs.microsoft.com/en-us/dotnet/standard/parallel-programming/dataflow-task-parallel-library</a></li>
			</ul>
		</div>
	</body></html>