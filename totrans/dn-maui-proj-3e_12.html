<html><head></head><body>
<div><p><a id="_idTextAnchor916"/><a id="_idTextAnchor917"/><a id="_idTextAnchor918"/></p>
<h1 class="chapter-number" id="_idParaDest-187"><a id="_idTextAnchor919"/>12</h1>
<h1 id="_idParaDest-188"><a id="_idTextAnchor920"/>Hot Dog or Not Hot Dog Using Machine Learning</h1>
<p>In this chapter, we will learn how to use machine learning to create a model that we can use for image classification. We will export the model as an <strong class="bold">Onnx</strong> model that we can use on all platforms – that is, Android, iOS, macOS, and Windows. To train and export models, we will use Azure Cognitive Services and the Custom Vision service.</p>
<p>Once we have exported the models, we will learn how to use them in a .NET MAUI app.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Training a model with Azure Cognitive Services and the Custom Vision service</li>
<li>Using Onnx models for image classification using ML.NET</li>
<li>Using custom routes in .NET MAUI for navigation</li>
</ul>
<h1 id="_idParaDest-189"><a id="_idTextAnchor921"/>Technical requirements</h1>
<p>To be able to complete this project, you need to have Visual Studio for Mac or PC installed, as well as the .NET MAUI components. See <em class="italic">Chapter 1</em>, <em class="italic">Introduction to .NET MAUI</em>, for more details on how to set up your environment. You also need an Azure account. If you have a Visual Studio subscription, there are a specific amount of Azure credits included each month. To activate your Azure benefits, go to <a href="https://my.visualstudio.com">https://my.visualstudio.com</a>.</p>
<p>You can also create a free account, where you can use selected services for free over 12 months. You will get $200 worth of credit to explore any Azure service for 30 days, and you can also use the free services at any time. Read more at <a href="https://azure.microsoft.com/en-us/free/">https://azure.microsoft.com/en-us/free/</a>.<a id="_idTextAnchor922"/></p>
<p>If you do not have and do not want to sign up for a free Azure account, the trained model is available in the source code for this chapter. You can download and use the pre-trained model instead.</p>
<p>The source code for this chapter is available at the GitHub repository for the book at <a href="https://github.com/PacktPublishing/MAUI-Projects-3rd-Edition">https://github.com/PacktPublishing/MAUI-Projects-3rd-Edition</a>.</p>
<h1 id="_idParaDest-190"><a id="_idTextAnchor923"/>Machine learning</h1>
<p>The term <strong class="bold">machine learning</strong> was coined in 1959 by <a id="_idIndexMarker1279"/><a id="_idIndexMarker1280"/>Arthur Samuel, an American pioneer in <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>). Tom M. Mitchell, an <a id="_idIndexMarker1281"/><a id="_idIndexMarker1282"/>American computer scientist, provided the following more formal definition of machine learning later:</p>
<p class="author-quote">“A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.”</p>
<p>In simpler terms, this quote describes a computer program that can learn without being explicitly programmed. In machine learning, algorithms are used to build a mathematical model of sample data or training data. The models are used for computer programs to make predictions and decisions without being explicitly <a id="_idTextAnchor924"/>programmed for the task in question.</p>
<p>In this section, we will learn about a few different machine learning services and APIs that are available when developing a .NET MAUI application. Some APIs are only available for specific platforms, such as Core ML, while others are cross-platform.</p>
<h2 id="_idParaDest-191"><a id="_idTextAnchor925"/>Azure Cognitive Services – Custom Vision</h2>
<p>Custom Vision is a tool or service that can be<a id="_idIndexMarker1283"/><a id="_idIndexMarker1284"/> used to train models for image classification and to detect objects in images. With Custom Vision, we can upload our own images and tag them so that they can be trained for image classification. If we train a <a id="_idIndexMarker1285"/><a id="_idIndexMarker1286"/>model for object detection, we can also tag specific areas of an image. Because models are already pre-trained for basic image recognition, we don’t need a large amount of data to get a great result. The recommendation<a id="_idIndexMarker1287"/><a id="_idIndexMarker1288"/> is to have at least 30 images per tag.</p>
<p>When we have trained a model, we can use it with an API, which is part of the Custom Vision service. We can also export models for <strong class="bold">Core ML</strong> (<strong class="bold">iOS</strong>), <strong class="bold">TensorFlow</strong> (<strong class="bold">Android</strong>), the <strong class="bold">Open Neural Network Exchange </strong>(<strong class="bold">ONNX</strong>), and a <strong class="bold">Dockerfile</strong> (<strong class="bold">Azure IoT Edge</strong>, <strong class="bold">Azure Functions</strong>, and <strong class="bold">Azure ML</strong>). These models can be used to carry out classification or object detection without being connected to the Custom Vision service.</p>
<p><a id="_idTextAnchor926"/>You will need an Azure subscription to use it – go to <a href="https://azure.com/free">https://azure.com/free</a> to create a <a id="_idTextAnchor927"/>free subscription, which should be enough to complete this project.</p>
<h2 id="_idParaDest-192"><a id="_idTextAnchor928"/>Core ML</h2>
<p>Core ML is a framework that was introduced<a id="_idIndexMarker1289"/><a id="_idIndexMarker1290"/> in iOS 11. Core ML makes it possible to integrate machine learning models into iOS apps. On top of Core <a id="_idIndexMarker1291"/><a id="_idIndexMarker1292"/>ML, we have high-level APIs, as follows:</p>
<ul>
<li>Vision APIs for image analysis</li>
<li>Natural language APIs for natural language processing</li>
<li>Speech to convert audio to text</li>
<li>Sound analysis to identify sounds in audio</li>
<li>GameplayKit to evaluate learned decision trees and strategies</li>
</ul>
<p class="callout-heading">More information</p>
<p class="callout">More information about Core ML can be found in the official documentation from Apple at <a href="https://developer.apple.com/documentation/coreml">https://developer.apple.com/documentation/coreml</a>.</p>
<h2 id="_idParaDest-193"><a id="_idTextAnchor929"/>TensorFlow</h2>
<p>TensorFlow is an open source machine <a id="_idIndexMarker1293"/><a id="_idIndexMarker1294"/>learning framework. However, TensorFlow can be used for more than simply running models on mobile devices – it can also be used to train models. To run it on mobile devices, we have TensorFlow Lite. The <a id="_idIndexMarker1295"/><a id="_idIndexMarker1296"/>models that are exported from Azure Cognitive Services are for TensorFlow Lite. There are also C# bindings for TensorFlow Lite that are available as a NuGet package.</p>
<p class="callout-heading">More information</p>
<p class="callout">More information about TensorFlow can be found in the official documentation at <a href="https://www.tensorflow.org/">https://www.tensorflow.org/</a>.</p>
<h2 id="_idParaDest-194"><a id="_idTextAnchor930"/>ML.Net</h2>
<p>ML.NET is an open source and<a id="_idIndexMarker1297"/><a id="_idIndexMarker1298"/> cross-platform machine learning framework with support for iOS, macOS, Android, and Windows, all from a familiar environment – C#. ML.NET provides <strong class="bold">AutoML</strong>, a set of productivity tools that make<a id="_idIndexMarker1299"/><a id="_idIndexMarker1300"/> building, training, and deploying custom models easy. ML.NET can be used in the following scenarios and more:</p>
<ul>
<li>Sentiment analysis and product recommendation</li>
<li>Object detection and image classification</li>
<li>Price prediction, sales spike detection, and forecasting</li>
<li>Fraud detection</li>
<li>Customer segmentation</li>
</ul>
<p>Now that we have a broad overview of the technologies at play, we will focus on using ML.NET, since it is a cross-platform framework and built for C#. Let’s look at the project we are going to build next.</p>
<h1 id="_idParaDest-195"><a id="_idTextAnchor931"/><a id="_idTextAnchor932"/><a id="_idTextAnchor933"/><a id="_idTextAnchor934"/>The project overview</h1>
<p>If you have seen the TV series <em class="italic">Silicon Valley</em>, you have probably heard of the <em class="italic">Not Hotdog</em> application. In this chapter, we will learn how to<a id="_idIndexMarker1301"/><a id="_idIndexMarker1302"/> build that app. The first part of this chapter will involve collecting the data that we will use to create a machine learning model that can detect whether a photo contains a hot dog.</p>
<p><a id="_idTextAnchor935"/>In the second part of the chapter, we will build an app using .NET MAUI and ML.NET, where the user can either take a new photo or pick a photo in the photo library, analyzing it to see whether <a id="_idTextAnchor936"/>it contains a hot dog. The estimated time for completing this project is 120 minutes.</p>
<h1 id="_idParaDest-196"><a id="_idTextAnchor937"/>Getting started</h1>
<p>We can use either Visual Studio 2022 on a PC or Visual Studio for Mac to do this project. To build an iOS app using Visual Studio for PC, you must have a Mac connected. If you don’t have access to a Mac at all, you can choose to just do the Android and Windows parts of this project.</p>
<p>Similarly, if you only have a Mac, you can choose to just do the iOS and macOS or Android parts of this <a id="_idTextAnchor938"/>project.</p>
<h1 id="_idParaDest-197"><a id="_idTextAnchor939"/>Building the Hot Dog or Not Hot Dog application using machine learning</h1>
<p>Let’s get started! We will first train a <a id="_idIndexMarker1303"/><a id="_idIndexMarker1304"/>model for image classification that we can use later in the chapter to decide whether a photo contains a hot <a id="_idIndexMarker1305"/><a id="_idIndexMarker1306"/>dog.</p>
<p class="callout-heading">Note</p>
<p class="callout">If you do not want to go through the effort of training a model, you can download a pre-trained model from the following URL: <a href="https://github.com/PacktPublishing/MAUI-Projects-3rd-Edition/tree/main/Chapter12/HotdogOrNot/Resources/Raw">https://github.com/PacktPublishing/MAUI-Projects-3rd-Edition/tree/main/Chapter12/HotdogOrNot/Resources/Raw</a>.</p>
<h2 id="_idParaDest-198"><a id="_idTextAnchor940"/>Training a model</h2>
<p>To train a model for image classification, we need to collect photos of hot dogs and photos that aren’t of hot dogs. Because most items in the world are not hot dogs, we need more photos that don’t contain hot dogs. It’s better if the photos of hot dogs cover a lot of different hot dog scenarios – with bread, ketchup, or mustard. This is so that the model will <a id="_idIndexMarker1307"/><a id="_idIndexMarker1308"/>be able to recognize hot dogs in different situations. When we collect photos that aren’t of hot dogs, we also need to have a large variety of photos that are both of items that are like hot dogs and that are completely different from hot dogs.</p>
<p>The model that is in the solution on GitHub was<a id="_idIndexMarker1309"/><a id="_idIndexMarker1310"/> trained with 240 photos, 60 of which were of hot dogs, and 180 of which were not.</p>
<p>Once we have collected all the photos, we will be ready to start training the model by following these steps:</p>
<ol>
<li>Go to <a href="https://customvision.ai">https://customvision.ai</a>.</li>
<li>Log in and create a new project.</li>
<li>Give the project a name – in our case, <code>HotDogOrNot</code>.</li>
<li><a id="_idTextAnchor941"/>Select a resource or create a new one by clicking <strong class="bold">Create new</strong>. Fill in the dialog box, and select <strong class="bold">CustomVision.Training</strong> in the <strong class="bold">Kind</strong> dropdown.<p class="list-inset">The project type should be <strong class="bold">Classification</strong>, and the classification type should be <strong class="bold">Multiclass (Single tag </strong><strong class="bold">per image)</strong>.</p></li>
<li>Select <strong class="bold">General (compact)</strong> as the domain. We use a compact domain if we want to export models and run them on a mobile device.</li>
<li>Click <strong class="bold">Create project</strong> to <a id="_idIndexMarker1311"/><a id="_idIndexMarker1312"/>continue, as shown in the following screenshot:</li>
</ol>
<div><div><img alt="Figure 12.1 – Creating a new AI project" src="img/B19214_12_1.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.1 – Creating a new AI project</p>
<p>Once <a id="_idIndexMarker1313"/>we <a id="_idIndexMarker1314"/>have created a project, we can start to upload images and tag them.<a id="_idTextAnchor942"/><a id="_idTextAnchor943"/><a id="_idTextAnchor944"/></p>
<h3>Tagging images</h3>
<p>The easiest way to get images is to go to Google and search for them. We will start by adding photos <a id="_idIndexMarker1315"/>of hot dogs by following<a id="_idIndexMarker1316"/> these steps:</p>
<ol>
<li>Click <strong class="bold">Add images</strong>.</li>
<li>Select the photos of hot dogs that should be uploaded.</li>
<li>Tag the photos with <code>hotdog</code>, as shown in the following screenshot:</li>
</ol>
<div><div><img alt="Figure 12.2 – Uploading images of hot dogs" src="img/B19214_12_2.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.2 – Uploading images of hot dogs</p>
<p>Once we have uploaded all the photos of hot dogs, it’s time to upload photos that aren’t of hot dogs by following the following steps. For best results, we should also include photos of objects that look similar to hot dogs but are not:</p>
<ol>
<li>Click the <strong class="bold">Add images</strong> button above the gallery of uploaded images.</li>
<li>Select the photos that aren’t of hot dogs.</li>
<li>Tag the photos with <code>Negative</code>.<p class="list-inset">A Negative tag is used for photos that don’t contain any objects that we have created other tags for. In this case, none of the photos we will upload contain hot dogs, as can<a id="_idIndexMarker1317"/> be seen in the following screenshot:</p></li>
</ol>
<div><div><img alt="Figure 12.3 – Uploading images that aren’t hot dogs" src="img/B19214_12_3.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.3 – Uploading images that aren’t hot dogs</p>
<p>Once we<a id="_idIndexMarker1318"/> have uploaded the photos, it’s time to train a mode<a id="_idTextAnchor945"/><a id="_idTextAnchor946"/><a id="_idTextAnchor947"/>l.</p>
<h3>Training a model</h3>
<p>Not all the <a id="_idIndexMarker1319"/>photos that we are uploading will be used for training; some will be used for verification to give us a score about how good the model is. If we upload photos in chunks and train the model after each chunk, we will be able to see our scores improving. To train a model, click the green <strong class="bold">Train</strong> button at the top of the page, as illustrated in the following screenshot:</p>
<div><div><img alt="Figure 12.4 – Training the mode﻿l" src="img/B19214_12_4.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.4 – Training the mode<a id="_idTextAnchor948"/>l</p>
<p>The following screenshot shows the result of a training iteration where the precision of the model is <strong class="bold">91.7%<a id="_idTextAnchor949"/><a id="_idTextAnchor950"/><a id="_idTextAnchor951"/></strong>:</p>
<div><div><img alt="Figure 12.5 – Model verification results" src="img/B19214_12_5.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.5 – Model verification results</p>
<p>Once we<a id="_idIndexMarker1320"/> have trained a model, we will export it so that it can be used on a device.</p>
<h3>Exporting a model</h3>
<p>We can <a id="_idIndexMarker1321"/>use the APIs if we want to, but to make fast classifications and to be able to do this offline, we will add the models to the app packages. Click the <strong class="bold">Export</strong> button and then on <strong class="bold">ONNX</strong> to download the model, as shown in the following screenshot:</p>
<div><div><img alt="Figure 12.6 – Exporting the model" src="img/B19214_12_6.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.6 – Exporting the model</p>
<p>Once we<a id="_idIndexMarker1322"/> have downloaded the ONNX model, it’s time to build the a<a id="_idTextAnchor952"/><a id="_idTextAnchor953"/><a id="_idTextAnchor954"/>pp.</p>
<h2 id="_idParaDest-199"><a id="_idTextAnchor955"/>Building the app</h2>
<p>Our app <a id="_idIndexMarker1323"/>will use the trained models to classify photos, according to whether they are photos of hot dogs. We will use the same ONNX model for all platforms in the .NET MAUI <a id="_idTextAnchor956"/>app.</p>
<h3>Creating the new project</h3>
<p>Let’s begin, as foll<a id="_idTextAnchor957"/>ows.</p>
<p>The <a id="_idIndexMarker1324"/>first step is to create a new .NET MAUI projec:</p>
<ol>
<li>Open Visual Studio 2022, and select <strong class="bold">Create a </strong><strong class="bold">new project</strong>:</li>
</ol>
<div><div><img alt="Figure 12.7 – Visual Studio﻿ 2022" src="img/B19214_12_7.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.7 – Visual Studio<a id="_idTextAnchor958"/> 2022</p>
<p class="list-inset">This will open the <strong class="bold">Create a new </strong><strong class="bold">project</strong> wizard.</p>
<ol>
<li value="2">In the<a id="_idIndexMarker1325"/> search field, type in <code>maui</code>, and select the <strong class="bold">.NET MAUI App</strong> item from the list:</li>
</ol>
<div><div><img alt="Figure 12.8 – Create a new pro﻿ject" src="img/B19214_12_8.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.8 – Create a new pro<a id="_idTextAnchor959"/>ject</p>
<ol>
<li value="3">Click <strong class="bold">Next</strong>.</li>
<li>Complete <a id="_idIndexMarker1326"/>the next step of the wizard by naming your project. We will call our application <code>HotdogOrNot</code> in this case. Move on to the next dialog box by clicking <strong class="bold">Next</strong>, as illustrated in the following screenshot:</li>
</ol>
<div><div><img alt="Figure 12.9 – Configure your new pro﻿ject" src="img/B19214_12_9.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.9 – Configure your new pro<a id="_idTextAnchor960"/>ject</p>
<ol>
<li value="5">The<a id="_idIndexMarker1327"/> last step will prompt you for the version of .NET Core to support. At the time of writing, .NET 6 is available<a id="_idIndexMarker1328"/> as <strong class="bold">Long-Term Support</strong> (<strong class="bold">LTS</strong>), and .NET 7 is <a id="_idIndexMarker1329"/>available as <strong class="bold">Standard Term Support</strong>. For the purposes of this book, we will assume that you are using .NET 7.</li>
</ol>
<div><div><img alt="Figure 12.10 – Additional information" src="img/B19214_12_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.10 – Additional information</p>
<ol>
<li value="6">Finalize <a id="_idIndexMarker1330"/>the setup by clicking <strong class="bold">Create</strong>, and wait for Visual Studio to create the project.</li>
</ol>
<p>If you run the app now, you should see something like the following:</p>
<div><div><img alt="Figure 12.11 – The HotdogOrNot applicaton" src="img/B19214_12_11.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.11 – The HotdogOrNot applicaton</p>
<p>Just like that, the <a id="_idIndexMarker1331"/>app is created. Next, let’s start creating the image classifier.</p>
<h3>Classifying images with machine learning</h3>
<p>The <a id="_idIndexMarker1332"/>first thing we will do is add the ONNX ML model to the project, by following these steps:</p>
<ol>
<li>Extract the <code>.zip</code> file that we got from the Custom Vision service.</li>
<li>Find the <code>.onnx</code> file, and rename it <code>hotdog-or-not.onnx</code>.</li>
<li>Add it to the <code>Resources/Raw</code> folder in the project.</li>
</ol>
<p>Once we add the file to the project, we are ready to create the implementation of the image classifier. The code that we will use for image classification will be shared between the .NET MAUI-supported platforms. We can create an interface for the classifier by following these steps:</p>
<ol>
<li>Create a new folder named <code>ImageClassifier</code>.</li>
<li>Create a new class called <code>ClassifierOutput</code> in the <code>ImageClassifier</code> folder.</li>
<li>Modify the <code>ClassifierOutput</code> class to look like the following:<pre class="source-code">
namespace HotdogOrNot.ImageClassifier;
internal sealed class ClassifierOutput
{
    ClassifierOutput() { }
}</pre></li> <li>Create <a id="_idIndexMarker1333"/>a new interface called <code>IClassifier</code> in the <code>ImageClassifier</code> folder.</li>
<li>Add a method called <code>Classify</code> that returns <code>ClassifierOutput</code> and takes <code>byte[]</code> as an argument.</li>
<li>Your interface should look like the following code block:<pre class="source-code">
namespace HotdogOrNot.ImageClassifier;
public interface IClassifier
{
    ClassifierOutput Classify(byte[] <a id="_idTextAnchor961"/><a id="_idTextAnchor962"/>bytes);
}</pre></li> </ol>
<p>Now that we have the interface for the classifier, we can move on to the implementation.</p>
<h4>Using ML.NET for image classiﬁcation</h4>
<p>We are now <a id="_idIndexMarker1334"/>ready to create the implementation of the <code>IClassifier</code> interface. Before we jump right into the implementation, let’s take a look at the high-level steps that will need to happen so that we understand the flow a little better.</p>
<p>Our trained model, <code>hotdog-or-not.onnx</code>, has specific input and output parameters, and we will need to convert the image that we want to classify into the input format before submitting it to the ML.NET framework. Additionally, we need to ensure that the image is in the correct shape before submitting it. The shape of the image is defined by the size, width, height, and color format. If the image does not match the input format, then it needs to be resized and converted before submission, or you will run the risk of the image being classified incorrectly. For image classification models that are generated by the Custom Vision service, such as the <em class="italic">hotdog-or-not</em> model, the inputs and outputs look like the following:</p>
<div><div><img alt="Figure 12.12 – Model inputs and outputs from Netron" src="img/B19214_12_12.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.12 – Model inputs and outputs from Netron</p>
<p>The input to the model is formatted into a multidimensional array named <code>data</code>. There are four dimensions that make up the array:</p>
<ul>
<li><strong class="bold">The image</strong>: The format allows you to submit multiple images at once; however, for this app, we will only submit one image at a time</li>
<li><code>0</code> is blue, <code>1</code> is green, and <code>2</code> is red</li>
<li><strong class="bold">The height</strong>: Each index is a position along the <em class="italic">y</em>, or vertical, axis of an image, in the range between 0 and 223</li>
<li><strong class="bold">The width</strong>: Each index is a position along the <em class="italic">x</em>, or horizontal, axis of an image, in the range between 0 and 223</li>
</ul>
<p>The<a id="_idIndexMarker1335"/> value is the color value for that specific image, color, and <em class="italic">x</em> and <em class="italic">y</em> positions. For example, <code>data[0,2,64,64]</code> would be the value of the green channel in the first image at a position of 64 pixels from the left and 64 pixels from the bottom of the image.</p>
<p>To reduce the number of incorrect classifications, we need to scale all the images for submission to 224 x 224 pixels and order the color channels properly.</p>
<p>We can do that by following these steps:</p>
<ol>
<li>Create a new class called <code>MLNetClassifier</code> in the <code>ImageClassifier</code> folder of the project.</li>
<li>Add the <code>IClassifier</code> interface.</li>
<li>Implement the <code>Classify</code> method from the interface, as shown in the following code block:<pre class="source-code">
namespace HotdogOrNot.ImageClassifier;
internal class MLNetClassifier : Iclassifier
{
    public MLNetClassifier(byte[] model)
    {
        // Initialize Model here
    }
    public ClassifierOutput Classify(byte[] imageBytes)
    {
        // Code will be added here
    }
}</pre></li> </ol>
<p>So far, we <a id="_idIndexMarker1336"/>have not referenced any classes from ML.NET. To use the ML.NET APIs, we will need to add a reference to the NuGet package, by following these steps:</p>
<ol>
<li>In the project, install the <code>Microsoft.ML.OnnxRuntime</code> NuGet package.</li>
<li>Accept any license dialog boxes.</li>
</ol>
<p>This will install the relevant NuGet packages.</p>
<p>Now that we are referencing the ML.NET package, we can compile the ONNX ML model by following these steps:</p>
<ol>
<li>At the top of the <code>MLNetClassifier</code> file, add the <code>using Microsoft.ML.Onnx</code><strong class="source-inline">
</strong><code>Runtime;</code> declaration.</li>
<li>In the <code>MLNetClassifier</code> class, add the following fields:<pre class="source-code">
    readonly InferenceSession session;
    readonly bool isBgr;
    readonly bool isRange255;
    readonly string inputName;
    readonly int inputSize;</pre></li> <li>In the <code>MLNetClassifier</code> constructor, add the following lines of code to initialize the <code>OnnxRuntime</code> session, replacing the <code>// Initialize Model </code><code>here</code> comment:<pre class="source-code">
        Session = new InferenceSession(model);
        isBgr = session.ModelMetadata.CustomMetadataMap["Image.BitmapPixelFormat"] == "Bgr8";
        isRange255 = session.ModelMetadata.CustomMetadataMap["Image.NominalPixelRange"] == "NominalRange_0_255";
        inputName = session.InputMetadata.Keys.First();
        inputSize = session.InputMetadata[inputName].Dimensions[2];</pre><p class="list-inset">Let’s <a id="_idIndexMarker1337"/>discuss the preceding code before moving on. The constructor for the <code>MLNetClassifier</code> class accepts <code>byte[]</code> as a parameter. This represents the ML model file. <code>byte[]</code> is then passed into a new instance of <code>InferenceSession</code>, which is the main entry point into the ML.NET API. Once the model is loaded into the session, we can then inspect the model for certain properties, such as the image format (<code>isBGR</code>), the color value range (<code>isRange255</code>), the input name, and the input size. We cache these values in the class fields for use during classification. Your <code>MLNetClassifier</code> class should now look like the following:</p><pre class="source-code">using Microsoft.ML.OnnxRuntime;
namespace HotdogOrNot.ImageClassifier;
internal class MLNetClassifier : Iclassifier
{
    readonly InferenceSession session;
    readonly bool isBgr;
    readonly bool isRange255;
    readonly string inputName;
    readonly int inputSize;
    public MLNetClassifier(byte[] model)
    {
        session = new InferenceSession(model);
        isBgr = session.ModelMetadata.CustomMetadataMap["Image.BitmapPixelFormat"] == "Bgr8";
        isRange255 = session.ModelMetadata.CustomMetadataMap["Image.NominalPixelRange"] == "NominalRange_0_255";
        inputName = session.InputMetadata.Keys.First();
        inputSize = session.InputMetadata[inputName].Dimensions[2];
    }
    public ClassifierOutput Classify(byte[] imageBytes)
    {
        // Code will be added <a id="_idTextAnchor963"/>here
    }
}</pre></li> </ol>
<p>We can now move on to implementing the <code>Classify</code> method of the <code>MLNetClassifier</code> class.</p>
<p>The <a id="_idIndexMarker1338"/>first step in running a classification is to get the input into the correct format. For image classification, that means resizing the image to the right dimensions and organizing the color values into the expected format. The image data is then loaded into <code>Tensor</code>, which is how we pass data into an ML.NET model. The following steps will create a method named <code>LoadInputTensor</code> to do just that:</p>
<ol>
<li>Add a new method named <code>LoadInputTensor</code> after the <code>Classify</code> method in the <code>MLNetClassififier</code> class. This method will accept four parameters, <code>byte[]</code>, <code>int</code>, and two Booleans, and return a tuple of <code>Tensor&lt;float&gt;</code> and <code>byte[]</code>. Your method should look like the following:<pre class="source-code">
static (Tensor&lt;float&gt;, byte[] resizedImage) LoadInputTensor(byte[] imageBytes, int imageSize, bool isBgr, bool isRange255)
    {
    }</pre></li> <li>Inside <code>LoadInputTensor</code>, we will create the <code>return</code> objects and add the following highlighted lines of code:<pre class="source-code">
    {
<strong class="bold">        var input = new DenseTensor&lt;float&gt;(new[] { 1, 3, imageSize, imageSize });</strong>
<strong class="bold">        byte[] pixelBytes;</strong>
        // Add code here
<strong class="bold">        return (input, pixelBytes);</strong>
    }</pre><p class="list-inset">The next step is to resize the image; we will use<a id="_idIndexMarker1339"/> the <strong class="bold">ImageSharp</strong> NuGet library to make this very easy.</p></li> <li>Add the ImageSharp NuGet package to the project.</li>
<li>Add <a id="_idIndexMarker1340"/>the following lines of code to resize the image, replacing the <code>\\ Add code </code><code>here</code> comment:<pre class="source-code">
using (var image = Image.Load&lt;Rgb24&gt;(imageBytes))
    {
        image.Mutate(x =&gt; x.Resize(imageSize, imageSize));
        pixelBytes = new byte[image.Width * image.Height * Unsafe.SizeOf&lt;Rgba32&gt;()];
        image.ProcessPixelRows(source =&gt;
        {
            // Add Code here
        });
    }</pre><p class="list-inset">This code uses the ImageSharp library to load the image from <code>byte[]</code>. The image is then resized to the size required by the model. We use the <code>imageSize</code> field, whose value captures the model requirement from the constructor. Finally, we set up a call to the <code>ProcessPixelRows</code> method that will allow us to manipulate the individual pixels in the image.</p></li> <li>Due to conflicts in naming between .NET MAUI and ImageSharp, we must add a declaration that tells the compiler which class we really want to use at the top of the file:<pre class="source-code">
using Image = SixLabors.ImageSharp.Image;</pre></li> <li>The next section of code will also need the following highlighted declarations:<pre class="source-code">
using Microsoft.ML.OnnxRuntime;
<strong class="bold">using SixLabors.ImageSharp.Formats.Png</strong>
<strong class="bold">using Microsoft.ML.OnnxRuntime.Tensors;</strong>
using Image = SixLabors.ImageSharp.Image;</pre></li> <li>To get<a id="_idIndexMarker1341"/> the input image into the correct color format required by the model, we use the <code>ProcessPixelRows</code> method from the <code>ImageSharp</code> library. This method provides a writable buffer for us to manipulate. Use the following highlighted code, in place of the <code>// Add Code here</code> comment, to iterate over the resized image data, putting the color values into the right order and clamping the values between 0 and 255, if required:<pre class="source-code">
image.ProcessPixelRows(source =&gt;
{
<strong class="bold">    for (int y = 0; y &lt; image.Height; y++)</strong>
<strong class="bold">    {</strong>
<strong class="bold">        Span&lt;Rgb24&gt; pixelSpan = source.GetRowSpan(y);</strong>
<strong class="bold">        for (int x = 0; x &lt; image.Width; x++)</strong>
<strong class="bold">        {</strong>
<strong class="bold">            if (isBgr)</strong>
<strong class="bold">            {</strong>
<strong class="bold">                input[0, 0, y, x] = pixelSpan[x].B;</strong>
<strong class="bold">                input[0, 1, y, x] = pixelSpan[x].G;</strong>
<strong class="bold">                input[0, 2, y, x] = pixelSpan[x].R;</strong>
<strong class="bold">            }</strong>
<strong class="bold">            else</strong>
<strong class="bold">            {</strong>
<strong class="bold">                input[0, 0, y, x] = pixelSpan[x].R;</strong>
<strong class="bold">                input[0, 1, y, x] = pixelSpan[x].G;</strong>
<strong class="bold">                input[0, 2, y, x] = pixelSpan[x].B;</strong>
<strong class="bold">            }</strong>
<strong class="bold">            if (!isRange255)</strong>
<strong class="bold">            {</strong>
<strong class="bold">                input[0, 0, y, x] = input[0, 0, y, x] / 255;</strong>
<strong class="bold">                input[0, 1, y, x] = input[0, 1, y, x] / 255;</strong>
<strong class="bold">                input[0, 2, y, x] = input[0, 2, y, x] / 255;</strong>
<strong class="bold">             }</strong>
<strong class="bold">         }</strong>
<strong class="bold">     }</strong>
});</pre><p class="list-inset">What <a id="_idIndexMarker1342"/>this code does is simple – using the provided source variable, it iterates over each row in the image, and each pixel in the row. If the model expects the colors to be in the blue, green, and red order, <code>isBGR</code> is <code>true</code>, and then the extracted color values are placed in the input tensor in that order; otherwise, they are added to the input tensor in the red, green, and blue order. The tricky part here is accessing the correct element for each pixel. The tensor is organized into four dimensions, as explained previously. The first element will always be zero for this model, since we are only processing one image at a time. The second dimension is the color channel, so you will see that change for the red, green, and blue color values.</p><p class="list-inset">Finally, if the model expects color values to be in the range of 0 to 255, <code>isRange255</code>, then each color channel is clamped to that range.</p></li> <li>The last <a id="_idIndexMarker1343"/>thing that we will do is copy the contents of the resized image to the <code>pixelBytes</code> array so that we can display the image to the user. Add the following highlighted code to do this; note that the previous code has been omitted for brevity:<pre class="source-code">
            });
<strong class="bold">            var outStream = new MemoryStream();</strong>
<strong class="bold">            image.Save(outStream, new PngEncoder());</strong>
<strong class="bold">            pixelBytes = outStream.ToArray();</strong>
        }
        return (input, pixelBytes);</pre></li> </ol>
<p>Now that we have written the code to process the image and populate the input tensor, we can complete the <code>Classify</code> method by following these steps:</p>
<ol>
<li>Replace the <code>// Code will be added here</code> comment with a call to the <code>LoadInputTensor</code> method:<pre class="source-code">
    public ClassifierOutput Classify(byte[] imageBytes)
    {
<strong class="bold">        (Tensor&lt;float&gt; tensor, byte[] resizedImage) = LoadInputTensor(imageBytes, inputSize, isBgr, isRange255);</strong>
    }</pre></li> <li>Next, we can run the session, passing in the newly created input tensor and capturing the result:<pre class="source-code">
    public ClassifierOutput Classify(byte[] imageBytes)
    {
        (Tensor&lt;float&gt; tensor, byte[] resizedImage) = LoadInputTensor(imageBytes, inputSize, isBgr, isRange255);
<strong class="bold">        var resultsCollection = session.Run(new List&lt;NamedOnnxValue&gt;</strong>
<strong class="bold">        {</strong>
<strong class="bold">                    NamedOnnxValue.CreateFromTensor&lt;float&gt;(inputName, tensor)</strong>
<strong class="bold">         });</strong>
    }</pre></li> <li>We <a id="_idIndexMarker1344"/>grab the label from the output result, which will be used to determine whether this image contains a hotdog or not:<pre class="source-code">
    public ClassifierOutput Classify(byte[] imageBytes)
    {
        (Tensor&lt;float&gt; tensor, byte[] resizedImage) = LoadInputTensor(imageBytes, inputSize, isBgr, isRange255);
        var resultsCollection = session.Run(new List&lt;NamedOnnxValue&gt;
                {
                    NamedOnnxValue.CreateFromTensor&lt;float&gt;(inputName, tensor)
                });
<strong class="bold">        var topLabel = resultsCollection</strong>
<strong class="bold">            ?.FirstOrDefault(i =&gt; i.Name == "classLabel")</strong>
<strong class="bold">            ?.AsTensor&lt;string&gt;()</strong>
<strong class="bold">            ?.First();</strong>
    }</pre></li> <li>Then, we <a id="_idIndexMarker1345"/>can get the confidence level of the result, which tells us how sure the model is of the classification. This will be used when we<a id="_idTextAnchor964"/> display the result:<pre class="source-code">
    public ClassifierOutput Classify(byte[] imageBytes)
    {
        (Tensor&lt;float&gt; tensor, byte[] resizedImage) = LoadInputTensor(imageBytes, inputSize, isBgr, isRange255);
        var resultsCollection = session.Run(new List&lt;NamedOnnxValue&gt;
                {
                    NamedOnnxValue.CreateFromTensor&lt;float&gt;(inputName, tensor)
                });
        var topLabel = resultsCollection
            ?.FirstOrDefault(i =&gt; i.Name == "classLabel")
            ?.AsTensor&lt;string&gt;()
            ?.First();
<strong class="bold">        var labelScores = resultsCollection</strong>
<strong class="bold">            ?.FirstOrDefault(i =&gt; i.Name == "loss")</strong>
<strong class="bold">            ?.AsEnumerable&lt;NamedOnnxValue&gt;()</strong>
<strong class="bold">            ?.First()</strong>
<strong class="bold">            ?.AsDictionary&lt;string, float&gt;();</strong>
    }</pre></li> <li>Finally, we<a id="_idIndexMarker1346"/> can return the result of the classification using the <code>ClassifierOutput</code> class:<pre class="source-code">
    public ClassifierOutput Classify(byte[] imageBytes)
    {
        (Tensor&lt;float&gt; tensor, byte[] resizedImage) = LoadInputTensor(imageBytes, inputSize, isBgr, isRange255);
        var resultsCollection = session.Run(new List&lt;NamedOnnxValue&gt;
                {
                    NamedOnnxValue.CreateFromTensor&lt;float&gt;(inputName, tensor)
                });
        var topLabel = resultsCollection
            ?.FirstOrDefault(i =&gt; i.Name == "classLabel")
            ?.AsTensor&lt;string&gt;()
            ?.First();
        var labelScores = resultsCollection
            ?.FirstOrDefault(i =&gt; i.Name == "loss")
            ?.AsEnumerable&lt;NamedOnnxValue&gt;()
            ?.First()
            ?.AsDictionary&lt;string, float&gt;();
<strong class="bold">        return ClassifierOutput.Create(topLabel, labelScores, resizedImage);</strong>
    }</pre></li> <li>The last <a id="_idIndexMarker1347"/>step is to finish the <code>MLNetClassifier</code> implementation by implementing the <code>ClassifierOutput</code> class. Update your <code>ClassifierOutput</code> class by adding the highlighted code:<pre class="source-code">
internal sealed class ClassifierOutput
{
<strong class="bold">    public string TopResultLabel { get; private set; }</strong>
<strong class="bold">    public float TopResultScore { get; private set; }</strong>
<strong class="bold">    public IDictionary&lt;string, float&gt; LabelScores { get; private set; }</strong>
<strong class="bold">    public byte[] Image { get; private set; }</strong>
    ClassifierOutput() { }
<strong class="bold">    public static ClassifierOutput Create(string topLabel, IDictionary&lt;string, float&gt; labelScores, byte[] image)</strong>
<strong class="bold">    {</strong>
<strong class="bold">        var topLabelValue = topLabel ?? throw new ArgumentException(nameof(topLabel));</strong>
<strong class="bold">        var labelScoresValue = labelScores ?? throw new ArgumentException(nameof(labelScores));</strong>
<strong class="bold">        return new ClassifierOutput</strong>
<strong class="bold">        {</strong>
<strong class="bold">            TopResultLabel = topLabelValue,</strong>
<strong class="bold">            TopResultScore = labelScoresValue.First(i =&gt; i.Key == topLabelValue).Value,</strong>
<strong class="bold">            LabelScores = labelScoresValue,</strong>
<strong class="bold">            Image = image,</strong>
<strong class="bold">        };</strong>
<strong class="bold">    }</strong>
}</pre></li> </ol>
<p>The <code>ClassifierOutput</code> class is used to encapsulate the four values that will be used in the UI and expose them as public properties. The <code>Create</code> static method is used to create an<a id="_idIndexMarker1348"/> instance of the class. The <code>Create</code> method validates the arguments provided and sets the public properties appropriately for use by the UI.</p>
<p>We have now written the code to recognize hot dogs in an image.</p>
<p>Now, we can build the user interface for the application and call <code>MLNetClasssifier</code> to classify an image.</p>
<h3>Requesting app permissions</h3>
<p>Before we<a id="_idIndexMarker1349"/> dive right into building the rest of the app functionality, we need to address permissions. This app will have two buttons that the user will use, one to take a photo and another to select a photo from the device. This is similar to the functionality that we saw in <em class="italic">Chapter 6</em>, <em class="italic">Building a Photo Gallery App Using CollectionView and CarouselView</em>, where we needed to request permission from the user before accessing the camera or device storage. However, we will implement the permissions differently than we did in that chapter. Since gaining access to the camera and accessing photos on the user’s device requires separate permissions, we will request them from each button handler.</p>
<p>Follow these steps to add a class to help us with the permission checks:</p>
<ol>
<li>Create a new class named <code>AppPermissions</code> in the project.</li>
<li>Modify the class definition to add a <code>partial</code> modifier, and remove the default constructor:<pre class="source-code">
namespace HotdogOrNot;
internal partial class AppPermissions
{
}</pre></li> <li>Add the following method to the <code>AppPermissions</code> class:<pre class="source-code">
    public static async Task&lt;PermissionStatus&gt; CheckRequiredPermission&lt;TPermission&gt;() where TPermission : Permissions.BasePermission, new() =&gt; await Permissions.CheckStatusAsync&lt;TPermission&gt;();</pre><p class="list-inset">The <code>CheckRequiredPermission</code> method is used to ensure that our app has the<a id="_idIndexMarker1350"/> right permissions before we attempt any operations that might fail if we don’t. Its implementation is to call the .NET MAUI <code>CheckSyncStatus</code> with the provided permission type in <code>TPermission</code>. It returns <code>PermissionStatus</code>, which is <code>enum</code>. We are mostly interested in the <code>Denied</code> and <code>Granted</code> values.</p></li> <li>Add the <code>CheckAndRequestRequiredPermission</code> method to the <code>AppPermissions</code> class:<pre class="source-code">
    public static async Task&lt;PermissionStatus&gt; CheckAndRequestRequiredPermission() &lt;TPermission&gt;() where TPermission : Permissions.BasePermission, new()
    {
        PermissionStatus status = await Permissions.CheckStatusAsync&lt; TPermission &gt;();
        if (status == PermissionStatus.Granted)
            return status;
        if (status == PermissionStatus.Denied &amp;&amp; DeviceInfo.Platform == DevicePlatform.iOS)
        {
            // Prompt the user to turn on in settings
            // On iOS once a permission has been denied it may not be requested again from the application
            await App.Current.MainPage.DisplayAlert("Required App Permissions", "Please enable all permissions in Settings for this App, it is useless without them.", "Ok");
        }
        if (Permissions.ShouldShowRationale&lt; TPermission &gt;())
        {
            // Prompt the user with additional information as to why the permission is needed
            await App.Current.MainPage.DisplayAlert("Required App Permissions", "This app uses photos, without these permissions it is useless.", "Ok");
        }
        status = await MainThread.InvokeOnMainThreadAsync(Permissions.RequestAsync&lt;TPermission&gt;);
        return status;
    }
}</pre><p class="list-inset">The <code>CheckAndRequestRequiredPermission</code> method handles the intricacies of <a id="_idIndexMarker1351"/>requesting access from the user. The first step is to simply check and see whether the permission has already been granted and, if it has, return the status. Next, if we are on iOS and the permission has been denied, it cannot be requested again, so you must instruct the user on how to grant permission to the app by using the settings panel. Android includes in the request behavior the ability to nag the user if they have denied access. This behavior is exposed through .NET MAUI with the <code>ShouldShowRationale</code> method. It will return <code>false</code> for any platform that does not support this behavior, and on Android, it will return <code>true</code> the first time after the user denies access and <code>false</code> if the user denies it a <a id="_idIndexMarker1352"/>second time. Finally, we request access to the permission from the user. Again, .NET MAUI hides all the platform implementation details from us, making checking and requesting access to certain resources very straightforward.</p></li> </ol>
<p class="callout-heading">Look familiar?</p>
<p class="callout">If the preceding code looks familiar, then you are right. It is based on the implementation that is described in the .NET MAUI documentation. You can find it at <a href="https://learn.microsoft.com/en-us/dotnet/maui/platform-integration/appmodel/permissions">https://learn.microsoft.com/en-us/dotnet/maui/platform-integration/appmodel/permissions</a>.</p>
<p>Now that we have the shared <code>AppPermissions</code> in place, we can start with<a id="_idTextAnchor965"/> the platform configuration. Before we can use the media picker, however, we need to do some configuration for each platform. We will start with Android.</p>
<p>In Android API version 33, three new permissions were added to enable read access to media files – <code>ReadMediaImages</code>, <code>ReadMediaVideos</code>, and <code>ReadMediaAudio</code>. Prior to API version 33, all that was required was the <code>ReadExternalStorage</code> permission. To access the camera, we will need both <code>Camera</code> and <code>WriteExternalStorage</code> permissions. To properly request the correct permission for the API version of the device, open <code>MauiApplication.cs</code> in the <code>Platform/Android</code> folder and modify it to look like the following:</p>
<pre class="source-code">
using Android.App;
using Android.Runtime;
<strong class="bold">// Needed for Picking photo/video</strong>
<strong class="bold">[assembly: UsesPermission(Android.Manifest.Permission.ReadExternalStorage, MaxSdkVersion = 32)]</strong>
<strong class="bold">[assembly: UsesPermission(Android.Manifest.Permission.ReadMediaImages)]</strong>
<strong class="bold">// Needed for Taking photo/video</strong>
<strong class="bold">[assembly: UsesPermission(Android.Manifest.Permission.Camera)]</strong>
<code>IMAGE_CAPTURE</code> intent as follows in the <code>AndroidManifest.xml</code> file:</p>
<pre class="source-code">
  &lt;queries&gt;
    &lt;intent&gt;
      &lt;action android:name="android.media.action.IMAGE_CAPTURE" /&gt;
    &lt;/intent&gt;
  &lt;/queries&gt;</pre> <p>For iOS and Mac Catalyst, the only thing we need to do is add the following four usage descriptions to the <code>info.plist</code> file in the <code>platform/ios</code> and <code>platform/maccatalyst</code> folders:</p>
<pre class="source-code">
&lt;key&gt;NSCameraUsageDescription&lt;/key&gt;
&lt;string&gt;This app needs access to the camera to take photos.&lt;/string&gt;
&lt;key&gt;NSPhotoLibraryUsageDescription&lt;/key&gt;
&lt;string&gt;This app needs access to photos.&lt;/string&gt;
&lt;key&gt;NSMicrophoneUsageDescription&lt;/key&gt;
&lt;string&gt;This app needs access to microphone.&lt;/string&gt;
&lt;key&gt;NSPhotoLibraryAddUsageDescription&lt;/key&gt;
&lt;string&gt;This app needs access to the photo gallery.&lt;/string&gt;</pre> <p>For <a id="_idIndexMarker1354"/>Windows, we need to add the following highlighted code to the <code>Capabilities</code> section of the <code>package.appxmanifest</code> file in the <code>platforms/windows</code> folder:</p>
<pre class="source-code">
&lt;Capabilities&gt;
  &lt;rescap:Capability Name="runFullTrust" /&gt;
  <strong class="bold">&lt;DeviceCapability Name="webcam"/&gt;</strong>
&lt;/Capabilities&gt;</pre> <p>Now that we have declared the permissions we need for each platform, we can implement the remaining functionality to take a photo or pick an existing image.</p>
<h3>Building the first view</h3>
<p>The<a id="_idIndexMarker1355"/> first view in this app will be a simple view <a id="_idIndexMarker1356"/>with two buttons. One button will be to start the camera so that users can take a photo of something to determine whether it is a hot dog. The other button will be to pick a photo from the photo library of the device. We will continue to use the MVVM pattern in this chapter, so we will split the view into two classes, <code>MainView</code> for the UI visible to the user and <code>MainViewModel</code> for the actual implementation.</p>
<h3>Building the ViewModel class</h3>
<p>We will <a id="_idIndexMarker1357"/>start by creating the <code>MainViewModel</code> class, which will handle what will happen when a user taps one of the buttons. Let’s set this up by going through the following steps:</p>
<ol>
<li>Create a new folder called <code>ViewModels</code>.</li>
<li>Add a NuGet reference to <code>CommunityToolkit.Mvvm</code>; we use <code>CommunityToolkit.Mvvm</code> to implement the <code>INotifyPropertyChanged</code> interface and commands, as we did in other chapters.</li>
<li>Create <a id="_idIndexMarker1358"/>a new partial class called <code>MainViewModel</code> in the <code>ViewModels</code> folder, using <code>ObservableObject</code> from the <code>CommunityToolkit.Mvvm.ComponentModel</code> namespace as a base class.</li>
<li>Create a private field of the <code>IClassifier</code> type and call it <code>classifier</code>, as shown in the following code block:<pre class="source-code">
using CommunityToolkit.Mvvm.ComponentModel;
using HotdogOrNot.ImageClassifier;
namespace HotdogOrNot.ViewModels;
public partial class MainViewModel : ObservableObject
{
    <strong class="bold">private IClassifier classifier;</strong>
    public MainViewModel()
    {
    }
}</pre></li> </ol>
<p>Initializing the ONNX model requires the use of asynchronous methods, so we need to handle them carefully, since we will be calling them from the constructor and the button handlers. The<a id="_idIndexMarker1359"/> following steps will create the model initializer:</p>
<ol>
<li>Create an <code>InitTask</code> property that is of the <code>Task</code> type.</li>
<li>Use a property initializer to set it to a new <code>Task</code>, using <code>Task.Run</code>.</li>
<li>Initialize the model from the raw resources of the .NET MAUI app. The method should look like the following code:<pre class="source-code">
    Task InitTask() =&gt; Task.Run(async () =&gt;
    {
        using var modelStream = await FileSystem.OpenAppPackageFileAsync("hotdog-or-not.onnx");
        using var modelMemoryStream = new MemoryStream();
        modelStream.CopyTo(modelMemoryStream);
        var model = modelMemoryStream.ToArray();
        _classifier = new MLNetClassifier(model);
    });</pre><p class="list-inset">The <code>InitTask</code> property holds a reference to <code>Task</code> that does the following:</p><ul><li>Loads the <code>hotdog-or-not.onnx</code> file into <code>Stream</code></li><li>Copies the bytes from the original stream to an array of bytes so that the original stream can be closed and any native resources, such as file handles, can be released.</li><li>Creates and returns a new instance of the <code>MLNetClassifier</code> class using the loaded model.</li></ul></li> <li>To<a id="_idIndexMarker1360"/> ensure that <code>InitTask</code> will only run successfully once, add the following highlighted code:<pre class="source-code">
public partial class MainViewModel : ObservableObject
{
    IClassifier _classifier;
<strong class="bold">    Task initTask;</strong>
    public MainViewModel()
    {
<strong class="bold">        _ = InitAsync();</strong>
    }
<strong class="bold">    public Task InitAsync()</strong>
<strong class="bold">    {</strong>
<strong class="bold">        if (initTask == null || initTask.IsFaulted)</strong>
<strong class="bold">            initTask = InitTask();</strong>
<strong class="bold">        return initTask;</strong>
<strong class="bold">    }</strong>
  // Code omitted for brevity
}</pre><p class="list-inset">In <code>InitAsync</code>, the initialization task is captured by a field only if the field is <code>null</code> or its value has faulted. This ensures that we only run the initialization successfully once. The value of the field is then returned to the caller, which, in this case, is the constructor. Unwinding this, the constructor calls <code>InitAsync</code> and throws away the return value. <code>InitAsync</code>, meanwhile, captures the value returned by the <code>InitTask</code> property, which is <code>Task</code> that has already been queued for execution. Since <code>InitAsync</code> and <code>InitTask</code> and their closure are all asynchronous, they complete sometime after the constructor completes.</p></li> </ol>
<p>Now that <a id="_idIndexMarker1361"/>we have initialized the <code>hotdog-or-not</code> ONNX model, we can now implement the two buttons, one that takes a photo and another that allows the user to pick a photo from their device storage. Let’s start by implementing a couple of helper methods to use in both use cases.</p>
<p>The first helper method is used to convert <code>FileResult</code> to <code>byte[]</code>. To implement <code>ConvertPhotoToBytes</code>, follow these steps:</p>
<ol>
<li>Open the <code>MainViewModel.cs</code> file.</li>
<li>Add a new method named <code>ConvertPhotoToBytes</code>, which takes <code>FileResult</code> as a parameter and returns <code>byte []</code>. Since the method is <code>async</code>, you’ll need to return <code>Task</code> and use the <code>async</code> modifier.</li>
<li>In the method, check whether <code>FileResult</code> is <code>null</code> and that it returns an empty array.</li>
<li>Next, open a stream from <code>FileResult</code> using the <code>OpenStreamAsync</code> method.</li>
<li>Create a new variable of the <code>MemoryStream</code> type and initialize it using the default constructor.</li>
<li>Use the <code>Copy</code> method to copy <code>stream</code> to <code>MemoryStream</code>.</li>
<li>Finally, return <code>MemoryStream</code> as <code>byte[]</code>; your method should look like the following:<pre class="source-code">
    private async Task&lt;byte[]&gt; ConvertPhotoToBytes(FileResult photo)
    {
        if (photo == null) return Array.Empty&lt;byte&gt;();
        using var stream = await photo.OpenReadAsync();
        using MemoryStream memoryStream = new();
        stream.CopyTo(memoryStream);
        return memoryStream.ToArray();
    }</pre></li> </ol>
<p>The other <a id="_idIndexMarker1362"/>helper method we will need is to use our classification model to get the results of a photo and return the results. We will need a new type to return the results. Follow these steps to implement the new class:</p>
<ol>
<li>Create a new folder named <code>Models</code> in the project.</li>
<li>In the <code>Models</code> folder, create a new class, <code>Result</code>, in a file named <code>Result.cs</code>.</li>
<li>Add a public property, <code>IsHotdog</code>, as <code>bool</code>.</li>
<li>Add a public property, <code>Confidence</code>, as <code>float</code>.</li>
<li>Add a public property, <code>PhotoBytes</code>, as <code>byte[]</code>; the class should now look like the following:<pre class="source-code">
namespace HotdogOrNot.Models;
public class Result
{
    public bool IsHotdog { get; set; }
    public float Confidence { get; set; }
    public byte[] PhotoBytes { get; set; }
}</pre><p class="list-inset">The <code>IsHotdog</code> property is used to capture whether the label returned from the model is “hotdog.” <code>Confidence</code> is a score of how sure the model is that this is a hotdog or not. Finally, since we transform the image prior to processing, we store the transformed image in the <code>PhotoBytes</code> property.</p></li> </ol>
<p>Now, we <a id="_idIndexMarker1363"/>can implement the method that will run and process the classification result, by following these steps:</p>
<ol>
<li>Open the <code>MainViewModel.cs</code> file.</li>
<li>In the <code>MainViewModel</code> class, add a new field, <code>isClassifying</code>, with a <code>bool</code> type.</li>
<li>Add the <code>ObservableAttribute</code> attribute to the field; it should look like the following:<pre class="source-code">
    [ObservableProperty]
    private bool isClassifying;</pre></li> <li>Add a new method to the <code>MainViewModel</code> class, named <code>RunClassificationAsync</code>. The method will accept a <code>byte[]</code> parameter and return <code>Result</code>, wrapped in <code>Task</code>, since it is <code>async</code>:<pre class="source-code">
async Task&lt;Result&gt; RunClassificationAsync(byte[] imageToClassify)
    {
    }</pre></li> <li>In the method, the first thing we do is set the <code>IsClassifying</code> property to <code>true</code>; this will be used to disable the buttons later in the chapter.</li>
<li>Add a <code>try..catch..finally</code> statement.</li>
<li>Inside the <code>try</code> statement, ensure the model is initialized by calling <code>InitAsync</code>.</li>
<li>Then, call <code>Classify</code> on the <code>classifier</code> field passing <code>byte[]</code>, representing the image as a parameter and storing the result.</li>
<li>The <a id="_idIndexMarker1364"/>last statement in the <code>try</code> statement block is to return a new <code>Result</code>, setting <code>IsHotdog</code> to <code>true</code> only if the classification result’s <code>TopResultLabel</code> property is “hotdog,” <code>Confidence</code> is set to the classification result’s <code>TopResultScore</code> property, and <code>PhotoBytes</code> is set to the classification result’s <code>Image</code> property. The <code>try</code> portion should look like the following:<pre class="source-code">
        try
        {
            await InitAsync().ConfigureAwait(false);
            var result = _classifier.Classify(imageToClassify);
            return new Result()
            {
                IsHotdog = result.TopResultLabel == "hotdog",
                Confidence = result.TopResultScore,
                PhotoBytes = result.Image
            };
        }
        catch</pre></li> <li>Now, in the <code>catch</code> statement block, return a new <code>Result</code>, setting the <code>IsHotdog</code> property to <code>false</code>, <code>Confidence</code> to <code>0.0f</code>, and the <code>PhotoBytes</code> property to the bytes passed into the method. The <code>catch</code> block should look like the following:<pre class="source-code">
        catch
        {
            return new Result
            {
                IsHotdog = false,
                Confidence = 0.0f,
                PhotoBytes = imageToClassify
            };
        }
        finally</pre></li> <li>Lastly, for<a id="_idIndexMarker1365"/> the <code>finally</code> block, we want to set the <code>IsClassiying</code> property back to <code>false</code>; however, we will need to do this on the main UI thread using the <code>MainThread.BeginInvokeOnMainThread</code> method from .NET MAUI, as shown in the following code:<pre class="source-code">
        finally
        {
            MainThread.BeginInvokeOnMainThread(() =&gt; IsClassifying = false);
        }</pre></li> </ol>
<p>Now that we have written the helper methods, we can create two methods, one to handle capturing an image from the camera and another to pick a photo from user storage. We wi<a id="_idTextAnchor966"/>ll start with the camera capture method.</p>
<p>Let’s set this up by following these steps:</p>
<ol>
<li>Open the <code>MainViewModel.cs</code> file.</li>
<li>Create a public async void method called <code>TakePhoto</code>.</li>
<li>Add the <code>RelayCommand</code> attribute to make the method bindable.</li>
<li>Add an <code>if</code> statement to check whether the <code>MediaPicker.Default.IsCaptureSupported</code> parameter is <code>true</code>.</li>
<li>In the <code>true</code> statement block of <code>if</code>, get the status of the <code>Camera</code> permission using the <code>CheckAndRequestPermission</code> method.</li>
<li>If the status is <code>Granted</code>, then use <code>CheckAndRequestMethod</code> again to check the <code>WriteExternalStorage</code> permission.</li>
<li>If the status is <code>Granted</code>, use <code>MediaPicker</code> to capture a photo using the <code>Capture</code><strong class="source-inline">
</strong><code>PhotoAsync</code> method.</li>
<li>Call a method named <code>ConvertPhotoToBytes</code>, passing in the file returned from <code>MediaPicker</code>.</li>
<li>Pass the photo byt<a id="_idTextAnchor967"/>es to the <code>RunClassificationAsync</code> method.</li>
<li>Finally, we<a id="_idIndexMarker1366"/> will dynamically navigate to the <code>Result</code> view, which we will create in the next section, passing the result from <code>RunClassificationAsync</code> as a parameter. We do this by using <code>Shell.Current.GotoAsync</code> and ensuring that the app uses the main thread to do so, as shown in the following code block:<pre class="source-code">
    [RelayCommand()]
    public async void TakePhoto()
    {
        if (MediaPicker.Default.IsCaptureSupported)
        {
            var status = await AppPermissions.CheckAndRequestRequiredPermissionAsync&lt;Permissions.Camera&gt;();
            if (status == PermissionStatus.Granted) {
                status = await AppPermissions.CheckAndRequestRequiredPermissionAsync&lt;Permissions.StorageWrite&gt;();
            }
            if (status == PermissionStatus.Granted)
            {
                FileResult photo = await MediaPicker.Default.CapturePhotoAsync(new MediaPickerOptions() { Title = "Hotdog or Not?" });
                var imageToClassify = await ConvertPhotoToBytes(photo);
                var result = await RunClassificationAsync(imageToClassify);
                await MainThread.InvokeOnMainThreadAsync(async () =&gt; await
                    Shell.Current.GoToAsync("Result", new Dictionary&lt;string, object&gt;() { { "result", result } })
                );
            }
        }
    }</pre></li> </ol>
<p><code>Shell.Current.GotoAsync</code> takes two parameters – the first is the route that <code>Shell</code> is to <a id="_idIndexMarker1367"/>navigate to, and the second is a dictionary of key-value pairs to send to the destination view. Later in this chapter, we will see how to configure a route to a view without using XAML and, when we create the <code>Result</code> view, how to access the parameters passed to it.</p>
<p>We will now create the <code>PickPhoto</code> method to allow a user to use an image from their device. Use the following steps to create the method:</p>
<ol>
<li>Create a public async void method called <code>PickPhoto</code>.</li>
<li>Add the <code>RelayCommand</code> attribute to make the method bindable.</li>
<li>Grant the status of the <code>Photos</code> permission using the <code>CheckAndRequestPermission</code> method.</li>
<li>If the status is <code>Granted</code>, use <code>MediaPicker</code> to capture a photo using the <code>Pick</code><strong class="source-inline">
</strong><code>PhotoAsync</code> method.</li>
<li>Call a method named <code>ConvertPhotoToBytes</code>, passing in the file returned from <code>MediaPicker</code>.</li>
<li>Pass the photo bytes to the <code>RunClassificationAsync</code> method.</li>
<li>Finally, we will dynamically navigate to the <code>Result</code> view, which we will create in the next section, passing the result from <code>RunClassificationAsync</code> as a parameter. We will do this by using <code>Shell.Current.GotoAsync</code> and ensuring that the app uses the main thread to do so, as shown in the following code block:<pre class="source-code">
    [RelayCommand()]
    public async void PickPhoto()
    {
        var status = await AppPermissions.CheckAndRequestRequiredPermissionAsync&lt;Permissions.Photos&gt;();
        if (status == PermissionStatus.Granted)
        {
            FileResult photo = await MediaPicker.Default.PickPhotoAsync();
            var imageToClassify = await ConvertPhotoToBytes(photo);
            var result = await RunClassificationAsync(imageToClassify);
            await MainThread.InvokeOnMainThreadAsync(async () =&gt; await
                Shell.Current.GoToAsync("Result", new Dictionary&lt;string, object&gt;() { { "result", <a id="_idTextAnchor968"/>result } })
            );
        }
    }</pre></li> </ol>
<p>When a <a id="_idIndexMarker1368"/>user clicks on a button, the classification could take a noticeable amount of time. To prevent the user from clicking the button again because they think it’s not working, we will disable the buttons until the operation completes. The <code>IsClassifying</code> property is already set; we just need to use that value to restrict <code>RelayCommands</code>, by following these steps:</p>
<ol>
<li>Add a new method that returns a Boolean named <code>CanExecuteClassification</code>, and return the inverse of the <code>IsClassifying</code> property, as shown in the following code:<pre class="source-code">
private bool CanExecuteClassification() =&gt; !IsClassifying;</pre></li> <li>Update the <code>RelayCommand</code> attribute for the <code>TakePhoto</code> method, as highlighted here:<pre class="source-code">
[RelayCommand(<strong class="bold">CanExecute = nameof(CanExecuteClassification)</strong>)]
public async void TakePhoto()</pre></li> <li>Update the <code>RelayCommand</code> attribute for the <code>PickPhoto</code> method, as highlighted here:<pre class="source-code">
[RelayCommand(<strong class="bold">CanExecute = nameof(CanExecuteClassification)</strong>)]
public async void PickPhoto()</pre></li> </ol>
<p>Now that ViewModel for the main<a id="_idIndexMarker1369"/> page is complete, we can build View for the main page.</p>
<h3>Building the view</h3>
<p>Now, once <a id="_idIndexMarker1370"/>we have created the <code>MainViewModel</code> class, it is time to <a id="_idIndexMarker1371"/>create the code for the <code>MainView</code> view:</p>
<ol>
<li>Create a new folder called <code>Views</code>.</li>
<li>Add<a id="_idIndexMarker1372"/> a new <code>MainView</code>.</li>
<li>Set the <code>Title</code> property of <code>ContentPage</code> as <code>Hotdog or </code><code>Not hotdog</code>.</li>
<li>Add <code>HorizontalStackLayout</code> to the page, and set its <code>VerticalOptions</code> property to <code>Center</code> and its <code>HorizontalOptions</code> property to <code>CenterAndExpand</code>.</li>
<li>Add <code>Button</code> to the <code>HorizontalStackLayout</code>, with the text <code>Take Photo</code>. For the <code>Command</code> property, add a binding to the<a id="_idTextAnchor970"/> <code>TakePhoto</code> property in the <code>MainViewModel</code> class.</li>
<li>Add <code>Button</code> to <code>HorizontalStackLayout</code>, with the text <code>Pick Photo</code>. For the <code>Command</code> property, add a binding to the <code>PickPhoto</code> property in the <code>MainViewModel</code> class, as shown in the following code block:<pre class="source-code">
&lt;ContentPage 
             
             
             x:Class="HotdogOrNot.Views.MainView"
             x:DataType="viewModels:MainViewModel"
             Title="Hotdog or Not hotdog"&gt;
    &lt;HorizontalStackLayout VerticalOptions="Center" HorizontalOptions="CenterAndExpand"&gt;
        &lt;Button Text="Take Photo" Command="{Binding TakePhotoCommand}" WidthRequest="150" HeightRequest="150" Margin="20" FontSize="Large"/&gt;
        &lt;Button Text="Pick Photo" Command="{Binding PickPhotoCommand}" WidthRequest="150" HeightRequest="150" Margin="20" FontSize="Large"/&gt;
    &lt;/HorizontalStackLayout&gt;
&lt;/ContentPage&gt;</pre></li> </ol>
<p>In the <a id="_idIndexMarker1373"/>code-behind <code>MainView.xaml.cs</code> file, we will set the binding context of the view by following these steps:</p>
<ol>
<li>Add <code>MainViewModel</code> as a parameter of the constructor.</li>
<li>After the <code>InitialComponent</code> method call, set the <code>BindingContext</code> property of the view to the <code>MainViewModel</code> parameter.</li>
<li>Use the <code>SetBackButtonTitle</code> static method on the <code>NavigationPage</code> class so that an arrow to navigate back to this view will be shown in the navigation bar on the result view, as shown in the following code block:<pre class="source-code">
public MainView(Main<a id="_idTextAnchor971"/>ViewModel viewModel)
{
    InitializeComponent();
    BindingContext = viewModel; Navigati<a id="_idTextAnchor972"/><a id="_idTextAnchor973"/>onPage.SetBackButtonTitle(this, string.Empty);
}</pre></li> </ol>
<h3>Building the result view</h3>
<p>The<a id="_idIndexMarker1374"/> last thing we need to do in this <a id="_idIndexMarker1375"/>project<a id="_idTextAnchor974"/> is to create the result view. This view will show the input<a id="_idTextAnchor975"/> photo and the classification of a hot dog or not.</p>
<h4>Building the ResultViewModel class</h4>
<p>Before <a id="_idIndexMarker1376"/>we create the view, we will create a <code>ResultViewModel</code> class that will handle all the logic for the view, by following these steps:</p>
<ol>
<li>Create a <code>partial</code> class called <code>ResultViewModel</code> in <code>ViewModels</code>.</li>
<li>Add <code>ObservableObject</code> as a base class to the <code>ResultViewModel</code> class.</li>
<li>Create a <code>private</code> field of the <code>string</code> type, called <code>title</code>. Add the <code>ObservableProperty</code> attribute to the field to make it a bindable property.</li>
<li>Create a <code>private</code> field of the <code>string</code> type, called <code>description</code>. Add the <code>ObservableProperty</code> attribute to the field to make it a bindable property.</li>
<li>Create a <code>private</code> field of the <code>string</code> type, called <code>Title</code>. Add the <code>ObservableProperty</code> attribute to the field to make it a bindable property, as shown in the following code block:<pre class="source-code">
using CommunityToolkit.Mvvm.ComponentModel;
using HotdogOrNot.Models;
namespace HotdogOrNot.ViewModels;
public partial class ResultViewModel : ObservableObject
{
    [ObservableProperty]
    private string title;
    [ObservableProperty]
    private string description;
     [ObservableProperty]
    byte[] photoBytes;
    public ResultViewModel()
    {
    }
}</pre></li> </ol>
<p>The next <a id="_idIndexMarker1377"/>thing we will do in <code>ResultViewModel</code> is to create an <code>Initialize</code> method that will have the result as a parameter. Let’s set this up by following these steps:</p>
<ol>
<li>Add a <code>private</code> method named <code>Initialize</code> to the <code>ResultViewModel</code> class that accepts a parameter of the <code>Result</code> type, named <code>result</code>, and returns <code>void</code>.</li>
<li>In the <code>Initialize</code> method, set the <code>PhotoBytes</code> property to the value of the <code>PhotoBytes</code> property of the <code>result</code> parameter.</li>
<li>Add an <code>if</code> statement that checks whether the <code>IsHotDog</code> property of the <code>result</code> parameter is <code>true</code> and whether <code>Confidence</code> is higher than <code>90%</code>. If this is the case, set <code>Title</code> to <code>"Hot dog"</code> and <code>Description</code> to <code>"This is for sure </code><code>a hotdog"</code>.</li>
<li>Add<a id="_idIndexMarker1378"/> an <code>else if</code> statement to check whether the <code>IsHotdog</code> property of the <code>result</code> parameter is <code>true</code>. If this is the case, set <code>Title</code> to <code>"Maybe"</code> and <code>Description</code> to <code>"This is maybe </code><code>a hotdog"</code>.</li>
<li>Add an <code>else</code> statement that sets <code>Title</code> to <code>"Not a hot dog"</code> and <code>Description</code> to <code>"This is <a id="_idTextAnchor976"/>not a hot dog"</code>, as shown in the following code block:<pre class="source-code">
public void Initialize(Result result)
{
    PhotoBytes = result.PhotoBytes;
    if (result.IsHotdog &amp;&amp; result.Confidence &gt; 0.9)
    {
        Title = "Hot dog";
        Description = "This is for sure a hot dog";
    }
    else if (result.IsHotdog)
    {
        Title = "Maybe";
        Description = "This is maybe a hot dog";
    }
    else
    {
        Title = "Not a hot dog";
        Description = "This is not a hot dog";
    }
}</pre></li> </ol>
<p>The<a id="_idIndexMarker1379"/> final thing we need to do is call the <code>Initialize</code> method with the result. If you recall from the previous section on building the main view, we navigated to the <code>Result</code> view and passed the <code>Result</code> object as a parameter. To access the parameter and call the <code>Initialize</code> method properly, follow these steps:</p>
<ol>
<li>Add the <code>IQueryAttributable</code> interface to the list of inherited interfaces:<pre class="source-code">
public partial class ResultViewModel : ObservableObject<code>void</code> method, <code>ApplyQueryAttributes</code>, that accepts a parameter named <code>query</code> of the <code>IDictionary&lt;string, </code><code>object&gt;</code> type:<pre class="source-code">
public void ApplyQueryAttributes(IDictionary&lt;string, object&gt; query)
{
}</pre></li> <li>Now, in the method, call the <code>Initialize</code> method, passing the <code>“result”</code> object from the query dictionary and casting it to a <code>Result</code> type, as shown in the following code:<pre class="source-code">
public void ApplyQueryAttributes(IDictionary&lt;string, object&gt; query)
{
    Initialize(query["result"] as Result);
}/</pre></li> </ol>
<p><code>ViewM<a id="_idTextAnchor977"/><a id="_idTextAnchor978"/>odel</code> is now <a id="_idIndexMarker1380"/>complete, and we are ready to create <code>View</code>.</p>
<h4>Building the view</h4>
<p>Because we <a id="_idIndexMarker1381"/>want to show the input photo in the result view, we need to convert it from <code>byte[]</code> to <code>Microsft.Maui.Controls.ImageSource</code>. We will do this in a value converter that we can use together with the binding<a id="_idIndexMarker1382"/> in the <strong class="bold">XAML</strong>, by following these steps:</p>
<ol>
<li>Create a new folder called <code>Converters</code>.</li>
<li>Create a new class called <code>BytesToImageConverter</code> in the <code>Converters</code> folder.</li>
<li>Add and implement the <code>IValueConverter</code> interface, as shown in the following code block:<pre class="source-code">
using<a id="_idTextAnchor979"/> System.Globalization;
namespace HotdogOrNot.Converters;
public class BytesToImageConverter : IvalueConverter
{
    public object Convert(object value, Type targetType, object parameter, CultureInfo culture)
    {
        throw new NotImplementedException();
    }
    public object ConvertBack(object value, Type targetType, object parameter, CultureInfo culture)
    {
        throw new NotImplementedException();
    }
}</pre></li> </ol>
<p>The <code>Convert</code> method<a id="_idIndexMarker1383"/> will be used when <code>ViewModel</code> updates a view. The <code>ConvertBack</code> method will be used in two-way bindings when <code>View</code> updates <code>ViewModel</code>. In this case, we only need to write code for the <code>Convert</code> method, by following these steps:</p>
<ol>
<li>First, check whether the <code>value</code> parameter is <code>null</code>. If so, we should return <code>null</code>.</li>
<li>If the value is not <code>null</code>, cast it as <code>byte[]</code>.</li>
<li>Create a <code>MemoryStream</code> object from the <code>byte</code> array.</li>
<li>Return the result of the <code>ImageSource.FromStream</code> method to which we w<a id="_idTextAnchor980"/>ill pass the stream, as shown in the following code block:<pre class="source-code">
public object Convert(object value, Type targetType, object parameter, CultureInfo culture)
{
    if(value == null)
    {
        return null;
    }
    var bytes = (byte[])value;
    var stream = new MemoryStream(bytes);
    return ImageSource.FromStream(() =&gt; stream);
}</pre></li> </ol>
<p>The view<a id="_idIndexMarker1384"/> will contain the photo, which will take up two-thirds of the screen. Under the photo, we will add a description of the result. Let’s set this up by going through the following steps:</p>
<ol>
<li>In the <code>Views</code> folder, create a new file using the .NET MAUI ContentPage (XAML) file template, and name it <code>ResultView</code>.</li>
<li>Import the namespace for the converter.</li>
<li>Add <code>BytesToImageConverter</code> to <code>Resources</code> for the page and give it <code>“</code><code>ToImage”</code> key.</li>
<li>Bind the <code>Title</code> property of <code>ContentPage</code> as the <code>Title</code> property of <code>ViewModel</code>.</li>
<li>Add <code>Grid</code> to the page with two rows. The <code>Height</code> value for the first <code>RowDefinition</code> should be <code>2*</code>. The height of the second row should be <code>*</code>. These are relative values that mean that the first row will take up two-thirds of <code>Grid</code>, while the second row will take up one-third of <code>Grid</code>.</li>
<li>Add <code>Image</code> to <code>Grid</code>, and bind the <code>Source</code> property to the <code>PhotoBytes</code> property in <code>ViewModel</code>. Use the converter to convert the <a id="_idTextAnchor981"/>bytes to an <code>ImageSource</code> object and set the <code>Source</code> property.</li>
<li>Add <code>Label</code>, and bind the <code>Text</code> property to the <code>Description</code> property of <code>ViewModel</code>, as shown in the following code block:<pre class="source-code">
&lt;ContentPage     xmlns:converters="clr-
    namespace:HotdogOrNot.Converters"
    x:Cl<a id="_idTextAnchor982"/>ass="HotdogOrNot.Views.ResultView" Title="{Binding Title}"&gt;
  &lt;ContentPage.Resources&gt;
    &lt;converters:BytesToImageConverter x:Key="ToImage" /&gt;
  &lt;/ContentPage.Resources&gt;
  &lt;Grid&gt;
    &lt;Grid.RowDefinitions&gt;
      &lt;RowDefinition Height="2*" /&gt;
      &lt;RowDefinition Height="*" /&gt;
    &lt;/Grid.RowDefinitions&gt;
    &lt;Image Source="{Binding PhotoBytes, Converter=
{StaticResource ToImage}}" Aspect="AspectFill" /&gt;
    &lt;Label Grid.Row="1" HorizontalOptions="Center" FontAttributes="Bold" Margin="10" Text="{Binding Description}" /&gt;
  &lt;/Grid&gt;
&lt;/ContentPage&gt;</pre></li> </ol>
<p>We also <a id="_idIndexMarker1385"/>need to set <code>BindingContext</code> of the view. We will do this in the same way as we did in <code>MainView</code> – in the code-behind file (<code>ResultView.xaml.cs<a id="_idTextAnchor983"/></code>), as shown in the following code snippet:</p>
<pre class="source-code">
public ResultView (ResultViewModel viewModel)
{
    InitializeComponent ();
    BindingContext = viewModel;
}</pre> <p>We<a id="_idTextAnchor984"/><a id="_idTextAnchor985"/> are now ready to write the initialization code for the app.</p>
<h3>Initializing the app</h3>
<p>We <a id="_idIndexMarker1386"/>will set up <code>Shell</code>.</p>
<p>Open <code>App.xaml.cs</code>, and set <code>MainPage</code> to <code>MainView</code> by following these steps:</p>
<ol>
<li>Delete the <code>MainPage.xaml</code> and <code>MainPage.xaml.cs</code> files from the root of the project, since we won’t be needing those.</li>
<li>Open the <code>AppShell.xaml</code> file in the root of the project, and modify it to look like the following code:<pre class="source-code">
&lt;?xml version="1.0" encoding="UTF-8" ?&gt;
&lt;Shell
    x:Class="HotdogOrNot.AppShell"
    
    
    <strong class="bold"></strong>
    Shell.FlyoutBehavior="Disabled"&gt;
    <strong class="bold">&lt;ShellContent</strong>
<strong class="bold">        Title="Home"</strong>
<strong class="bold">        ContentTemplate="{DataTemplate views:MainView}"</strong>
<strong class="bold">        Route="MainView" /&gt;</strong>
&lt;/Shell&gt;</pre></li> </ol>
<p>Now, configure the <code>View</code> and <code>ViewModel</code> classes in the IoC container by following these steps:</p>
<ol>
<li>Open the <code>MauiProgram.cs</code> file.</li>
<li>In the <code>CreateMauiApp</code> method before the <code>return</code> statement, add the following highlighted lines of code:<pre class="source-code">
#if DEBUG
    builder.Logging.AddDebug();
#endif
<strong class="bold">    builder.Services.AddTransient&lt;Views.MainView&gt;();</strong>
<strong class="bold">    builder.Services.AddTransient&lt;Views.ResultView&gt;();</strong>
<strong class="bold">builder.Services.AddTransient&lt;ViewModels.MainViewModel&gt;();</strong>
<strong class="bold">        builder.Services.AddTransient&lt;ViewModels.ResultViewModel&gt;();</strong>
        return builder.Build();</pre></li> </ol>
<p>The <a id="_idIndexMarker1388"/>very last thing we need to do is add the route to <code>ResultView</code> to enable navigation from <code>MainView</code>. We will do this by adding the following highlighted code to the constructor of <code>AppShell</code> in <code>AppShell.xaml.cs</code>:</p>
<pre class="source-code">
public AppShell()
{
<strong class="bold">    Routing.RegisterRoute("Result", type<a id="_idTextAnchor986"/>of(HotdogOrNot.Views.ResultView));</strong>
    InitializeComponent();
}</pre> <p>Now, we are ready to run the app. If we use the simulator/emulator, we can just drag and drop photos to it if we need photos to test with. When the app has started, we can now pick a photo and run it against the model. The following screenshot shows how the app will <a id="_idIndexMarker1389"/>look if we upload a photo of a hot dog:</p>
<div><div><img alt="Figure 12.13 – HotdogOrNot running in an Android emulator" src="img/B19214_12_13.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.13 – HotdogOrNot running in an Android emulator</p>
<p class="callout-heading">Note</p>
<p class="callout">The prediction result for Android may not be as accurate as the web portal at <a href="https://github.com/Azure-Samples/cognitive-services-android-customvision-sample/issues/12">https://github.com/Azure-Samples/cognitive-services-android-customvision-sample/issues/12</a>. If you desire better, more consistent results, you can use the REST APIs.</p>
<h1 id="_idParaDest-200"><a id="_idTextAnchor987"/>Summary</h1>
<p>In this chapter, we built an app that can recognize whether a photo contains a hot dog or not. We accomplished this by training a machine learning model for image classification, using Azure Cognitive Services and the Custom Vision service.</p>
<p>We exported models for ML.NET, and we learned how to use it in an MAUI app that targets iOS, Mac Catalyst, Windows, and Android. In the app, a user can take a photo or pick one from their photo library. This photo will be sent to the model to be classified, and we will get a result that tells us whether the photo is of a hot dog.</p>
<p>Now, we can continue to build other apps and use what we have learned in this chapter regarding machine learning, both on-device and in the cloud using Azure Cognitive Services. Even if we are building other apps, the concept will be the same.</p>
<p>Now, we have completed all the chapters in this book. We have learned the following:</p>
<ul>
<li>What .NET MAUI is and how we can get started building apps</li>
<li>How to use the basic layouts and controls of .NET MAUI</li>
<li>How to work with navigation</li>
<li>How to make the user experience better with animations</li>
<li>How to use sensors such as the <strong class="bold">Global Positioning System</strong> (<strong class="bold">GPS</strong>) in the background</li>
<li>How to build apps for multiple form factors</li>
<li>How to build real-time apps powered by Azure</li>
<li>How to make apps smarter with machine learning</li>
</ul>
<p>The next step is to start to build your own apps. To stay up to date and learn more about .NET MAUI, our recommendation is to read the official Microsoft dev blogs and watch live streams on Twitch and YouTube videos from the .NET MAUI team.</p>
<p>Thank you for reading the book!</p>
</div>
</body></html>