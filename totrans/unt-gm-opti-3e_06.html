<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Optimizing Your Art Assets</h1>
                </header>
            
            <article>
                
<p>Art is a famously subjective discipline, dominated by personal opinion and preference. It can be challenging to say whether, and why, one piece of art is better than another. Oftentimes, <span>our opinions </span><span>won't be able to find a complete consensus. The technical aspects behind art assets that support a game's artistry can also be very subjective. Multiple workarounds can be implemented to improve performance, but these tend to result in a loss of quality for the sake of speed. If we're trying to reach peak performance, then we must consult with our team members whenever we decide to make any changes to our art assets, as it is primarily a balancing act, which can be an art form in itself.</span></p>
<p><span>Whether we're trying to minimize our runtime memory footprint, keep the smallest possible executable size, maximize loading speed, or maintain consistency in frame rate, there are plenty of options to explore. Some methods are clearly always ideal, but most require a little more care and forethought before being adopted, as they would result in reduced quality or could increase the chances of developing bottlenecks in other subsystems.</span><br/></p>
<p>In this chapter, we will explore how to improve performance for the following asset types:</p>
<ul>
<li>Audio files</li>
<li>Texture files</li>
<li>Mesh and animation files</li>
<li>Asset bundles and resources</li>
</ul>
<p>In each case, we will investigate how Unity stores, loads, and manipulates these assets both during application build time and runtime. We will also examine our options in the event of performance issues, and what we can do to avoid behavior that might generate performance bottlenecks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Audio</h1>
                </header>
            
            <article>
                
<p><span>As a framework, </span>Unity can be used to build anything from small applications that require only a handful of sound effects and a single background track to huge role-playing games that need millions of lines of spoken dialog, music tracks, and ambient sound effects. Regardless of the actual scope of the application, audio files are often a significant contributor to the application size after it is built (sometimes called its <em>disk footprint</em>). Moreover, many developers are surprised to find that runtime audio processing can turn into a significant source of CPU and memory consumption.</p>
<p>Audio is often neglected on both sides of the gaming industry: developers tend not to commit many resources to it until the last minute and users rarely pay attention to it. Nobody notices when audio is handled well, but we all know what lousy audio sounds like—it's instantly recognizable, jarring, and guaranteed to draw unwanted attention. This makes it crucial not to sacrifice too much audio clarity in the name of performance.</p>
<p>Audio bottlenecks can come from a variety of sources. Excessive compression, too much audio manipulation, too many active audio components, inefficient memory storage methods, and access speeds all lead to poor memory and CPU performance.</p>
<p>Fortunately, you can learn to avoid such issues with just a little effort and understanding. In the following sections, we will learn some useful tricks to save us from a user experience disaster. We will learn how to choose among the different audio loading options, how to choose the right audio format for our game, and some other relevant performance tweaks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Importing audio files</h1>
                </header>
            
            <article>
                
<p>When we select an imported audio file in the <span class="packt_screen">Project</span> window, the <span class="packt_screen">Inspector</span> window will reveal multiple <span class="packt_screen">Import Settings</span>. These settings dictate everything from loading behavior, compression behavior, quality, sample rate, and (in later versions of Unity) whether to support ambisonic audio (multichannel audio, which combines tracks via spherical harmonics to create more realistic audio experiences).</p>
<div class="packt_tip">Many of the audio import options can be configured on a per-platform basis, allowing us to customize behavior between different target platforms.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loading audio files</h1>
                </header>
            
            <article>
                
<p>The following are the three settings that dictate how an audio file is loaded:</p>
<ul>
<li><span class="packt_screen">Preload Audio Data</span></li>
<li><span class="packt_screen">Load In Background</span></li>
<li><span class="packt_screen">Load Type</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img src="assets/37aa4295-17e2-481c-97b0-0d03f5de0d70.png" style="width:20.83em;height:22.08em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">What we see when we look at an imported file in the inspector.</div>
<p>Our audio files are initially packaged as binary data files that are bundled with our application, which reside on the hard disk of the device (although in some cases they are downloaded from somewhere on the internet). <em>Loading</em> audio data simply means pulling it into main memory (RAM) so that it can be later processed by audio decoders, which then convert the data into audio signals to our headphones or speakers. However, how loading happens will vary enormously based on the previous three settings, which are as follows:</p>
<ul>
<li>The first setting, <span class="packt_screen">Preload Audio Data</span>, determines whether audio data will be automatically loaded during scene initialization or at a later time.</li>
<li>When the loading of audio data does occur, the second setting, <span><span class="packt_screen">Load In Background</span></span>, <span>determines whether this activity blocks the main thread until it is finished or loads it asynchronously in the background.</span></li>
<li><span>Finally, the</span> <span class="packt_screen">Load Type</span> setting defines what kind of data gets pulled into memory and how much data gets pulled at a time.</li>
</ul>
<p>All three of these settings can have a dramatically negative effect on performance if they are not used wisely.</p>
<p>The typical use case of an audio file is to assign it to the<span> </span><span class="packt_screen">AudioClip</span><span> </span>property of an <kbd>AudioSource</kbd> component, which will wrap it in an <kbd>AudioClip</kbd> object. We can then trigger playback via <kbd>AudioSource.Play()</kbd> or<span> </span><kbd>AudioSource.PlayOneShot()</kbd>. Each audio clip assigned in this way would be loaded into memory during scene initialization as the scene contains immediate references to these files, which it must resolve before they are needed. This is the default behavior when <span class="packt_screen">Preload Audio Data</span> is enabled.</p>
<p><span>Disabling <span class="packt_screen">Preload Audio Data</span> tells the Unity engine to skip audio file asset loading during scene initialization, which defers loading activity to the first moment it is needed—that is, when <kbd>Play()</kbd> or <kbd>PlayOneShot()</kbd> are called. Disabling this option will speed up scene initialization, but it also means that the first time we play the file, the CPU will need to immediately access the disk, retrieve the file, load it into memory, decompress it, and play it. This is a synchronous operation and will block the main thread until it is completed. We can prove this with a simple test:</span></p>
<pre>public class PreloadAudioDataTest : MonoBehaviour {<br/>  [SerializeField] AudioSource _source;<br/><br/>  void Update() {<br/>    if (Input.GetKeyDown(KeyCode.Space)) {<br/>        using (new CustomTimer("Time to play audio file", 1)) {<br/>        _source.Play();<br/>    }<br/>  }<br/>}</pre>
<p>If we add an <kbd>AudioSource</kbd> object to our scene, assign a large audio file to it, and assign it to the <kbd>_source</kbd><span> </span>field of <span>the </span><kbd>PreloadAudioDataTest</kbd> component, we can press the spacebar and take a look at a printout of how long the<span> </span><kbd>Play()</kbd><span> </span>function took to complete. A simple test of this code against a 10-MB audio file with <span class="packt_screen">Preload Audio Data</span> enabled will reveal that the call was practically instantaneous; however, disabling <span class="packt_screen">Preload Audio Data</span>, applying the changes to the file, and repeating the test shows that it takes significantly longer (around 700 ms on a desktop PC with an Intel i5 3570K). This completely blows past our budget for a single frame, so to use this toggle responsibly, we will need to load the majority of our audio assets into memory ahead of time.</p>
<p>This can be achieved by calling <kbd>AudioClip.LoadAudioData()</kbd><span> </span>(which can be acquired through an <kbd>AudioSource</kbd> component's <kbd>clip</kbd><span> </span>property). However, this activity will still block the main thread for the same amount of time it takes to load it in the previous example, and so loading our audio file will still cause frame drops, regardless of whether we choose to load it ahead of time. <span>Data can also be unloaded through  </span><kbd>AudioClip.UnloadAudioData()</kbd><span>.</span></p>
<p>This is where the <span class="packt_screen">Load In Background</span> option comes in. This changes audio loading into an asynchronous task, which means that loading will not block the main thread. With this option enabled, the actual call to  <kbd>AudioClip.LoadAudioData()</kbd><span> </span>will complete instantly, but keep in mind that the file won't be ready to play until loading completes on a separate thread. We can double-check <span>an <kbd>AudioClip</kbd> component's current loading state through the </span><kbd>AudioClip.loadState</kbd><span> property. If <span class="packt_screen">Load In Background</span> is enabled and we call</span> <kbd>AudioSource.Play()</kbd><span> </span>without loading the data<span> first</span>, Unity will still require the file to be loaded into memory before it can be played, and so there will be a delay between when we called <kbd>AudioSource.Play()</kbd><span> </span>and when the audio file begins playback. This risks <span>introducing jarring behavior if we try to access a sound file before it is fully loaded, causing it to be out of sync with other tasks, such as animations.</span></p>
<p><span>Modern games typically implement convenient stopping points in levels to perform tasks such as loading or unloading audio data—for example, an elevator between floors, or long corridors where minimal action is taking place. Solutions involving custom loading and unloading of audio data via these methods would need to be tailor-made to the particular game, depending on when audio files are required, how long they're needed for, how scenes are put together, and how players traverse them.</span></p>
<p><span>This can require a significant number of special case changes, testing, and asset management tweaks, so it is recommended that you save this approach as a <em>nuclear option</em> to be used late in production, in the event that all other techniques have not succeeded as well as we hoped.</span></p>
<p><span>Finally, there is the <span class="packt_screen">Load Type</span> option, which dictates how audio data loads when it occurs. There are three options available:</span></p>
<ul>
<li><span><span class="packt_screen">Decompress On Load</span></span></li>
<li><span><span class="packt_screen">Compressed In Memory</span></span></li>
<li><span><span class="packt_screen">Streaming</span></span></li>
</ul>
<p>These three options are explained in detail in the following list:</p>
<ul>
<li><span class="packt_screen">Decompress On Load</span>: This setting compresses the file on disk to save space and decompresses it into memory when it is first loaded. This is the standard method of loading an audio file and should be used in most cases. It takes some time to decompress the file, which leads to a little extra overhead during loading, but reduces the amount of work required when the audio file is played.</li>
<li><span class="packt_screen">Compressed In Memory</span>: This setting copies the compressed file straight from disk into memory when it is loaded. It will only decompress the audio file during runtime when it is being played. This will sacrifice runtime CPU when the audio clip is played, but improves loading speed and reduces runtime memory consumption while the audio clip remains dormant. Hence, this option is best used for very large audio files that are used relatively frequently, or if we're incredibly bottlenecked on memory consumption and are willing to sacrifice some CPU cycles to play the audio clip.</li>
<li><span class="packt_screen">Streaming</span>: Fin<span>ally, this </span>setting <span>(also known as</span> <span><em>Buffered</em>) </span>will load, decode, and play files on the fly at runtime by gradually pushing the file through a small buffer where only one small piece of the overall file is present in memory at a time. This method uses the least amount of memory for a particular audio clip, but the largest amount of runtime CPU. Since each instance of playback of the file will need to generate its buffer, this setting comes with the unfortunate drawback of referencing the audio clip more than once, which leads to multiple copies of the same audio clip in memory that must all be processed separately, resulting in a runtime CPU cost if used recklessly. Consequently, this option is best reserved for single-instance audio clips that play regularly and never need to overlap with other instances of themselves or even with other streamed audio clips—for example, this setting is best used with background music and ambient sound effects that need to be played during the majority of a scene's lifetime.</li>
</ul>
<p><span>So, let's recap. The default case, with <span class="packt_screen">Preload Audio Data</span> enabled, <span class="packt_screen">Load In Background</span> disabled, and a <span class="packt_screen">Load Type</span> of <span class="packt_screen">Decompress On Load</span>, causes a long scene loading time, but ensures that every audio clip we reference in the scene is ready immediately when we need it. There will be no loading delays when the audio clip is needed, and the audio clip will play back the moment we call <kbd>Play()</kbd>.</span></p>
<p><span>A good compromise to improve scene loading time is to enable <span class="packt_screen">Load In Background</span> for audio clips we won't need until later, but this should not be used for audio clips we need shortly after scene initialization. We then control when our audio data is loaded manually through <kbd>AudioClip.LoadAudioData()</kbd> and <kbd>AudioClip.UnloadAudioData()</kbd>. We should be willing to use all of these methods in a single scene to reach optimal performance.</span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Encoding formats and quality levels</h1>
                </header>
            
            <article>
                
<p>Unity supports three general case encoding formats for audio clips, which are determined by the <span class="packt_screen">Compression Format</span> option when we view an audio clip's properties in the <span class="packt_screen">Inspector</span> window:</p>
<ul>
<li><span class="packt_screen">Compressed</span> (the actual text for this option can appear differently, depending on the platform)</li>
<li><span class="packt_screen">PCM</span></li>
<li><span class="packt_screen">ADPCM</span></li>
</ul>
<p>The audio files we import into the Unity engine can be one of many popular audio file formats, such as Ogg Vorbis, MPEG-3 (MP3), and Wave, but the actual encoding that is bundled into the executable will be converted into a different format. </p>
<p><span>The compression algorithm used with the <span class="packt_screen">Compressed</span> setting will depend on the platform being targeted. Standalone applications and other nonmobile platforms will convert the file into Ogg Vorbis format, whereas mobile platforms use MP3.</span></p>
<div class="packt_infobox"><span>There are a few platforms that always use a specific type of compression, such as HEVAG for the PS Vita, XMA for Xbox One, and AAC for WebGL.</span></div>
<p>Statistics are provided in the <span class="packt_screen">Inspector</span> window for the currently selected format in the area following the <span class="packt_screen">Compression Format</span> option, giving you an idea of how much <span>disk space </span><span>the compression is saving. Note that the first value displays the original file size and the second displays the size cost on disk. How much memory the audio file will consume at runtime once loaded will be determined by how efficient the chosen compression format is—for example, the Ogg Vorbis compression will generally decompress to about ten times its compressed size, whereas ADPCM will decompress to about four times the compressed size.</span></p>
<div class="packt_tip"><span>The cost savings displayed in the <span class="packt_screen">Inspector</span> window for an audio file only apply for the currently selected platform and most recently used settings. Ensure that the editor is switched to the correct platform in <span class="packt_screen">File</span></span> | <span><span class="packt_screen">Build Settings</span>, and that you click on <span class="packt_screen">Apply</span> after making changes in order to see the actual cost savings (or cost inflation) for the current configuration. This is particularly important for WebGL applications since the AAC format generally leads to very inflated audio file sizes.</span></div>
<p>The encoding/compression format used can have a dramatic effect on the quality, file size, and memory consumption of the audio file during runtime, and only the <span class="packt_screen">Compressed</span> setting gives us the ability to alter the quality without affecting the sampling rate of the file. Meanwhile, the <span class="packt_screen">PCM</span> and <span class="packt_screen">ADPCM</span> settings do not provide this luxury, and we're stuck with whatever file size those compression formats decide to give us—that is, unless we're willing to reduce audio quality for the sake of file size by reducing the sampling rate.</p>
<p>In the following table, you can take a glance at the differences and use cases for each format:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td><strong>Format</strong></td>
<td><strong>Lossless</strong></td>
<td><strong>Size</strong></td>
<td><strong>Quality</strong></td>
<td><strong>Usage</strong></td>
</tr>
<tr>
<td><strong>PCM</strong></td>
<td>Yes</td>
<td>Large</td>
<td>High</td>
<td><span>Very short sound effects that require a lot of clarity where any compression would otherwise distort the experience.</span></td>
</tr>
<tr>
<td><strong>ADPCM</strong></td>
<td>No</td>
<td>Very Small</td>
<td>Poor</td>
<td><span><span>Compression results in a fair amount of noise, and therefore it is used for </span></span><span>short sound effects with a lot of chaos, such as explosions, collisions, and impact sounds.</span></td>
</tr>
<tr>
<td><strong>Compressed</strong></td>
<td>No</td>
<td>Small/Medium</td>
<td>Variable</td>
<td>This consumes more CPU for decoding and s<span><span>hould be used in most cases. </span></span><span>This option allows us to customize the resulting quality level of the compression algorithm to tweak the quality against the file size.</span></td>
</tr>
</tbody>
</table>
<p> </p>
<div class="packt_tip">Do not forget that any additional audio effects applied to the file at runtime will not play through the editor in <em>Edit Mode</em>, so any changes should be thoroughly tested through the application in <em>Play Mode</em>.</div>
<p><span>Now that we have a better </span>understanding<span> of audio file formats, loading methods, and compression modes, let's explore some approaches that we can use to improve performance by </span>tweaking audio behavior.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Audio performance enhancements</h1>
                </header>
            
            <article>
                
<p>In this section, we explore some other small but important enhancements you can add to your game's sound architecture to improve the overall player experience. We will see why it is important to minimize the audio sources in a scene, in which situation we should prefer mono sounds over stereo sounds, when we should prefer streaming over preloading, and much more.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Minimizing active audio source count</h1>
                </header>
            
            <article>
                
<p>Since each actively playing audio source consumes a particular amount of CPU, it stands to reason that we can save CPU cycles by disabling redundant audio sources in our scene. One approach is to limit how many instances of an audio clip can be played simultaneously. This involves sending audio playback requests through an intermediary that controls our audio sources in such a way that it puts a hard cap on how many instances of an audio clip can be played simultaneously.</p>
<p>Almost every audio management asset available in the Unity Asset Store implements an audio-throttling feature of some kind (often known as <em>audio pooling</em>), and for good reason: it's the best trade-off in minimizing excessive audio playback with the least cost in quality—for example, having 20 footstep sounds playing simultaneously won't sound too much different to playing 10 of them simultaneously, and is less likely to become distracting by being too loud. For this reason, and because these tools often provide many more subtle performance-enhancing features, it is recommended that you use a preexisting solution rather than rolling out your own, as there is a lot of complexity to consider from audio file types, stereo/3D audio, layering, compression, filters, cross-platform capability, efficient memory management, and so on.</p>
<p>When it comes to ambient sound effects, they still need to be placed at specific locations in the scene to make use of the logarithmic volume effect, which gives it a pseudo-3D effect, so an audio pooling system would probably not be an ideal solution. Limiting playback on ambient sound effects is best achieved by reducing the total number of audio sources. The best approach is to either remove some of them or reduce them down to one larger, louder audio source. Naturally, this approach affects the quality of the user experience since it would appear that the sound is coming from a single source and not multiple sources; therefore, it should be used with care.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Enabling Force to Mono for 3D sounds</h1>
                </header>
            
            <article>
                
<p>Enabling the <span class="packt_screen">Force to Mono</span> setting on a stereo audio file will mix together the data from both audio channels into a single channel, saving 50 percent of the file's total disk and memory space usage<span> effectively</span>. Enabling this option is generally not a good idea for some 2D sound effects, where the stereo effect is often used to create a specific audio experience; however, we can enable this option for some good space savings on 3D positional audio clips, where the two channels are effectively identical. These audio source types will let the direction between the audio source and the player determine how the audio file gets played into the left/right ear, and playing a stereo effect in this case is generally meaningless.</p>
<p>Forcing 2D sounds (sounds that play into the player's ears at full volume, regardless of distance/direction to the audio source) to mono might also make sense if there is no need for a stereo effect.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Resampling to lower frequencies</h1>
                </header>
            
            <article>
                
<p>Resampling imported audio files to lower frequencies will reduce the file size and runtime memory footprint. This can be achieved by setting an audio file's <span class="packt_screen">Sample Rate Setting</span> to <span class="packt_screen">Override Sample Rate</span>, at which point we can configure the sample rate through the <span class="packt_screen">Sample Rate</span> option. Some files require high sample rates to sound reasonable, such as files with high pitches and most music files; however, lower settings can reduce the file's size without noticeable quality degradation in most cases. Most use a 22,050 Hertz sampling rate for sources that involve human speech and classical music; some sound effects may be able to get away with even lower frequency values. However, each sound effect will be affected by this setting in a unique way, so it would be wise to spend some time running a few tests before we finalize our decision on the sampling rate.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Considering all compression formats</h1>
                </header>
            
            <article>
                
<p>Each of the <span class="packt_screen">Compressed</span>, <span class="packt_screen">PCM</span>, and <span class="packt_screen">ADPCM</span> compression formats have their own benefits and drawbacks, as explained previously. It's possible to make some compromises in memory footprint, disk footprint, CPU usage, and audio quality using different encoding formats for different files where appropriate. We should be willing to use all of them in the same application and come up with a system that works for the kinds of audio files we're using so that we don't need to treat each file individually; otherwise, we would need to do a prohibitive amount of testing to ensure that audio quality hasn't been degraded for each file.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Being cautious of streaming</h1>
                </header>
            
            <article>
                
<p>The upside of the <span class="packt_screen">Str</span><span class="packt_screen">eaming</span> loading type is a low runtime memory cost, since a small buffer is allocated and the file is continuously pushed through it like a data queue. This can seem quite appealing, but streaming files from the disk should be restricted to large, single-instance files only, as it requires runtime hard disk access, which is one of the slowest forms of data access available to us (second only to pulling a file through a network). Layered or transitioning music clips may run into major hiccups using the <span class="packt_screen">Streaming</span> option, at which point it would be wise to consider using a different <span class="packt_screen">Load Type</span> and control loading/unloading manually. </p>
<p>We should also avoid streaming more than one file at a time, as it's likely to inflict a lot of cache misses on the disk that will interrupt gameplay. This is why this option is primarily used for background music/ambient sound effects, since we only need one at a time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applying filter effects through mixer groups to reduce duplication</h1>
                </header>
            
            <article>
                
<p>Filter effects can be used to modify the sound effect playing through an audio source, and can be accomplished through <kbd>FilterEffect</kbd> components. Each individual filter effect will cost a certain amount of both memory and CPU, and can be a good way to achieve disk space savings while maintaining a lot of variety in audio playback since one file could be tweaked by a different set of filters to generate completely different sound effects.</p>
<p>Because of the additional overhead, overusing filter effects in our scene can result in dire consequences in performance. A better approach is to make use of Unity's audio mixer utility (<span class="packt_screen">Window</span> | <span class="packt_screen">Audio</span> | <span class="packt_screen">Audio Mixer</span>) to generate common filter effect templates that multiple audio sources can reference to minimize the amount of memory overhead.</p>
<p>The official tutorial on audio mixers <span>at </span><a href="https://learn.unity.com/tutorial/audio-mixing">https://learn.unity.com/tutorial/audio-mixing</a> <span>covers the topic in excellent detail.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using remote content streaming responsibly</h1>
                </header>
            
            <article>
                
<p>It is possible to dynamically load game content via the web through Unity, which can be an effective means of reducing an application's disk footprint since fewer data files need to be bundled into the executable. This also provides a means to present dynamic content using web services to determine what is presented to the user at runtime. Asset streaming can be accomplished through <span>the <kbd>UnityWebRequest</kbd> class in Unity 2017 and later.</span></p>
<p>The <kbd>UnityWebRequest</kbd> class makes use of the new HLAPI and LLAPI networking layers. This class provides various utilities to download and access what are primarily text files. Multimedia-based requests should go through the <kbd>UnityWebRequestMultimedia</kbd> helper class. So, if an <kbd>AudioClip</kbd> is requested, we should call<span> </span><kbd>UnityWebRequestMultimedia.GetAudioClip()</kbd> to create the request and<span> </span><kbd>DownloadHandlerAudioClip.GetContent()</kbd><span> </span>to retrieve it once the download is complete.<span> </span></p>
<p>This new version of the API is designed to be more efficient at storing and providing the data we requested, and so reacquiring an <kbd>AudioClip</kbd> multiple times through<span> </span><kbd>DownloadHandlerAudioClip.GetContent()</kbd><span> </span>will not lead to additional allocations. Instead, it will merely return a reference to the originally downloaded <kbd>AudioClip</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Consider using audio module files for background music</h1>
                </header>
            
            <article>
                
<p>Audio module files, also known as <strong>tracker modules</strong>, are an excellent means of saving a significant amount of space without any noticeable quality loss. Supported file extensions in Unity are <kbd>.it</kbd>, <kbd>.s3m</kbd>, <kbd>.xm</kbd>, and <kbd>.mod</kbd>. Unlike the common audio formats, which are read like streams of bits that must be decoded at runtime to generate a specific sound, tracker modules contain lots of small, high-quality samples and organize the entire track similar to a music sheet, defining when, where, how loud, with what pitch, and with what special effects each sample should be played with. This can provide significant size savings while maintaining high-quality sampling, so if the opportunity is available for us to make use of tracker module versions of our music files, then it is worth exploring. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Texture files</h1>
                </header>
            
            <article>
                
<p>The terms <em>texture</em> and <em>sprite</em> often get confused in game development, so it's worth making the distinction: a texture is simply an image file, a big list of color data telling the interpreting program what color each pixel of the image should be, whereas a sprite can be seen as the 2D equivalent of a mesh—it defines how and where the image will appear in the game scene. Usually, a sprite is just a single <em>quad</em> (a pair of triangles combined to make a rectangular mesh) that renders flat against the current camera.</p>
<p>There are also things called sprite sheets, which are large collections of individual images contained within a larger texture file, commonly used to contain the animations of a 2D character. These files can be split apart by tools, such as Unity's Sprite Atlas tool, to form individual textures for the character's animated frames.</p>
<div class="packt_infobox">Of course, you can render a 2D sprite in a 3D environment; however, in essence, a sprite is still a 2D element in the same way a playing card is still a flat card, even when it is used to build a house of cards.</div>
<p>Both meshes and sprites use textures to render an image onto its surface. Texture image files are typically generated <span>in tools such as Adobe Photoshop or GIMP and then imported into our project in much the same way as audio files. At runtime, these files are loaded into memory, pushed to the GPU's VRAM, and rendered by a shader over the target sprite or mesh during a given draw call.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Texture compression formats</h1>
                </header>
            
            <article>
                
<p>Much like audio files, Unity will import texture files with a default list of settings that tend to keep things simple and work okay in the general case, but there are many import settings available, allowing us to improve a texture's quality and performance with some custom tweaking. Of course, making changes is just as likely to reduce quality and performance if we blindly make changes without fully understanding the internal processes going on.</p>
<p>The first option is the file's <span class="packt_screen">Texture Type</span>. This setting will determine what other options are available, particularly under the <span class="packt_screen">Advanced</span> dropdown. Not all importing options are available to all types, so it is best to configure this option for the texture's intended purpose, whether it is set to <span class="packt_screen">Normal Map</span>, <span class="packt_screen">Sprite</span>, <span class="packt_screen">Lightmap</span>, and so on, as this will reveal the options appropriate for that type:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6d0b6b2d-afb7-4ade-9d9b-380415c75309.png" style="width:36.08em;height:33.83em;"/></p>
<p>Similar to audio files, we can import texture files in multiple common formats (such as <kbd>.jpg</kbd> and <kbd>.png</kbd>), but the actual compression format built into the application could be one of many different texture compression formats ideally suited for GPUs of the given platform. These formats represent different ways of organizing the texture's color information, which includes the following:</p>
<ul>
<li>Different numbers of bits used to represent each channel (the more bits that are used, the more colors that can be represented)</li>
<li>Different numbers of bits per channel (for example, the red channel may use more bits than the green channel)</li>
</ul>
<ul>
<li>Different total number of bits used for all channels (more bits naturally mean larger textures and more disk and memory consumption)</li>
<li>Whether or not an alpha channel is included</li>
<li>Perhaps the most important, different ways of packing the data together, which can allow for efficient memory access for the GPU (or incredibly inefficient access if the wrong packing type is chosen!)</li>
</ul>
<p>The simple way of altering compression is to use the <span class="packt_screen">Compression</span> texture import option to select one of the following options:</p>
<ul>
<li><span class="packt_screen">None</span></li>
<li><span class="packt_screen">Low Quality</span></li>
<li><span class="packt_screen">Normal Quality</span></li>
<li><span class="packt_screen">High Quality</span></li>
</ul>
<p>Selecting <span class="packt_screen">None</span> means that no compression will be applied. In this case, the final texture will still change the format from the file type we imported, but it will select a format that makes no attempt at compression, and so we should see little or no quality loss at the expense of large texture files. The other three settings will pick a compression format, which, again, will vary depending on the platform, and Unity will try to pick a compression format that matches the option. For instance, selecting <span class="packt_screen">Low Quality</span> will mean that Unity picks a compression format that greatly reduces the texture size, but will generate some compression artifacts, whereas selecting <span class="packt_screen">High Quality</span> will consume more memory with much larger texture sizes and minimal artifacts. Again, this is an automatic selection made by Unity.</p>
<div class="packt_infobox">The exact formats Unity picks for each platform for each of these <span class="packt_screen">Compression</span> settings can be found at <a href="https://docs.unity3d.com/Manual/class-TextureImporterOverride.html">https://docs.unity3d.com/Manual/class-TextureImporterOverride.html</a>.</div>
<p>The exact compression format Unity chooses can be overridden, although the available options vary per platform since practically every platform has its own custom formats that work best for it. If we click on one of the platform-specific tabs beside the <span class="packt_screen">Default</span> tab (just above the <span class="packt_screen">Max Size</span> option), we will expose the settings for a specific platform and can choose the exact compression format we want Unity to use.</p>
<div class="packt_tip">There is also the <span class="packt_screen">Crunch Compression</span> setting, which will apply an additional level of lossy compression on top of the DXT compression format. This option is only revealed if the other compression settings result in a DXT level of compression. This setting can save even more space at the cost of potentially glaring compression artifacts, depending on the <span class="packt_screen">Compressor Quality</span> setting.</div>
<p>Several of a texture's import settings are fairly mundane, such as determining whether the file contains an alpha channel, how to wrap the texture at its extents, the filtering method, and the maximum possible resolution of the file (a global limit so that we don't accidentally overscale the texture beyond its original size on certain platforms). However, there are several other interesting options in these import settings, which we will cover in other sections where appropriate.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Texture performance enhancements</h1>
                </header>
            
            <article>
                
<p>Let's explore some changes that we can make to our texture files, which might help improve performance, depending on the situation and the content of the files we're importing. In each case, we'll explore the changes that need to be made and the overall effect they have, whether this results in a positive or negative impact on memory or CPU, an increase or decrease in the texture quality, and under what conditions we can expect to make use of these techniques.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reducing texture file size</h1>
                </header>
            
            <article>
                
<p>The larger a given texture file, the more GPU memory bandwidth will be consumed, pushing the texture when it is needed. If the total memory pushed per second exceeds the graphics card's total memory bandwidth, then we will have a bottleneck, as the GPU must wait for all textures to be uploaded before the next rendering pass can begin. Smaller textures are naturally easier to push through the pipeline than larger textures, so we will need to find a good middle ground between high quality and performance.</p>
<p>A simple test to find out whether we're bottlenecked in memory bandwidth is to reduce the resolution of our game's <span>largest</span><span> and</span><span> most abundant texture files and relaunch the scene. If the frame rate suddenly improves, then the application was most likely bound by texture throughput. If the frame rate does not improve or improves very little, then either we still have some memory bandwidth to make use of or there are bottlenecks elsewhere in the rendering pipeline, preventing us from seeing any further improvement.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using mipmaps wisely</h1>
                </header>
            
            <article>
                
<p>There would be no point rendering small, distant objects, such as rocks and trees, with a high-detail texture if there's no way the player would ever be able to see that detail. Of course, they may see some slight improvement, but the performance cost may not be worth the minor detail increase. Mipmaps were invented as a way to solve this problem (as well as to help eliminate aliasing problems that were plaguing video games at around the same time) by pregenerating lower-resolution alternatives of the same texture and keeping them together in the same memory space. At runtime, the GPU picks the appropriate mipmap level based on how large the surface appears within the perspective view (essentially based on the texel-to-pixel ratio when the object is rendered).</p>
<p>By enabling the <span class="packt_screen">Generate Mip Maps</span> setting, Unity automatically handles the generation of these lower-resolution copies of the texture. These alternatives are generated using high-quality resampling and filtering methods within the editor rather than during runtime. There are several other options available for mipmap generation that can affect the quality of the generated levels, so some tweaking may be required to get a high-quality set of mipmaps. We will need to decide whether the time spent tweaking these values is worth it since the whole purpose of mipmaps is to intentionally reduce quality to save performance in the first place.</p>
<p>The following image shows how a 1024 x 1024 image that has been mipmapped into multiple lower-resolution images duplicates:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/436083f8-d2a6-418c-88d0-90545e3bfd0b.png" style="width:34.58em;height:16.75em;"/></p>
<p>These images will be packed together to save space, essentially creating a final texture file that will be about 33 percent larger than the original image. This will cost some disk space and GPU memory bandwidth to upload. </p>
<p>Since Unity 2018.2, there is another way to load mipmaps: streaming. As in the audio case, mipmap streaming is used to reduce the memory needed to keep in memory the multiple textures of a mipmap without sacrificing the quality. In fact, if we enable mipmap streaming, then Unity will try to load on the fly from disk only the correct resolution of a texture on the basis of the camera position in the scene. This can save up to 30% of texture memory depending on the scene (and the player's position).</p>
<p>However, this comes at a price. First of all, streaming a mipmap is slower than generation; therefore, if you have instantaneous camera cuts or you move quickly, you can start noticing the texture quality change as the mipmaps are loaded. This can be mitigated by using the mipmap streaming API in order to preload the mipmaps in the destination location.</p>
<p>Second, mipmap streaming may not be supported on all platforms <span>at the moment</span><span>. If you want to be sure that mipmap streaming is supported on your platform, you can check the </span><kbd>SystemInfo.supportsMipStreaming</kbd><span> </span><span>property.</span></p>
<div class="packt_infobox">If you want more information on texture streaming, you can check the detailed page in the manual at <a href="https://docs.unity3d.com/Manual/TextureStreaming-API.html">https://docs.unity3d.com/Manual/TextureStreaming-API.html</a>.</div>
<p>It's possible to see which mipmap levels are being used by our application at certain points by changing the <span class="packt_screen">Draw Mode</span> setting of the <span class="packt_screen">Scene</span> window to <span class="packt_screen">Mipmaps</span>. This will highlight textures in red if they are larger than they should be, given the player's current view (the extra detail is wasted), whereas being<span> </span>highlighted blue means that they are too small (the player is observing a low-quality texture with a poor texel-to-pixel ratio).</p>
<p>Remember that mipmapping is only useful if we have textures that need to be rendered at varying distances from the camera. If we have textures that always render at a common distance from the main camera in such a way that the mipmapped alternatives are never used, then enabling mipmaps is just a waste of space. Similarly, if we happen to have a texture that always resolves to the same mipmap level because the player's camera never gets too close/far away to switch levels, then it would be wiser to simply downscale the original texture. </p>
<p>Good examples of this would be any texture file used in a 2D game, textures used by UI systems, or those used in a Skybox or distant background, since, by design, these textures will always be about the same distance from the camera, so mipmapping would be essentially pointless. Other good examples include objects that only appear near the player, such as player-centric particle effects, characters, objects that only appear near the player, and objects that only the player can hold/carry.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Managing resolution downscaling externally</h1>
                </header>
            
            <article>
                
<p>Unity puts a lot of effort into making things as easy to use as possible and provides us with the ability to place the project files from external tools to our project workspace, such as <kbd>.PSD</kbd> and <kbd>.TIFF</kbd> files, which are often large and split into multiple layered images. Unity automatically generates a texture file from the file's contents for the rest of the engine to make use of, which can be very convenient, as we only need to maintain a single copy of the file through source control, and the Unity copy is automatically updated when an artist makes changes.</p>
<p>The problem is that the aliasing introduced by Unity's autotexture generation and compression techniques from these files may not be as good as what the texture-editing tools we use could generate for us. Unity is very feature-rich and, first and foremost, focuses on being a game-development platform, which means that it can have difficulty competing in areas that other software developers work on full time. Unity may<span> </span>be introducing artifacts through aliasing as a result of downscaling the image for us, and so we might find ourselves working around it by importing image files with a higher resolution than necessary just to keep the intended quality level; however, had we downscaled the image through the external application first, we might have suffered much less aliasing. In these cases, we may achieve an acceptable level of quality with a lower resolution, while consuming less overall disk and memory space.</p>
<p>We can either avoid using <kbd>.PSD</kbd> and <kbd>.TIFF</kbd> files within our Unity project as a matter<span> </span>of habit (storing them elsewhere and importing the downscaled version into Unity) or just perform some occasional testing to ensure that we're not wasting file size, memory, and GPU memory bandwidth using larger resolution files than necessary. This costs us some convenience in project file management, but can provide some significant savings for some textures if we're willing to spend the time comparing the different downscaled versions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adjusting anisotropic filtering levels</h1>
                </header>
            
            <article>
                
<p>Anisotropic filtering is a feature that improves the image quality of textures when they are viewed at very oblique (shallow) angles. The following screenshot shows the classic example of painted lines on a road with and without anisotropic filtering applied:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6be0a82d-eefa-46a4-a0f1-cf73ecf345a8.png" style="width:31.67em;height:7.75em;"/></p>
<p><span>In either case, the painted lines close to the camera appear fairly clear, but things change as they get further away from the camera. Without anisotropic filtering, the distant painted lines get more and more blurry and distorted, whereas these lines remain crisp and clear with anisotropic filtering applied.</span></p>
<p>The strength of anisotropic filtering applied to the texture can be hand modified on a per-texture basis with the <span class="packt_screen">Aniso Level</span> setting, as well as globally enabled/disabled using the <span class="packt_screen">Anisotropic Textures</span> option within the <span class="packt_screen">Edit</span> | <span class="packt_screen">Project</span> | <span class="packt_screen">Quality</span> settings.</p>
<p>Much like mipmapping, this effect can be costly and, sometimes, unnecessary. If there are textures in our scene that we are certain will never be viewed at an oblique angle (such as distant background objects, UI elements, and billboard particle effect textures), then we can safely disable anisotropic filtering for them to save runtime overhead. We can also consider adjusting the strength of the anisotropic filtering effect on a per-texture basis to find the magic spot between quality and performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Consider atlasing</h1>
                </header>
            
            <article>
                
<p>Atlasing is the technique of combining lots of smaller, isolated textures together into a single, large texture file in order to minimize the number of materials, and therefore draw calls, we need to use. This is effectively a means to exploit dynamic batching. Conceptually, this technique is very similar to the approaches of minimizing material usage that you learned in<span> </span><a href="">Chapter 3</a>, <em>The Benefits of Batching</em>.</p>
<p>Each unique material will require an additional draw call, but each material<span> </span>only supports a single primary texture. Of course, they can also support multiple secondary textures, such as normal maps and emission maps. However, by combining multiple primary textures into a single large texture file, we can minimize the number of draw calls used to render objects that share this texture:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/26f35ace-e607-4955-94a9-15fd539ae6e8.png" style="width:22.08em;height:15.75em;"/></p>
<p>Extra work is required to modify the UV coordinates used by the mesh or sprite object to only sample the portion of the larger texture file that it needs, but the benefits are clear: reducing draw calls results in the reduction of CPU workload and improvement in the frame rate if our application is bottlenecked on the CPU. Assuming that the merged texture file's resolution is equivalent to that of all of the combined images, there will be no loss of quality, and memory consumption will be essentially identical. Note that atlasing does not result in reduced memory bandwidth consumption since the amount of data being pushed to the GPU would also be identical. It just happens to be bundled together in one bigger texture file.</p>
<div class="packt_tip">Atlasing is only an option when all of the given textures require the same shader. If some of the textures need unique graphical effects applied through shaders, then they must be isolated into their own materials and atlased in separate groups.</div>
<p>Atlasing is a common tactic applied to UI elements and in games that feature a lot of 2D graphics. Atlasing becomes practically essential when developing mobile games with Unity since draw calls tend to be the most common bottleneck on those platforms. However, we would not want to generate these atlas files manually. Life would be much simpler if we could continue to edit our textures individually and automate the task of combining them into a larger file.</p>
<p>Many GUI-related tools in the Unity Asset Store provide an automated texture-atlasing feature. There are some standalone programs scattered across the internet that can handle this work, and Unity can generate atlases for sprites in the form of assets. These can be created by going to <span class="packt_screen">Asset</span> | <span class="packt_screen">Create</span> | <span class="packt_screen">Sprite Atlas</span>. </p>
<p>Check out the Unity documentation to discover more about this useful feature at <a href="https://docs.unity3d.com/Manual/class-SpriteAtlas.html">https://docs.unity3d.com/Manual/class-SpriteAtlas.html</a><span>.</span></p>
<div class="packt_tip">Note that the sprite atlas feature effectively supplants the sprite packer tool from older versions of Unity.</div>
<p>Atlasing does not need to be applied to 2D graphics and UI elements either. We can apply this technique to 3D meshes if we happen to be creating a lot of low-resolution textures. 3D games that feature simple texture resolutions or a flat-shaded, low-poly art style are ideal candidates for atlasing in this way.</p>
<p>However, because dynamic batching affects<span> only</span> nonanimated meshes (that is, <kbd>MeshRenderer</kbd>, but not <kbd>SkinnedMeshRenderer</kbd>), there is no reason to combine texture files for animated characters into an atlas. Since they are animated, the GPU needs to multiply each object's bones by the transform of the current animation state. This means that a unique calculation is needed for each character, and they will result in an extra draw call regardless of any attempts we make to have them share materials.</p>
<p>As a result, combining textures for animated characters should be done<span> only</span> as a matter of convenience and space-saving; for example, in a flat-shaded, low-poly art style game, where everything happens to use a common color palette, we can make some space savings using a single texture for the entire game world, objects, and characters.</p>
<p>The disadvantages of atlasing are mostly in terms of development time and workflow costs. It requires a lot of effort to overhaul an existing project to make use of atlasing, which can be a lot of work just to figure out whether it is worth the effort or not. In addition, we need to be aware of generating texture files that are too large for the target platform.</p>
<p>Some devices (specifically mobile devices) have a relatively low limit on the size of the textures that can be pulled into the lowest memory cache of the GPU. If the atlased texture file is too large, then it must be broken up into smaller textures in order to fit the target memory space. If the device's GPU happens to need textures from different pieces of the atlas every other draw call, then not only will <span>we </span><span>inflict a lot of cache misses, but we also might find that we choke the memory bandwidth, as textures are constantly pulled from VRAM and the lower-level cache.</span></p>
<p>We would probably not have this problem if the atlas was left as individual textures. The same texture swapping will occur, but will result in much smaller files being swapped at the cost of additional draw calls. Our best options at this stage would be to lower the Atlas resolution or generate multiple smaller atlases to have better control over how they will be dynamically batched.</p>
<p>Atlasing is clearly not a perfect solution, and if it is not clear whether it would result in a performance benefit, then we should be careful not to waste too much time on its implementation. Speaking very generally, mobile games with a very simplistic 2D art style probably won't need to make use of atlasing; however, mobile games attempting to compete with high-quality assets or use any kind of 3D graphics should probably start integrating atlasing from the very beginning of development, since it is likely that the project will reach texture throughput limits very quickly. They may even need to apply many per-platform and per-device optimizations in order to reach a wide audience.</p>
<p>Meanwhile, we should consider applying atlasing to high-quality desktop games only if our draw call count exceeds reasonable hardware expectations, since we will want many of our textures to maintain high resolutions for maximum quality. Low-quality desktop games can probably afford to avoid atlasing since draw calls are unlikely to be the biggest bottleneck.</p>
<p>Of course, no matter what the product is, if we're ever limited in CPU by too many draw calls and have already exhausted many of the alternative techniques, then atlasing is a very effective performance enhancement in most cases.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adjusting compression rates for nonsquare textures</h1>
                </header>
            
            <article>
                
<p>Texture files are normally stored in a square, power-of-two format, meaning that their height and width are equal in length, and its size is a power of two—for example, some typical sizes are 256 x 256 pixels, 512 x 512, and 1024 x 1024, and so on.</p>
<p>It is possible to provide rectangular power-of-two textures (such as 256 x 512) or those with a non-power-of-two format (such as 192 x 192), but creating textures such as these is not recommended. Some GPUs require square texture formats, so Unity will compensate by automatically expanding the texture to include additional empty space in order to fit the form factor that the GPU expects, which will result in additional memory bandwidth costs, pushing what is essentially unused and useless data to the GPU. Other GPUs may support non-power-of-two textures, but this is likely to result in slower sampling than a square texture.</p>
<p>So the first recommendation is to avoid nonsquare and/or non-power-of-two textures altogether. If the image can be placed within a square, power-of-two texture and does not result in too much quality degradation due to squeezing/stretching, then we should apply those changes just to keep the CPU and GPU happy. As a second option, we can customize this scaling behavior in Unity through the texture file's <kbd>Non Power of 2</kbd> import setting, though because this is an automated process, it might not give us the graphical quality we expect.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sparse textures</h1>
                </header>
            
            <article>
                
<p>Sparse textures, also known as <strong>mega-textures</strong> or <strong>tiled-textures</strong>, provide a way of effectively streaming texture data from disk at runtime. Relatively speaking, if the CPU performs operations in the order of seconds, then the disk would operate in the order of days. So the common advice is that hard-disk access during gameplay should be avoided as much as possible since any such technique risks inflicting more disk access than available, causing our application to grind to a halt.</p>
<p>However, sparse texturing offers some interesting performance-saving techniques if we're smart about starting data transfer for portions of the texture before we need them. Sparse texturing is prepared by combining many textures into an enormous texture file that would be far too large to load into graphics memory as a single texture file. This is similar to the concept of atlasing, except the file containing the textures is incredibly large—for example,  32,768 x 32,768 pixels—and would contain considerable color detail, such as 32 bits per pixel <span>(this would result in a texture file that consumes 4 GBs of disk space)</span>. The idea is to save large amounts of runtime memory and memory bandwidth by hand-picking small subsections of the texture to load from the disk dynamically, pulling them from the disk moments before they are needed in the game. The main cost of this technique is the file size requirement and the potentially continuous disk access. Other costs for this technique can be overcome, but normally take a great deal of scene preparation work.</p>
<p>The game world needs to be created in such a way that it minimizes the amount of texture swapping taking place. In order to avoid very noticeable <em>texture popping</em> problems, texture subsections must be pulled from a disk into RAM with just enough time to spare that the GPU does not need to wait before the transfer to VRAM can begin (in much the same way that it normally doesn't need to wait for ordinary texture files that are preloaded into RAM). This takes place in the design of the texture file itself by keeping common elements for a given scene in the same general area of the texture, and the design of the scene, by triggering new texture subsection loading at key moments during gameplay and making sure that disk access of the new tile is quickly located by the disk without extreme cache misses. If it is handled with care, then sparse texturing can result in impressive benefits in both scene quality and memory savings.</p>
<p><span>It is a highly specialized technique in the gaming industry and has not yet been widely adopted, partly because it </span>requires specialized hardware and platform support and partly because it is difficult to pull it off well. The Unity documentation on sparse texturing has improved somewhat over time and provides an example scene showing the effect at work, which can be found at<span> </span><a href="http://docs.unity3d.com/Manual/SparseTextures.html">http://docs.unity3d.com/Manual/SparseTextures.html</a>.</p>
<p>For Unity developers who consider themselves advanced enough to experiment with sparse texturing, it might be worth taking the time to perform some research to check whether sparse texturing is right for their project since it promises some significant performance savings.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Procedural materials</h1>
                </header>
            
            <article>
                
<p>Procedural materials, also known as <strong>substances</strong>, are a means of procedurally generating textures at runtime by combining small, high-quality texture samples with custom mathematical formulas. The goal of procedural materials is to greatly minimize the application disk footprint at the cost of additional runtime memory and CPU processing during initialization to generate the texture via mathematical operations rather than static color data.</p>
<p>Texture files are, sometimes, the biggest disk space consumer of a game project, and it's fairly common knowledge that download times have a tremendous negative impact on the completed download rate and getting people to try our game (even if it's free). Procedural materials offer us the ability to sacrifice some initialization and runtime processing power for much faster downloads. This is very important for mobile games that are trying to compete via graphical fidelity.</p>
<p>As for Unity 2019, procedural materials are no longer part of Unity. Instead, they are offered as a separate plugin. You can check more about substances on the official page at <a href="https://www.substance3d.com/integrations/substance-in-unity">https://www.substance3d.com/integrations/substance-in-unity</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Asynchronous texture uploading</h1>
                </header>
            
            <article>
                
<p>The last texture import option we haven't covered is the read/write enabled option. By default, this option is disabled, which is good, because this allows textures to make use of the asynchronous texture uploading feature, which has two benefits: the texture will be uploaded asynchronously from disk to RAM, and when the texture data is needed by the GPU, the transfer happens on the render thread, not the main thread. Textures will be pushed into a circular buffer, which pushes data to the GPU continuously so long as the buffer contains new data. If not, then it early-exits the process and waits until new texture data is requested.</p>
<p>Ultimately, this reduces the time spent preparing the render states for each frame and <span>allows more CPU resources to be spent on gameplay logic, the physics engine, and so on. Of course, some time is still spent on the main thread preparing the render state, but moving the texture uploading task to a separate thread saves a significant chunk of CPU time on the main thread.</span></p>
<p>However, enabling read/write access to the texture <span>essentially tells Unity that we might be reading and editing this texture at any time. This implies that the GPU will </span>need fresh access to it every time, so it <span>will disable asynchronous texture uploading for that texture; all uploading must occur on the main thread</span>. <span>We might want to enable this option for things such as simulating </span><span>painting colors onto a canvas or writing image data from the internet into a premade texture, but the downside is that t</span>he GPU must always wait for any changes to be made to the texture before it can be uploaded since it cannot predict when those changes will happen.</p>
<p><span>In addition, asynchronous texture uploading only works for textures we explicitly imported into the project and that were present during build time since the feature only works if the texture was packed together into special streamable assets. Therefore, a</span><span>ny textures generated via <span><kbd>LoadImage(byte[])</kbd>, texture </span>assets imported/downloaded from external locations, or loaded from a <em>resources</em> folder via <kbd>Resources.Load()</kbd> (which all implicitly call <kbd>LoadImage(byte[])</kbd>  themselves) will not be converted into streamable content, and therefore will be unable to make use of asynchronous texture uploading.</span></p>
<p>It is possible to tweak both the upper limit of the maximum allowed time so that it can be spent on asynchronous texture uploads, and the total circular buffer size Unity should use to push the textures we want to upload. These settings can be tweaked under <span class="packt_screen">Edit</span> | <span class="packt_screen">Project Settings</span> | <span class="packt_screen">Quality</span> | <span class="packt_screen">Other</span> and are named <span class="packt_screen">Async Upload Time Slice</span> and <span class="packt_screen">Async Upload Buffer Size</span>, respectively. We should set the <span class="packt_screen">Async Upload Time Slice</span> value to the maximum number of milliseconds we want Unity to spend on asynchronous texture uploads on the render thread. It might be wise to set the <span class="packt_screen">Async Upload Buffer Size</span> value to the largest texture file we might need to use, plus a little extra buffer if multiple fresh textures are needed in the same frame. The circular buffer that texture data is copied into will expand as needed, but this is often costly. Since we probably already know ahead of time how large we need that circular buffer to be, we might as well set it to the maximum expected size to avoid potential frame drops when it needs to resize the buffer. We now move on to our next topic— the mesh and animation file types. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mesh and animation files</h1>
                </header>
            
            <article>
                
<p>The mesh and animation file types are essentially large arrays of vertex and skinned bone data, and there are a variety of techniques we can apply to minimize file size while keeping similar, if not identical, appearances. There are also ways to lower the cost of rendering large groups of these objects through batching techniques. Let's take a look at a series of performance-enhancing techniques that we can apply to such files.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reducing the polygon count</h1>
                </header>
            
            <article>
                
<p>Reducing the polygon count is the most obvious way to gain performance and should always be considered. In fact, since we cannot batch objects using skinned mesh renderers, it's one of the good ways of reducing CPU and GPU runtime overhead for animated objects.</p>
<p>Reducing the polygon count is simple, straightforward, and provides both CPU and memory cost savings for the time required for artists to clean up the mesh. I<span>n this day and age, m</span><span>uch of an object's detail is almost entirely based on detailed texturing and complex shading, so we can often get away with stripping away a lot of vertices on modern meshes, and most users would be unable to tell the difference.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tweaking mesh compression</h1>
                </header>
            
            <article>
                
<p>Unity offers four different <span class="packt_screen">Mesh Compression</span> settings for imported mesh files: <span class="packt_screen">Off</span>, <span class="packt_screen">Low</span>, <span class="packt_screen">Medium</span>, and <span class="packt_screen">High</span>. Increasing this setting will convert floating-point data into fixed values, reducing the accuracy in the vertex position/normal direction, simplifying vertex color information, and so on. This can have a noticeable effect on meshes that contain lots of small parts near one another, such as a fence or grate. If we're generating meshes procedurally, we can achieve the same type of compression by calling the <kbd>Optimize()</kbd> method of a<span> </span><kbd>MeshRenderer</kbd> component (of course, this will take some time to complete).</p>
<p>There are also two global settings found in <span class="packt_screen">Edit</span> | <span class="packt_screen">Project Settings</span> | <span class="packt_screen">Player</span> | <span class="packt_screen">Other Settings</span> that can affect how mesh data is imported. They are as follows:</p>
<ul>
<li><span class="packt_screen">Vertex Compression</span>: We can use this option to configure the type of data that will be optimized when we import a mesh file with <span class="packt_screen">Mesh Compression</span> enabled, so if we want accurate normal data (for lighting), but are less worried about positional data, then we can configure it here. Unfortunately, this is a global setting, and will affect all imported meshes (although it can be configured on a per-platform basis since it is a <span class="packt_screen">Player</span> setting).</li>
<li><span><span class="packt_screen">Optimize Mesh Data</span></span>: Enabling <span class="packt_screen">Optimize Mesh Data</span> will strip away any data from the mesh that isn't required by the material(s) assigned to it. So, if the mesh contains tangent information, but the shader never requires it, then Unity will ignore it during build time.</li>
</ul>
<p>In each case, the benefits reduce the application's disk footprint at the cost of extra time loading the mesh, since extra time must be spent decompressing the data before it's needed.</p>
<div class="packt_tip">The 3D mesh-building/animation tools often provide their own built-in ways of automated mesh optimization in the form of estimating the overall shape and stripping the mesh down to fewer total polygons. This can cause a significant loss of quality and should be tested vigorously if used.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using Read-Write Enabled appropriately</h1>
                </header>
            
            <article>
                
<p>The <span class="packt_screen">Read-Write Enabled</span> flag allows changes to be made to the mesh at runtime either via scripting or automatically by Unity during runtime, similar to how it is used for texture files. Internally, this means that it will keep the original mesh data in memory until we want to duplicate it and make changes dynamically. Disabling this option will allow Unity to discard the original mesh data from memory once it has determined the final mesh to use, since it knows it will never change.</p>
<p>If we use<span> only </span>a uniformly scaled version of a mesh throughout the entire game, then disabling this option will save runtime memory since we will no longer need the original mesh data to make further rescaled duplicates of the mesh (incidentally, this is how Unity organizes objects by scale factor when it comes to dynamic batching). Unity can, therefore, discard this unwanted data early since we won't need it again until the next time the application is launched.</p>
<p>However, if the mesh often reappears at runtime with different scales, then Unity needs to keep this data in memory so that it can recalculate a new mesh more quickly; therefore, it would be wise to enable the <span class="packt_screen">Read-Write Enabled</span> flag. Disabling it will require Unity to not only reload the mesh data each time the mesh is reintroduced, but also make the rescaled duplicate at the same time, causing a potential performance hiccup.</p>
<p>Unity tries to detect the correct behavior for this setting at initialization time, but when meshes are instantiated and scaled in a dynamic fashion at runtime, we must force the issue by enabling this setting. This will improve the instantiation speed of the objects, but cost some memory overhead since the original mesh data is kept around until it's needed.</p>
<div class="packt_tip">Note that this potential overhead cost also applies when using the <span class="packt_screen">Generate Colliders</span> option.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Considering baked animations</h1>
                </header>
            
            <article>
                
<p>Using baked animations will require changes in the asset by using the 3D rigging and animation tool that we are using, since Unity does not provide such tools itself. Animations are normally stored as keyframe information, which it uses to keep track of specific mesh positions and interpolate between them at runtime using skinning data (bone shapes, assignments, animation curves, and so on). Baking animations means effectively sampling and hardcoding each position of each vertex into the mesh/animation file <span>per frame </span><span>without the need for interpolation and skinning data.</span></p>
<p>Using baked animations can sometimes result in much smaller file sizes and memory overhead than blended/skinned animations for some objects since skinning data can take up a surprisingly large amount of space to store. <span>This is most likely to be the case for relatively simple objects or objects with short animations since we would effectively be replacing procedural data with a hardcoded series of vertex positions. So i</span>f the mesh's polygon count is low enough where storing lots of vertex information is cheaper than skinning data, then we may see some significant savings through this simple change.</p>
<p>In addition, how often the baked sample is taken can usually be customized by the exporting application. Different sample rates should be tested to find a good value where the key moments of the animation still shine through what is essentially a simplified estimate.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Combining meshes</h1>
                </header>
            
            <article>
                
<p>Forcefully combining meshes into a large, single mesh can be a convenient option to reduce draw calls, particularly if the meshes are too large for dynamic batching and don't play well with other statically batched groups. This is essentially the equivalent of static batching, but it is performed manually, so sometimes it's a wasted effort if static batching could take care of the process for us.</p>
<p>Be aware that if any single vertex of the mesh is visible in the scene, then the entire object will be rendered together as one whole. This can lead to a lot of wasted processing if the mesh is only partially visible most of the time. This technique also comes with the drawback that it generates a whole new mesh asset file that we must deposit into our scene, which means that any changes we make to the original meshes will not be reflected in the combined one. This results in a lot of tedious workflow effort every time changes need to be made, so if static batching is an option, it should be used instead.</p>
<p>There are several tools available online that can combine mesh files together for us in Unity. They are only an Asset Store or Google search away.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Asset bundles and resources</h1>
                </header>
            
            <article>
                
<p>We touched upon the topic of resources and serialization in <a href="">Chapter 2</a>, <em>Scripting Strategies</em>, and it should be fairly clear that the resource system <span>can be a great benefit during prototyping, as well as during the early stages of our project, and can be used relatively effectively in games of limited scope. </span></p>
<p>However, professional Unity projects should instead favor the asset bundle system. There are a number of reasons for this. Firstly, the resource system is not very scalable when it comes to builds. All resources are merged together into a single massive serialized file binary data blob with an index list of where various assets can be found within it. This can be hard to manage, and take a long time to build as we add more data to the list.</p>
<p>Secondly, the resource system's ability to acquire data from the serialized file scales in an <em>Nlog(N)</em> fashion, which should make us very wary of increasing the value of <em>N</em>. Thirdly, the resource system makes it unwieldy for our application to provide different asset data on a per-device basis, whereas asset bundles tend to make this matter trivial. Finally, asset bundles can be used to provide small, periodic custom content updates to the application, while the resource system would require updates that completely replace the entire application to achieve the same effect.</p>
<p>Asset bundles share a lot of common functionality with resources, such as loading from files, loading data asynchronously, and unloading data we no longer need. However, they also offer much more functionality, such as content streaming, content updates, and content generation and sharing. These can all be used to improve the performance of our application to great effect. We can deliver applications with much smaller disk footprints and have the user download additional content before or during gameplay, stream assets at runtime to minimize the initial loading time of the application, and provide more optimized assets to the application on a per-platform basis without the need to push a complete application to overwrite to the user.</p>
<p><span>Of course, there are downsides to asset bundles. They are much more complicated to set up and maintain than resources, they're more complicated to understand since they use a much more sophisticated system for accessing asset data than the resources system, and making full use of their functionality (such as streaming and content updates) would require a lot of additional QA testing to make sure that the server is delivering content properly, and that the game is reading and updating its content to match. Ergo, asset bundles are best used only when our team size is able to support the extra workload they require.</span></p>
<p>A tutorial on the asset bundle system is beyond the scope of this book, but there are dozens of useful guides online and in the Unity documentation.</p>
<p>Check out the Unity tutorial at<span> </span><a href="https://learn.unity.com/tutorial/assets-resources-and-assetbundles">https://learn.unity.com/tutorial/assets-resources-and-assetbundles</a><span> to find out more about the asset bundle system.</span></p>
<p>If you require further convincing, then a Unity blog post from April 2017 should help reveal how the asset bundle system can use memory more efficiently during runtime in ways that the resources system cannot provide through memory pooling. You can find this blog at <a href="https://blogs.unity3d.com/2017/04/12/asset-bundles-vs-resources-a-memory-showdown/">https://blogs.unity3d.com/2017/04/12/asset-bundles-vs-resources-a-memory-showdown/</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>There are many different opportunities that we can explore to achieve performance gains for our application just by tinkering with our imported assets. Alternatively, from another perspective, there are plenty of ways to ruin our application's performance through asset mismanagement. Almost every single import configuration opportunity is a trade-off between one performance metric or workflow task and another. Typically, this means saving the disk footprint via compression at the expense of CPU at runtime to decompress the data, or faster access while reducing the quality level of the final presentation. So we must remain vigilant and only pick the right techniques for the right assets for the right reasons.</p>
<p>This concludes our exploration of improving performance through art asset manipulation. In the next chapter, we will be investigating how to improve our usage of Unity's physics engine.</p>


            </article>

            
        </section>
    </body></html>