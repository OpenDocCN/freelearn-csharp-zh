<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-169"><a id="_idTextAnchor028"/>9</h1>
<h1 id="_idParaDest-170">Best Practices and Future Trends in XR Development</h1>
<p>Throughout this book, you have acquired and mastered a diverse set of skills for XR development in Unity. While you should feel comfortable creating a wide array of VR, MR, and AR applications in Unity by now using the XR Interaction Toolkit, AR Foundation, ARKit, and ARCore using all the techniques you learned about in this book, we also want to help you and guide you on your XR development journey from here onward. This chapter is specifically designed for this goal. It will not only provide you with best practices in XR development that you should familiarize yourself with to become a more versatile and powerful XR developer, but also offer a comprehensive overview of XR toolkits and plugins that you might want to focus on mastering next to deepen your XR development knowledge in a particular field.</p>
<p>You will also learn about current trends in XR technology and applications, and we will even provide you with future trend predictions in XR made by some leading professionals and researchers in this area. By the end of this chapter, you will not only be a great XR developer, but will also feel comfortable evaluating the future path of XR technology. You will be able to better assess how you can match your XR projects to be in line with and at the forefront of these developments, as well as which skills might be interesting for you to acquire next.</p>
<p>This chapter is split into the following sections:</p>
<ul>
<li>Exploring current and future trends in XR technology and applications</li>
<li>Learning the best practices for XR development</li>
<li>Other useful toolkits and plugins for XR development</li>
</ul>
<h1 id="_idParaDest-171">Exploring current and future trends in XR technology and applications</h1>
<p>Did you know that in 2023, consumer XR headset shipments reached 17.8 million, which is over triple the<a id="_idIndexMarker721"/> number of commercial XR shipments, at 5.6 million? The business-to-consumer XR market surged to 31.1 billion US dollars, marking a 23% growth from 2022. Mobile AR dominates the XR landscape with a market value of 21.1 billion US dollars, overshadowing the VR market, which stands at 15.8 billion US dollars (<a href="https://www.statista.com/topics/6072/extended-reality-xr">https://www.statista.com/topics/6072/extended-reality-xr</a>).</p>
<p>Being armed with the capabilities and tools to create immersive XR experiences and witnessing the rapid expansion of the XR market, you might ponder which XR niche to prioritize. This section will explore the primary domains employing XR. You’ll gain insights into ongoing XR research, discover leading companies in each segment, and understand emerging trends and applications. Let’s begin by examining a sector showing significant XR adoption: education.</p>
<h2 id="_idParaDest-172">Learning in new realities – XR in education</h2>
<p>Imagine a<a id="_idIndexMarker722"/> world where classrooms aren’t limited by four walls, and where students can traverse the universe, delve into the human body, or explore limestone caves from the other side of the globe – not through textbooks, but through immersive experiences. This isn’t the realm of science fiction, but a rapidly evolving frontier in education made possible by XR, and your skills as an XR developer are crucial in bringing it to life.</p>
<p>An innovative technology itself is worth nothing if no one is keen on using it. At a Mexican university, more than 90% of the students involved in a research study concluded that VR aided in their understanding of 3D vectors, visualization of vector-related mathematical problems, and comprehension of angles between vectors and 3D axes (<a href="https://repositorio.unab.cl/xmlui/handle/ria/24387">https://repositorio.unab.cl/xmlui/handle/ria/24387</a>). Additionally, over 80% expressed an interest in seeking out courses with VR integration in the future and felt that getting acquainted with the VR tool was quick and straightforward.</p>
<p>The same also goes for teachers. A large-scale survey investigated the attitudes of over 20,000 teachers toward the use of VR for education (<a href="https://link.springer.com/article/10.1007/s10639-022-11061-0">https://link.springer.com/article/10.1007/s10639-022-11061-0</a>). This survey found that educators held a moderately favorable view of VR’s role in education. The findings also revealed that as the level of VR integration increased, so did its usage frequency.</p>
<p>VR has been<a id="_idIndexMarker723"/> found to enhance collaborative learning by boosting engagement and motivation, facilitating remote teamwork, offering interdisciplinary collaboration spaces, cultivating social skills, and aligning with collaborative learning strategies (<a href="https://www.frontiersin.org/articles/10.3389/frvir.2023.1159905/full">https://www.frontiersin.org/articles/10.3389/frvir.2023.1159905/full</a>).</p>
<p>There’s been a year-on-year increase in publications revolving around immersive VR in education over the past 20 years. This is especially true of the time period from 2017 to 2022, with an annual publication volume of 2.6 times more than the preceding period (<a href="https://www.mdpi.com/2071-1050/15/9/7531">https://www.mdpi.com/2071-1050/15/9/7531</a>). These findings prove a growing research interest in using XR for educational purposes. When quantitative learning outcomes from VR applications are compared to those of less immersive teaching techniques, such as desktop computers and presentations, VR is found to provide a notable benefit in most cases (<a href="https://link.springer.com/article/10.1007/s40692-020-00169-2">https://link.springer.com/article/10.1007/s40692-020-00169-2</a>).</p>
<p>Several companies are leveraging the recent research findings to enhance educational experiences through VR and AR. <em class="italic">ClassVR</em> provides an immersive educational platform that pairs VR headsets with a library of curriculum-matched content. This content, known as <em class="italic">Avanti’s World</em>, can also be accessed without the headsets. It’s a comprehensive virtual learning environment set in a theme park format, containing six VR lands, each dedicated to various educational areas (<a href="https://www.classvr.com/">https://www.classvr.com/</a>).</p>
<p>Similarly, <em class="italic">zSpace</em> offers an integrated XR solution with both software and unique hardware, enhancing hands-on learning. Students, using lightweight glasses, can interact with 3D virtual models via a stylus pen. The platform offers rich content across diverse subjects, from K-8 and high-school courses in math, science, biology, and more, to vocational topics such as health science and manufacturing (<a href="https://zspace.com/">https://zspace.com/</a>).</p>
<p><em class="italic">Kai XR</em> is an educational XR platform crafted specifically for teachers. This platform offers 360-degree virtual excursions, enabling students to virtually traverse global locations from their classrooms. With a commitment to child-friendly, immersive experiences, <em class="italic">Kai XR</em> offers diverse VR content, from historical sites to modern marvels. The company emphasizes its adaptability, inviting educators to either use pre-designed lessons or craft their own using <em class="italic">Kai XR</em>’s resources. The aim is to facilitate enriched learning experiences for middle-school students while honing their skills for the modern world. The platform is compatible with numerous devices, from smart TVs to VR headsets such as <em class="italic">Oculus Quest</em>.</p>
<p>Numerous <a id="_idIndexMarker724"/>educational XR applications have emerged on platforms such as Apple’s App Store and Google’s Play Store. Noteworthy examples include apps such as <em class="italic">Human Anatomy Atlas</em> and <em class="italic">Catchy Words</em>, which utilize AR, and <em class="italic">Ancient Egypt</em>, a VR experience. Additionally, VR headset manufacturers such as Meta offer a range of educational VR apps on their native platforms. This includes offerings such as <em class="italic">Mondly</em> for language learning, <em class="italic">Star Chart</em> for space exploration, and <em class="italic">Painting VR</em> for art enthusiasts.</p>
<p>If you are an XR developer and enthusiast, there’s never been a better time to specialize in educational VR. The demand is escalating, research is encouraging, and the pedagogical implications are profound. The education sector is ripe for an XR revolution, and your expertise could shape its future.</p>
<p>Another driving force of XR innovation is industrial and manufacturing use cases, about which you will learn more in the upcoming section.</p>
<h2 id="_idParaDest-173">XR in industrial settings and smart manufacturing</h2>
<p>Almost 13% of <a id="_idIndexMarker725"/>entrepreneurs in the US are currently integrating XR solutions into their companies. This percentage underscores a growing interest in and acceptance of the technology. Moreover, projections indicate that close to 23% of companies plan to incorporate these technologies within the next three years. This provides a substantial window of opportunity for XR developers to capitalize on a market that’s set to expand.</p>
<p>One of the<a id="_idIndexMarker726"/> significant benefits of integrating XR technologies into the industrial sector is the potential for a drastic reduction in workplace accidents – by up to 70%. By identifying potential hazards and dangerous maneuvers using XR, industries can create a safer working environment. This focus on safety and efficiency is likely to drive demand for XR solutions that can deliver such results.</p>
<p>Data captured from the US market suggests that around 2.5% of companies are fully utilizing XR technologies. These modern entities, often referred to as <em class="italic">smart factories of the future</em>, have a holistic technological ecosystem that’s conducive to XR solutions. Tapping into this segment allows developers to push the boundaries of what’s possible with XR, setting benchmarks for the industry (<a href="https://doi.org/10.1016/j.procs.2022.09.357">https://doi.org/10.1016/j.procs.2022.09.357</a>).</p>
<p>In the realm of smart manufacturing, AR in particular is making strides. By using AR devices such as smartphones or headsets, workers receive real-time information and guidance while executing tasks. For instance, an AR headset can display step-by-step assembly instructions and provide feedback if an error occurs or if a tool is nearing the end of its usability. Another useful application of AR in manufacturing is one where a customer, requiring assistance with repairs, is guided remotely by an expert using AR glasses, such as Microsoft’s <em class="italic">HoloLens</em>.</p>
<p>As the usage <a id="_idIndexMarker727"/>of AR increases workers’ accuracy and efficiency, adopting this technology in a smart manufacturing context leads to <a id="_idIndexMarker728"/>overall increased productivity in manufacturing facilities. For onboarding and training of new staff, AR can visually represent intricate tasks and processes, ensuring better understanding and retention. Managers can quickly ascertain the production status via their mobile devices as they navigate the factory floor (<a href="https://www.mdpi.com/2079-9292/12/7/1719">https://www.mdpi.com/2079-9292/12/7/1719</a>).</p>
<p>In recent literature, assembly has been found to be a leader in AR adoption in manufacturing. This prominence stems from assembly activities being visual-centric, demanding significant operator involvement. Comparative studies have shown AR improves efficiency in maintenance and assembly, shortens maintenance time, and enhances task quality over conventional tools.</p>
<p>Quality assurance, too, has benefited from AR. Initially using AR for projecting 2D information, it now uses spatial AR for quality checks, as with welding spot inspections. Spatial AR, combined with real-time 3D metrology data, aids in assessing parts’ surface quality, ensuring precise welding, and supporting packaging sectors such as die-cutter setups, reducing errors and costs.</p>
<p>Recent studies have also utilized AR for quality control on automotive parts, eliminating the need for operators to constantly refer to static documentation, streamlining processes, and increasing efficiency. Tests have revealed that AR systems reduced execution time for complex quality control procedures by 36%. Other applications, based on a user-centered design, have showcased AR’s potential in assisting workers with design discrepancy detection. The AR tool’s efficiency is evidenced by the minimized cognitive load it places on users, as they must no longer split their attention between design data and physical prototypes.</p>
<p>Another study in the automotive domain developed an AR system to correct alignment errors during car body fitting, significantly expediting the process. The AR system provided real-time guidance, making the procedure almost four times quicker, with more consistent results regardless of operator experience. Future enhancements aim to further minimize setup times and implement artificial neural networks for error detection (<a href="https://www.mdpi.com/2076-3417/12/4/1961">https://www.mdpi.com/2076-3417/12/4/1961</a>).</p>
<p>In industrial AR settings, tracking<a id="_idIndexMarker729"/> techniques play a pivotal role in accurately determining the position and orientation of objects and devices, ensuring an immersive and interactive AR experience. Based on the provided information, the most used tracking techniques in these industrial scenarios are the following:</p>
<ul>
<li><strong class="bold">Marker-based tracking:</strong> Among <a id="_idIndexMarker730"/>the most prevalent tracking methods in the manufacturing context, marker-based tracking holds a dominant position. We discussed this technique in great detail in the <em class="italic">What are marker-based and markerless AR?</em> section of <a href="B20869_04.xhtml#_idTextAnchor011"><em class="italic">Chapter 4</em></a>. The primary advantages of marker-based tracking for industrial use cases are its speed, simplicity, and robustness. It has been extensively applied in scenarios such as facilitating AR instructional systems for assembly processes, assisting maintenance processes, guiding shop-floor operators, and supporting welding processes in the automotive industry. The method’s wide application can be attested by the fact that out of 200 selected research articles related to AR usage in an industrial setting, 63 articles (31.5%) focused on marker-based tracking, indicating its significance in AR-based manufacturing applications.</li>
<li><strong class="bold">Markerless tracking</strong>: Markerless <a id="_idIndexMarker731"/>tracking, another method that we discussed extensively in the <em class="italic">What are marker-based and markerless AR?</em> section of <a href="B20869_04.xhtml#_idTextAnchor011"><em class="italic">Chapter 4</em></a>, takes the second spot in terms of the dominance of frequently used AR tracking techniques in industrial settings. Examples of its application include using 3D point clouds of workstations for quality control procedures in the automotive industry, or integrating cloud databases for fault diagnosis.</li>
<li><strong class="bold">Hybrid tracking</strong>: An<a id="_idIndexMarker732"/> emergent trend, hybrid tracking combines the strengths of computer vision-based techniques, such as marker-based or markerless methods, with sensor-based approaches. By doing so, it achieves faster tracking speeds, reduces latency, and alleviates the computational burden inherent in markerless tracking algorithms. Incorporating additional sensor data can also enhance the performance of other tracking methods.</li>
<li><strong class="bold">Model-based tracking</strong>: Used <a id="_idIndexMarker733"/>when a 3D CAD model of<a id="_idIndexMarker734"/> the tracked object or part is available, this method analyzes and recognizes the pose and position of objects based on these models. Even though its popularity has been on the rise due to its integration into AR software development platforms such as Unity and <em class="italic">Vuforia</em>, it’s still less dominant than the other methods, with 13 out of 200 research articles (6.5%) implementing this technique.</li>
<li><strong class="bold">Sensor-based tracking</strong>: This <a id="_idIndexMarker735"/>method is relatively less favored in AR solutions for manufacturing, primarily because of the complexities and costs associated with deploying sensors, especially in indoor environments. Challenges include potential blockages of sensor signals by equipment, machines, or other objects, making it less effective (<a href="https://www.mdpi.com/2076-3417/12/4/1961">https://www.mdpi.com/2076-3417/12/4/1961</a>).</li>
</ul>
<p>While various tracking techniques cater to different AR applications in manufacturing, marker-based tracking remains the most favored due to its simplicity and robustness. However, as industrial settings evolve and technology continues to advance, we’re witnessing a gradual shift toward more complex yet versatile tracking techniques such as markerless and hybrid methods. If you’re interested in exploring the world of industrial XR, getting to know the pros and cons of these tracking techniques first-hand should be one of your main priorities moving forward.</p>
<p>A rising star in the industrial XR space <a id="_idIndexMarker736"/>called <em class="italic">RealWear</em> specializes in hands-free AR wearable solutions. Its AR wearable devices allow workers in fields such as energy and manufacturing to access crucial information while keeping their hands free for tasks (<a href="https://www.realwear.com/">https://www.realwear.com/</a>).</p>
<p><em class="italic">TeamViewer</em> is also betting <a id="_idIndexMarker737"/>on AR for industrial use cases. Its AR platform <em class="italic">TeamViewer Frontline</em> is designed to usher the deskless industrial workforce into the digital age. Leveraging state-of-the-art wearable technology, <em class="italic">Frontline</em> streamlines <a id="_idIndexMarker738"/>manual processes across industries, bridging the gap between digitally proficient office workers and hands-on industrial teams. Its suite of XR solutions offers seamless integration, extensive hardware compatibility, and user-friendly interfaces (<a href="https://www.teamviewer.com/en/products/frontline/">https://www.teamviewer.com/en/products/frontline/</a>).</p>
<p>Similarly, <em class="italic">PTC</em>’s Vuforia offers <a id="_idIndexMarker739"/>a comprehensive suite of AR solutions tailored for industrial use cases. From enabling service technicians to visualize and get insights about machinery using AR to assisting in design reviews and factory layout, Vuforia’s AR platform caters to a vast array of industrial needs.</p>
<p>As you can see, the industrial sector presents a fertile ground for XR developers that’s ripe with opportunities for innovation, disruption, and growth. The tangible benefits of XR, coupled with the trends in adoption and the challenges faced by industries, make it a lucrative domain for XR developers to focus their efforts on.</p>
<p>Another sector that benefits hugely from the integration of XR is healthcare.</p>
<h2 id="_idParaDest-174">Bridging the gap from real bodies to virtual worlds – XR in healthcare</h2>
<p>VR <a id="_idIndexMarker740"/>has been garnering attention for its potential in medical education. A comprehensive meta-analysis, spanning studies from 1990 to 2019, aimed to gauge VR’s efficacy in anatomy teaching. The study’s findings revealed that VR adoption in anatomy instruction improves students’ test results compared to traditional methods. The authors’ analysis underscores the potential of VR as a valuable tool in elevating students’ grasp of anatomy (<a href="https://bmcmededuc.biomedcentral.com/articles/10.1186/s12909-020-1994-z">https://bmcmededuc.biomedcentral.com/articles/10.1186/s12909-020-1994-z</a>).</p>
<p><em class="italic">UbiSim</em> is an interesting company in this domain. It provides nursing students with a fully virtual simulation lab, immersing them in environments where they engage with realistic patients. This tool aims to refine their patient engagement, clinical reasoning, and decision-making abilities, offering varied scenarios from emergency responses to childcare (<a href="https://www.ubisimvr.com/">https://www.ubisimvr.com/</a>). Other companies offering similar platforms for surgical VR trainings are <em class="italic">PeriopSim VR</em>, <em class="italic">Osso VR</em>, and <em class="italic">Oxford Medical Simulation</em>. The latter delivers comprehensive VR training for both the medical and nursing sectors. The platform’s content is available on regular computers or VR headsets. Emphasizing learner-centric methods, it offers realistic patient care scenarios and teamwork simulations. Several esteemed institutions such as Oxford, Manchester, and Edinburgh Universities have incorporated this platform into their curricula (<a href="https://careers.oxfordmedicalsimulation.com/">https://careers.oxfordmedicalsimulation.com/</a>).</p>
<p>Another big leap<a id="_idIndexMarker741"/> in XR technologies for healthcare is the use of VR for rehabilitation purposes. There’s been a surge in the number of individuals requiring rehabilitation, especially among older individuals and those with disabilities, chronic diseases, and functional and cognitive impairments. These individuals face challenges in movement, sensation, balance, and cognition, which significantly impact their quality of life, careers, and social engagements.</p>
<p>Traditional rehabilitation encounters issues such as stationary rehabilitation centers, resource scarcity, monotony in the training process, soaring treatment expenses, and a lack of self-driven encouragement and automatic guidance. This often leads to decreased confidence and diminished outcomes in the rehabilitation journey.</p>
<p>With VR, the rehabilitation process is being completely transformed. VR provides an immersive experience, motivating patients and enhancing their active involvement. This not only overcomes the stationary nature of traditional centers but also fills the gap of lacking resources. Moreover, VR-based systems paired with appropriate sensors can closely monitor and record patient movements and biological data, offering an avenue for refining and personalizing rehabilitation programs.</p>
<p>Contributions to this research domain stem from 63 countries and 1,921 institutes, highlighting the global attention and collaboration that VR rehabilitation is garnering. Research trends in VR rehabilitation are areas such as kinematics, neurorehabilitation, brain injury, exergames, aging, motor rehabilitation, and cerebral palsy, among others. This diversity underscores the broad applications and scope of VR in various rehabilitation sectors (<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10028519/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10028519/</a>).</p>
<p>An interesting company in this space that specializes in offering a wide range of different rehabilitation programs in VR, from<a id="_idIndexMarker742"/> chronic back pain to Parkinson’s disease, is <em class="italic">Penumbra</em> (<a href="https://www.realsystem.com/">https://www.realsystem.com/</a>).</p>
<p>VR development for surgical trainings, rehabilitation, or other fields of healthcare is not just an exciting intersection of healthcare and technology, but also an area that’s ripe for innovation, collaboration, and immense growth potential. Given the evident gaps and burgeoning demand, there’s a compelling reason for XR enthusiasts and developers like you to specialize in this field, driving the next wave of meaningful XR applications.</p>
<p>The idea of<a id="_idIndexMarker743"/> enriching the gaming industry with XR applications may not initially seem as profound as developing VR trainings for surgical procedures. However, as you’ll discover in the following section, XR holds the potential to enable entirely novel gaming experiences, thereby influencing this multi-billion-dollar industry in ways that we cannot fully predict at this moment.</p>
<h2 id="_idParaDest-175">Gaming across realities – XR trends in gaming</h2>
<p>Gaming has a<a id="_idIndexMarker744"/> historical symbiosis with XR; it is in the rich and diverse gaming landscapes that XR found its early applications and extensive testing grounds. The industry has continually pushed the boundaries, seeking to offer increasingly immersive, interactive, and realistic experiences, which naturally positioned it at the forefront of XR adoption and innovation.</p>
<p>Top-tier companies and gaming studios have been pivotal in nurturing this space. Renowned for its graphics processing units, <em class="italic">NVIDIA</em> is<a id="_idIndexMarker745"/> pivotal in the XR world. Its <em class="italic">RTX</em> series<a id="_idIndexMarker746"/> GPUs, designed with ray tracing capabilities, offer enhanced realism in VR environments. It has also worked on AI-driven advancements that improve XR experiences, such as <em class="italic">deep learning </em><em class="italic">super sampling</em>.</p>
<p>Sony introduced<a id="_idIndexMarker747"/> the <em class="italic">PlayStation VR</em>, a VR headset that integrates with its PlayStation gaming consoles. Games such as <em class="italic">Astro Bot Rescue Mission</em> and <em class="italic">Blood &amp; Truth</em> offer immersive VR experiences exclusive to the platform.</p>
<p>Renowned game studios such as <em class="italic">Ubisoft</em>, <em class="italic">EA Sports</em>, and <em class="italic">Epic Games</em> have also been proactive in infusing XR into their gaming ecosystems, thereby transforming the way we game today. They’ve partnered with other tech leaders to create XR-supported games that offer unprecedented levels of immersion and interactivity.</p>
<p>Apart from these tech and gaming giants, several start-ups and smaller entities are throwing their hats into the ring, eager to carve niches for themselves. Companies such as <em class="italic">Magic Leap</em> and <em class="italic">Niantic</em> (the name behind <em class="italic">Pokémon GO</em>) have been working tirelessly to build ARs that merge the physical and digital worlds in gaming scenarios. Other companies to watch include <em class="italic">Resolution Games</em>, a studio focusing on VR games and publisher of well-known VR games such as <em class="italic">Demeo</em>, and <em class="italic">Rec Room Inc.</em>, known for its VR social platform that integrates gaming elements.</p>
<p>For XR developers looking to immerse themselves in an arena rife with opportunity, the gaming industry presents a landscape fertile with potential. The industry offers a rich variety of genres and narratives to explore and innovate upon, from fantasy worlds to simulations grounded in reality.</p>
<p>Furthermore, with gaming being a multi-billion-dollar industry, it promises not only creative satisfaction but also substantial financial rewards. Developers have the chance to work on cutting-edge technology, pushing the boundaries of what is possible and shaping the future of entertainment.</p>
<p>Moreover, the <a id="_idIndexMarker748"/>gaming audience, traditionally receptive to new technologies and innovations, provides a ready market that is eager to embrace the novelties that XR can bring to the gaming space. This receptive audience can offer a particularly straightforward path to commercial success for well-conceived XR gaming products.</p>
<p>A research study explored the factors that influence people’s acceptance and use of VR games through an online survey involving 473 VR gamers. The researchers investigated the impact of hedonic and utilitarian aspects on VR gaming. Hedonic aspects refer to the pleasure and enjoyment derived from playing VR games. It encompasses aspects such as fun, enjoyment, and the immersive experience that these games offer. Utilitarian aspects relate to the practical benefits that can be derived from playing VR games, such as health and well-being improvements that some games can facilitate. The findings of this study showed that the hedonic aspects were a stronger driver for people playing VR games compared to the utilitarian aspects. Despite potential inconveniences such as physical discomfort and VR sickness, the researchers found that these factors did not significantly reduce the intention to use VR games or the level of immersion experienced by the players. The ease of using VR systems was found to be crucial in encouraging both the pleasure-driven (hedonic) and practical (utilitarian) usage of VR games (<a href="https://link.springer.com/article/10.1007/s10055-023-00749-4">https://link.springer.com/article/10.1007/s10055-023-00749-4</a>).</p>
<p>The results provide XR developers with important insights; understanding that players prioritize enjoyable and immersive experiences over practical benefits could guide the design process. Additionally, ensuring the system is user friendly can foster both enjoyment and practical utility. Thus, XR developers aiming to specialize in gaming should focus on creating immersive, fun, and easy-to-use experiences, while not being overly concerned with the potential physical discomfort and VR sickness that might be associated with VR gaming.</p>
<p>Another study aimed to understand player complaints about PC-based VR games by carrying out an empirical study of 750 PC-based VR games and 17,635 user reviews on the gaming platform <em class="italic">Steam</em>. It <a id="_idIndexMarker749"/>found that most newly released VR games support multiple headset categories and play areas. There has been an increase in support for smaller-scale <a id="_idIndexMarker750"/>play areas over time. Even though the median price for VR games has doubled, complaints about games being overpriced for their content have sharply decreased. Similarly, complaints about cybersickness are low and have been declining steadily over time. From 2018 onward, most of the complaints have shifted from VR comfort issues to game-specific problems (<a href="https://ieeexplore.ieee.org/document/9347709/">https://ieeexplore.ieee.org/document/9347709/</a>).</p>
<p>The findings suggest that the PC-based VR gaming market is becoming more sophisticated. Concerns traditionally centered on VR comfort, such as cybersickness, have lessened in prevalence. This means XR developers specializing in gaming should now prioritize enhancing game design and gameplay aspects, while ensuring VR comfort remains at its current standard or better.</p>
<p>A study using data<a id="_idIndexMarker751"/> from 515 Pokémon GO players found that nostalgic feelings about the Pokémon franchise fueled the players’ imagination of AR game content in the real world. This, in turn, nurtured affection toward the AR content, enhancing the sense of meaningfulness derived from playing. Similarly, social involvement, including community identification and social self-efficacy, elevated the meaningfulness of the gaming experience ( <a href="https://www.sciencedirect.com/science/article/pii/S0747563221001394#sec7">https://www.sciencedirect.com/science/article/pii/S0747563221001394#sec7</a>).</p>
<p>Researchers from another study developed <em class="italic">ARQuiz</em>, a game for public exhibition attendees of a Finnish science center. The ARQuiz game <a id="_idIndexMarker752"/>enabled visitors to engage in a virtual quiz related to the exhibits. During the exhibition visit, players used their smartphones to locate AR hints for answers to the quiz questions related to the nearby exhibits. In addition to the main quiz gameplay, users could interact socially by leaving messages for other players. Their study found that the AR application positively influenced the overall visitor experience at the public exhibition. Visitors who enjoyed ARQuiz also enjoyed the exhibition more, performed better in the quiz, and felt more sociable after the exhibition. ARQuiz was perceived as enjoyable and offered a different user experience compared to traditional exhibitions (<a href="https://ieeexplore.ieee.org/document/8861040">https://ieeexplore.ieee.org/document/8861040</a>).</p>
<p>The gaming industry emerges as yet another compelling avenue for XR developers. Regardless of the specific industry you intend to target with XR applications, there exist certain essential best practices and design principles that your XR applications should consistently follow. These will be presented in the upcoming section.</p>
<h1 id="_idParaDest-176">Learning the best practices for XR development</h1>
<p>This <a id="_idIndexMarker753"/>section <a id="_idIndexMarker754"/>will provide you with best practices and design patterns for all your XR projects, regardless of the industry you are creating them for. With this knowledge, you will be able to make the right hardware, software, and design decisions for each of your XR projects. Let’s start by exploring the hardware considerations you should keep in mind at the beginning of your XR projects.</p>
<h2 id="_idParaDest-177">Hardware considerations in XR development</h2>
<p>In the realm<a id="_idIndexMarker755"/> of XR, hardware is more than just a vessel for experiences; it’s a determinant of their quality. The most powerful VR device isn’t universally ideal for every XR application, much like how buying a supercomputer <a id="_idIndexMarker756"/>would be overkill for the sake of creating a PowerPoint presentation. Consider a fair scenario: would you prefer a lengthy VR setup with external sensors, demanding PC requirements, and full-body tracking to show off a VR representation of a solar panel? Or would standalone VR devices such as those from Meta or PICO, known for their user-friendly operation, be more adequate for this task?</p>
<p>The right hardware strikes a harmony of performance, adaptability, and ease of use. Some pivotal factors to consider are the device type, performance, and testing. Let’s have a closer look at different device types and their strengths in the next section.</p>
<h3>XR device types and their strengths</h3>
<p>Understanding the<a id="_idIndexMarker757"/> constraints of XR devices is essential for XR developers aiming to craft immersive and smooth experiences. Each XR device, whether it’s a smartphone, AR glasses, or a dedicated VR headset, is tailored for different scenarios based on its computational power.</p>
<p>For instance, a smartphone, with its versatility and widespread use, is ideal for AR applications such as Pokémon GO or AR-driven shopping experiences where the user can visualize products in their real environment. AR glasses, such as Microsoft’s HoloLens, are more suitable for specialized tasks such as hands-free work instructions or architectural visualizations. In contrast, dedicated VR headsets, such as the Oculus Quest or the <em class="italic">HTC Vive</em>, are designed for deep, immersive experiences where the user is transported into<a id="_idIndexMarker758"/> an entirely virtual world. The following are some of the most common use cases that suit the different types of XR hardware:</p>
<ul>
<li>VR headsets:<ul><li>Fully immersive gaming experiences with a 360-degree field of view</li><li>Simulation training where users can practice tasks such as surgeries or navigating airplanes in safe virtual spaces</li><li>Experiencing places and historic sites without leaving home</li><li>Architectural visualization to walk through your future home before it’s built</li><li>Virtual offices where global teams can collaborate in real time with avatars</li></ul></li>
<li>AR glasses:<ul><li>Step-by-step overlay guidance for complex tasks</li><li>Real-time directions overlay on the real world, combined in an industrial setting where users should move hands-free</li><li>Surgeons receiving overlays of important info while operating without the need to move their heads constantly between the patient and a screen</li><li>Technicians getting schematics or guidance overlaid on machinery they’re fixing</li></ul></li>
<li>Smartphones (AR):<ul><li>Gaming experiences such as Pokémon GO<em class="italic"> </em>where players interact with real-world locations</li><li>Previewing furniture or clothing items in your space before buying</li><li>AR-driven walking or driving directions such as the ones offered by Google Maps</li><li>Providing tourists with historical info on landmarks or interactive learning modules</li><li>Augmenting video calls or video content with digital enhancements</li><li>Real-time guidance on tasks such as home repairs</li></ul></li>
</ul>
<p>Once you have<a id="_idIndexMarker759"/> chosen the right hardware for your XR project, you will need to be aware of the performance challenges that are common in this field. Let’s go through them in the upcoming section.</p>
<h3>Performance challenges of XR devices</h3>
<p>Creating <a id="_idIndexMarker760"/>XR applications that depict detailed 3D environments, such as architectural visualizations in VR, requires addressing performance challenges. Older devices or entry-level VR headsets with limited computational capabilities may not render these details smoothly, resulting in lags or visual disturbances. Similarly, an AR application loaded with high-resolution textures and complex 3D models may run effortlessly on advanced devices such as the latest iPhone, but might falter on older ones due to memory constraints.</p>
<p>The preliminary step in creating an XR application is to assess the 3D models you’re incorporating. Unity, for example, provides information on the polygon or triangle count of an imported 3D model. Typically, a 3D model’s polygon often equates to a triangle consisting of three vertices. Thus, a model’s intricacy directly relates to its polygon count, which becomes pivotal for devices with limited processing capabilities such as standalone VR headsets or smartphones.</p>
<p>Referencing documentation provides an understanding of device-specific polygon count limitations. For example, the Oculus Quest 1 documentation suggests a triangle count cap of 350,000-500,000, while Quest 2 can handle 750,000 to 1 million polygons (<a href="https://developer.oculus.com/documentation/unity/unity-perf/">https://developer.oculus.com/documentation/unity/unity-perf/</a>). Note that this pertains to the cumulative polygons rendered in a scene, not just an individual model. Hence, for optimal performance, the aggregate polygon count of all entities in a scene shouldn’t surpass this figure, though exceptions exist based on other optimized factors such as shaders or physics.</p>
<p>Suppose <a id="_idIndexMarker761"/>a client wants to showcase five high-resolution car prototypes in VR using the standalone mode of the Quest 2 headset. However, each car model has 1 million polygons, so the total number of polygons for the five models would be 5 million, which exceeds Quest 2’s recommended limit of 2 million polygons. There are a few things that developers can do to address this issue:</p>
<ul>
<li><strong class="bold">Use PC VR mode</strong>: Opt to run Quest 2 in PC VR mode paired with a robust computer, sidestepping the need for optimizations. The reasoning behind this is that PCs possess greater computational prowess, eliminating the typical hardware constraints of standalone VR headsets.</li>
<li><strong class="bold">Utilize mesh decimation</strong>: Mesh decimation is a powerful technique to significantly diminish a model’s polygon count without notably downgrading its visual appeal. Implementing this method requires proficiency in 3D modeling software and an understanding of the underlying geometry of the models being simplified. There’s no hard limit, but the key is to strike a balance: reducing polygons effectively while ensuring the model retains its essential characteristics and aesthetic value.<p class="list-inset">To learn more about mesh decimation, please refer to this insightful blog post: <a href="https://odgy.medium.com/mesh-decimation-done-right-95245c4b5f52">https://odgy.medium.com/mesh-decimation-done-right-95245c4b5f52</a></p></li>
<li><strong class="bold">Optimization techniques</strong>: Although standalone VR headsets can accommodate scenes exceeding their recommended polygon count, achieving this demands numerous optimizations. Adopting straightforward shaders, implementing baked lighting, and opting for reduced texture resolution are a few methods of doing so. However, from our experience, it’s advisable to stay within a million polygons over the stipulated limit for such optimizations.</li>
</ul>
<p>Lastly, you should continuously test and assess your XR application throughout its entire development process to make sure it is in line with your hardware’s capabilities. Tools such as Unity’s <strong class="bold">Profiler</strong> serve as valuable aids. In the next section, you will learn how you can use it.</p>
<h3>Testing and profiling the application</h3>
<p>Unity’s Profiler is <a id="_idIndexMarker762"/>an invaluable tool for diagnosing performance issues and optimizing your VR experiences. The Profiler provides real-time data about your application, capturing details on CPU, GPU, memory usage, rendering, physics, and more. Implementing and using the Profiler within a VR experience is similar to using it for any other Unity project, with some VR-specific considerations. The following is a step-by-step guide <a id="_idIndexMarker763"/>on how to implement Unity’s Profiler for your VR experience:</p>
<ol>
<li>Open your project in Unity. Go to <strong class="bold">Window</strong> | <strong class="bold">Analysis</strong> | <strong class="bold">Profiler</strong>, or simply press <em class="italic">Ctrl/Cmd</em> + <em class="italic">7</em> to open the <strong class="bold">Profiler</strong> window. If you are using a standalone headset, ensure that both your PC and the headset are on the same network.</li>
<li>If you have a PC-based VR headset, simply click the <strong class="bold">Play</strong> button in Unity to start your VR experience. While your VR experience is running, the <strong class="bold">Profiler</strong> window will update in real time, showing data captured from your application. For standalone VR headsets, you must first build and deploy your application to your VR device.</li>
<li>In Unity’s <strong class="bold">Profiler</strong> window, click on the <strong class="bold">Active Profiler</strong> dropdown. You should see your VR device listed there (given it’s on the same network). If not, ensure Unity and your device can communicate over your network. Select your VR device from the list. Unity will start profiling directly from the device, and the <strong class="bold">Profiler</strong> window will update in real time.</li>
</ol>
<p>While many of the metrics in the Profiler are applicable to all Unity applications, with VR, you should pay special attention to the following:</p>
<ul>
<li><strong class="bold">Frame time</strong>: Ensure you’re consistently hitting the frame rate required for your VR headset (e.g., 90 Hz for Oculus Rift, 72 Hz for Oculus Quest). Missing frame rates can lead to motion sickness in VR. You will find the recommended frame rate in the documentation of your VR hardware.</li>
<li><strong class="bold">Render thread</strong>: VR requires rendering two views (one for each eye), which can be more demanding. Look for any spikes or bottlenecks here.</li>
<li><strong class="bold">GPU</strong>: High-quality shaders and post-processing effects can be especially demanding in VR due to the need to render them twice (for each eye). Always test their impact in VR.</li>
</ul>
<p>Once you’ve <a id="_idIndexMarker764"/>identified areas where your VR experience might be encountering performance hitches, you can dive deeper into the specific sections (such as <strong class="bold">CPU</strong>, <strong class="bold">GPU</strong>, and <strong class="bold">Memory</strong>) in the Profiler to get a detailed performance breakdown. Look for any unexpected spikes or prolonged high usage. These could be indicators of potential issues such as inefficient scripts, too many active physics objects, or overly detailed models. Make adjustments in your project based on your findings. For instance, simplify complex meshes, optimize shaders, and reduce the number of active physics objects. After making changes, test your VR experience again to see whether the issue has been resolved and to ensure no new issues have cropped up.</p>
<p class="callout-heading">Tips</p>
<p class="callout">These three important aspects are often forgotten when profiling XR applications in Unity:</p>
<p class="callout">1. Unity has a <em class="italic">Deep Profiling</em> mode that provides more detailed information, but it comes at a performance cost. Use it when you need an in-depth analysis.</p>
<p class="callout">2. Remember to turn off any debug logs in the final build as they can impact performance.</p>
<p class="callout">3. For VR, always strive for consistent and smooth performance. Even minor hitches can be disorienting and uncomfortable for users.</p>
<p>By using the Profiler regularly throughout your VR development process, you can ensure that your applications run smoothly and provide the best possible experience for your users. However, a smooth-running experience does not guarantee that users will enjoy your app. Let’s have a closer look at the most important aspect to focus on before designing your app: your audience.</p>
<h2 id="_idParaDest-178">Understanding your audience</h2>
<p>One of the <a id="_idIndexMarker765"/>foundational steps in crafting a successful XR application is understanding who will be using it. Just as a carpenter wouldn’t start building without a blueprint, a developer shouldn’t start creating without a clear picture of their audience. This section will guide you through the process of identifying and understanding your target users.</p>
<p>Before you dive into the development of your VR application, you need to ask: “Who is this for?” The answer will influence almost every other decision you make. For instance, a VR application for children might prioritize colorful graphics and simplicity, whereas one for professionals in a given industry might focus on functionality and data visualization. Consider age, gender, education, and occupation. A VR game targeted at teenagers will have different design elements and challenges than one aimed at adults.</p>
<p>Another important question to answer in this context is whether your audience is familiar with VR. If not, your application might need to incorporate introductory tutorials or more intuitive controls. A tech-savvy user might crave advanced features and customizations, while others might want a more streamlined, plug-and-play experience. By identifying these user needs, you can ensure that your application will be relevant, user friendly, and appealing to your audience.</p>
<p>It’s always wise to assess the landscape before planting your flag. Look at the VR applications currently available that serve a similar purpose or target audience to yours. Identify what’s missing in current offerings. Is there a feature or an experience that users are asking for in the reviews but isn’t yet available? Staying with user reviews, the insights you will receive just by reading through reviews of similar applications are invaluable. Users will often detail what they love, what they hate, and what they wish was included.</p>
<p>Creating user personas is another common method in product design, and its importance is magnified in the world of VR, where experiences are so personal and immersive. A user persona is a fictional character that represents a segment of your target audience. They come with a name, a background, preferences, and challenges. Personas help make abstract user needs more tangible. When making decisions, you can ask: “Would this feature benefit Anna, the tech-savvy college student who loves gaming?” or “Would this interface be intuitive for Raj, the middle-aged architect who’s new to VR?” Incorporating user personas into your planning phase can be the difference between a VR application that feels generic and one that feels tailor-made for the user.</p>
<p>Understanding your audience isn’t just the first step in VR application development; it’s a compass that should guide every subsequent decision. Making the effort to truly understand<a id="_idIndexMarker766"/> your users ensures that the final product will not only meet but exceed their expectations. After you’ve understood your audience, you must also ensure that you can cater to its needs. No matter whether you’re working alone or in a team, efficient project management is crucial to completing your XR application in a reasonable time and manner. Let’s dive deeper into this topic in the next section.</p>
<h2 id="_idParaDest-179">Efficient project management for XR</h2>
<p>Every great<a id="_idIndexMarker767"/> creation begins with an intention, a clear vision of what one hopes to achieve. Setting tangible goals ensures that the development process remains focused and the end product aligns with the envisioned purpose. This section sheds light on how to set effective goals, prioritize features, and determine the criteria for success.</p>
<p>Just as a sailor wouldn’t set out without a destination in mind, developers should have a clear objective for their VR application. Begin by asking: “Why are we building this?” Are you looking to entertain, educate, train, or provide a unique service? Pinpointing the purpose will shape the app’s design, content, and user experience. Once you can answer this question, determine the boundaries of your application. If it’s an educational VR app, will it cover one subject or offer a broad curriculum? Setting the scope ensures your efforts remain targeted and manageable. In this context, you should also specify what the target user hopes to achieve with your application. Whether it’s mastering a new skill, being entertained, or solving a specific problem, understanding this can help refine your objectives.</p>
<p>With all of these questions settled and an array of possible features to integrate, you need a strategy to prioritize. The following are some important considerations you should make:</p>
<ul>
<li>List out all the features you envision for your application. Then, categorize them into essential functions and those that are additional enhancements. This helps in focusing on what’s crucial first.</li>
<li>Use your user personas or early feedback to determine which features are most desired by your audience.</li>
<li>Be realistic about time, budget, and technical constraints. Some features might be fantastic on paper but could be resource-intensive to implement. Keep in mind the number of people and the competencies within your team.</li>
</ul>
<p>Defining success <a id="_idIndexMarker768"/>is as crucial as setting the initial goals. How will you know whether your VR application meets, or hopefully exceeds, expectations? For this, we recommend you establish success metrics right at the start of your XR endeavor that you can evaluate post-launch, as well as during user testing. Track metrics such as the average session duration, frequency of use, and user progression within the application to gauge engagement levels. Likewise, monitor crash rates, glitches, and other performance-related issues. A technically sound application is key to retaining users. Use your metrics not just as a report card but as a roadmap for future updates, refinements, and even potential expansions.</p>
<p>For your XR endeavors to succeed, you should also be familiar with some essential techniques and tools for managing your projects in XR. Let’s go through them step by step:</p>
<ul>
<li><strong class="bold">Mind mapping and brainstorming sessions</strong>: These enable you to harness collective intelligence, propelling the clarity and creativity of your concept. For these sessions, assemble a diverse group of people, combining technical developers, designers, and even potential end users. You should do this even if you are a solo developer. Their combined insights can lead to enriched ideas. For these sessions, we recommend you use tools such as <em class="italic">Miro</em> or <em class="italic">Lucidchart</em>.</li>
<li><strong class="bold">Game design documentation (GDD)</strong>: A GDD acts as a project’s blueprint, weaving together the narrative, mechanics, interactions, and level designs. Rather than just sketching a user’s journey, as with traditional storyboarding, a GDD delves deeper. It outlines the intricacies of user interactions in the XR environment, lists mechanics, sets the governing rules, maps out the challenges within each level, and ensures a seamless user experience by detailing menus. This holistic approach streamlines development and sets the stage for a successful project.</li>
<li><strong class="bold">Creating 3D wireframes</strong>: While traditional wireframing tools assist in plotting out digital interfaces, XR demands a more 3D approach. Tools such as <em class="italic">Sketchbox</em> and <em class="italic">Gravity Sketch</em> let developers and designers craft 3D wireframes, allowing a spatial understanding of the XR environment.</li>
<li><strong class="bold">Managing your code base</strong>: Once the project is planned, it needs to be managed. In particular, managing code base changes in complex XR projects can be a daunting task. <strong class="bold">Git</strong>, a <a id="_idIndexMarker769"/>distributed version control system, will become your daily companion in such scenarios. Git allows developers to work simultaneously on different features or bugs without affecting the main code base. Once a feature or <a id="_idIndexMarker770"/>fix is ready, it can be merged back into the main branch. Every change, every merge, and every version is tracked. This means if something goes wrong, you can revert to a previous state of your application easily. Git provides a platform where multiple developers can contribute without stepping on each other’s toes. Each contributor can work on their local copy and then push changes to a shared repository.<p class="list-inset">Both <em class="italic">GitLab</em> and <em class="italic">GitHub</em> offer platforms that extend the capabilities of Git, making the collaborative process even smoother. Beyond just version control, these platforms offer a suite of tools such as code reviews, continuous integration, and continuous deployment, which streamline the development process. You can also report, track, and assign bugs or features with them. This keeps the team aligned on priorities and ensures that issues are resolved methodically. Both tools also come <a id="_idIndexMarker771"/>with <strong class="bold">kanban boards</strong>. A kanban board is a visual tool to manage tasks and workflows. As tasks move from one stage to another, such as from <strong class="bold">To Do</strong> to <strong class="bold">In Progress</strong> or <strong class="bold">Completed</strong>, they can be dragged across columns on the board. This provides a quick overview of project progress and bottlenecks and is perfectly suited for agile projects.</p></li>
</ul>
<p>Planning an XR application is a multifaceted process, blending traditional techniques with XR-specific tools. This integration ensures that the foundation of the XR application is robust, clear, and primed for successful development. Whether you’re sketching out the user’s journey or building a 3D prototype, each planning stage is a step closer to realizing a compelling and impactful XR experience.</p>
<p>In the next section, we are focusing on best practices that ensure a good user experience.</p>
<h2 id="_idParaDest-180">Ensuring a good user experience</h2>
<p>Ensuring a good <a id="_idIndexMarker772"/>user experience in XR applications goes beyond just graphics and performance; it delves deep into intuitive design and user interaction. From grasping the basics of XR UI/UX design to creating inclusive interfaces and understanding the importance of feedback, every aspect plays a pivotal role in defining a seamless and immersive experience for the user.</p>
<p>Traditional UI/UX, whether for web or mobile apps, focuses on the spatial relation on a flat screen. Designers ask: “How does one element look next to another?” In XR, the questions evolve. The spatial relation is not just side-to-side, but also in-depth. It’s about how one element exists in relation to another within a 3D space.</p>
<p>XR environments also introduce challenges not found in conventional design. For instance, how do you ensure that a user can comfortably read text in VR? Or in AR, how do you design an interface that complements, rather than clashes with, the real world?</p>
<p>As XR has matured, certain patterns and guidelines have emerged that aid in creating intuitive and immersive experiences. Just as a hamburger icon signifies a menu in traditional apps, certain gestures or symbols are becoming standardized in XR. For instance, a hand pinch in VR might signify selection, while a swipe in AR might rotate an object. Physical feedback, such as a controller’s vibration in response to an action, can significantly reinforce a user’s actions within the XR environment. No matter what kind of XR application you are developing, a good rule of thumb is the following: leverage familiar actions from the real world (such as grabbing or throwing) to make virtual interactions in your XR application instantly understandable.</p>
<p>Similarly, XR interfaces need to be designed with user comfort at the forefront. Elements should be within a comfortable reach and view, ensuring users don’t need to make awkward movements or strain their eyes. This also involves considering ergonomics, ensuring that interactions don’t lead to physical strain over extended periods. There’s a temptation to include a plethora of interactions and details in XR applications, but this can often lead to confusion. Clarity and simplicity should be the watchwords. If an interface element or interaction doesn’t serve a clear purpose, it’s worth reconsidering its inclusion.</p>
<p>With the digital revolution, one would assume that accessibility barriers have been significantly reduced. However, without careful design, XR can inadvertently introduce new ones. It’s essential that XR applications are not just for the many but for all. There are three<a id="_idIndexMarker773"/> important things to keep in mind for any of your future XR applications:</p>
<ul>
<li>Not all users interact with XR in the same way. Some might have visual impairments, others could have auditory challenges, and yet others might face mobility restrictions. For instance, subtitled audio can aid those with hearing impairments, while voice commands can help those with mobility challenges.</li>
<li>One-size-fits-all rarely works in design, especially in XR. Providing users with the ability to adjust settings to their comfort can make a massive difference. This includes scaling the UI for those who might find default sizes hard to read or adjusting brightness for those sensitive to intense light. Audio levels, especially in VR, can be adjusted to ensure users can comfortably immerse themselves in the environment.</li>
<li>Inclusivity in XR is not just about adding features; it’s about adopting a mindset. When the design process starts with considering all potential users, regardless of their challenges, it leads to a product that’s not only more robust but also more universally enjoyable.</li>
</ul>
<p>The potential of XR is boundless. However, this potential can only be realized when designers place users at the heart of the design process. By focusing on intuitive interactions and championing inclusivity, XR applications can offer transformative experiences for everyone.</p>
<p>Another important aspect to ensure a good user experience is the gathering of user feedback. The development of any XR application is only as good as the feedback it receives. In XR, where users are navigating complex 3D environments and engaging in novel interactions, understanding their experiences is crucial. There are a couple of possible ways in which to collect user feedback:</p>
<ul>
<li><strong class="bold">Playtesting</strong>: This is one of the most direct ways to gather insights. By observing users as they navigate through an XR experience, you can identify pain points, moments of confusion, or elements that work exceptionally well. It provides a firsthand look into how users engage with your application, making it invaluable for XR development.</li>
<li><strong class="bold">Surveys and questionnaires</strong>: These can be used post-experience to gather structured feedback. They can focus on specific aspects of the XR application, such as the intuitiveness of interactions, visual aesthetics, or overall user satisfaction. Tailored questions can extract deep insights into particular elements, while open-ended questions can provide unexpected but valuable perspectives.</li>
<li><strong class="bold">Spontaneous feedback</strong>: Beyond structured playtests, sometimes just allowing users to interact with an XR application and share their thoughts spontaneously can yield the most candid feedback. These sessions can be more free-form, letting users explore at their own pace, with developers noting reactions, comments, and behaviors.</li>
</ul>
<p>Not all <a id="_idIndexMarker774"/>feedback will be actionable, and some might even be contradictory. The key is to look for patterns and consistent pain points or praises across users. Tools such as heat maps in combination with eye-tracking, to visually represent where users look or interact the most, can be invaluable. Likewise, feedback about physical discomfort, confusion, or moments of delight should be given priority.</p>
<p>Based on such analyses, your XR experience should undergo adjustments. This could mean redesigning certain UI elements, optimizing performance, or even introducing new features. The goal is to enhance what works and fix or eliminate what doesn’t.</p>
<p>To avoid criticism on interactions not feeling natural or intuitive, you should focus on choosing common input configurations from the beginning. Let’s discuss them in the next section.</p>
<h2 id="_idParaDest-181">Input devices</h2>
<p>In the realm<a id="_idIndexMarker775"/> of XR, the mode of interaction is a <a id="_idIndexMarker776"/>pivotal aspect that can significantly influence user immersion and engagement. Modern XR experiences offer a spectrum of interaction modalities, from controllers to direct hand-tracking and gesture recognition. As developers, ensuring seamless, intuitive, and robust interactions is paramount. Let’s delve into the best practices for these XR interaction methods by starting with VR controller input configurations.</p>
<h3>VR controller input configuration</h3>
<p>In the <a id="_idIndexMarker777"/>dynamic world of VR, the way the user interacts with the environment can significantly shape their overall experience. VR controllers are central to this interaction, and there are general conventions that many developers follow to ensure a consistent and intuitive user experience. The following is a look at common VR controller inputs and their frequently associated actions:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-1">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold" lang="en-US" xml:lang="en-US">VR </strong><strong class="bold" lang="en-US" xml:lang="en-US">controller input</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold" lang="en-US" xml:lang="en-US">Primary actions</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold" lang="en-US" xml:lang="en-US">Secondary actions</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Trigger button</p>
</td>
<td class="No-Table-Style">
<p>Grabbing or picking up objects</p>
</td>
<td class="No-Table-Style">
<p>Shooting or activating a highlighted item</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Grip button</p>
</td>
<td class="No-Table-Style">
<p>Grasping or holding objects, mimicking the action of closing one’s hand</p>
</td>
<td class="No-Table-Style">
<p>Object manipulation, such as scaling or rotating</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Thumbstick/touchpad</p>
</td>
<td class="No-Table-Style">
<p>Navigation, usually through teleportation or smooth locomotion</p>
</td>
<td class="No-Table-Style">
<p>Rotating the user’s view, scrolling through menus, or adjusting settings</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Face buttons (A, B, X, Y, or equivalent)</p>
</td>
<td class="No-Table-Style">
<p>In-game actions, such as jumping, interacting, or accessing menus</p>
</td>
<td class="No-Table-Style">
<p>Secondary modes or tools within the VR experience</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Menu or system button</p>
</td>
<td class="No-Table-Style">
<p>Accessing in-game menus, pausing, or pulling up system settings</p>
</td>
<td class="No-Table-Style">
<p>Often a shortcut to calibrate or reset the VR view</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Haptic feedback</p>
</td>
<td class="No-Table-Style">
<p>While not a button, haptic feedback provides tactile responses to the user, signaling successful interactions, collisions, or other in-game events</p>
</td>
<td class="No-Table-Style"/>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 9.1 – The actions associated with different VR controller inputs</p>
<p>In the next section, we will dive into common hand gestures in VR.</p>
<h3>Hand gesture recognition in VR</h3>
<p>As VR technology<a id="_idIndexMarker778"/> has evolved, hand-tracking and gesture recognition have emerged as powerful tools for a more natural and intuitive user interaction. Unlike traditional controller inputs, hand gestures utilize the nuances of human hand movement, creating a more immersive and direct interaction within the virtual environment. The following is a breakdown of commonly recognized hand gestures and their typical VR actions:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table002">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold" lang="en-US" xml:lang="en-US">Hand gesture</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold" lang="en-US" xml:lang="en-US">Primary actions</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold" lang="en-US" xml:lang="en-US">Secondary actions</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Open hand</p>
</td>
<td class="No-Table-Style">
<p>Often signifies a relaxed state or neutral position</p>
</td>
<td class="No-Table-Style">
<p>Hovering over objects or menu items</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Closed fist</p>
</td>
<td class="No-Table-Style">
<p>Grasping, holding, or picking up objects; mirrors the action of clenching one’s hand</p>
</td>
<td class="No-Table-Style">
<p>Can sometimes trigger a punch or hit, especially in combat-oriented experiences</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Pinch (thumb and index finger)</p>
</td>
<td class="No-Table-Style">
<p>Selecting or interacting with a specific item or UI element</p>
</td>
<td class="No-Table-Style">
<p>Fine-tuning or adjusting objects, such as resizing or rotating</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Point (extended index finger)</p>
</td>
<td class="No-Table-Style">
<p>Pointing to or highlighting specific items or areas</p>
</td>
<td class="No-Table-Style">
<p>Initiating teleportation or drawing attention in social VR settings</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Thumbs up</p>
</td>
<td class="No-Table-Style">
<p>Often represents affirmation, agreement, or a positive response within social VR contexts</p>
</td>
<td class="No-Table-Style">
<p>Can sometimes trigger specific in-game actions or emotes</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Palm facing outward</p>
</td>
<td class="No-Table-Style" colspan="2">
<p>Stopping or blocking, particularly in narrative-driven experiences</p>
<p>Pushing objects away or activating barriers</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Swipe (horizontal or vertical)</p>
</td>
<td class="No-Table-Style">
<p>Navigating through menus, scrolling, or changing views</p>
</td>
<td class="No-Table-Style">
<p>Can also be employed in certain games for actions such as slashing</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Rotation (twisting hand from side to side)</p>
</td>
<td class="No-Table-Style">
<p>Rotating objects or adjusting settings, such as volume or brightness</p>
</td>
<td class="No-Table-Style">
<p>Might be used to change perspective or view in some applications</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 9.2 – The actions associated with different hand gestures</p>
<p>To round off <a id="_idIndexMarker779"/>this topic, let’s have a look at common input methods for mobile AR applications.</p>
<h3>Mobile AR input methods</h3>
<p>Mobile AR <a id="_idIndexMarker780"/>has opened a world of possibilities by blending the digital realm with our physical environment, all through the lens of a mobile device. Interactivity in mobile AR varies from traditional VR since it often doesn’t involve specialized controllers or hand-tracking hardware. Instead, it capitalizes on the existing capabilities of smartphones and tablets. The following is a breakdown of commonly utilized mobile AR inputs and their standard actions:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table003">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold" lang="en-US" xml:lang="en-US">Mobile </strong><strong class="bold" lang="en-US" xml:lang="en-US">AR input</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold" lang="en-US" xml:lang="en-US">Primary actions</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold" lang="en-US" xml:lang="en-US">Secondary actions</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Touch (single tap)</p>
</td>
<td class="No-Table-Style">
<p>Selecting or interacting with an AR object or UI element</p>
</td>
<td class="No-Table-Style">
<p>Triggering animations, accessing information, or playing videos</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Touch (double tap)</p>
</td>
<td class="No-Table-Style">
<p>Resetting or recentering an AR experience</p>
</td>
<td class="No-Table-Style">
<p>Toggling between AR modes or views</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Touch (long press)</p>
</td>
<td class="No-Table-Style">
<p>Initiating a drag or move function for AR objects</p>
</td>
<td class="No-Table-Style">
<p>Can also activate context-specific menus or options</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Pinch and zoom</p>
</td>
<td class="No-Table-Style">
<p>Scaling AR objects, making them larger or smaller</p>
</td>
<td class="No-Table-Style">
<p>Zooming into detailed information or images</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Swipe</p>
</td>
<td class="No-Table-Style">
<p>Rotating AR objects or navigating through AR menus or slides</p>
</td>
<td class="No-Table-Style">
<p>Dismissing AR elements or accessing additional content</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Device movement (tilting or panning)</p>
</td>
<td class="No-Table-Style">
<p>Exploring the AR environment, changing the view, or influencing AR object behavior</p>
</td>
<td class="No-Table-Style">
<p>Can also be employed in games or experiences to control avatars or vehicles</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Camera feed</p>
</td>
<td class="No-Table-Style">
<p>Scanning or recognizing markers, patterns, or objects to initiate AR overlays</p>
</td>
<td class="No-Table-Style">
<p>Capturing images or videos with AR elements superimposed</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Voice commands</p>
</td>
<td class="No-Table-Style">
<p>Controlling the AR experience or interacting with AR objects through speech</p>
</td>
<td class="No-Table-Style">
<p>Searching for information, initiating calls, or accessing device functions</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Gyroscope and accelerometer</p>
</td>
<td class="No-Table-Style">
<p>Detecting device orientation and movement, which can influence the AR content’s positioning and behavior</p>
</td>
<td class="No-Table-Style">
<p>Used in games or simulations to steer, navigate, or control AR elements based on device tilt</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 9.3 – The actions associated with different mobile AR inputs</p>
<p>Having <a id="_idIndexMarker781"/>delved into the best practices in the XR space, the next section of this chapter explores some useful toolkits and plugins that you might want to consider for your upcoming XR endeavors.</p>
<h1 id="_idParaDest-182">Other useful toolkits and plugins for XR development</h1>
<p>After learning about the trends<a id="_idIndexMarker782"/> and best practices of XR development in<a id="_idIndexMarker783"/> this chapter, you’re likely already formulating an idea about which area of XR development you’re passionate about. As you venture further into your chosen specialization, this section will guide you by offering a detailed look at valuable XR toolkits and plugins for Unity. While we’ve already <a id="_idIndexMarker784"/>discussed <a id="_idIndexMarker785"/>the XR Interaction Toolkit, AR Foundation, ARKit, and ARCore, there’s a plethora of other tools out there. Some of these toolkits are tailored for specific projects, while others might become staple tools for a wide range of your XR endeavors, depending on the direction you take.</p>
<h2 id="_idParaDest-183">Exploring powerful AR toolkits from the Unity Asset Store</h2>
<p>Within the <a id="_idIndexMarker786"/>expansive <strong class="bold">Unity Asset Store</strong>, there <a id="_idIndexMarker787"/>exists a multitude of AR toolkits, each bringing its own unique capabilities to the table. This chapter will introduce you to four powerful AR toolkits and help you discern the best fit for your project needs. One of these toolkits is the <strong class="bold">Vuforia Engine</strong>.</p>
<h3>Vuforia Engine, the AR pioneer</h3>
<p>Vuforia Engine<a id="_idIndexMarker788"/> stands as one of the <a id="_idIndexMarker789"/>most widely used AR platforms globally, renowned for its robust capabilities in developing cross-platform AR applications. It targets both handheld devices and digital eyewear and offers advanced computer vision capabilities, including recognition of images, objects, and spaces.</p>
<p>Powering over 80,000 apps, Vuforia Engine boasts a clientele that includes big names such as <em class="italic">EA</em>, <em class="italic">Square Enix</em>, <em class="italic">LEGO</em>, and <em class="italic">Activision</em>. If giants in the industry are putting their trust in Vuforia, there’s a compelling reason to consider it for your AR endeavors. If you are looking to find a job in the field of AR, gaining experience with the Vuforia Engine should be at the top of your list once you start working on further AR projects beyond this book. From adding Vuforia Engine to your Unity project and setting up <strong class="bold">Image Targets</strong> to <a id="_idIndexMarker790"/>testing your AR app, every step is detailed in its documentation (<a href="https://library.vuforia.com/getting-started/getting-started-vuforia-engine-unity">https://library.vuforia.com/getting-started/getting-started-vuforia-engine-unity</a>).</p>
<p>You can find the toolkit by searching for it on the Unity Asset Store or by clicking on the following link: <a href="https://assetstore.unity.com/packages/templates/packs/vuforia-engine-163598">https://assetstore.unity.com/packages/templates/packs/vuforia-engine-163598</a></p>
<p>Another non-negotiable toolkit for anyone who’s serious about AR development is <strong class="bold">AR Foundation Remote 2.0</strong>, which facilitates testing AR applications on smartphones a lot. You will learn more about it in the next section.</p>
<h3>AR Foundation Remote 2.0 toolkit, the AR development life cycle companion</h3>
<p>Alongside<a id="_idIndexMarker791"/> the <a id="_idIndexMarker792"/>Vuforia Engine, the AR Foundation Remote 2.0 toolkit is one of those tools you should familiarize yourself with if you want to venture further into the world of AR development. This tool will dramatically transform your AR development journey as it allows for rapid iteration and an efficient workflow.</p>
<p>Traditional AR development involves making changes, building the project, deploying it to a device, and then testing it. This cycle, while essential, is time-consuming. With AR Foundation Remote 2.0, developers can run and debug AR apps directly within the Unity Editor. This drastically cuts down development time, enabling more focus on refining and improving the AR experience.</p>
<p>The toolkit goes beyond the basic <code>Debug.Log()</code> method by offering real-time debugging capabilities. One of the challenges of AR development is visualizing how an app looks and feels on a real device. AR Foundation Remote 2.0 streams video from the Unity Editor to a real AR device such as a smartphone, letting you preview your work without a full build. This real-time feedback is invaluable for fine-tuning the user experience. With full access to the scene hierarchy and all object properties right in the Unity Editor, the debugging process becomes more intuitive and comprehensive.</p>
<p>Another notable feature of this tool <a id="_idIndexMarker793"/>is its <strong class="bold">Input Remoting</strong> feature, which lets you stream multi-touch inputs from an AR device such as a smartphone or even simulate touch using a mouse within the Unity Editor. This flexibility makes testing and tweaking user interactions more thorough and efficient.</p>
<p>Written in pure C# without third-party libraries, and with full source code available, this toolkit offers developers the freedom to tweak, modify, and customize according to their specific needs. It also offers seamless integration with ARKit and ARCore.</p>
<p>Given all of these and many more features that this toolkit offers, we highly recommend you invest in this asset to continue diving into the world of AR development. By slashing the development and debugging time, the toolkit will provide a tangible return on investment for you. Your initial cost is quickly offset by the saved hours and the elevated quality of your AR apps, and by making you a more specialized AR developer.</p>
<p>You can find the toolkit by searching for <code>AR Foundation Remote 2.0</code> in the Unity Asset Store or by clicking on this link: <a href="https://assetstore.unity.com/packages/tools/utilities/ar-foundation-remote-2-0-201106">https://assetstore.unity.com/packages/tools/utilities/ar-foundation-remote-2-0-201106</a></p>
<p>Now that you <a id="_idIndexMarker794"/>have gained <a id="_idIndexMarker795"/>insights into which toolkits will elevate your AR development capabilities, let’s have a look at some more specialized toolkits that will help you to develop specific AR applications more quickly and efficiently.</p>
<h3>GO Map – 3D Map for AR Gaming, a hidden gem for location-based AR apps</h3>
<p>Whenever <a id="_idIndexMarker796"/>you are developing a location-based <a id="_idIndexMarker797"/>AR game or require a rich selection of map visualization styles, such as terrains, satellite imagery, and hybrid maps, the <strong class="bold">GO Map – 3D Map for AR Gaming</strong> asset from the Unity Asset Store will be invaluable to you. It is also loaded with demo scenes showcasing a myriad of styles – from classic flat maps to hybrid maps, and from real building renderings to terrains.</p>
<p>The GO Map package allows a seamless integration of points of interest and real-world landscapes into the gameplay. As every setting and graphic control can be manipulated directly from the <strong class="bold">Inspector</strong> window, you can save precious development time, allowing you to focus on the content and gameplay.</p>
<p>You can find the toolkit by visiting the Unity Asset Store and searching for <code>GO Map - 3D Map for AR Gaming</code> or by directly navigating to its URL (<a href="https://assetstore.unity.com/packages/tools/integration/go-map-3d-map-for-ar-gaming-68889">https://assetstore.unity.com/packages/tools/integration/go-map-3d-map-for-ar-gaming-68889</a>).</p>
<p>If you are more interested in real-world AR navigation or tours instead of 3D maps, the next toolkit is for you.</p>
<h3>AR+GPS Location, enabling navigation, tours, and location-based games of the future</h3>
<p>If games such as Pokémon GO and <a id="_idIndexMarker798"/>features such as Google Maps’ AR navigation excite you, Unity’s <strong class="bold">AR+GPS Location</strong> package<a id="_idIndexMarker799"/> will be a very interesting asset for you, as it allows you to create similar applications of your own in a relatively short amount of time.</p>
<p>At the forefront of this asset’s offering is its AR navigation system. Drawing its strength from the <em class="italic">Mapbox Directions API</em>, the system allows developers to not just create, but also visualize routes within real-world landscapes. Whether it’s a path charted by Mapbox or a personalized route crafted for lesser-known terrains, the user’s journey is augmented with detailed signposts, arrows, and visual cues. A complementary feature provides a simultaneous 2D map view alongside the AR display using the integrated Mapbox SDK, offering an enriched navigational experience.</p>
<p>With the asset, the world truly becomes your canvas. Developers can anchor 3D objects within specific geographical locations. Whether it’s a monument in a bustling city or an artifact in a remote village, these objects can be virtually situated with latitude, longitude, and altitude coordinates, breathing life into the surroundings.</p>
<p>Further enhancing user engagement are the AR hotspots. These are specialized zones that, when entered or approached, activate specific AR manifestations. Imagine walking through a historic alley and having a virtual guide recount its rich past, all triggered by your mere presence in that location.</p>
<p>Textual representations get a makeover with the asset’s ability to establish 3D text markers over real-world landmarks. A tourist navigating a city could instantly get information about a site, with hovering textual cues guiding their exploration.</p>
<p>As the user moves, the AR overlays respond naturally and with precision, ensuring a harmonious blend between the user’s movement and the augmented display. Additionally, objects can be orchestrated to move or remain stationary along intricate paths.</p>
<p>Adopting the AR+GPS Location asset necessitates a certain alignment of tools and hardware. Compatibility with AR Foundation versions 4.x or 5.x, or Vuforia version 10 or newer, is essential. Devices must either support ARKit for iOS or ARCore for Android when deploying with AR Foundation. For those working with Vuforia, a device with ground plane capabilities is mandatory.</p>
<p>You can find this asset in the Unity Asset Store by searching for <code>AR + GPS Location</code> or by clicking this link: <a href="https://assetstore.unity.com/packages/tools/integration/ar-gps-location-134882">https://assetstore.unity.com/packages/tools/integration/ar-gps-location-134882</a></p>
<p>By exploring the<a id="_idIndexMarker800"/> AR toolkits mentioned in this section, you are on your way to <a id="_idIndexMarker801"/>mastering AR development. The next section introduces you to similarly powerful VR toolkits that you should keep an eye on if you want to venture further into the world of VR development.</p>
<h2 id="_idParaDest-184">Exploring powerful VR toolkits from the Unity Asset Store</h2>
<p>The XR Interaction<a id="_idIndexMarker802"/> Toolkit is undeniably a cornerstone for VR development in Unity and should be the tool you focus most of your time and energy on. However, there are several other toolkits that can complement or even substitute it in certain contexts. These alternatives often provide specialized features not currently found in the XR Interaction Toolkit. Diving into these toolkits can not only enhance your VR development expertise, making you a more versatile VR developer, but it can also optimize your workflow based on each project’s requirements. Although the XR Interaction Toolkit is continuously evolving, getting acquainted with other hardware- or software-specific toolkits and plugins remains a crucial step for any committed VR developer, as you will learn in the next section.</p>
<h3>Oculus Integration, SteamVR, VIVE Input Utility, and similar VR toolkits</h3>
<p>Delving into the XR Interaction Toolkit is a great starting point, but to truly advance your VR development prowess, it’s beneficial to explore the toolkits and plugins offered by VR hardware manufacturers directly. This encompasses the Oculus Integration package tailored for Meta Quest series devices, the PICO Unity Integration SDK for PICO headsets, the SteamVR plugin compatible with a broad range of VR devices, and the VIVE Input Utility asset, suitable for various headsets from VIVE and other brands. Let’s explore each of these in depth to understand their significance in your journey to further refine your VR expertise:</p>
<ul>
<li><strong class="bold">Oculus Integration package</strong>: This<a id="_idIndexMarker803"/> package provides comprehensive support for Oculus VR devices<a id="_idIndexMarker804"/> and some devices compatible with OpenVR. It enables seamless handling of all in-app audio and sound effects, provides the tools to incorporate Oculus Avatars into apps, facilitates the synchronization of avatar lip movements with speech to enhance realism, allows the inclusion <a id="_idIndexMarker805"/>of Oculus platform solutions in apps to broaden their functionality, and introduces immersive voice<a id="_idIndexMarker806"/> interactions to let players interact with the virtual world using their voice, among many other powerful features. As you can see, diving into this toolkit is pivotal for XR developers targeting Oculus devices, as it provides a comprehensive suite of tools that ensures apps are <a id="_idIndexMarker807"/>optimized for this platform (<a href="https://assetstore.unity.com/packages/tools/integration/oculus-integration-82022">https://assetstore.unity.com/packages/tools/integration/oculus-integration-82022</a>).</li>
<li><strong class="bold">PICO Unity Integration SDK</strong>: This<a id="_idIndexMarker808"/> is a dedicated Unity SDK crafted by PICO. It <a id="_idIndexMarker809"/>offers a wide array of features, ranging from rendering and tracking to MR features. The SDK comes packed with resources tailored<a id="_idIndexMarker810"/> for spatial audio, <strong class="bold">eye-tracked foveated rendering</strong>, which optimizes rendering based on the eye’s gaze, <strong class="bold">fixed foveated rendering</strong>, which is a static gaze-centric rendering <a id="_idIndexMarker811"/>method to decrease the resolution from center to edge, eye, face, hand, and body tracking, the ability to capture mixed reality videos, passthrough, and many other features. Similar to the Oculus Integration package, this SDK is an all-inclusive package for developers aiming to create XR experiences tailored to the PICO ecosystem (<a href="https://developer-global.pico-interactive.com/resources/#sdk">https://developer-global.pico-interactive.com/resources/#sdk</a>).</li>
<li><strong class="bold">SteamVR plugin</strong>: Maintained <a id="_idIndexMarker812"/>by <a id="_idIndexMarker813"/>Valve, this plugin is a universal interface compatible with a variety of PC VR headsets. It streamlines the representation of VR controllers in the virtual space, offers skeletal hand data, and enables intricate hand-object interactions. While this plugin has many intersections with the XR Interaction Toolkit, VR developers should feel comfortable with it too, as the toolkit or parts of its scripts are still used by various companies (<a href="https://assetstore.unity.com/packages/tools/integration/steamvr-plugin-32647">https://assetstore.unity.com/packages/tools/integration/steamvr-plugin-32647</a>).</li>
<li><strong class="bold">VIVE Input Utility</strong>: This is a <a id="_idIndexMarker814"/>versatile <a id="_idIndexMarker815"/>toolkit especially oriented toward VIVE devices but it also supports a multitude of platforms. It empowers developers to create immersive movement and object interactions. It also facilitates specific device-role associations, promotes standardized VR development, and provides cross-platform hand-tracking. For those XR developers eyeing the VIVE ecosystem, VIVE Input Utility is an essential toolkit (<a href="https://assetstore.unity.com/packages/tools/integration/vive-input-utility-64219">https://assetstore.unity.com/packages/tools/integration/vive-input-utility-64219</a>).</li>
</ul>
<p>There are many other interesting VR plugins provided by each of these manufacturers and companies. We advise you to explore each company’s Unity Asset Store page to discover additional VR plugins that may spark your interest:</p>
<ul>
<li>Meta Quest: <a href="https://assetstore.unity.com/publishers/25353">https://assetstore.unity.com/publishers/25353</a></li>
<li>Valve: <a href="https://assetstore.unity.com/publishers/12026">https://assetstore.unity.com/publishers/12026</a></li>
<li>HTC Vive: <a href="https://assetstore.unity.com/publishers/21581">https://assetstore.unity.com/publishers/21581</a></li>
</ul>
<p>If you’re keen on delving further into VR game development, the toolkit presented in the next section will certainly interest you.</p>
<h3>Hurricane VR, an XR Interaction Toolkit alternative for VR game development</h3>
<p>While the <a id="_idIndexMarker816"/>XR Interaction Toolkit is the leading toolkit for handling interactions in XR, its generalist approach can sometimes<a id="_idIndexMarker817"/> fall short for specific VR gaming applications. If you want to venture into the avenue of VR game development, <strong class="bold">Hurricane VR</strong> could be an interesting alternative for you as it leans heavily into the gaming domain.</p>
<p>This toolkit offers a rich palette of features tailored for VR games such as weapon systems, specialized physics interactions such as flipping a knife in your hand, or intricate player controllers. While the XR Interaction Toolkit provides a wide array of movements, Hurricane VR offers more nuanced player controller features for VR gaming, such as seamlessly switching between sitting and standing modes, sprinting, crouching, and many more.</p>
<p>The<a id="_idIndexMarker818"/> toolkit isn’t just a toolbox; it’s an entire <a id="_idIndexMarker819"/>workshop. With diverse samples ranging from physics interactables (such as doors and dials) to over-the-shoulder backpack inventories and weapons, developers get a hands-on experience of the toolkit’s potential. Moreover, it seamlessly integrates with other assets such as <em class="italic">VR - Physics Interactions Bundle</em> and <em class="italic">Final IK</em>, amplifying its utility. You can find the toolkit when searching for it via the Unity Asset Store or<a id="_idIndexMarker820"/> directly navigating to <a href="https://assetstore.unity.com/packages/tools/physics/hurricane-vr-physics-interaction-toolkit-177300">https://assetstore.unity.com/packages/tools/physics/hurricane-vr-physics-interaction-toolkit-177300</a>.</p>
<p>Having delved deep into toolkits that enhance your XR capabilities, it’s now time to refine your XR development process. In the upcoming section, you’ll discover how to elevate your XR development efficiency by leveraging the strengths of AI.</p>
<h2 id="_idParaDest-185">Exploring powerful AI toolkits from the Unity Asset Store</h2>
<p>Regardless of <a id="_idIndexMarker821"/>your specific interest within the realm of XR, it’s essential to utilize cutting-edge tools throughout the development <a id="_idIndexMarker822"/>life cycle of your XR projects. Leveraging AI allows you to concentrate on the core game mechanics and logic of your XR application, eliminating the need to spend countless hours on basic scripting. To be a proficient XR developer who remains updated with contemporary trends and advancements, we advise you to explore the AI toolkit outlined in the following section.</p>
<h3>AI Toolbox for ChatGPT and DALL·E, your companion for XR development</h3>
<p>The interplay of game development and AI has never been so seamless or potent. The <strong class="bold">AI Toolbox for ChatGPT and DALL·E</strong> is an embodiment of this evolution, presenting a paradigm shift in how developers perceive, interact with, and utilize code in their projects.</p>
<p>Imagine, instead of manually <a id="_idIndexMarker823"/>crafting each line of your C# script or shader, you simply write your requirements in human <a id="_idIndexMarker824"/>language. <strong class="bold">ChatGPT</strong> translates your requirements into functional and contextually relevant code, directly inside Unity. By using this toolkit, seasoned developers can streamline their workflows, dedicating more time to the heart and soul of game design.</p>
<p>The power of this <a id="_idIndexMarker825"/>toolkit doesn’t end at scripting. <strong class="bold">DALL·E</strong>, a cutting-edge diffusion model, can easily transform your text<a id="_idIndexMarker826"/> descriptions into vivid images inside the Unity Engine as well. Whether it’s a unique texture for your terrains or a defining logo for your game’s brand, it’s all achievable via a simple description.</p>
<p>With this asset, the Unity environment remains unaltered. Just replace your traditional <strong class="bold">Add Script</strong> button with the new <strong class="bold">Generate Script</strong> option inside the Unity Editor after installing the toolkit, and watch ChatGPT in action.</p>
<p>While this toolbox revolutionizes the XR development process, it’s pivotal to understand its boundaries. The AI, remarkable as it is, may not consistently achieve precision for intricate requirements. It is a complement to your developmental journey, not an outright replacement.</p>
<p>In essence, AI Toolbox for ChatGPT and DALL·E is less a tool and more a partner, ready to share the load and elevate your game development pursuits to unparalleled heights. You can find it in the Unity Asset Store by searching for <code>AI Toolbox for ChatGPT and DALL·E</code> or by directly navigating to its URL (<a href="https://assetstore.unity.com/packages/tools/ai-ml-integration/ai-toolbox-for-chatgpt-and-dall-e-250892">https://assetstore.unity.com/packages/tools/ai-ml-integration/ai-toolbox-for-chatgpt-and-dall-e-250892</a>).</p>
<p class="callout-heading">Important note</p>
<p class="callout">At the time of writing this book, this asset provides experimental support for Google Bard. As you are reading this, the asset might fully support Google Bard and hence might be renamed slightly. If you have trouble finding this asset in the Unity Asset Store, search for the asset’s publisher, Dustyroom (<a href="https://assetstore.unity.com/publishers/16150">https://assetstore.unity.com/publishers/16150</a>), in the Unity Asset Store and check out its assets to find the potentially renamed asset.</p>
<p>In the following section, you will discover another powerful AI tool for creating characters in your XR scene.</p>
<h3>Blaze AI Engine, a powerhouse for universal AI character creation</h3>
<p>Whether you are sketching out a simple cube that metamorphoses into a vigilant patrolling <a id="_idIndexMarker827"/>agent or sculpting a tactically advanced enemy that adapts to the game environment, the <strong class="bold">Blaze AI Engine</strong> toolkit <a id="_idIndexMarker828"/>allows you to add these characters into your project with ease. It infuses any GameObject with intelligent and realistic behaviors without requiring you to write a single line of code.</p>
<p>The beauty <a id="_idIndexMarker829"/>of this asset is that you don’t have to abide by a strict framework. You’re free to sculpt AI behaviors as you envision them. This asset is compatible with virtually any system or asset, including visual scripting.</p>
<p>This toolkit isn’t only about intelligent enemy creation. It also enables you to craft loyal companions to accompany players through their quests, responding dynamically to a variety of commands.</p>
<p>To enrich your future XR endeavors with an endless range of AI characters, check out the asset in the Unity Asset Store (<a href="https://assetstore.unity.com/packages/tools/behavior-ai/blaze-ai-engine-194525">https://assetstore.unity.com/packages/tools/behavior-ai/blaze-ai-engine-194525</a>) by searching for <code>Blaze </code><code>AI Engine</code>.</p>
<p>The next section introduces you to a useful tool that simplifies prototyping your XR applications.</p>
<h2 id="_idParaDest-186">Prototyping XR applications</h2>
<p><strong class="bold">ShapesXR</strong> (<a href="https://www.shapesxr.com">https://www.shapesxr.com</a>) offers a<a id="_idIndexMarker830"/> range of features for UI/UX and spatial model <a id="_idIndexMarker831"/>prototyping. It simplifies 3D creation, allowing users to start with basic elements or choose from an extensive library of primitives. With the ability to customize colors, materials, and text, the platform offers precise control through its snapping system.</p>
<p>Collaboration is also a central focus of ShapesXR. It facilitates real-time co-creation by enabling team members to enter VR environments easily. This immersive approach to designing reviews can enhance understanding and productivity.</p>
<p>ShapesXR supports various 2D and 3D formats, simplifying the integration of assets. A <em class="italic">Figma</em> plugin keeps assets synchronized, and you can import and export assets seamlessly from your browser.</p>
<p>ShapesXR also offers MR capabilities, enabling designs to be brought into the real world. Features such as passthrough materials provide valuable insights for XR development.</p>
<h1 id="_idParaDest-187">Summary</h1>
<p>Reflecting on your journey through this book, it began with you mastering the foundational elements of Unity, progressed to the creation of your first VR and AR applications, and culminated in the development of an immersive drumming experience, experimenting with eye-tracking, multiplayer functionalities, and sound and visual effects. With this chapter’s close, you should not only possess the confidence to design, develop, and launch intermediate XR applications but also be primed to delve deeper into XR development. Armed with thoughts of toolkits you’re eager to discover next and comprehensive guidance on structuring your XR initiatives, you’re now poised to not just create XR solutions on your own, but also oversee or guide them.</p>
</div>
</body></html>