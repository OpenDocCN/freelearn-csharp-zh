- en: The Transport Layer - TCP and UDP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we've looked at the interactions of different application
    layer protocols and how to program those interactions in .NET Core. In this chapter,
    we'll go one step closer to the hardware and start looking at transport layer
    protocols with **Transmission Control Protocol** (**TCP**) and **User Datagram
    Protocol** (**UDP**). We'll look at the connection-based and connectionless communication
    patterns that each implements, and we'll look at the strengths and weaknesses
    inherent to each approach. In addition, we'll examine how to write and interact
    with a software client that implements each protocol and use that to extend the
    functionality of our networked applications with custom behavior. Finally, we'll
    look at some of the advanced features of transport layer protocols, such as multicasting,
    for interacting with several hosts simultaneously to improve the performance of
    our network software.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Which responsibilities are delegated to the transport layer and how this layer
    meaningfully differs from the application layer and HTTP/SMTP/FTP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The distinction between connection-based and connectionless protocols and the
    challenges they seek to solve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to initiate a TCP connection and send and receive TCP requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to establish and leverage UDP communication in C#
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to leverage multi-casting to improve performance in our TCP client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll be using sample applications available in the GitHub repo for the book
    here: [https://github.com/PacktPublishing/Hands-On-Network-Programming-with-CSharp-and-.NET-Core/tree/master/Chapter
    11](https://github.com/PacktPublishing/Hands-On-Network-Programming-with-CSharp-and-.NET-Core/tree/master/Chapter%2011).
  prefs: []
  type: TYPE_NORMAL
- en: Check out the following video to see the code in action: [http://bit.ly/2HY61eo](http://bit.ly/2HY61eo)
  prefs: []
  type: TYPE_NORMAL
- en: We'll also be continuing to leverage the tools we used in [Chapter 8](a0c3481a-daca-484d-95f8-f08867c8c7b8.xhtml), *Sockets
    and Ports*. Specifically, if you haven't already done so, I recommend installing
    Postman, from here: [https://www.getpostman.com/apps](https://www.getpostman.com/apps)
     Or you can install the Insomnia REST client, which can be found here: [https://insomnia.rest/](https://insomnia.rest/)
  prefs: []
  type: TYPE_NORMAL
- en: The transport layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we start to examine the intricacies of the transport layer, it's important
    to remember one of the most fundamental distinctions between the protocols of
    the transport layer and the protocols of the application layer; specifically,
    the distinction between what kinds of interactions each layer is concerned with. The
    protocols of the application layer are concerned with the communication between
    business objects. They should only deal with the high-level representations of
    your application's domain entities, and how those entities move through your system.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, with transport layer protocols, the concern is around **atomic network
    packets**, which are used to transmit context-agnostic data packets as well as
    to establish and negotiate connections.
  prefs: []
  type: TYPE_NORMAL
- en: The objectives of the transport layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In all of the application layer protocols we've examined thus far, we've been
    able to make some generous assumptions about the network requests we were transmitting.
    We just assumed that provided our **Uniform Resource Identifier** (**URI**) was
    correct and the remote host was active, we could establish a **connection** to
    our target system. Additionally, we could assume that the connection we established
    was a **reliable** one and that any requests we transmitted would be delivered,
    in their entirety, in such a way as to be readable by the remote host's listening
    application. We could comfortably assume that if an error occurred in transit,
    we would get sufficient information to identify the nature of the error and attempt
    to **correct** it.
  prefs: []
  type: TYPE_NORMAL
- en: Let's review if any of these assumptions can also be applied to the transport
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: Establishing a connection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the transport layer, we can't make assumptions about an existing connection,
    because that's the layer at which the connections are established in the first
    place. Transport layer protocols are what expose specific ports on the local machine
    and negotiate the delivery of a packet to the designated port of a remote machine.
    If that connection requires a session to be maintained for the duration of the
    interaction, it's the transport layer protocol that's responsible for maintaining
    the state of that session (we'll see more about this when we explore connection-based
    communication).
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring reliability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to reliability, the stable and consistent delivery of network
    packets, and the acceptance of response packets, this the job of the transport
    layer. If a session is broken due to a break in the chain of communication between
    two hosts, transport layer protocols are responsible for attempting to re-establish
    a connection and resume the network session based on its previous state. Transport
    protocols that guarantee successful delivery of packets must accept the responsibility
    of communicating with the transport layer of the remote host to validate that
    the application layer data was received successfully.
  prefs: []
  type: TYPE_NORMAL
- en: This is especially important for application layer software that treats an open
    connection like a serial data stream. The notion of incoming data being processed
    in order requires that it can be read in order. That means the transport layer
    must have some mechanism for ensuring reliable, same-order delivery of network
    packets.
  prefs: []
  type: TYPE_NORMAL
- en: Error correction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This reliability is also key to another responsibility of the transport layer:
    error correction. This includes being able to fix discrepancies in data received
    due to complications or interruptions at the network layer of the interaction.
    And, make no mistake, there are a lot of opportunities for potential interference,
    manipulation, or loss of the content of a network packet. The transport layer
    is responsible for mitigating these eventualities and re-requesting a fresh packet
    in the event of corruption. This data correction is usually accomplished with
    a simple `checksum` value, which can give a reliable indicator of any change being
    made to the data in transit.'
  prefs: []
  type: TYPE_NORMAL
- en: Error handling should also be present to ensure the reliable ordering of packets.
    Because physical network infrastructure can, and often does, route multiple requests
    from one single host to another over multiple available network connections, and
    through multiple different switches, it's not uncommon for a packet that was sent
    later in the stream to arrive before packets that were sent earlier. The transport
    layer must have some way of identifying when that has happened, and be able to
    re-request the missing packet or re-arrange the received packets into their proper
    ordering.
  prefs: []
  type: TYPE_NORMAL
- en: Managing traffic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It might not be obvious initially, but when we talk about thousands of ports
    being available to listen on a given machine, those thousands of ports exist only
    virtually. Obviously, there aren't 65, 536 wires plugged into the motherboard
    of your PC. Those ports are just a way for your (usually only one) network adapter
    to route traffic to the appropriate process currently running on your operating
    system. All incoming and outgoing network traffic has to pass through that single
    network adapter.
  prefs: []
  type: TYPE_NORMAL
- en: While it's the network layer software that manages direct traffic control, it
    typically does so by only provisioning access to the physical connection in short
    segments of uptime for the transport layer. It's the job of the transport layer
    software to manage a queue of incoming, unprocessed data, as well as one for outbound
    requests, and provision their delivery to the network layer when the resources
    are made available to do so. This use of resources with limited, intermittent
    availability can be a major boost to performance when done well, and a major bottleneck
    when implemented poorly.
  prefs: []
  type: TYPE_NORMAL
- en: Segmentation of data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As I mentioned when I discussed this topic in [Chapter 3](84e54d31-1726-477b-b753-4408a3ee6286.xhtml),
    *Communication Protocols*, the large, contiguous objects that are used to encapsulate
    data at the application layer are unsuitable for transport over a network. If
    you tried to block your network adapter for the duration of the transport of a
    20 MB file, or a 13 GB file for that matter, the impact on the performance of
    any other network-dependent software on your machine would be absolutely unacceptable.
    Attempting to do so would block operations for any outgoing or incoming requests
    for far too long.
  prefs: []
  type: TYPE_NORMAL
- en: While application layer protocols can send massive payloads with all of their
    requests and just assume they'll be delivered correctly, the same cannot be said
    of transport layer packets. There is no other intermediary between the transport
    layer and the network adapter, so it's the responsibility of the transport layer
    to decompose large request payloads into smaller, discrete network packets that
    are suitable for transport over the network layer. This means that transport layer
    protocols have the added responsibility of applying sufficient context for the
    decomposed packets of an application layer payload to be reconstructed by the
    recipient machine, regardless of delivery order, typically accomplished with packet
    headers.
  prefs: []
  type: TYPE_NORMAL
- en: This isn't typically something you'll be implementing yourself with a language
    as high-level as C#, but understanding that it is going on behind the scenes will
    make concepts such as packet-sniffing and network-tracing much easier to grasp
    down the line.
  prefs: []
  type: TYPE_NORMAL
- en: The classes of transport layer protocols
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While we just discussed a number of responsibilities that transport layer protocols
    might assume, not every protocol at the transport layer implements every one of
    these features. Since it's important to understand what optional features will
    be available in a given implementation, standards organizations have defined a
    classification system for connection mode protocols based on the features they
    implement. According to this classification scheme, there are five different classes
    of connection mode (or transport layer) protocols, with each implementing different
    combinations of the list of services that a transport protocol might implement.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the classification scheme for different implementation classes of transport
    protocols was actually a joint effort between standards organizations. The **International
    Organization for Standardization** (**ISO**), along with the **International Telecommunication
    Union** (**ITU**), issued recommendation X.224 for this exact purpose.
  prefs: []
  type: TYPE_NORMAL
- en: The list of classifications is zero-indexed, from class `0` to class `4`, and
    they are described as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Class 0 – Simple class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is described as providing the simplest type of transport connection, with
    sufficient data segmentation. It is explicitly described in the standard as being
    suitable only for network connections with acceptable residual error rate and
    an acceptable rate of signaled errors. Basically, given the simplicity of the
    protocol, it is only suitable for use on highly reliable local networks with nearly
    guaranteed error-free connections between hosts.
  prefs: []
  type: TYPE_NORMAL
- en: Class 1 – Basic recovery class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Protocols that fall under class `1` are specified to provide basic transport
    connection with minimal overhead. However, what distinguishes class `1` from class
    `0` is that class `1` protocols are expected to recover from signaled errors,
    or errors that are immediately detectable, such as a network disconnection or
    a network reset. Protocols of this class are sufficient for use on networks with
    an acceptable residual error rate, but an unacceptable rate of signaled errors.
  prefs: []
  type: TYPE_NORMAL
- en: Class 2 – Multiplexing class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The defining characteristic of class `2` protocols is their ability to multiplex
    several transport connections on to a single network connection. It's designed
    to work on the same exceptionally reliable networks as class `0` protocols. Because
    of the potential for multiple network connections to be leveraged over a single
    transport layer protocol, protocols within this classification may end up leveraging
    explicit flow control for the optimized use of the network layer resources. However,
    that explicit flow control is not a guaranteed property of class `2` protocols.
    In fact, it may be avoided in cases where multiplexing isn't necessary, as not
    managing flow control explicitly can reduce the overhead applied to packets in
    transit.
  prefs: []
  type: TYPE_NORMAL
- en: Class 3 – Error recovery and the multiplexing class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The multiplexing class is, essentially, a combination of classes `1` and `2`.
    Protocols in class `3` introduce the performance benefits (or packet overhead)
    of the class `2` multiplexing functionality into a protocol with sufficient error
    recovery for a network with less-reliable signaled error rates, where class `1`
    would otherwise be preferred.
  prefs: []
  type: TYPE_NORMAL
- en: Class 4 – Detecting errors and recovery class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Class `4` protocols are by far the most robust of any protocol. They are explicitly
    stated to be suitable for networks with an unacceptable residual error rate and
    an unacceptable signal error rate, which is to say, basically, any large-scale
    distributed network with a high probability of interference of interruption of
    service. Given the suitability of a class `4` protocol for use on such an unreliable
    network, it should come as no surprise that class `4` protocols are expected to
    both detect, and recover from, errors on the network. The errors for which a class
    `4` protocol should provide recovery include, but are not limited to, the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Data packet loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delivery of a data packet out of sequence in a data stream
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data packet duplication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data packet corruption
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Protocols in class `4` are also expected to provide the highest degree of resiliency
    against network failure, as well as increased throughput by way of improved multiplexing
    and packet segmentation. Needless to say, this is also the class of transport
    layer protocols with the highest amount of per-packet overhead introduced with
    each transaction over the network.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, the classification of a protocol only determines the minimum
    set of services you should expect the protocol to implement. That doesn't preclude
    that protocol from implementing a broader set of services than specified by its
    classification. Such is the case with TCP, which actually provides a handful of
    additional services that might be provisioned by software higher up in the network
    stack under more rigid implementations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Class `4` captures both TCP/IP, which is broadly considered the most robust
    (or at least the most complex/complicated) transport layer protocol in wide use
    today, as well as UDP, which is its connectionless peer in terms of broad support
    and adoption. These are the classes of transport layer protocols you''ll be interacting
    with directly when working in C#. To that end, let''s look at perhaps the biggest
    distinction between TCP and UDP: their connection-based and connectionless communication
    patterns. In doing so, we''ll have a much better idea of when and how to leverage
    each protocol.'
  prefs: []
  type: TYPE_NORMAL
- en: Connection-based and connectionless communication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are two primary transport layer protocols we'll be working with in C#.
    The first is the TCP. Commonly called TCP/IP due to its prevalent use on the internet-based
    network software and tight coupling with the **Internet Protocol** (**IP**), TCP
    is the transport layer protocol underlying all of the application layer protocols
    we've looked at so far. The second protocol we'll be looking at is the  UDP. It
    stands as an alternative approach to TCP with respect to transport layer implementations,
    aiming to provide better performance in more tightly constrained use cases.
  prefs: []
  type: TYPE_NORMAL
- en: The primary distinction between these two protocols, however, is that TCP operates
    in what's known as a **connection-based communication mode**, whereas UDP operates
    in what's called a **connectionless communication mode**. So, what exactly are
    these communication modes?
  prefs: []
  type: TYPE_NORMAL
- en: Connection-based communication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It may seem obvious what connection-based communication is at first glance.
    By its name, you might conclude it's just any communication that leverages a connection
    between two hosts. But what exactly do we mean when we say *connection*? It can't
    simply be some physical route between two hosts. After all, under that definition,
    how could two hosts communicate without connecting in some way? How would data
    travel between two machines if not over a connection?
  prefs: []
  type: TYPE_NORMAL
- en: The shortcomings of such a definition become even more obvious when you consider
    that connectionless communication is a valid mode of communication. With that
    point in mind, it's apparent that a connection, in this context, must refer to
    more than a simple channel between two hosts for data to travel across. So, then,
    what exactly is a connection? How do connection-based modes of communication leverage
    it?
  prefs: []
  type: TYPE_NORMAL
- en: Connections to establish sessions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the purposes of clarity and understanding, I think we would benefit from
    thinking of a connection as a session*.* In connection-oriented protocols, a session
    must first be established between the two hosts prior to any meaningful work being
    done. That session must be negotiated with a handshake between the two hosts,
    and it should coordinate the nature of the pending data transfer. The session
    allows the two hosts to determine what, if any, orchestration must happen between
    the two machines over the lifetime of the request to fulfill the request reliably.
  prefs: []
  type: TYPE_NORMAL
- en: Once the session is established, the benefits of a connection-based communication
    mechanism can be realized. This includes a guarantee of the ordered delivery of
    data, as well as the reliable re-transmission of lost data packets. This can happen
    because the session context gives both machines an interaction mechanism that
    will allow them to communicate when a message has been delivered and received.
    This shared, active context is important for our understanding of connection-based
    protocols, so let's look at how that session context is provisioned by the underlying
    network layer.
  prefs: []
  type: TYPE_NORMAL
- en: Circuit-switched versus packet-switched connections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are two ways that a session is provided for two hosts wanting to establish
    a connection. The first is to establish the session by way of a direct, hardware
    circuit link between hosts. This is what's known as a **circuit-switched connection**.
    The data needs no routing information applied to the header because it travels
    over a closed circuit between the two devices. This physical circuit connection
    is how public telephone networks were set up to establish connections. If you've
    ever seen old photographs of telephone operators using quarter-inch cables to
    connect two different ports in a gigantic circuit board, you've seen this exact
    routing mechanism in action (albeit in a very primitive implementation).
  prefs: []
  type: TYPE_NORMAL
- en: Establishing exclusive, direct, physical connections between two hosts has a
    lot of benefits. It guarantees that all packets will arrive in constant time since
    there's no downtime for intermediary routers and switches to parse the addressing
    information of the packet, or to wait for an opening in the data channel. It also
    guarantees the ordering of packets, since each one will travel across the same
    channel exactly ahead of the next packet transmitted.
  prefs: []
  type: TYPE_NORMAL
- en: The downside to this kind of connection-based communication, of course, is that
    it is incredibly costly to implement. There must be a mechanism at every possible
    intersection on the network to establish a dedicated circuit between any two other
    connections without interfering with other possible connections that may pass
    through that same intersection. Separately, it is incredibly costly to manage
    the mechanical switching necessary to engage and disengage a specific circuit
    as connections are established and closed. Thus, these kinds of physical networks
    haven't been in wide use for computational networks in decades.
  prefs: []
  type: TYPE_NORMAL
- en: The alternative approach is what's known as a **packet-switched connection**.
    These connections are established through the use of hardware switches and software
    deployed on routing devices that virtualize the behavior or a circuit-switched
    connection. With connection mode, routers and switches set up an in-memory circuit
    that manages a queue of all inbound requests for a target location. Those devices
    parse the addressing information of each incoming packet and pass it into the
    queue for the appropriate circuit accordingly, and then forward along messages
    from those queues, in order, as soon as the physical resources become available.
    In doing so, the expectations for the behavior of a physical circuit-switched
    connection are maintained. So, for any software that is written to leverage a
    connection-based communication scheme, there's no functional difference between
    a circuit-switched connection or a packet-switched connection.
  prefs: []
  type: TYPE_NORMAL
- en: With this virtualization, the costs of implementing circuit-switched connection
    functionality at a physical level are mitigated. Of course, by mitigating the
    physical costs of a circuit-switch setup, we pay for it in performance costs.
    With packet-switched connections, there's added overhead with each connection
    because each packet must be parsed by each switch in the network path between
    the two hosts. Moreover, unless there is no other traffic on a given network switch,
    there is undoubtedly going to be downtime for a packet-switched connection every
    time the packets associated with that connection are put into a queue to wait
    for physical resources to be made available. However, as most of these operations
    are implemented at a firmware level, the total time cost for any given connection
    is actually reasonably small. This model of a packet-switched network describes
    almost all modern **Wide-area networks** (**WAN**), including the internet.
  prefs: []
  type: TYPE_NORMAL
- en: TCP as a connection-oriented protocol
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Establishing a connection is one of the most important functions a TCP implementation
    provides for application-layer software that is leveraging it. When the TCP layer
    breaks up a request into packets and applies its headers, it does so on the assumption
    that the packets will be sent over a packet-switched network.
  prefs: []
  type: TYPE_NORMAL
- en: 'That means it must be certain that switches and routers along the network path
    have provisioned a virtual circuit for each payload between the two hosts. This
    certainty is provided by a multi-step handshake between the two hosts. The specific
    details of the handshake are a bit more complicated, but it can be boiled down
    to three fundamental transactions for each step in the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '**SYN**: This stands for **synchronization** and is a request sent from the
    client to the server indicating a desire to establish a connection. The synchronization
    happens because the client generates a random integer, *n,* and transmits it along
    in the SYN request as a sequence number, which the server uses to establish that
    the appropriate message was received.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**SYN-ACK**: This stands for **synchronization and acknowledgment** and is
    the response a server sends to an initial SYN request. To acknowledge that the
    request was received in the same state it was sent, the server increments and
    then returns the random synchronization integer it received from the client, *n+1*,
    as the acknowledgment message. It also sends a random integer of its own, *m,*
    as the sequence number.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**ACK**: At this point, the client acknowledges that its own synchronization
    request was sent and received correctly, and confirms the same for the server
    by sending a payload setting the sequence number to the acknowledgment value it
    received from the server, *n+1*, and then incrementing and returning the sequence
    number from the server as its own acknowledgment value, *m+1*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once these three signals have been sent and received accordingly, the connection
    has been established and data transfer can proceed accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Having this agreement between two hosts prior to data transmission is what allows
    TCP to achieve the resiliency that sets it apart from UDP. Since the host knows
    to expect an ordered sequence of packets, it can re-arrange packets that were
    received out of order once they arrive to ensure they are delivered in the appropriate
    order to the higher-level protocols that are expecting them. Moreover, if it doesn't
    receive all of the packets it's expecting, it can identify the missing packets
    based on the missing sequence numbers, and request re-transmission of exactly
    what was lost. Finally, this initialization of a connection gives the server an
    opportunity to communicate information about its processing capability and maximum
    throughput. By telling the client how much data can be processed at any given
    time, the client can throttle its own output to minimize data loss and network
    congestion.
  prefs: []
  type: TYPE_NORMAL
- en: The obvious downside, of course, is that all of these steps to establishing
    and leveraging a connection-parsing sequence of numbers and re-ordering data streams
    accordingly, and re-transmitting data, incurs a major time cost for the interactions.
    When such reliability is necessary (and in most enterprise network software, it
    is), you have no other choice but to leverage TCP or similarly resilient protocols.
    However, when the nature of your software, or the network infrastructure supporting
    it, can support less reliability, you have many high-performing alternatives for
    transport-layer protocols.
  prefs: []
  type: TYPE_NORMAL
- en: Connectionless communication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we previously established, a connection in transport-layer communication
    can be thought of instead as a session for communication. Thus, **connectionless
    communication** is a mode of communication in which data is transmitted without
    first establishing a mutual session between hosts. Instead, packets are sent out
    with their appropriate addressing information, and the responsibility of ensuring
    delivery falls entirely to the lower layers of the network stack. This obviously
    introduces the risk of a failed delivery being undetected: since, without any
    acknowledgment expected from the server, the client wouldn''t know the packet
    delivery failed and required re-transmission, whereas without synchronizing a
    session first, the server wouldn''t know to expect an inbound message. So, why
    is this mechanism used, and when is this sort of risk acceptable?'
  prefs: []
  type: TYPE_NORMAL
- en: Stateless protocols
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Without any session to manage, connectionless protocols are typically described
    as being stateless. Without a state to manage, each transaction happens without
    any broader context telling the recipient how an individual packet fits into the
    wider stream of incoming packets. As such, there is almost no ability to determine
    and ensure the proper sequencing of packets for re-construction by the recipient.
    Without that ability, connectionless protocols are typically leveraged in cases
    where packets can be wholly self-contained, or where the information lost in a
    dropped packet can be reconstructed by the recipient application based on the
    next packet received.
  prefs: []
  type: TYPE_NORMAL
- en: In the latter case, we can account for the statelessness of the protocol with
    state management in our applications. For example, imagine your server hosting
    an application that keeps track of a deterministic state. Now let's say that state
    is updated by a remote client, and those updates are sent in real time, with minimum
    latency, over a connectionless protocol, such as UDP. Because the state of the
    application is deterministic, if a single packet is lost, the server may still
    be able to determine which update was made based on the next packet received if
    its own update could only be reached from a specific state set in the lost packet.
  prefs: []
  type: TYPE_NORMAL
- en: Using this architecture, there would be a time cost incurred by the application,
    as every time a packet was lost, some processing would need to happen to deduce
    the value of the lost packet and update its internal state accordingly. However,
    in cases where the network is reliable enough that packet loss is an infrequent
    occurrence, the reduced latency of a connectionless communication mode can more
    than make up for the occasional processing cost of a dropped packet over the lifetime
    of the application. So, on a reliable enough connection, the trade-off could prove
    extremely worthwhile. While less common in business applications, these protocols
    are frequently leveraged in high-throughput, low-latency interactive applications
    such as networked multiplayer video games.
  prefs: []
  type: TYPE_NORMAL
- en: Broadcasting over connectionless communication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the benefits of this lack of shared state being managed between two hosts
    is the ability of connectionless communication to multicast. In this way, a single
    host can transmit the same packet out to multiple recipients simultaneously, as
    the outbound port isn't bound by a single active connection with a single other
    host. This multicasting, or broadcasting, is especially useful for services such
    as a live video stream or feed, where a single source server is transmitting to
    an arbitrary number of potential consumers simultaneously. With the already-low
    overhead of connectionless packet transmission, this can allow high throughput
    of data to a broad spectrum of consumers.
  prefs: []
  type: TYPE_NORMAL
- en: Establishing connections over connectionless communication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you're anything like me, you probably noticed a bit of a chicken-and-egg
    problem with connection-based communication modes as I initially described them.
    Specifically, how can you establish a session between two hosts that rely on connection-based
    communication without first having a session between those two hosts?
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the obvious answer is that connections are established by way of
    an initial, connectionless communication request. The initial SYN message of a
    TCP connection request is sent over the connectionless communication IP. In this
    way, you could say that connection-based communication is built on the back of
    connectionless communication. In fact, in the case of TCP, the connection-based
    interactions are so dependent on the connectionless interactions of IP that the
    two are typically lumped together and identified as the TCP/IP suite.
  prefs: []
  type: TYPE_NORMAL
- en: UDP as a connectionless communication protocol
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just as TCP is the connection-based communication protocol of choice on the
    internet, UDP often serves as the connectionless communication protocol of choice.
    UDP exhibits all of the expected traits of a connectionless protocol, including
    the lack of any handshake or session negotiation prior to data transfer, and minimal
    error-checking and error correction techniques. So, what are the contexts in which
    UDP is useful?
  prefs: []
  type: TYPE_NORMAL
- en: The need for such speed and the acceptability of intermittent packet loss is
    perfectly suited to low-level network operations to send out notifications or
    basic queries of other devices on the network. It's for that reason that UDP is
    the protocol of choice for **Domain Name System** (**DNS**) lookups and the **Dynamic
    Host Configuration Protocol** (**DHCP**). In both of these contexts, the requesting
    host needs an immediate response to a single, simple query. In the case of DNS
    lookup, the request is for each IP address registered for a given domain name.
    The UDP packet can simply address the DNS server directly, and can contain only
    the domain name of the resource being looked up. Once that information is received,
    the DNS server can respond with addressing information on its own time, trusting
    that whichever application requested the IP addresses will likely be listening
    for the response. Once the DNS lookup request is initially sent out by the client,
    if there's been no response after a given timeout period, an identical packet
    will be transmitted from the client. This way, in the off-chance of a lost packet,
    there's a mechanism for error recovery (the timeout period); meanwhile, in the
    far-more-likely scenario that the packet is successfully transmitted, the query
    result will be returned substantially faster than if a connection were established
    first.
  prefs: []
  type: TYPE_NORMAL
- en: This same behavior is what enables a DHCP request to be satisfied in near-real
    time. When a new network device requests an IP address from the DHCP server, it
    has no specific information about the other devices on its own network. Therefore,
    it must broadcast out a DHCP request and hope that an adjacent node is available
    to act as the DHCP server and provision an IP address for the device. These needs,
    for low-latency and the need to broadcast packets, mean that DHCP requests are
    the ideal use case for a connectionless protocol such as UDP.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting errors in connectionless protocols
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I have discussed at length that connectionless transport layer protocols are
    far more susceptible to errors, as there is no mechanism inherent to the protocol
    for detecting errors. I've already discussed how we can detect and even correct
    for errors in a connectionless transport layer protocol from the application layer
    that's leveraging it. However, at least within UDP, there is at least one simple
    error-detection mechanism transmitted with each packet, and that is a **checksum**.
  prefs: []
  type: TYPE_NORMAL
- en: If you've never heard the term before, a checksum is similar to a hash function
    where each input will provide a drastically different output. In UDP packets,
    the checksum input is essentially the entirety of the headers and body of the
    packet. Those bytes are sent through a standard algorithm for generating the checksum.
    Then, once the packet is received, the recipient puts the content of the packet
    through the same checksum algorithm as the client, and validates that it received
    the same response as was delivered. If there is even a minor discrepancy, the
    recipient can be certain that some data was modified in transit and an error has
    occurred.
  prefs: []
  type: TYPE_NORMAL
- en: Responding to, or correcting, this error is outside the scope of the error-handling
    mechanisms of UDP. Typically, if the value of the packet was critical for continued
    operation of the recipient system, that system may request re-transmission of
    the packet. However, in most cases, a mismatched checksum simply indicates to
    the recipient that the packet is invalid and can be discarded from the processing
    queue.
  prefs: []
  type: TYPE_NORMAL
- en: TCP in C#
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, now that we've explored in-depth the objectives, functions, and limitations
    of various transport layer protocols, let's take a look at how we can interact
    with those protocols in C#. We'll start by taking a close look at the classes
    and features exposed by .NET Core for implementing TCP requests directly from
    our application code. We'll see how stepping down in the network stack gives us
    a degree of flexibility and control over our network operations that wasn't previously
    available in the application layer protocols we've explored in previous chapters.
    To do this, we'll be creating two applications, as we did in [Chapter 9](e93c024e-3366-46f3-b565-adc20317e6ec.xhtml),
    *HTTP in .NET*. One of the applications will be our TCP client, and one will be
    the listening TCP server. We'll see the results from each request and response,
    confirming the expected behavior of our software, by writing to the standard output
    for each of our two applications.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing a TCP server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s first create our `TCP` client, the same as we have with every application
    before, by creating a directory for it, and then using the CLI to create a console
    app:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, within the same directory, we''ll create our `TCP` server application
    with the same command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now we're ready to start setting up our interactions. When we were last interacting
    directly with sockets exposing a port back in [Chapter 8](a0c3481a-daca-484d-95f8-f08867c8c7b8.xhtml)*,
    Sockets and Ports*, we were using Postman to generate HTTP requests against a
    given endpoint. Now, however, since we'll be writing our own TCP messages directly
    in code, we won't be constrained to processing the standardized HTTP headers generated
    by Postman. We can define our own mechanism for interactions between hosts. For
    ease of processing, we'll just let our client and server work with simple string
    messages.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start these interactions, we''ll set up a listening server. We need to do
    this to know what port our client will be connecting to. So, navigating to the
    `Main()` method of your `SampleTcpServer` application, we''ll start by defining
    our listening ports, and then starting up an instance of the `TcpListener` class,
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `TcpListener` class is a custom wrapper around a bare `Socket` instance.
    With the constructor, we designate the port and IP we want to listen on for incoming
    requests. If we had used a bare socket, we'd have to process and either respond
    to or discard every single incoming network request that ran against our designated
    port. With the `TcpListener` instance, though, we won't have to respond to any
    requests that aren't sent via TCP. We'll take a look at this once we set up our
    client class, but this is immensely useful when you're listening for such low-level
    network requests on an open port.
  prefs: []
  type: TYPE_NORMAL
- en: 'The constructor we used accepts an instance of the `IPAddress` class, and any
    `int` that designates a valid port (so nothing negative, and nothing above 65,535).
    So, for this project, we''ll be using port `54321` to listen for incoming TCP
    requests. For our `IPAddress` instance, we''re using the `Any` static `IPAddress`
    instance that is exposed by the class. By doing this, we''ll see and be able to
    respond to any TCP request whose target host IP address or domain name would resolve
    to our host machine. If we didn''t do this, and instead specified an individual
    IP address, we wouldn''t respond to any requests whose IP address didn''t match
    that exactly, even if the address resolved to the same machine. So, we would do
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: After doing so, we could send a TCP request to `tcp://0.0.0.0:54321`, and you
    wouldn't see any request register on our `TcpListener` instance. You might expect
    that our request would be detected, since the `0.0.0.0` IP addresses and `127.0.0.1`
    both resolve to the same local machine, but because, in this example, we designated
    our `TcpListener` to only listen for requests to the `127.0.0.1` IP address, that's
    exactly what it does. Meanwhile, our request to `0.0.0.0` goes unresolved. So,
    unless you're writing distinct listeners for distinct IP addresses held by your
    host machine, (or by a series of host machines your application might be deployed
    across), I would recommend using `IPAddress.Any` wherever possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have to set up our server to run and listen for requests against that
    port. First, we''ll start the server, and then we''ll set up a context where we
    listen indefinitely for incoming requests. This is typically done with an intentionally
    infinite loop. Now, if you''ve ever accidentally found yourself stuck inside an
    infinite loop, you know it''s something you should only ever start when you mean
    to. However, since we want our application to listen indefinitely, the simplest
    and most reliable way to do so is to prevent our `Main()` method from resolving
    by encapsulating our primary business logic in a simple infinite loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If you compile and build what we've written so far, you'll see the two console
    statements print to the screen, and then your application will look as if it's
    hanging for quite some time, and that's exactly what you would hope to see. What's
    happening behind the scenes is that you've initiated the `TcpListener` instance
    by calling `Start()` on it.
  prefs: []
  type: TYPE_NORMAL
- en: This will cause the instance to accept incoming requests on its designated port
    until either you explicitly call the `Stop()` method on the class, or it receives
    a total number of connections greater than the `MaxConnections` property of the
    `SocketOptionName` enum (which is set to over two billion, so it's unlikely that
    limit will be reached in our little local TCP server).
  prefs: []
  type: TYPE_NORMAL
- en: Once our server is listening, we start our listening loop and check to see whether
    our socket has received any pending requests. If it hasn't (and we haven't logged
    it since the last request), we indicate as much with a simple console log, and
    then move along, continuing with the `while` loop until we have something to process.
    For now, we shouldn't see anything in the pending state, so let's set up our client
    project to change that.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing a TCP client
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we'll need to initialize our TCP client application in much the same way
    as we did with our server. Instead of using the `TcpListener` class, though, we'll
    be using the `TcpClient` class to create connections with our server that we can
    write to and read from within our project. The difference between the two in our
    case is that, when we created a `TcpListener`, we needed to initialize it with
    the address and port on which it would be listening. There is no default constructor,
    because without a port on which to listen, the class can't perform its most basic
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'With an instance of the `TcpClient`, however, we don''t need to initialize
    it with an address or port specification. The client instance could feasibly be
    used to connect to multiple, distinct remote processes (ports on a single remote
    host) or hosts (different IP addresses altogether). As such, we only need to specify
    our connection target when we attempt to make a connection. For now, let''s just
    establish the connection to confirm that our server responds to listening requests
    appropriately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here, we specified IP address `127.0.0.1`, but, as I said before, we could have
    specified any alias IP address that would resolve to our local machine. Once we've
    created our client, we can use it to connect to the port we designated as listening
    on our server application. Then, just to confirm that the connection was established
    and that our client knows about it, we write a simple log statement, sleep the
    thread for `10` seconds to observe the message in our console, and then terminate
    the program.
  prefs: []
  type: TYPE_NORMAL
- en: In order to see this succeed, start your server application first, so that it's
    started and listening on the designated port. Then, once you see the messages
    show up in your console window indicating that the server is waiting for a pending
    request, start your client application. You should see the We've connected...
    message in your client window, and the Pending TCP request... message in your
    server window. Once you see both messages, you know your connections are being
    established, and you can terminate both applications.
  prefs: []
  type: TYPE_NORMAL
- en: And, here, let's consider why we use the `loggedPending` flag. It should be
    pretty obvious why we used the `loggedNoRequest` flag to prevent us from printing
    out the log messages every time we stepped through our loop until we received
    an incoming request. However, the reason we have to do the same thing when we
    have a pending request is the server will hold the `Pending` state until its inbound
    message queue has been read from and flushed. So, since our server doesn't yet
    read from and empty the incoming request stream if we didn't have that check,
    and we connected to our server, our console would quickly overflow with Pending
    TCP request... messages.
  prefs: []
  type: TYPE_NORMAL
- en: Connection information without data transfer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we get to the work of building and parsing TCP requests in our projects,
    I just want to take a moment to note the benefit of the connection-based approach,
    and how .NET Core leverages it to give engineers fine-tuned control over their
    network transactions. Note that once we send the connection request from our client,
    we get an immediate notification from the server that a connection was established.
    No message was actually sent, and no response was received by the server. In fact,
    the connection remains open even if the server is synchronously locked and prevented
    from actively transmitting anything. This is the handshake interaction of TCP
    at work, and gives us access to a lot of information about the state of the connection
    prior to actually sending a message.
  prefs: []
  type: TYPE_NORMAL
- en: What's especially nice for application developers, though, is that the connection
    is established and managed by the `TcpClient` class itself. With only a single
    call to the `Connect(IPAddress, int)` method, the TcpClient library notified our
    server that we wished to establish a connection, await the acknowledgment, and
    finally acknowledge the server's response to open the connection. This is one
    of the greatest strengths of .NET Core; the ease of use of a high-level application
    programming language, coupled with access to, and control over, low-level network
    interactions.
  prefs: []
  type: TYPE_NORMAL
- en: Transmitting data on an active connection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we've established a connection, our server can decide what to do with
    that connection, and the requests transmitted across it. However, before we change
    gears back to our server, let's generate a message from our client for the server
    to process in the first place. We'll be using something of a mutation test to
    confirm that all of our data is being processed and returned by the server accordingly.
    So, at each step of the way, we'll be modifying our initial message and logging
    the results. Each step of the way, our message should look different than the
    last system that wrote it.
  prefs: []
  type: TYPE_NORMAL
- en: If you've never heard the term **mutation test**, it's a simple way of tracking
    that changes to your system are detected by the tests that validate your system.
    The idea is that you make a change or a mutation somewhere in your code, and confirm
    that somewhere downstream, usually in your unit tests, that change has an impact,
    typically by failing a previously passing unit test.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start by writing a message with a header and a payload. This will just
    be a simple greeting for our server, and a message we expect our server to return
    to us, unchanged, as part of its response. We''ll separate the two messages with
    a simple `|` delimiter. Then we''ll convert it to a byte array that''s suitable
    for transmission over our connection, and send the request. So, let''s set that
    up before moving on to the server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `requestStream` variable we created is an instance of the `NetworkStream`
    class created to write and read data over an open socket. With this, we'll be
    able to send our initial message, and then, eventually, read the response from
    the server. But, first, let's take a look at how to use our `TcpListener` instance
    to accept and parse an incoming request.
  prefs: []
  type: TYPE_NORMAL
- en: Accepting an incoming TCP request on the server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that our client is actually sending a readable message, let''s listen for
    the request on our pending connection. To do that, we''ll actually get another
    instance of the `TcpClient` class directly from our listener. This is simply the
    class is used to interact with the open connection, so once we accept it, we''ll
    be reading from and writing to that open connection in much the same way that
    our sample client program has been. First, though, we''ll have to accept the pending
    connection, using the thread-blocking `AcceptTcpClient()` call. Since we''re now
    responding to our pending request, we can get rid of our log message and replace
    it with our new code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Starting our server, we should see in our server log that it's listening for
    pending connection requests. Then, once we run our client, we should see our request
    message from the client logged to the server's console, followed by another indicator
    that the server has started listening for incoming requests again. If we run the
    client again, we'll see the same sequence of events until we eventually shut down
    the server.
  prefs: []
  type: TYPE_NORMAL
- en: The request/response model on the server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To finish the request/response interaction, we''ll generate a new message,
    using the payload of the original request, and return it to our client. As we
    complete these two applications, we''ll have the client drive the interactions
    with the server from here on out. So, our server will be up and running, returning
    responses that echo the payload of the requests, until it receives a signal message
    indicating it should shut itself down. Meanwhile, our client will send intermittent
    requests with new payloads, until eventually sending the termination signal to
    our server. To serve that purpose, we''ll add the following lines to our server
    application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll use this as a signal that we should stop listening for requests and
    terminate the server. Next, we''ll add the following conditional code to our server''s
    listening loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Our response transmission code will go inside the `else` statement in that
    conditional block, and so our loop will simply continue logging the request message,
    and then appending the payload to the response, until the terminating signal is
    received, at which point the loop is broken and we''ll shut our server down. So,
    lastly, we''ll modify our `while` loop to check for the value of our `done` condition
    instead of running in an infinite loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s go ahead and parse the message for its payload, using our delimiter
    to separate the two components of our message, and then apply the result to our
    server''s response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, on the line after the closing brace for our listening loop, let''s
    shut down our server, and if you''re running the application in Debug-mode from
    Visual Studio, allow our program to end after a brief delay to check the log results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: And, with that, our `SampleTcpServer` application is complete. It will stay
    active and listen for requests until it's explicitly instructed to terminate itself.
    And the whole time, it will log each request it receives and return its own custom
    response. You can use the source code in the GitHub repository for this chapter
    to check your implementation against my own, but, as always, I'd encourage you
    to modify it on your own and start investigating what other methods are available.
    And, as you do so, always be thinking about how you could use this code in your
    own custom networking software.
  prefs: []
  type: TYPE_NORMAL
- en: Finalizing the TCP client
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our server is designed and written to remain active and listening for any potential
    incoming requests. A client, on the other hand, should only ever be set up for
    a single purpose, execute on that purpose, and then close its connection, freeing
    up the resources of the server for any other consumers that may need to access
    it. For this reason, we won't be writing any persistent listening loops. Instead,
    we will simply process each of a handful of request/response round trips before
    terminating the server and then shutting down our own application. However, to
    create a slightly more realistic simulation of multiple clients accessing our
    TCP server, we'll be dropping and recreating our `TcpClient` instance for each
    subsequent request, and injecting a random delay in between each request.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first order of business, though, is accepting the response from our server.
    So, inside our `SampleTcpClient` application, we''ll be adding a few lines to
    create a new byte array for use as a message buffer for the response and then
    reading our `requestStream` into our buffer for processing and logging. So, let''s
    add that code and then we''ll see how we can extend it to finish our simulation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: I would think none of this is surprising at this point. We're essentially executing
    the exact same thing as the server, but in reverse order. Where as on the server,
    we were reading from the stream, and then writing *to* the stream, in the client
    code, we're first writing *to* the stream, and then reading from the stream. Mechanically
    though, this is the same sort of interaction we've seen since we first looked
    at how to interact with raw C# Stream objects back in [Chapter 4](9d6266fb-4428-4044-b63b-44f1317f64e7.xhtml)*,
    Packets and Streams.* Hopefully, by now, you're starting to see the value in the
    incremental, brick-by-brick approach we've taken to building a foundation for
    network programming up to this point (assuming you haven't already).
  prefs: []
  type: TYPE_NORMAL
- en: 'At any rate, let''s modify our client to transmit a handful of pre-defined
    messages before finally sending the termination signal. To do that, let''s build
    out a short array of the messages we''ll be sending to the server so that we can
    easily increment through them in our code, sending distinct messages with each
    outbound request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s wrap the request/response transactions in a `while` loop (not
    an active listening loop as we saw with our server, but a simple incremental loop).
    We''ll use an iterator variable, starting at zero, to move through the our messages,
    checking its value against the length of our messages array to determine when
    to break out of our loop and let our application terminate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Because our `TcpClient` instance is created by the `using` statement within
    our `while` loop, the variable goes out of scope with each iteration. We thus
    create a new connection every time we step back through the beginning of the loop.
    Next, we have to change the code that builds our request message byte-array to
    iterate through the `messages` string array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, at the end of our `while` loop, we''ll sleep our thread for a random
    amount of time between `2` and `10` seconds, logging the `sleepDuration` each
    time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Finally, if you're running in Debug-mode, you'll want to throw in one last `Thread.Sleep()`
    for good measure, after the `while` loop, to ensure we have enough time to examine
    the results of our requests before our application shuts down.
  prefs: []
  type: TYPE_NORMAL
- en: 'After completing the client and running both applications, my terminals logged
    exactly the messages that I hoped they would:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50954a2a-67e4-4336-85d4-5f4c2da42e86.png)'
  prefs: []
  type: TYPE_IMG
- en: And, with this, we've written our own custom TCP server and clients. While this
    example was fairly trivial in its function, I hope you can see the high degree
    of flexibility these .NET classes open up for you with respect to custom TCP implementations.
    With these two sample applications, you have all the tools at your disposal necessary
    to write your own custom application layer protocol with a custom TCP server optimized
    to support it. Or you could write applications whose network interactions side-step
    the application layer protocol overhead altogether! The problems you encounter
    in your personal or professional projects will dictate how you choose to use this
    toolset, but now, hopefully, you'll be ready to leverage it when you need to.
  prefs: []
  type: TYPE_NORMAL
- en: UDP in C#
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we've looked at how to implement TCP in C#, let's take a look at its
    connectionless counterpart in the suite of transport layer protocols, UDP. By
    its very nature, the sample client and server we'll be writing will be a fair
    bit simpler than the TCP in terms of setup code, but we'll be using the same pattern
    we used in the previous section for defining the behavior of our sample application.
    So, we'll be transmitting requests and accepting and logging responses between
    a client and a server.
  prefs: []
  type: TYPE_NORMAL
- en: The difference here, however, is that both the client and the server will be
    implemented in the exact same way. This is because there is no `UdpListener` class,
    because UDP doesn't actively listen for connections. Instead, a UDP server simply
    accepts in bound packets whenever it is set up to look for a new one. For this
    reason, we'll only be looking at the client application's implementation, and
    I'll leave the server source code for you to pull down from GitHub and use to
    test and validate the behavior of the client.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing a UDP client
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll start by creating a new console app that will serve as our `UDP` client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Then inside our project, the first thing we'll want to do is define a well-known
    IP endpoint that our client will be interacting with. We'll be working against
    localhost once again, with an arbitrary port exposed, just as we did in the previous
    section about TCP. Once we have that defined, though, we're just about ready to
    start generating requests. The beauty of a connection-less protocol is that we
    don't have to first establish any sort of interaction with our remote host. Provided
    we know what the address of the host is, we can simply send out our datagrams.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: And, just like that, if you run the server application and then run the client,
    you'll see your message logged to the server's console! So, what exactly is going
    on here, and what are we doing when we initialize our `UdpClient`?
  prefs: []
  type: TYPE_NORMAL
- en: The first thing we do is initialize our `UdpClient` with a port number. If we
    intend to use this client to receive incoming UDP datagrams (which we eventually
    will), it will be accepting them on the port it was initialized with. So, our
    client will be listening on port `34567`. Next, we take the time to define the
    explicit `IPEndPoint` that we would be sending our datagrams to.
  prefs: []
  type: TYPE_NORMAL
- en: This isn't technically necessary, as you can define your request target with
    their hostname and port as part of the `SendAsync()` method using an overloaded
    method signature. However, since we'll be extending this method to also accept
    responses, it's easier for our purposes to explicitly define the `IPEndPoint`
    instance once at the start of the method. Finally, we build our datagram as an
    array of bytes representing the characters of our message string, just as we did
    in the previous section, and send the message along with the help of our newly
    initialized `UdpClient`.
  prefs: []
  type: TYPE_NORMAL
- en: The send/receive paradigm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One thing you might have noticed about using a `UdpClient`, as opposed to the
    more robust `TcpClient` class, is that UDP doesn't leverage streams at all. With
    no underlying connection for a `Stream` to represent, and the potential for UDP
    data to be lost, or delivered out of order, there is no direct correlation between
    the behavior of a UDP request and the abstraction provided by a `Stream` instance.
    Because of this, the `UdpClient` class provided by .NET Core implements a simple
    call/response mechanism through its `Send` and `Receive` methods. Neither of these
    two methods requires a prior communication or interaction with the remote host
    to execute. Instead, they behave as more of a fire-and-forget trigger for some
    events to happen on the network.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, though, when you want to leverage the `SendAsync()` method, which
    doesn't block your application's thread, you *can* choose to first establish a
    connection with your remote host. Keep in mind, though, that this isn't quite
    the same as establishing a connection in TCP. Instead, this simply configures
    your `UdpClient` so that it attempts to send all outgoing packets to the specific
    remote host to which it is connected.
  prefs: []
  type: TYPE_NORMAL
- en: The connection in this context is only a logical one and it only exists within
    the application it's established in. So, while an established TCP connection was
    detectable from both our client and server applications simultaneously, the same
    is not true in our UDP application. While running our UDP client and server simultaneously,
    the server application has no way of detecting the connection established by the
    client.
  prefs: []
  type: TYPE_NORMAL
- en: Once we've connected our `UdpClient` to a given `IPEndPoint`, every `SendAsync()`
    call is assumed to be configured for the connected endpoint. If you want to send
    a message to an arbitrary endpoint while your `UdpClient` instance is connected
    to a different endpoint, you'll have to disconnect your client first, or explicitly
    pass the new endpoint as a parameter for your `SendAsync()` call. In the context
    of our sample application, this won't come up as an issue, but it could come up
    fairly quickly in real-world contexts, so it's important you keep that in mind
    as you define your send/receive patterns for a given application.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that understanding in mind, let''s prepare to receive the response from
    our UDP server application. First, though, we''ll modify our application to connect
    to our remote endpoint at the outset. Next, to demonstrate how to establish a
    connection with a `UdpClient` instance, we''ll remove the endpoint parameter from
    our `SendAsync()` call. Finally, we''ll listen for a message with `ReceiveAsync()`.
    At that point, we''ll be handling the packet''s buffer object just as we have
    with every byte-array buffer before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: And, with that, we've got our UDP client wired up to send a packet and await
    a response from our server.
  prefs: []
  type: TYPE_NORMAL
- en: You may have deduced this from our discussions about connectionless communication
    throughout this chapter, but whenever you're sending a message using UDP (or any
    other connectionless protocol), it is an inherently non-blocking operation. This
    is due to the lack of any sort of acknowledgment from the server. So, from our
    application's perspective, once a UDP packet has reached our network card for
    transmission, its delivery is out of our hands.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, the `Receive()` operation in UDP is inherently blocking*.* Since
    there's no established connection or stream buffer to hold an incoming message
    until our server or client is ready to process the packet, any software we right
    that must accept and receive UDP packets will have to be very explicit about when
    and how long it is acceptable to block our execution while we wait for a packet
    that may never arrive. The asynchronous versions of the transmission methods provide
    some flexibility, but, ultimately, it's a limitation of the protocol that we can't
    escape. Given that, it's in your best interest to be mindful of that limitation
    and design your UDP software around it from the start.
  prefs: []
  type: TYPE_NORMAL
- en: Multicasting packets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perhaps one of the single greatest advantages of using connectionless communication,
    such as UDP, is the ability to send out packets to a large number of recipients
    in a single transaction. This is commonly called **multicasting**, or **broadcasting**,
    and it enables everything from network device discovery and host registration
    to most live television or video streams broadcast over the internet. It's a somewhat
    niche feature that, if I had to guess, most of the people reading this will never
    have a good reason to leverage, but it is certainly worth understanding. With
    that said, let's look at how to enable this feature in our .NET Core apps.
  prefs: []
  type: TYPE_NORMAL
- en: Multicasting in .NET
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With most of the packet transmissions we've looked at so far, we've been addressing
    a specific port on a specific machine, addressed via host name or IP address.
    However, this obviously won't suit our needs if our goal is to send the same packet
    to as many IP addresses as can listen for it. And it certainly won't work if we're
    trying to discover devices on our network and aren't even sure of their IP addresses
    in the first place. Instead, most network devices will listen for requests to
    their specific IP addresses as well as a special range of IP addresses designed
    specifically to catch broadcast packets from other devices on their network (typically,
    that multicast IP address is going to be `255.255.255.255`, but not necessarily).
  prefs: []
  type: TYPE_NORMAL
- en: If you want to multicast a number of packets out of a single port from your
    host, you can do so simply by configuring your `UdpClient` instance to allow multiple
    clients to access an open port with the `ExclusiveAddressUse` boolean property.
    By setting that property to `false`, you enable multiple `UdpClient` to leverage
    the same port at the same time, giving your application the ability to transmit
    messages to as many remote hosts as you have clients configured to interact with
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Separately, if you want to listen for multicast packets, you can set up a `UdpClient`
    to be a part of a `MulticastGroup` by applying the appropriate `MulticastGroupOptions`
    settings to your client or socket. Doing so sets your client to listen along with
    any other registered listeners to packets being multicast by a single transmitting
    host.
  prefs: []
  type: TYPE_NORMAL
- en: As I said at the start of this section, multicasting and listening for multicast
    packets is an incredibly niche operation, and it's unlikely you'll find yourself
    needing to account for it in your daily work. As such, I won't be spending any
    more time on the subject. However, if you're curious, I would strongly encourage
    you to plumb the documentation for it online. For now, though, I just wanted to
    make sure you had at least some exposure to the concept and understood that there
    were features available to you in the `UdpClient` class that you could leverage
    to achieve or listen for multicast data transmission. For now, though, I think
    it's time we transition to a much more ubiquitously used transport layer protocol.
    And so, let's get our hands dirty with the internet protocol. It's time we explored
    IP.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter served as a major paradigm shift for our understanding of network
    programming. We looked at how the responsibilities of the transport layer are
    wholly distinct from those of the application layer and we took an extremely close
    look at just what those transport layer responsibilities are. We learned that
    the **Internet Engineering Task Force** (**IETF**) has classified the various
    approaches to transport layer responsibilities based around the services and features
    a protocol might support, and how we can use those classifications to determine
    the best circumstances in which to employ a given transport layer protocol.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we learned how connection-based protocols, such as TCP, use preliminary
    handshakes between clients and servers to establish an active connection, or session,
    between two hosts prior to the transmission of any data between the two. We saw
    how these sessions enable connection-based communication protocols to provide
    reliable interactions between the hosts, with substantial error-detection and
    error-correction support. Then we considered how connectionless protocols provide
    a number of advantages in their own right, including low-overhead and low-latency
    interactions between hosts over sufficiently reliable networks. Then we took a
    look at some of the strategies that can be employed by connectionless protocols,
    or the application layer protocols on top of them, to mitigate the unreliability
    of connectionless communication.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, with this perspective in our minds, we were able to dive head-first
    into implementing both connection-based and connectionless clients and servers
    in C# and .NET using some incredibly simple libraries provided by the framework.
    We used a client and server designed to simulate interactions over TCP and UDP,
    and, in doing so, saw how the designers of .NET Core have conceptualized some
    of the characteristics of each protocol and implemented those characteristics
    in code. And now that we have such an in-depth understanding of both of these
    transport layer protocols, we're ready to fully examine the intricacies and nuances
    of the most ubiquitous transport layer protocol of all, the **Internet Protocol**.
    And that's exactly what we'll be doing in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are the four classifications of transport layer protocols?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the primary functions and responsibilities of transport layer protocols?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is meant by a connection in connection-based communication modes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does TCP stand for? Why is it typically referred to as TCP/IP?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Describe the handshake process used to establish a connection over TCP.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does UDP stand for? What are some of the advantages of UDP?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the biggest drawbacks of connectionless communication?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is multicasting? What is broadcasting, and how is it enabled in UDP?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For additional information about TCP, UDP, and the transport layer generally,
    I recommend reading *Understanding TCP/IP* by Alena Kabelová and Libor Dostálek,
    available from Packt Publishing at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packtpub.com/networking-and-servers/understanding-tcpip](https://www.packtpub.com/networking-and-servers/understanding-tcpip).'
  prefs: []
  type: TYPE_NORMAL
