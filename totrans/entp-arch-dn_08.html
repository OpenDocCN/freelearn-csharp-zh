<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-160"><a id="_idTextAnchor271"/>8</h1>
<h1 id="_idParaDest-161"><a id="_idTextAnchor272"/>Service Orientation and APIs</h1>
<p>Now that we have explained many principles and several methods, we are going to move on to chapters that will be a bit more technical and thus will show more examples being applied to our demonstration application. In this chapter, we will explain the notion of <em class="italic">service</em> from the IT perspective and will try to place services in the history of IT to give you a good understanding of what they are for and what they have brought to the industry. There are still shortcomings, of course, but web-oriented architecture and web services in general bring huge value to the software industry when they are correctly designed (which, sadly, is far from being common).</p>
<p>After this examination of the history of services, we will detail the characteristics of a good service-based architecture (I am not using the expression <em class="italic">Service-Oriented Architecture</em> for a precise reason, as you will discover shortly) and explain how their current evolution, namely REST APIs, can be of benefit to many software systems.</p>
<p>Finally, we will show how the architecture patterns seen in the previous chapter apply to the definition of services for the demo application. To do so, we will of course use REST APIs since they are at the core of every modern approach to IT system architecture. We will come back to the notion of standards, which was heavily discussed in <a href="B21293_02.xhtml#_idTextAnchor038"><em class="italic">Chapter 2</em></a>, and explain which ones can be used in our demonstration system. Finally, we will explain what we can do when no standards exist or apply, and this will form the transition point to the next chapter.</p>
<p>In this chapter, we’ll cover the following points:</p>
<ul>
<li>Looking at the history of service orientation</li>
<li>Characteristics of a service</li>
<li>Application to our demonstration system</li>
</ul>
<h1 id="_idParaDest-162"><a id="_idTextAnchor273"/>Looking at the history of service orientation</h1>
<p>First of all, let’s start<a id="_idIndexMarker386"/> with a bit of history. it may be because I am an old-timer and have been programming for the past 37 years, with 25 of them in industrial contexts, but I think it is always interesting to know where we are coming from as this explains a lot of what technologies today have been created for, and what they still miss. This way, not only can we anticipate the shortcomings of certain techniques and software artifacts, but we also avoid the risk of not using them to their full <a id="_idIndexMarker387"/>potential, as intended by their creators. Unrolling the history of technologies has yet another advantage: while strolling along this path, you may stumble upon an old but still interesting technology that may better fit your context than the new kid on the block. This does not happen so often, but when it does and you can solve your IT problem with a battle-hardened, yet simpler technology than the tools generally used at present, it can provide you a huge boost in maintenance time and performance. For example, files-based interop may seem laughable to someone using web APIs every day, but in particular, contexts where asynchronous is better, security is not a problem, independence of the process is an advantage, and avoiding the deployment of a web server is a time-saver, they can be the perfect solution.</p>
<p>So, let’s start this journey with interop techniques, and in particular begin with why we need them<a id="_idTextAnchor274"/>.</p>
<h2 id="_idParaDest-163"><a id="_idTextAnchor275"/>The long-awaited reusability</h2>
<p>Making two parts of a software entity interop with each other is a concept almost as old as programming itself since it is related to reusability. For a common function to not need to be typed twice, there needs to be a way to separate it from the rest of the code that is different, and make it callable, one way or another, by these pieces of code. This can be easily schematized as shown in <em class="italic">Figure 8</em><em class="italic">.1</em>:</p>
<div><div><img alt="Figure 8.1 – Reusing common code" src="img/B21293_08_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – Reusing common code</p>
<p>Code duplication is a problem (although the Don’t Repeat Yourself principle may have its own shortcomings and every programming choice is always a compromise), so putting some code in common is generally a valuable orientation. There are lots of ways to organize this, and reusability has been sought after like the Graal in IT for many years, even decades.<a id="_idTextAnchor276"/></p>
<h2 id="_idParaDest-164"><a id="_idTextAnchor277"/>Routines and avoiding punching additional paper cards</h2>
<p>The first <a id="_idIndexMarker388"/>attempt at reusability came from an era that most of you will not even know about, where programs existed in the form of paper cards punched with holes to provide instructions to the computer. This actually came from the Jacquard weaving mechanisms where these paper cards were used to control cloth threads in semi-automated machines to make certain figures and weaving patterns appear in the resulting cloth. Repeatability was already possible by using the same card again and again, but the reuse of a pattern on a given card was done by punching the card many times in the exact same way, resulting in a long manual process and involving the risk of errors. Also, each application, composed of boxes with punched cards in the exact order, had to contain every single instruction. Due to the fixed number of instructions that could fit on a card, it was practically impossible that a card could be reused in another stack. Even if it was possible, extracting the right card, using it, and then returning it to the right position in the stack would have been too dangerous for both applications involved, particularly with regard to duplicating the paper card, even if it was a manual operation.</p>
<p>Then came the idea of routines and sending the code back to a given address of the program to make it repeat some part of its instructions. The notion associated with the infamous (but still worthy) <code>GOTO</code> instruction was born, and it saved many instructions in the programs that followed. But there was a problem: routines could only be used inside a single program. Admittedly, it helped reduce their size. Nonetheless, when creating a new program, it was still necessary to type in the same code – and we’re talking about a time when copy-paste simply did not exist. So, there was a need for something better. This is what we are going to explore in the coming sections<a id="_idTextAnchor278"/>.</p>
<h2 id="_idParaDest-165"><a id="_idTextAnchor279"/>Libraries and the capacity to share instructions between programs</h2>
<p>The next evolution of reuse was the <a id="_idIndexMarker389"/>concept of <code>.dll</code> files, or Java modules in <code>.jar</code> archives, are still the foundations of reuse, and universality is not far when Base Class Libraries like the one from Microsoft are evidence for all .NET programmers. For this particular framework that started under the strict control of Microsoft and progressively flourished into open-source availability, history has been a blessing, since lots of libraries are simply implemented once and for all. Java, on the other side, started as a more open<a id="_idIndexMarker390"/> platform but became progressively more closed after the buying of Sun by Oracle. Though things start to unify a bit (at the price of a slower evolution), there still are many libraries to do the same thing within the Java ecosystem. I remember being flabbergasted with my first professional developments in Java, after five years of .NET, by the fact that there was not a single XML parser library, but many of them, each being the best at one thing: Xerces being good at streaming analysis; Xalan recognized as the quickest at fully loading an XML DOM; some other library handling the DTD schemas and validation better; and so on. For someone used to having only <code>System.Xml</code> to think about, that was a huge surprise and a big disappointment, since it made the learning curve all of a sudden dramatically steeper than the closeness of the platform and language led me to expect.</p>
<p>Anyhow, libraries are certainly the most widespread approach for reuse in programming platforms and exist in almost all modern languages, be it JavaScript, Python, C or C++, and so on. Though libraries also have their difficulties, not only in the versioning and forward-compatibility areas but also in the ease of copying a file, which sometimes ends up in multiple copies in a code base (which goes against the initial goal of reuse), they remain the go-to approach when reuse is necessary. Of course, as far as interop is needed, they have pronounced shortcomings since they are only usable in their own execution platforms: though bridges may exist, a Java library can only be used by a Java program, a .NET assembly can only be called by another .NET assembly, and so on. This is a problem that has occupied IT engineers for a long time and is where lots of solutions have appear<a id="_idTextAnchor280"/>ed.</p>
<h2 id="_idParaDest-166"><a id="_idTextAnchor281"/>Attempts at general interoperability</h2>
<p>The main solution to the aforementioned limitation was to create libraries that contained code in a compiled form, in such a way that the machine code was usable from any caller, whatever language was used to create the calling program, as long as it was also compiled in machine-readable code. The difficulty there was mostly technical: calling one such library was not as simple as using a function name and attributes. Also, there was still a limitation due to the platform of compilation. There was indeed no way to execute a Windows library inside a Linux operating system and vice versa.</p>
<p>Microsoft actually tried to go a bit further in this direction by introducing the notion of OS-controlled components. These reusable units were not directly available as files but as entities known by the operating system itself: their concrete form was still filed, with the <code>.dll</code> extension, but when <em class="italic">registered</em>, Windows would make the functions available to any program even if it did not have access to the original file. In addition to being a repository for application customization, the registry also stores the necessary information for <a id="_idIndexMarker391"/>the <strong class="bold">Component Object Model</strong> (<strong class="bold">COM</strong>); in fact, it even started its career in Windows 3.11 mostly for this use. Together with COM, a Microsoft technology called Dynamic Data Exchange allowed application components to be inserted into one another. This is what you use today when you open an Excel worksheet inside a Word document and see the menus adapt.</p>
<p>After <a id="_idIndexMarker392"/>COM came extensions such as COM+, then for <strong class="bold">Distributed COM</strong> (<strong class="bold">DCOM</strong>), which <a id="_idIndexMarker393"/>was an attempt at breaking out of the perimeter of the local computer and introducing remote execution of components. These did not have the same success as the latest innovation in this vein of components, called ActiveX. ActiveX was a technology built on COM to make it easier to integrate graphical components into applications, instead of just functions. It was even possible to embed these in web applications by delivering them within the browser. At a time when browser security was not as extended as today, it provided lots of interesting features, but the technology is now outdated.</p>
<p>Other technologies for distributed components existed, such as Enterprise Java Beans and all platforms using CORBA, but they had the same limitations as DCOM and did not exhibit the level of low coupling that was initially promised. Version control was left to the maintainer of the platform, no capacity existed for a relationship with the presentation layer, and other shortcomings made for a limited future for these technologies that are nowadays pure legacy.</p>
<p>In fact, interoperation in the form of components might have been too humble to really reach the state of an ever-lasting technology such as ASCII, Unicode, HTTP, and some other norms that are so widely used, including as the basis for new approaches, that they will be around for the foreseeable future of software. Components started inside the perimeter of a single machine and never found a way to step out. A completely universal approach was necessary to make the next step, and it concerned bringing interoperability and reuse to a whole network of computers – and to be worth it, this had to be in the biggest network of all, the Int<a id="_idTextAnchor282"/>ernet.</p>
<h2 id="_idParaDest-167"><a id="_idTextAnchor283"/>Using web standards to try and get universal</h2>
<p>The next milestone <a id="_idIndexMarker394"/>in our history of interop concerns web services, in the general acceptance of the term, meaning providing a service through web standards. The web was the obvious way to make this next step since its foundations of HTTP, TCP/IP, Unicode, and XML were already available and offered a good part of the foundation such a universal interop technology would need.</p>
<p>The first attempt at web services (or what we could call <em class="italic">reusable functions over the internet</em>) was implemented <a id="_idIndexMarker395"/>with technologies such as <strong class="bold">Simple Object Access Protocol (SOAP)</strong> and <strong class="bold">Web Service Description Language (WSDL)</strong>, and since these standards were alone in the field, they simply<a id="_idIndexMarker396"/> preempted the term <em class="italic">web service</em>, which became the accepted jargon for SOAP- and WSDL-compatible expositions. SOAP was about standardizing the XML content of requests and responses over HTTP to make them look like function calls, with an envelope, attributes with types, possible metadata, and so on. WSDL was the norm used to express the associated contract, in short, the grammar that was supposed to be used in SOAP messages. There were additional <a id="_idIndexMarker397"/>norms such as <strong class="bold">Universal Description, Discovery, and Integration (UDDI)</strong>, for example, but these fell short of their objectives and quickly declined. This was also the case for lots of the so-called <em class="italic">WS-*</em> standards – WS-Authentication, WS-Routing, and other syntax additions to the web-service grammar that were intended to allow for complementary features.</p>
<p>These solutions gained a lot of momentum in the industry in the 2000s and were at their strongest in the early 2010s. In fact, they generated a whole architecture known as <strong class="bold">Service Oriented Architecture (SOA)</strong>. SOA<a id="_idIndexMarker398"/> should have remained a generic term, but it has become associated with particular architectures and software. Also, software manufacturers heavily invested in <em class="italic">SOA tools</em>, making companies believe that central middleware was all they needed to reach interop, while people knowledgeable in interoperability knew that this was only one part of the deal, with semantic and functional interoperability being, in fact, more critical and more complex to obtain than technical interoperability, which is generally the last mile in the process.</p>
<p>This of course led to lots of critics of SOA, and many articles in the mid-2010s announced the death of SOA and its failure to reach its goals. In the meantime, its spread was still huge in the industry and lots of technologies had a rebirth <a id="_idTextAnchor284"/>due to SOA.</p>
<h2 id="_idParaDest-168"><a id="_idTextAnchor285"/>The steps in middleware</h2>
<p>Middleware applications in particular were pumped up by the SOA architecture and, even when they were not based on SOAP and WSDL, they were able to adapt to it. <strong class="bold">Enterprise Application Integration</strong> (<strong class="bold">EAI</strong>) was an old dream and supposed that centralized <a id="_idIndexMarker399"/>adapters made it possible for many applications in a system to talk to each other, with the EAI platform translating every message from one format to the target format. Of course, its centralized aspect was a <strong class="bold">Single Point of Failure</strong> (<strong class="bold">SPoF</strong>) and<a id="_idIndexMarker400"/> quite a drawback. If you add to this an update of the EAI bricks, which is needed every time any of the applications change its version, it is no wonder that these customized systems never reached maturity.</p>
<p><strong class="bold">Extract, Transform, Load</strong> (<strong class="bold">ETL</strong>) is a <a id="_idIndexMarker401"/>set of data manipulation tools, but they can be classified as middleware applications, particularly since lots of interoperability streams between applications <a id="_idIndexMarker402"/>in common information systems are really pure data transfer, and not business function calls. Of course, this is a crude middleware, but the quality of the data is more important than the sophistication of the tool, and a well-controlled ETL can go a long way in structuring streams of information. Still, ETLs are not completely adapted to digital transformation, and it can be easy to lose control of them. One of the companies I have consulted for had such a messy system of ETL jobs, with more than a thousand of them kicking up every night, that the whole system needed a dedicated tool to orchestrate the jobs in a precise sequence for them to end up in clean data in the morning. With the never-ending addition of new jobs, time started to become scarce during the low-activity periods, and after using simple solutions such as adding more server power and parallelizing what could be parallelized, it reached a point where the whole system would finish its work only after the offices had opened. This, of course, became an important problem that had to be dealt with using radical decisions. To make it worse, the jobs were so interdependent and brittle that there was never a night where all jobs passed and it was necessary to run a few correcting jobs during operations, or—for the riskiest ones—to simply wait for the next night to, hopefully, get the clean data. For informational purposes (and may it serve as a warning as well, since the sheer number of jobs makes the diagram almost unreadable), the following diagram shows a graph of the chronological execution of the jobs. This diagram is meant to be an overview of complex jobs; text readability is not intended.</p>
<div><div><img alt="Figure 8.2 – Complex job orchestration" src="img/B21293_08_2.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – Complex job orchestration</p>
<p><strong class="bold">Message-Oriented Middlewares (MOMs)</strong> already <a id="_idIndexMarker403"/>existed for some time but got a kick from SOA and AMQP, ActiveMQ, MSMQ, and RabbitMQ gaining some market visibility by introducing robustness in message delivery (the WS-Reliability and WS-ReliableMessaging standards never really made it to the top, particularly because reliability needs to be ensured at the application level and not only in the messaging layer). Even in<a id="_idIndexMarker404"/> today’s architecture, which does not use SOAP web services anymore, a MOM is useful to ensure full-featured transportation of particularly important messages in the system. Some MOM proponents argue that all messages should pass through the middleware, preventing applications from talking directly from one to the other, but this has a toll on performance and we will show that functional standards for messages allow the removal of the mediation layer.</p>
<p>As far as the mediation layer is concerned, MOMs benefited a lot from a standard way of manipulating messages that were defined by Hohpe and Wolf (<a href="https://www.enterpriseintegrationpatterns.com/">https://www.enterpriseintegrationpatterns.com/</a>), called <strong class="bold">Enterprise Integration Patterns</strong> (<strong class="bold">EIP</strong>). EIP<a id="_idIndexMarker405"/> defines some standard bricks for handling software messages, such as Multiplex, Content-Based Router, Enrich, and so on. By combining these basic bricks of message transformation or routing, a MOM was able to handle almost all possible functional situations. Apache Camel is the reference open-source EIP implementation, and it is used in many middlewares. The term <em class="italic">bricks</em> is particularly adapted to these patterns, as they can be explained with actual, concrete, Lego™ bricks: I have often used these to visually explain the concepts of software system architecture and in particular, how to make it evolve with minimal impact by introducing a mediation layer with composable actions, each of them handled by a simple assembly of Technical Lego™ bricks, as shown in <em class="italic">Figure 8</em><em class="italic">.3</em>:</p>
<div><div><img alt="Figure 8.3 – Enterprise Integration Patterns simulated with Lego(TM) bricks" src="img/B21293_08_3.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – Enterprise Integration Patterns simulated with Lego(TM) bricks</p>
<p><strong class="bold">Enterprise Service Buses</strong> (<strong class="bold">ESBs</strong>) are <a id="_idIndexMarker406"/>the natural evolution of MOM and SOA, colliding with the principles of the internet. An ESB integrates all the technologies we have talked about in a system where there is no centralization anymore: the network (in TCP/UDP) is the only thing that remains central and its ability to adapt delivery is used to improve <a id="_idIndexMarker407"/>robustness to a node failure. At the same time, the <em class="italic">Store &amp; Forward</em> pattern is used to make sure that the messages can almost never be lost since they are persisted and only deleted when the next destination has confirmed that they persisted under their control. ESBs had just about everything that was needed to reach the complete functional goal of interoperability in systems that were internet-scaled. But still, they failed, or at least did not succeed as much as would be expected for the ideal solution to such an important problem in the IT industry. In fact, ESBs added all the necessary features, but this was their doom. Since they could make everything, their heavy, complex machinery required extensive expertise to<a id="_idTextAnchor286"/> run and maintain.</p>
<h2 id="_idParaDest-169"><a id="_idTextAnchor287"/>The most recent evolution – REST APIs</h2>
<p>Then came REST, which<a id="_idIndexMarker408"/> is a much lighter way of creating web-based APIs, and this changed the ecosystem quite radically again. <strong class="bold">REpresentational State Transfer (REST)</strong> had been defined before 2000 but became really <a id="_idIndexMarker409"/>famous in the early 2010s. In the 2020s, though the part of the legacy software is huge and SOAP web services continue to be exploited, no new project would start based on these old technologies and virtually every new API project is using REST, or at least some degraded, not really “RESTful” approach.</p>
<p>In a few words, REST<a id="_idIndexMarker410"/> is about going back to the basic mechanisms of HTTP to allow function calls on the web. For example, instead of sending the code of the operation in the envelope as SOAP does, REST uses HTTP verbs such as <code>GET</code>, <code>POST</code>, <code>PUT</code>, <code>PATCH</code>, and <code>DELETE</code> to instruct the server on what should be done. Instead of sending function calls, it deals with resources just as HTTP does with the more-known HTML pages or images that are served through the web; it just happens that these resources are functionally oriented, such as a customer or a contract. Each of these business entities has URLs just like a web page or resource has. Their representation can be in HTML but is more suited to XML or JSON, the latter of which is also lighter than its predecessor XML. Hypermedia, format negotiation, and headers are also used for the equivalent interop function. Authorization is simply left to the equivalent feature in any HTTP call, using Basic Authentication, Bearer tokens, and so on. In short, REST stripped the web-based interop to the bone and eliminated every ounce of fat to focus on the pure and complete use of existing standards. REST does not actually need anything else beyond existing standards such as HTTP, JSON or XML, and Unicode. In this way, it is more a practice than a new protocol.</p>
<p>And it worked… It actually worked so well that commentators on the internet did not hesitate to talk about SOA 2.0, or even <em class="italic">SOA made right</em>. Some introduced new architectural terms such as <em class="italic">Web-Oriented Application</em> to separate this approach from the original SOA. The best proof of success for REST is that no editor has built on its fame to try and impose a proprietary implementation: REST works well because it does not add anything but reduces any software layer to nil since everything already exists, and engineers only have to use it in the<a id="_idTextAnchor288"/> way it was intended.</p>
<h2 id="_idParaDest-170"><a id="_idTextAnchor289"/>What we are missing to reach actual service reusability</h2>
<p>This is where we are at the moment of writing this book, and there is absolutely no doubt that the situation will keep evolving, but we have reached a point where actual web-based interop, including between two separate entities, is an everyday reality for lots of companies, which is already a huge victory in itself. Sure, we can always go further, but the main path has been paved and the remaining tasks now are mostly about spreading good practices in this way of interoperating rather than imagining a new approach that overcomes any current shortcomings.</p>
<p>In fact, most <a id="_idIndexMarker411"/>of the remaining problems are the presence of mediation connectors due to the lack of accepted formats for functional data exchange. If we want to reach the ideal place where global, universal interop will not be an issue anymore but rather a problem of the past, we would need to have an indisputable standard for each of the data streams. This is of course not possible and we are very far from such a satisfying state, but some precise, very widespread, and technically easy forms of exchange are currently covered. For example, authentication and identification are now well implemented by OpenID Connect, SAML, JSON Web Tokens, SCIM, and a few other norms. Sure, there are lots of legacy software and even expert engineers that do not use these, but the general orientation is that they are the future and everyone globally accepts this and works towards these norms, which will become convenience standards in the future, just as ASCII and Unicode are for text binary representations. A few other domains are covered, or at least have nice, fully-featured norms that could solve the problem, such as CMIS for electronic document exchanges or BPMN 2.0 for workflow modeling.</p>
<p>But the vast majority of exchanges are not covered by an indisputable standard and legions of connectors are still developed to establish correspondence between applications. This is a major waste of resources in global IT today, as these mediation connectors do not add any additional value to customers and end users. But the reality is that crafting a standard takes a lot of time, as we have seen in <a href="B21293_02.xhtml#_idTextAnchor038"><em class="italic">Chapter 2</em></a>. Let’s try to focus on the positive, though: the movement is now active and the situation is getting better every year, with a strong interop foundation where the technical bits are now considered solved. Only the semantics and functional interop remain to be handled. This will be the subject of the next chapter but, before talking about this, we need to come back to the very notion of <em class="italic">service</em> and explain how a good service should be defined. We will then use these principles to draft the first services for our demonstration system using the architecture principles shown in the previous chapter and applying them to the demo application that has been shown previously and that we will develop in greater detail throughout<a id="_idTextAnchor290"/> the rest of the book.</p>
<h1 id="_idParaDest-171"><a id="_idTextAnchor291"/>Characteristics of a service</h1>
<p><strong class="bold">Service</strong> is such<a id="_idIndexMarker412"/> a blurry designation that a complete section will be necessary to give a good sense—rather than a single defi<a id="_idTextAnchor292"/>nition—of this concept.</p>
<h2 id="_idParaDest-172"><a id="_idTextAnchor293"/>As a service explained</h2>
<p>The<a id="_idIndexMarker413"/> expression <em class="italic">as a service</em> is used in many formulations: <strong class="bold">SaaS</strong> for <strong class="bold">Software as a Service</strong>, <strong class="bold">PaaS </strong>for <strong class="bold">Platform as a Service</strong>, <strong class="bold">CaaS</strong> for <strong class="bold">Containers as a Service</strong>, and <a id="_idIndexMarker414"/>so on. Have you ever<a id="_idIndexMarker415"/> considered why such different things use this common denomination? This in itself gives maybe the best definition of what a service is: something that benefits from the advantages of something else without having to deal with the usually-associated externalities. A hotel room is a service because you benefit from a bed and a roof without needing to buy and maintain a house, or even clean the room. SaaS is a service because you can use the software (manipulating its interface, storing data and retrieving it, realizing complex computations, and exporting the results) without having to install the software, buy a long-term license, operate it, install new versions, and so on. IaaS is a service because it offers what you expect from infrastructure (CPU power, RAM, I/O, storage, network bandwidth, and use) without you needing to worry about the hardware aspect of buying servers, operating them, renting some room, sorting electricity and cooling, securing them physically, renewing the hardware when there is a failure, and so on.</p>
<p>This explanation of the <em class="italic">as a service</em> expression was necessary because the word <em class="italic">service</em> by itself is very generic and one may be a bit lost as to why we talk about <em class="italic">service-oriented architecture</em>, then web services in the sense of SOAP web services, then services in the context of the web, and so on. When we talk about service in this book, we really mean service as a software function that is proposed to a user without them having to work on its implementation: the user does not have to know which platform is used, where the servers are, and so on. They only have to know the minimal information possible, namely a URL and the contract defining the exchange grammar, to interoperate with the service.</p>
<p>Does this remind you of something? Depending on only the functional definition of something, without any software-associated constraint? This is something that has already been exposed in the book, in particular where we talked about the four-layer CIGREF map model:</p>
<div><div><img alt="Figure 8.4 – Decoupling illustrated with the CIGREF map" src="img/B21293_08_4.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – Decoupling illustrated with the CIGREF map</p>
<p>When talking<a id="_idIndexMarker416"/> about providing a <em class="italic">function</em> <em class="italic">as a service</em>, one can view it as having something in the second layer from the top (the Business Capability Map) without having to worry about how it is implemented in the third and fourth la<a id="_idTextAnchor294"/>yers (the technical ones).</p>
<h2 id="_idParaDest-173"><a id="_idTextAnchor295"/>Getting rid of middleware altogether</h2>
<p>The nice advantage of the <em class="italic">as-a-service</em> approach is that it allows us to get rid of the middleware altogether. Indeed, what we really want to avoid is the direct, point-to-point interop that causes a lot of coupling, as shown in the following diagram:</p>
<div><div><img alt="Figure 8.5 – Point-to-point interoperation" src="img/B21293_08_05.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – Point-to-point interoperation</p>
<p>But the <a id="_idIndexMarker417"/>middleware, while introducing an indirection layer, poses two problems. The first one is that it introduces an additional software complexity, which can be hard to maintain. The second one is that we are still in the software layer of the CIGREF map, and this means that, if done badly (without standardizing the messages), we could very well end up with two steps of coupling instead of simplifying it! The following schema expresses this potential danger:</p>
<div><div><img alt="Figure 8.6 – Interoperation through a middleware" src="img/B21293_08_06.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – Interoperation through a middleware</p>
<p>ESBs are often presented as a solution to avoid a centralized entity, but the way they actually work still implies the presence—though distributed—of software agents that can cause coupling:</p>
<div><div><img alt="Figure 8.7 – Interoperation with Enterprise Service Bus" src="img/B21293_08_07.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – Interoperation with Enterprise Service Bus</p>
<p>One way <a id="_idIndexMarker418"/>to avoid this coupling is to standardize the messages from a functional point of view:</p>
<div><div><img alt="Figure 8.8 – Interoperation with standardized decoupled functions" src="img/B21293_08_08.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.8 – Interoperation with standardized decoupled functions</p>
<p>But if we reach this state where a functional standard has been created, the middleware actually does not need to map data anymore or translate any format, because the <code>f</code> and <code>f'</code> functions are actually the same (otherwise they would not have been included in a single stream of data). The middleware’s sole functions remain routing, authentication, and some other features that can simply be realized by HTTP and do not need any middleware. Thus, the<a id="_idIndexMarker419"/> intermediate simply disappears and we reach the ideal situation that was expressed previously:</p>
<div><div><img alt="Figure 8.9 – Principle of decoupling by indirection" src="img/B21293_08_09.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.9 – Principle of decoupling by indirection</p>
<p>Here, the only difficulty remaining is a functional one, that of describing the business-related need. Admittedly, this can be a very difficult thing to do, but the main difference is that this is intrinsic complexity that we need to overcome in any case (otherwise the software will simply not work correctly) and not accidental, technical complexity that steps into our design phase and adds unnecessary problems of versioning, maintenance, and so on. This is the essence of decoupling and making it easier for the system to evolve.</p>
<p>Again, even though this is something we should strive to achieve in as many cases as possible and definitely a way to create some low-coupling interop, this kind of interaction is not always easy to realize. MOM and other middleware systems will not be retired any time soon, as they remain a good choice to interoperate complex messages, apply mediation, and ensure robustness of delivery when it is not possible to put in place a complete standardization of messages in<a id="_idTextAnchor296"/> the information system.</p>
<h2 id="_idParaDest-174"><a id="_idTextAnchor297"/>External interop finally becoming a reality</h2>
<p>All of this<a id="_idIndexMarker420"/> may sound a bit theoretical, but this approach is what enables us to finally reach the stage where the interop between software <code>A</code> and software <code>B</code> in the preceding diagrams (<em class="italic">Figures 8.5</em> to <em class="italic">8.9</em>) does not depend on middleware or other artifacts that get in the way and make it complicated. The best way to show this is to provide a few practical examples.</p>
<p>In a company I worked for in the past, two customers (a regional council and a town) wanted to interop in such a way that, when the regional council added an association to its list, the city would automatically receive the information and store it in its own database, provided that it was the given city of registration. The way this was done necessitated some important preliminary work that had been done by my employer, which was to define a standard format for French associations. Since we knew the subject well, this took only a few days and we proposed this format to the French government for publication in their open source forge as they did not have any existing standard for this. This format was the functional contract between the two customers. They agreed that, whatever changes they might make to their software, the content of the association JSON would always be the following (this extract is highly simplified and translated into English for improved readability):</p>
<pre class="source-code">
{
    "name": "Old-time developers of Brittany",
    "registrationNumber": "FR-56-973854763",
    "organizationType": "uri:ORGANIZATIONS:ASSOCIATIONS",
    "creationDate": "2019-01-04T12:00:00Z",
    "representatives": [
        {
            "role": "accountant",
            "lastName": "Gouigoux",
            "firstName": "JP"
        }
    ],
    "legalAddress" : {
        "streetNumber": 282,
        "cityName": "Saint-Nazaire",
        "zipCode": "44600"
    }
}</pre> <p>It happened<a id="_idIndexMarker421"/> that the regional council was already a customer before this project, so they already were using our moral person referential software based on this format. So, on this side, we only had to customize the event management system to call the second customer callback address whenever the events of creation, modification, or removal of an association happened. This was done with the following grammar:</p>
<pre class="source-code">
{
    "webhooks": [
        {
            "topic": "POST+*/api/organizations",
            "callback": "https://saint-nazaire.fr/referentiel_associations/modules/index.php?refOrga={registrationNumber},
            "method": "PUT",
            "filter": "organizationType=='uri:ORGANIZATIONS:ASSOCIATIONS' and zipCode=='44600'"
        }
        {
            "topic": "PUT+*/api/organizations/{registrationNumber}",
            "callback": "https://saint-nazaire.fr/referentiel_associations/modules/index.php?refOrga={registrationNumber},
            "method": "PUT",
            "filter": "organizationType=='uri:ORGANIZATIONS:ASSOCIATIONS' and zipCode=='44600'"
        }
        {
            "topic": "PATCH+*/api/organizations/{registrationNumber}",
            "callback": "https://saint-nazaire.fr/referentiel_associations/modules/index.php?refOrga={registrationNumber},
            "method": "PUT",
            "filter": "organizationType=='uri:ORGANIZATIONS:ASSOCIATIONS' and zipCode=='44600'"
        }
        {
            "topic": "DELETE+*/api/organizations/{registrationNumber}",
            "callback": "https://saint-nazaire.fr/referentiel_associations/modules/index.php?refOrga={registrationNumber}&amp;setActive=false,
            "method": "PUT",
            "filter": "organizationType=='uri:ORGANIZATIONS:ASSOCIATIONS' and zipCode=='44600'"
        }
    ]
}</pre> <p>To give a bit of explanation, webhooks are registrations of an external system to events emitted by the given application. In our case, when the regional council actors’ referential service received data of a new organization or a change in existing data, through the referential service’s API methods, associated events were raised and the aforementioned customization file extract associated these with calls of the provided URL. This URL was exposed by the second customer (the city of Saint-Nazaire) using PHP (but the specific technology doesn’t matter). When, for example, we applied <code>POST</code> to a new organization, the callback URL was called with the identifier of the created entity along with the <code>PUT</code> verb. This is also where we introduced the fact that the city was only interested in associations (not all organizations), and in particular, the ones based in their territory, with the <code>filter</code> attribute.</p>
<p>The URL<a id="_idIndexMarker422"/> implementation was then free to work as it pleased, without any dependence on the emitter. In some operations, the fact that there was an event on a given identifier was enough (for example, to deactivate the association in case of a <code>DELETE</code> order in the regional council information system). In some other cases, for example when an association was created, the JSON content—the exact grammar of which was agreed upon between the two participants—would be retrieved through a <code>GET</code> operation owing to the identifier obtained in the callback (where there was a use for all information of the association) or simply read in the body of the callback call (as the most important data were sent there, using the same contractual grammar, of course).</p>
<p>This example proved to be a successful experiment, as each of the customers was then free to evolve their systems in the way they wanted, changing technologies or other parameters without their partner even needing to know about it. At some point, the city would be interested in associations outside its own zip-code area and could simply register a new webhook content with the updated filter. This did not impact the emitter of the event, not even in its authorization scheme: if the city had requested to be called for associations outside its department (a French geographical unit between a region and a city), the event would have been sent, but reading the information with the help of the identifier received would simply end up in a <code>403 Forbidden</code> HTTP status code. This particular mechanism was something that initially made us decide to never send any data in the callback request in order to simplify authorization mechanisms. But, at some point, it was decided that forcing the called entity to always reply with a <code>GET</code> call to obtain the name and basic information of a new association was a waste of bandwidth. Performance was not so much an issue, but simplicity was more important in this context than the risk of authorization mishaps, since this data is public in<a id="_idTextAnchor298"/> France and easy to obtain.</p>
<h2 id="_idParaDest-175"><a id="_idTextAnchor299"/>Interop made real with standards</h2>
<p>The preceding example demonstrated a case where a particular data schema (we call this a <em class="italic">pivotal format</em>, but <a id="_idIndexMarker423"/>we will come back to this in more detail at the end of this chapter and in the next one) had to be devised to exchange data in a free and decoupled manner. But an even better case is where this contract already exists in the industry. This is another practical case I had the pleasure of dealing with, in particular, because the small company I worked for by then forced a much bigger one to comply with our way of working, simply because we used a recognized standard. Let me explain the situation better…</p>
<p>Our flagship <a id="_idIndexMarker424"/>application, a kind of ERP, generates PDF documents and other binary files, and these should be stored. For quite some time, those would be stored alongside the database in a network share or, sometimes, in a dedicated server accessed through a UNC link. Electronic document management systems started to become mainstream after a few years and we needed to adapt our application so that it could use these systems to store documents. The natural choice for this was the Content Management Interoperability Services norm, as OASIS published a fully-featured 1.1 version supporting multiple metadata schemas, classification, versioning, and many more functions that we did not even need. It also happened that this was the only standard in use in this functional area, which makes for a very easy architectural decision.</p>
<p>So we ended up using a few operations from the standard (in the first step, we only needed to create documents, add metadata and binary content to them, and then retrieve documents through a query on their metadata content), which took us a few weeks to add to our application. Customers were quite satisfied because a simple customization of the software would make documents appear in their Alfresco or Nuxeo EDM systems, since these applications are natively CMIS 1.1 compatible. But what really demonstrated the importance of such a normative approach was the first time we had to deal with a customer equipped with a proprietary EDM: the editor, a quite large company, with an important footprint in the information system of our common customer, wanted us to make changes to our application in order to support their proprietary web services in order to send documents and metadata. After an initial refusal from us, the situation got a bit tense but we were lucky that the information system owner was a clever person who understood perfectly the value of low coupling. She intelligently asked what the effort would be for one partner if she had to select another <a id="_idIndexMarker425"/>supplier for the services this one talked to. The EDM provider stated that they would not have to do anything if our company was replaced by another one. As far as our company was concerned, I explained that—in the reverse hypothesis—we would have to rewrite some parts of the code to adapt to another proprietary protocol. This was enough for the customer, even if she was not a technical expert, to realize that something was wrong with this way of operating and to demand that a standard-based, contractual communication channel was used. Disapproved by the customer, the EDM provider had no choice but to implement, at its own cost, support for the CMIS standard in its product.</p>
<p>This proved a very satisfying experience for many reasons:</p>
<ul>
<li>First, I have to admit that replaying David against Goliath was one of the best ego boosts I had in my career.</li>
<li>Second, we went out of the meeting without having anything to add to our software, since it was already CMIS-ready.</li>
<li>Third, the customer appreciated our expertise in helping them reach a better, more evolutive, system and not trying to push them into a vendor lock-in situation as the other <em class="italic">partner</em> did.</li>
<li>Fourth, the interop project was technically very easy to lead because we would simply provide the partner with a Postman collection of the API calls we needed to work and they were able to validate them from the CMIS norm point of view. There were no “hidden parameters” in the interop calls, everything was explicit and strictly regulated through the OASIS standard. We only had one tweak to add in the case of authentication.</li>
<li>Finally, even the initially reluctant partner admitted at the end of the project that this approach helped to avoid the ping-pong effect in the project, where both partners reject responsibility for a non-working call to the other, ending up in a global loss of time and the customer not being satisfied. And I am truly convinced that the CMIS support would open new opportunities for their <a id="_idTextAnchor300"/>product further down the line.</li>
</ul>
<h2 id="_idParaDest-176"><a id="_idTextAnchor301"/>Keeping complete compatibility</h2>
<p>All this sounds<a id="_idIndexMarker426"/> like a beautiful dream, with pink unicorns and rainbows everywhere, but having great APIs using international standards and norms does not prevent one last danger in interop. Actually, it is quite the reverse, and the cleaner and more usable an API is, the bigger this danger is. Sounds weird, doesn’t it? Welcome to<a id="_idIndexMarker427"/> Hyrum’s Law (<a href="https://www.hyrumslaw.com/">https://www.hyrumslaw.com/</a>), which states the following:</p>
<p class="author-quote">With a sufficient number of users of an API,</p>
<p class="author-quote">it does not matter what you promise in the contract:</p>
<p class="author-quote">all observable behaviors of your system</p>
<p class="author-quote">will be depended on by somebody.</p>
<p>The more successful your API gets, the more important forward compatibility becomes as it is impossible to break the uses of many clients. But after all, this is just the flip side of success and not a bad price to pay if your API is the most used in your context, which ensures a large market share and notable income. Hyrum’s Law is harsher because even some parts of the API that you have no formal engagement with will become things that get you into trouble. A sudden change in performance, for example, might make it impossible for one of your biggest customers to continue working with your API. Even a smaller, non-contractual, modification may get you into this kind of trouble. You know what? Even removing a bug might make some of your API users unhappy because—in some twisted way—their system depended on this particular behavior to operate. That may sound silly but it occurs more widely than you may imagine. After all, it is very common that some API users consume response attributes by their order instead of their identifier.</p>
<p>To a certain extent, Hyrum’s Law can be considered as the API equivalent to the Liskhov substitution principle in object-oriented programming: even if a class can replace another one by implementing the same interface, if its behavior is not the same when the function calls have the same parameter values, then actual compatibility (and thus su<a id="_idTextAnchor302"/>bstitutability) is not achieved.</p>
<h2 id="_idParaDest-177"><a id="_idTextAnchor303"/>Managing APIs</h2>
<p>Even if this is more of an operational concern, managing a number of APIs, with all the authorization <a id="_idIndexMarker428"/>access issues, logging, and possibly invoicing for API consumption, follow-up of versions, and so on can make for a tough challenge. Some dedicated software products exist for this under the common name of <em class="italic">API gateways</em>. They generally are implemented in the form of reverse proxies that act as a frontal server, hiding the actual API expositions.</p>
<p>Depending on whether you need a very low-coupled system or a very integrated one, you could respectively use systems such as WSO² or Ocelot (in the case of an ASP.NET im<a id="_idTextAnchor304"/>plementation of your API system).</p>
<h2 id="_idParaDest-178"><a id="_idTextAnchor305"/>Inversion of dependency for services</h2>
<p>If you remember the following schema from the previous chapter, you will recall that a port and adapter pattern is used in order for the satellite modules to depend on the main one that implements the business domain model, even if the calls come from the latter and go to the former:</p>
<div><div><img alt="Figure 8.10 – Hexagonal architecture" src="img/B21293_08_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.10 – Hexagonal architecture</p>
<p>This is simply the <a id="_idIndexMarker429"/>principle of dependency inversion applied to architecture, with the description of a conventional interface being called by one module, without knowing what implementation<a id="_idIndexMarker430"/> is used behind this interface. In <strong class="bold">Object-Oriented Programming (OOP)</strong> code, this is generally done by object injection.</p>
<p>In service-oriented systems, and in particular, when using web APIs, the indirection level is done by the URL that the caller uses without knowing what is behind it. If having a dependency on this module is not a problem, then the call can be direct. But if the business domain module calls an API, a direct dependency is not a good idea for evolution and a way has got to be found in order to reverse the dependency.</p>
<p>This is generally done by using some kind of callback mechanism, where the domain model module is instructed from the outside (the dependency, in our case) with the URL it should call, possibly in its customization but also in the runtime initialization steps. In the first explanation of the preceding webhooks, this is what happened when the town needed a change of filter on the events the regional council should take into account for informing the town: it would not be normal for the regional council to depend on the town, since the town is the functional requester of the information. This is why the best way for the town to provide the callback URL to the regional council is by registering for the events, possibly through a <code>/</code><code>subscribe</code> API.</p>
<p>This way, we<a id="_idIndexMarker431"/> reach a nice separation of responsibilities, as the regional council is responsible for the following:</p>
<ul>
<li>Exposing an API that allows clients to create, modify, and remove organizations from the data referential service’s persistence mechanism</li>
<li>Exposing an API that allows clients (possibly other ones, possible the same ones) to register for events on organizations</li>
<li>Calling the URL provided by these clients upon registration whenever the event appears in the code</li>
<li>Applying the filter provided upon registration to only emit requested events</li>
</ul>
<p>On the other end, the town is responsible for the following:</p>
<ul>
<li>Registering on the organization referential for the events it needs to observe</li>
<li>Providing a URL for callbacks that is reachable, and points to the necessary implementation</li>
</ul>
<p>When this kind of event-based mechanism is used for every interaction to provide a very low degree of coupling, the jargon term<a id="_idIndexMarker432"/> is <strong class="bold">Event-Driven Architecture</strong> (<strong class="bold">EDA</strong>). In its most advanced form, EDA adds lots of very precisely defined responsibilities to allow for the following:</p>
<ul>
<li>Different authentication and authorization methods for the registration and emission mechanisms</li>
<li>Management of robustness of delivery by reapplying the calls if necessary and, if needed, warning an administrator that, after a certain amount of tries, the event has been stored for later emission to certain registered clients</li>
<li>Handling high volumes of events</li>
<li>Handling large numbers of registered clients</li>
<li>Service-level agreement management, among many other features</li>
</ul>
<p>In its correct <a id="_idIndexMarker433"/>implementation, an EDA-based system is the most accomplished outcome of decoupling in software systems, allowing for a completely transparent evolution of the different modules and linear performance. But despite its long theoretical existence, there are very few actual implementations of this.</p>
<p>Now that the notion of <em class="italic">service</em> has been presented and studied from various points of view, we are going to return to our sample information syste<a id="_idTextAnchor306"/>m and apply this new knowledge to it.</p>
<h1 id="_idParaDest-179"><a id="_idTextAnchor307"/>Application to our demonstration system</h1>
<p>Now that<a id="_idIndexMarker434"/> the notion of <em class="italic">service</em> should hold any secrets for you, it is time to see some practical applications of what we have covered on our demonstration system to reinforce the takeaways from this chapter. Since we aim at something modern, the choice is quite obvious that the different modules of the example system will interact with each other through REST APIs. As much as possible, we will try to keep the middleware as transparent as we can. We may need some connectors for mediation in some cases, but other than that, applications will talk to centralized APIs that will then be implemented separately (this will be done using the concept of service in the container or<a id="_idTextAnchor308"/>chestrator that will be put in place).</p>
<h2 id="_idParaDest-180"><a id="_idTextAnchor309"/>Interfaces needing analysis</h2>
<p>First, we will start with a hexagonal architecture diagram to list all the business domain models and their dependencies. The C4 approach used in the previous chapter showed that we will need at least three business domains, namely books, authors, and sales.</p>
<p>If we concentrate on books, for example, the dependencies are the persistence mechanism, the authors cache module, the books’ GUI system, the books’ API controller, and some technical satellites such as logging, and identity and authorization management. This can be schematized as follows:</p>
<div><div><img alt="Figure 8.11 – An example of hexagonal architecture" src="img/B21293_08_11.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.11 – An example of hexagonal architecture</p>
<p>In terms of the <a id="_idIndexMarker435"/>Agile approach, I am not saying that this contains all the interfaces that will be present at the end of our journey together. But in order to keep this exercise as realistic as possible, I am creating the sample information system at the same time as I write the book, in order not to leave anything hidden and so that you can follow the precise method of design that I recommend, and that I of course try to follow myself.</p>
<p>So, now that the first interfaces have been listed, we need to be a bit more precise than just a name. What are they going to do? How will they be designed to provide for clean, future-proof usage? Most importantly, how are these choices going to reflect the business/IT alignment principles that I have pu<a id="_idTextAnchor310"/>shed forward since the first chapters?</p>
<h2 id="_idParaDest-181"><a id="_idTextAnchor311"/>Using norms and standards</h2>
<p>Since I have spoken so much about the crucial importance of norms and standards, it would have been a terrible signal not to start with them for the precise definition of the interfaces. And describing more precisely the interfaces we talked about in the previous section is as easy as can be when we use a standard because we simply need to name it (and possibly cite the version of it that will be used) and all the operations, formats, semantics, and other functions of the standard are immediately clearly defined through the documentation of the standard.</p>
<p>For example, let’s<a id="_idIndexMarker436"/> start with the authentication and identification service. For this particular interface, we will use the OpenID Connect protocol, based on OAuth 2.0 (RFC 6749) and using JSON Web Tokens (RFC 7519), the JWT profile for OAuth 2.0 itself being standardized (RFC 7523). Again, the great thing about norms and standards is that they greatly simplify our work. If I had to describe with the same degree of precision the use of an interface without a standard, this chapter would be extra long. For this service, citing a few RFCs (and of course, in the next chapters, using good implementations of these norms) is enough to make everything explicit.</p>
<p>How about the database interface or, to be more precise, the persistence interface? The decision is to use a NoSQL document-based approach since it sounds the most adapted to the business entities we talked about and the volumetry we want to address. It may not be a very well-known fact about MongoDB, but most protocols used are open standards, and are, in fact, used by many other NoSQL database implementations. If you want to improve on your local MongoDB database, all you need to do to switch to an Atlas service or an Azure CosmosDB instance is to change the connection string, as everything works the same. The MongoDB Wire Protocol Specification is licensed under a Creative Commons <em class="italic">Attribution-NonCommercial-ShareAlike 3.0</em> license. The BSON format (<a href="https://bsonspec.org/#/specification">https://bsonspec.org/#/specification</a>) used is documented openly and can be implemented by any software. And the list goes on. In addition to the appropriate adaptation of the software to our needs and the fact it is easy to create a free database, the standardized aspect is the cherry on top that makes MongoDB a sound choice for our sample application.</p>
<p>OK, now on to authorizations! There<a id="_idIndexMarker437"/> happens to<a id="_idIndexMarker438"/> exist two main norms around software authorization management, namely <code>admin</code> role has all rights, the <code>operator</code> can read and write entities based on a portfolio, and the <code>reader</code> role can only read data, for example), then using OPA would not be the right choice, as it would add lots of overhead. Of course, the real question, again, is to take time into account. Of course, by the end of the book, our sample application will be so simple that using OPA would be over-dimensioning. However the goal of this exercise is to show how to work if we aim at a real, industrial, freely evolving information system. And since we operate under the hypothesis that rights management is going to be more complicated, then we will start right away with the adapted interface, which means OPA 1.0 in our case.</p>
<p>The <a id="_idIndexMarker439"/>logging feature is a bit of a different situation because this is not something directly functional, but rather a technical feature. However this does not mean that the same approach of standardization should not be used. The only difference is that this level of indirection will not be standardized at the international level as with other norms, but rather locally to the platform. Our sample application being implemented mostly uses .NET Core, so we will use whatever is standard for this technology, and there happens to be a standard global interface in <code>Microsoft.Extensions.Logging</code> called <code>ILogger</code>, which exists also as a generic class <code>ILogger&lt;T&gt;</code>. We will return in the technical chapters to see how to use it and maybe we will even spice it up by using a semantic logging system such as Serilog. But for now, suffice it to say that the logging mechanism will be standardized as well.</p>
<p>It is worth noting that some players in the field currently work towards a first level of standardization, such as Elastic with the ECS specification (see <a href="https://www.elastic.co/guide/en/ecs/current/ecs-reference.html">https://www.elastic.co/guide/en/ecs/current/ecs-reference.html</a> for details). As Elastic is one of the major publishers of observation platforms and the specification is open source, we can place some hope in the spreading of this as<a id="_idTextAnchor312"/> a standard, although only time will tell.</p>
<h2 id="_idParaDest-182"><a id="_idTextAnchor313"/>Where do I find norms and standards?</h2>
<p>When I teach or consult about business/IT alignment and in particular about this need to refer to norms and standards, the same question always comes up at some point: <em class="italic">How do we search for norms?</em> I should say that I am really astonished by this question, and for several reasons:</p>
<ul>
<li>Finding them is as easy as any internet search and virtually all of them are public, with the need to be as visible as possible in order to achieve their goals, so there is absolutely no technical difficulty in finding them. It shows that most people working in the IT industry (and with the number of people I have trained or taught, I do have significant statistics) are ignorant of the standards of their industry, which is quite annoying. I can understand that not a lot of people know BPMN 2.0, for example, as processes are a specific use case, and not all applications need a workflow engine. But how can some architects not know about OAuth 2.0, since this is used almost everywhere on the internet and almost all software applications need some kind of authentication at some point?</li>
<li>Even some people outside of the profession know some of the most identified providers of norms and standards, such as ISO or IETF. Even just the term <strong class="bold">Request For Comments (RFC)</strong> is<a id="_idIndexMarker440"/> understood by many people. Granted, some IT-specific organizations producing norms, such as OASIS, are lesser known. But then again, the <strong class="bold">World Wide Web Consortium</strong> (<strong class="bold">W3C</strong>) is a <a id="_idIndexMarker441"/>very active and recognized institution. So how come people asking this question do not have the reflex to start with these organizations and search them for what they need?</li>
</ul>
<p>For some <a id="_idIndexMarker442"/>customers, I even created a whitepaper at some point with almost a hundred norms and standards used in the business context I was working in at the time (public and government organizations) because this question came back all the time and I wanted to have a quick answer, not only telling them where to find what they needed, but providing them with the answers already found for them. This is for a simple reason: because I found out that the real issue was not that these people did not know “where to find the norm,” but because it indicated doubts in their ability to use them. Norms and standards can be a bit intimidating with their hundreds of pages explaining all the possible cases. Even simple RFCs are indeed not easy to read.</p>
<p>But there are other answers to this as well:</p>
<ul>
<li>First, finding the right norm and starting to use it does not require you to read the norm specification. In fact, only if you need to implement large parts of it will you gain a benefit from reading it.</li>
<li>In most cases, you will use components that implement the norm and all you have to do is check that they are recognized, well-established modules.</li>
</ul>
<p>For example, in <a id="_idIndexMarker443"/>order to use OpenID Connect in our sample information system, we will basically need to know nothing about the protocol itself since we will rely on Apache Keycloak, which implements it in a transparent way for us. All we have to deal with is the choice of identity provider and some customizations made easy by the Keycloak GUI.</p>
<p>Even if you have to dive into the details of the norms, most of the time, you will only need to understand a very small portion of them. For example, in our sample application, we will certainly need at some point to implement some kind of support for binary documents for authors’ contracts; which means we will of course use CMIS 1.1 since this is the recognized standard for this use case. But as we will only send documents, add binaries and metadata, and query documents in return, we may only use 10% of the whole norm.</p>
<p>Finally, a good norm is normally quite spread out and used internationally already. So, reading the full-blown specification is always an interesting read but let’s be honest: the way you will be exposed to the standards in the first steps is simply by mimicking some sample calls that you will find on reference websites and adapt to your needs. Only if you reach a certain level of complexity will it be easier at some point to find the exact nitty-gritty detail <a id="_idTextAnchor314"/>of implementation in the full text of the RFC.</p>
<h2 id="_idParaDest-183"><a id="_idTextAnchor315"/>Pivotal format for the other interfaces</h2>
<p>And for the last part of this subject, the next question that arises is, logically: <em class="italic">What do we do when there is no norm or standard for </em><em class="italic">our context?</em></p>
<p>My first reflexive reply to this question is always, “<em class="italic">Are you willing to bet that there is indeed no norm I can show you on this?”</em> Most of the time, this question goes back to the previous one and just shows that the person asking it is simply not comfortable with norms, or is afraid as they think it is going to be difficult (where in reality, on the contrary, norms free you from all the difficult design aspects). Because, let’s face it, we have norms for virtually everything today. All right, there may be fewer norms in IT than in the mechanical domain. But there are standards for every common feature. You have norms for all generic techniques, norms for every entity used in international data transfers, and for every common human activity including banking, insurance, travel, and so on. You even have an ISO-Gender norm (ISO/CEI 5218) for representing human genders in numeric format.</p>
<p>The second part of the answer concerns what we should do when there is indeed no applicable norm for your context. And the answer to this has already been given a bit earlier in this chapter: you then create what is <a id="_idIndexMarker444"/>called a <strong class="bold">pivotal format</strong>, which has the same goal of standardization as a real norm, but limited to your own context. Of course, it is always better to aim at something universal. Not only because, you never know, but your format may become a norm if you put enough effort into it and other people have an interest in it (this is the way norms appear: it always starts with the effort of an individual <a id="_idIndexMarker445"/>who knows the business domain extremely well and makes the effort to transcribe their knowledge into something technical, which is then agreed upon by other participants as a sound basis for exchanges). But also because aiming at something universal will make your pivotal format as close to a norm as possible, with as many resulting advantages as possible.</p>
<p>And the rule for this is to fall back on existing norms as quickly as possible. Sure, there does not seem to exist an international norm for the concept of authoring (though the Dublin Core <code>creator</code> attribute allows us to draw a link between a resource and the person or organization that authored this resource), but since it points to individual persons, lots of other related norms will quickly apply, such as Social Security Numbers for unique identification, ISO 8601 for the date of authoring, and so on. The same applies to books: of course, we may not find the perfect standard to precisely address what we need for our sample application, and in particular its persistence system, but there are nonetheless norms for languages (ISO 639), internationally-recognized standard codes for registered book <a id="_idIndexMarker446"/>identification such as <strong class="bold">International Standard Book Numbers (ISBNs)</strong>, and standards for virtually everything we will set out to record in the descriptions of the books in our system.</p>
<p>Now, the real question is what to put in the book and author’s pivotal format? And this is such a huge question that it will necessitate a chapter on its own. The good news is that the f<a id="_idTextAnchor316"/>ollowing chapter will explain how to answer this.</p>
<h1 id="_idParaDest-184"><a id="_idTextAnchor317"/>Summary</h1>
<p>In this chapter, I have used a short historical approach (a detailed one would be a book in itself) to explain what the stakes at play are in service orientation and how this seemingly simple yet hard-to-define word of <em class="italic">service</em> has been implemented in the past decades. We are definitely not at the end of the story yet, but nowadays, it seems the best approach is to use REST APIs with a middleware, reduced as much as possible through the use of norms and standards. This not only avoids the costly mediation connectors that translate one format to another, since everybody in the interaction talks the same language but also helps us know whether our design is the right one since consortiums and experts have thought a lot about this business domain.</p>
<p>Standardized APIs are what make it easy today to change some parts of important information systems without breaking them. They allow for international banking, much more efficient insurance systems, simplified travel abroad, and many other feats of the industrialized IT world.</p>
<p>We talked about norms, but also compatibility, the evolution of services, how services will be integrated through interfaces, and much more. By the end of this chapter, we came back to our sample application and showed which norms would be used to implement a few of the services it will expose. Now a difficult question remains: when there is no standard format for a business need and we need to create a pivotal format (of course, using norms as much as possible for its inner attributes), how do we determine the content of this format? The best answer I have is to use <strong class="bold">Domain-Driven Design </strong>(<strong class="bold">DDD</strong>). And this is the subject of the next chapter.</p>
</div>
</body></html>