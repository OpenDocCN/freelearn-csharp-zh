<html><head></head><body>
<div id="_idContainer125">
<h1 class="chapter-number" id="_idParaDest-175"><a id="_idTextAnchor174"/><span class="koboSpan" id="kobo.1.1">11</span></h1>
<h1 id="_idParaDest-176"><a id="_idTextAnchor175"/><span class="koboSpan" id="kobo.2.1">Instrumenting Messaging Scenarios</span></h1>
<p><span class="koboSpan" id="kobo.3.1">Messaging and asynchronous processing improve distributed system scalability and reliability by reducing coupling between services. </span><span class="koboSpan" id="kobo.3.2">However, they also increase complexity and introduce a new failure mode, which makes observability even </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">more important.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">In this chapter, we’ll work on instrumenting a messaging producer and consumer with traces and metrics and cover individual and batch </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">message processing.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">In this chapter, you’ll learn how to do </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.9.1">Trace individual messages as they are created </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">and published</span></span></li>
<li><span class="koboSpan" id="kobo.11.1">Instrument receiving and </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">processing operations</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.13.1">Instrument batches</span></span></li>
<li><span class="koboSpan" id="kobo.14.1">Use instrumentation to diagnose common </span><span class="No-Break"><span class="koboSpan" id="kobo.15.1">messaging problems</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.16.1">By the end of this chapter, you should be able to instrument your messaging application from scratch or tune the existing messaging instrumentation to </span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">your needs.</span></span></p>
<h1 id="_idParaDest-177"><a id="_idTextAnchor176"/><span class="koboSpan" id="kobo.18.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.19.1">The code for this chapter is available in the book’s repository on GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">at </span></span><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter11"><span class="No-Break"><span class="koboSpan" id="kobo.21.1">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter11</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.22.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.23.1">To run the samples and perform analysis, we’ll need the </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">following tools:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.25.1">.NET SDK 7.0 </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">or later.</span></span></li>
<li><span class="koboSpan" id="kobo.27.1">Docker </span><span class="No-Break"><span class="koboSpan" id="kobo.28.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.29.1">docker-compose</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.31.1">Any HTTP benchmarking tool, for example, </span><strong class="source-inline"><span class="koboSpan" id="kobo.32.1">bombardier</span></strong><span class="koboSpan" id="kobo.33.1">. </span><span class="koboSpan" id="kobo.33.2">You can install it with </span><strong class="source-inline"><span class="koboSpan" id="kobo.34.1">$ go get -u github.com/codesenberg/bombardier</span></strong><span class="koboSpan" id="kobo.35.1"> if you have Go tools, or download bits directly from its GitHub repository </span><span class="No-Break"><span class="koboSpan" id="kobo.36.1">at </span></span><a href="https://github.com/codesenberg/bombardier/releases"><span class="No-Break"><span class="koboSpan" id="kobo.37.1">https://github.com/codesenberg/bombardier/releases</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.38.1">.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.39.1">We will also be using the Azure Storage emulator in Docker. </span><span class="koboSpan" id="kobo.39.2">No setup or Azure subscription </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">is necessary.</span></span></p>
<h1 id="_idParaDest-178"><a id="_idTextAnchor177"/><span class="koboSpan" id="kobo.41.1">Observability in messaging scenarios</span></h1>
<p><span class="koboSpan" id="kobo.42.1">In </span><a href="B19423_10.xhtml#_idTextAnchor161"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.43.1">Chapter 10</span></em></span></a><span class="koboSpan" id="kobo.44.1">, </span><em class="italic"><span class="koboSpan" id="kobo.45.1">Tracing Network Calls</span></em><span class="koboSpan" id="kobo.46.1">, we just started scratching the surface of tracing support for asynchronous processing. </span><span class="koboSpan" id="kobo.46.2">There, we saw how the client and server can send a </span><a id="_idIndexMarker586"/><span class="koboSpan" id="kobo.47.1">stream of potentially independent messages to </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">each other.</span></span></p>
<p><span class="koboSpan" id="kobo.49.1">In the case of messaging, things get even more complicated: in addition to asynchronous </span><a id="_idIndexMarker587"/><span class="koboSpan" id="kobo.50.1">communication, the producer and consumer interact through an intermediary – a </span><span class="No-Break"><span class="koboSpan" id="kobo.51.1">messaging </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.52.1">broker</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.53.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.54.1">Operation on the producer completes once the message is published to the broker without waiting for the consumer to process this message. </span><span class="koboSpan" id="kobo.54.2">Depending on the scenario and application health, the consumer may process it right away, in a few seconds, or in </span><span class="No-Break"><span class="koboSpan" id="kobo.55.1">several days.</span></span></p>
<p><span class="koboSpan" id="kobo.56.1">In some cases, producers get a notification that the message was processed, but this usually happens through another messaging queue or a different </span><span class="No-Break"><span class="koboSpan" id="kobo.57.1">communication channel.</span></span></p>
<p><span class="koboSpan" id="kobo.58.1">Essentially, the producer does not know whether the consumer exists – failures or delays in the processing pipeline are not visible on the producer side. </span><span class="koboSpan" id="kobo.58.2">This changes how we should look at latency, throughput, or error rate from an observability standpoint – now we need to think about end-to-end flows that consist of multiple </span><span class="No-Break"><span class="koboSpan" id="kobo.59.1">independent operations.</span></span></p>
<p><span class="koboSpan" id="kobo.60.1">For example, when using HTTP calls only, the latency of the original request covers almost everything that happened with the request. </span><span class="koboSpan" id="kobo.60.2">Once we introduce messaging, we need means to measure the end-to-end latency and identify failures between different components. </span><span class="koboSpan" id="kobo.60.3">An example of an application that uses messaging is shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.61.1">Figure 11</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.62.1">.1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.63.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer115">
<span class="koboSpan" id="kobo.64.1"><img alt="Figure 11.1 – Application using messaging to run tasks in the background " src="image/B19423_11_01.jpg"/></span>
</div>
</div>
<p class="IMG---Figure"><span class="koboSpan" id="kobo.65.1">Figure 11.1 – Application using messaging to run tasks in the background</span></p>
<p><span class="koboSpan" id="kobo.66.1">In such an application, when the user sends a request to the frontend, they receive a response once </span><a id="_idIndexMarker588"/><span class="koboSpan" id="kobo.67.1">the backend finishes processing and publishes a message to a topic. </span><span class="koboSpan" id="kobo.67.2">The indexer, replicator, archiver, and any other services that post-process the data run at their own speed. </span><span class="koboSpan" id="kobo.67.3">The indexer usually processes the latest messages, while the archiver would only look at the messages published </span><span class="No-Break"><span class="koboSpan" id="kobo.68.1">days ago.</span></span></p>
<p><span class="koboSpan" id="kobo.69.1">Some of these components can fail without affecting user scenarios directly, while others impact how soon the data the user published shows up in other parts of the system and therefore can </span><span class="No-Break"><span class="koboSpan" id="kobo.70.1">be critical.</span></span></p>
<p><span class="koboSpan" id="kobo.71.1">Let’s explore how we can instrument </span><span class="No-Break"><span class="koboSpan" id="kobo.72.1">such applications.</span></span></p>
<p><span class="koboSpan" id="kobo.73.1">Before writing our own instrumentation from scratch, we should always check whether there are existing instrumentation libraries we can start with, and if there are none available, we should consult with OpenTelemetry </span><span class="No-Break"><span class="koboSpan" id="kobo.74.1">semantic conventions.</span></span></p>
<p><span class="koboSpan" id="kobo.75.1">We’re going to instrument Azure Queue Storage as an example. </span><span class="koboSpan" id="kobo.75.2">The existing instrumentation does not cover the messaging aspects of queues because of the reasons we’ll see in the next couple of sections. </span><span class="koboSpan" id="kobo.75.3">So, we’ll have to write our own; we’ll do it according to </span><span class="No-Break"><span class="koboSpan" id="kobo.76.1">OpenTelemetry conventions.</span></span></p>
<h2 id="_idParaDest-179"><a id="_idTextAnchor178"/><span class="koboSpan" id="kobo.77.1">Messaging semantic conventions</span></h2>
<p><span class="koboSpan" id="kobo.78.1">The messaging </span><a id="_idIndexMarker589"/><span class="koboSpan" id="kobo.79.1">conventions for tracing are available </span><span class="No-Break"><span class="koboSpan" id="kobo.80.1">at </span></span><a href="https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/trace/semantic_conventions/messaging.md"><span class="No-Break"><span class="koboSpan" id="kobo.81.1">https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/trace/semantic_conventions/messaging.md</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.82.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.83.1">They currently have experimental status and are very likely to change. </span><span class="koboSpan" id="kobo.83.2">There are no general metrics conventions available yet, but you can find ones specific </span><span class="No-Break"><span class="koboSpan" id="kobo.84.1">to Kafka.</span></span></p>
<p><span class="koboSpan" id="kobo.85.1">Conventions </span><a id="_idIndexMarker590"/><span class="koboSpan" id="kobo.86.1">provide some considerations on context propagation (we’ll discuss this in the </span><em class="italic"><span class="koboSpan" id="kobo.87.1">Trace context propagation</span></em><span class="koboSpan" id="kobo.88.1"> section) and define generic attributes to describe messaging operations. </span><span class="koboSpan" id="kobo.88.2">Here are a few essential ones we’re going </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">to use:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.90.1">messaging.system</span></strong><span class="koboSpan" id="kobo.91.1">: Indicates that the span follows messaging semantics and describes the specific messaging system used, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.92.1">kafka</span></strong><span class="koboSpan" id="kobo.93.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.94.1">rabbitmq</span></strong><span class="koboSpan" id="kobo.95.1">. </span><span class="koboSpan" id="kobo.95.2">In our sample, we’ll </span><span class="No-Break"><span class="koboSpan" id="kobo.96.1">use </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.97.1">azqueues</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.98.1">.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.99.1">messaging.operation</span></strong><span class="koboSpan" id="kobo.100.1">: Identifies one of the standard operations: </span><strong class="source-inline"><span class="koboSpan" id="kobo.101.1">publish</span></strong><span class="koboSpan" id="kobo.102.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.103.1">receive</span></strong><span class="koboSpan" id="kobo.104.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.105.1">or </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.106.1">process</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.108.1">messaging.destination.name</span></strong><span class="koboSpan" id="kobo.109.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.110.1">messaging.source.name</span></strong><span class="koboSpan" id="kobo.111.1">: Describe a queue or topic name within a broker. </span><span class="koboSpan" id="kobo.111.2">The term </span><strong class="source-inline"><span class="koboSpan" id="kobo.112.1">destination</span></strong><span class="koboSpan" id="kobo.113.1"> is used on the producer and </span><strong class="source-inline"><span class="koboSpan" id="kobo.114.1">source</span></strong><span class="koboSpan" id="kobo.115.1"> is used on </span><span class="No-Break"><span class="koboSpan" id="kobo.116.1">the consumer.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.117.1">net.peer.name</span></strong><span class="koboSpan" id="kobo.118.1">: Identifies the broker </span><span class="No-Break"><span class="koboSpan" id="kobo.119.1">domain name.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.120.1">Let’s see how we can use the conventions to add observability signals that can help us document the application behavior or detect and resolve a new class of issues happening in </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">messaging scenarios.</span></span></p>
<h1 id="_idParaDest-180"><a id="_idTextAnchor179"/><span class="koboSpan" id="kobo.122.1">Instrumenting the producer</span></h1>
<p><span class="koboSpan" id="kobo.123.1">The producer is the component responsible for publishing messages to a broker. </span><span class="koboSpan" id="kobo.123.2">The publishing </span><a id="_idIndexMarker591"/><span class="koboSpan" id="kobo.124.1">process itself is usually synchronous: we send a request to the broker and get a response from it indicating whether the message was </span><span class="No-Break"><span class="koboSpan" id="kobo.125.1">published successfully.</span></span></p>
<p><span class="koboSpan" id="kobo.126.1">Depending on the messaging system and producer needs, one publish request may carry one or more messages. </span><span class="koboSpan" id="kobo.126.2">We’ll discuss batching in the </span><em class="italic"><span class="koboSpan" id="kobo.127.1">Instrumenting batching scenarios</span></em><span class="koboSpan" id="kobo.128.1"> section. </span><span class="koboSpan" id="kobo.128.2">For now, let’s focus on a single </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">message case.</span></span></p>
<p><span class="koboSpan" id="kobo.130.1">To trace it, we need to make sure we create an activity when we publish a message, so we can track the call duration and status and debug individual requests. </span><span class="koboSpan" id="kobo.130.2">We’d also be interested in metrics for duration, throughput, and failure rate – it’s important to budget cloud messaging solutions or scale </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">self-hosted brokers.</span></span></p>
<p><span class="koboSpan" id="kobo.132.1">Another essential part of producer instrumentation is context propagation. </span><span class="koboSpan" id="kobo.132.2">Let’s stop here for a second and </span><span class="No-Break"><span class="koboSpan" id="kobo.133.1">discuss it.</span></span></p>
<h2 id="_idParaDest-181"><a id="_idTextAnchor180"/><span class="koboSpan" id="kobo.134.1">Trace context propagation</span></h2>
<p><span class="koboSpan" id="kobo.135.1">When we instrument HTTP calls, context is propagated via HTTP request headers, which are part of </span><a id="_idIndexMarker592"/><span class="koboSpan" id="kobo.136.1">the request. </span><span class="koboSpan" id="kobo.136.2">In messaging, the context is carried via a transport call to the broker and is not propagated to a consumer. </span><span class="koboSpan" id="kobo.136.3">Transport call </span><a id="_idIndexMarker593"/><span class="koboSpan" id="kobo.137.1">trace context identifies the request, but not the message(s) </span><span class="No-Break"><span class="koboSpan" id="kobo.138.1">it carries.</span></span></p>
<p><span class="koboSpan" id="kobo.139.1">So, we need to propagate context inside the message to make sure it goes all the way to the consumer. </span><span class="koboSpan" id="kobo.139.2">But which </span><a id="_idIndexMarker594"/><span class="koboSpan" id="kobo.140.1">context should we inject? </span><span class="koboSpan" id="kobo.140.2">We have </span><span class="No-Break"><span class="koboSpan" id="kobo.141.1">several options:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.142.1">Use context from the current activity</span></strong><span class="koboSpan" id="kobo.143.1">: For instance, when we publish messages in the scope of an incoming HTTP request, we may use the context of the activity representing this HTTP server call. </span><span class="koboSpan" id="kobo.143.2">This works only if we send one message per incoming request. </span><span class="koboSpan" id="kobo.143.3">If we send more than one (each in an individual publish call), we’d not be able to tell which message the consumer call processed or identify whether we sent messages to the </span><span class="No-Break"><span class="koboSpan" id="kobo.144.1">right queues.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.145.1">Create an activity per message and inject its context</span></strong><span class="koboSpan" id="kobo.146.1">: Unique context allows us to trace messages individually and works in batching scenarios as well where we send multiple messages in one publish call. </span><span class="koboSpan" id="kobo.146.2">It also adds the overhead of creating an additional activity </span><span class="No-Break"><span class="koboSpan" id="kobo.147.1">per message.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.148.1">Reuse the publish activity</span></strong><span class="koboSpan" id="kobo.149.1">: When we publish one message in one call to the broker, we can uniquely identify a message and publish call with </span><span class="No-Break"><span class="koboSpan" id="kobo.150.1">one activity.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.151.1">The first option goes against OpenTelemetry messaging semantic conventions, which allow us to pick a suitable option from the last two. </span><span class="koboSpan" id="kobo.151.2">In our example, we’re using Azure Queue Storage, which does not support batching when publishing messages. </span><span class="koboSpan" id="kobo.151.3">So, we’re going to use the last option and create one activity to trace a publish call and inject its context into </span><span class="No-Break"><span class="koboSpan" id="kobo.152.1">the message.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.153.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.154.1">When forking or routing messages from one queue to another, the message might have pre-existing trace context injected in the upstream service. </span><span class="koboSpan" id="kobo.154.2">The default behavior in such a case should be to keep the message context intact. </span><span class="koboSpan" id="kobo.154.3">To correlate all operations that happen with the message, we can always add a link to an existing trace context in the message when publishing or </span><span class="No-Break"><span class="koboSpan" id="kobo.155.1">receiving it.</span></span></p>
<p><span class="koboSpan" id="kobo.156.1">Another interesting </span><a id="_idIndexMarker595"/><span class="koboSpan" id="kobo.157.1">aspect of Azure Queue Storage is that </span><a id="_idIndexMarker596"/><span class="koboSpan" id="kobo.158.1">it doesn’t support message metadata – the message is an opaque payload without any prescribed structure or format that the service carries over. </span><span class="koboSpan" id="kobo.158.2">So, similarly to gRPC streaming, which we covered in </span><a href="B19423_10.xhtml#_idTextAnchor161"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.159.1">Chapter 10</span></em></span></a><span class="koboSpan" id="kobo.160.1">, </span><em class="italic"><span class="koboSpan" id="kobo.161.1">Tracing Network Calls</span></em><span class="koboSpan" id="kobo.162.1">, we’ll need to define our own message structure or use one of the well-known event formats </span><a id="_idIndexMarker597"/><span class="koboSpan" id="kobo.163.1">available out there, such </span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">as </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.165.1">CloudEvents</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.166.1">.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.167.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.168.1">CloudEvents (</span><a href="https://cloudevents.io"><span class="koboSpan" id="kobo.169.1">https://cloudevents.io</span></a><span class="koboSpan" id="kobo.170.1">) is an </span><a id="_idIndexMarker598"/><span class="koboSpan" id="kobo.171.1">open standard that defines event structure in a vendor- and technology-agnostic way. </span><span class="koboSpan" id="kobo.171.2">It’s commonly used by cloud providers to notify applications about infrastructure changes or when implementing data change feeds. </span><span class="koboSpan" id="kobo.171.3">CloudEvents have distributed tracing extensions to carry W3C Trace Context as well as general-purpose metadata that can be used for other formats. </span><span class="koboSpan" id="kobo.171.4">OpenTelemetry also provides semantic conventions </span><span class="No-Break"><span class="koboSpan" id="kobo.172.1">for CloudEvents.</span></span></p>
<p><span class="koboSpan" id="kobo.173.1">For demo purposes, we’ll keep things simple and define our own tiny message model in the </span><span class="No-Break"><span class="koboSpan" id="kobo.174.1">following way:</span></span></p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.175.1">producer/Message.cs</span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.176.1">
public class Message
{
  ...
</span><span class="koboSpan" id="kobo.176.2">  </span><strong class="bold"><span class="koboSpan" id="kobo.177.1">public Dictionary&lt;string, string&gt; Headers { get; set; }</span></strong><span class="koboSpan" id="kobo.178.1"> =
    new ();
  public string? </span><span class="koboSpan" id="kobo.178.2">Text { get; set; }
}</span></pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Message.cs"><span class="koboSpan" id="kobo.179.1">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Message.cs</span></a></p>
<p><span class="koboSpan" id="kobo.180.1">We’ll use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.181.1">Headers</span></strong><span class="koboSpan" id="kobo.182.1"> property to propagate the trace context and will keep the payload in the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.183.1">Text</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.184.1"> property.</span></span></p>
<p><span class="koboSpan" id="kobo.185.1">Similarly to </span><a id="_idIndexMarker599"/><span class="koboSpan" id="kobo.186.1">the gRPC streaming examples we saw in </span><a href="B19423_10.xhtml#_idTextAnchor161"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.187.1">Chapter 10</span></em></span></a><span class="koboSpan" id="kobo.188.1">, </span><em class="italic"><span class="koboSpan" id="kobo.189.1">Tracing Network Calls</span></em><span class="koboSpan" id="kobo.190.1">, we can inject context into this message using the </span><a id="_idIndexMarker600"/><span class="koboSpan" id="kobo.191.1">OpenTelemetry propagator with the following </span><span class="No-Break"><span class="koboSpan" id="kobo.192.1">code snippet:</span></span></p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.193.1">producer/Controllers/SendController.cs</span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.194.1">
private void InjectContext(Message message, Activity? </span><span class="koboSpan" id="kobo.194.2">act)
{
  if (act != null)
  {
</span><strong class="bold"><span class="koboSpan" id="kobo.195.1">    _propagator.Inject(new (act.Context, Baggage.Current),</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.196.1">      message,</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.197.1">    static (m, k, v) =&gt; m.Headers[k] = v);</span></strong><span class="koboSpan" id="kobo.198.1">
  }
}</span></pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs"><span class="koboSpan" id="kobo.199.1">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs</span></a></p>
<p><span class="koboSpan" id="kobo.200.1">Now we have all we need to instrument a publish call – let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.201.1">do it.</span></span></p>
<h2 id="_idParaDest-182"><a id="_idTextAnchor181"/><span class="koboSpan" id="kobo.202.1">Tracing a publish call</span></h2>
<p><span class="koboSpan" id="kobo.203.1">We’ll need to create a new activity and put common messaging attributes on it to identify the broker, queue operation, and add other information. </span><span class="koboSpan" id="kobo.203.2">In the case of Azure Queue </span><a id="_idIndexMarker601"/><span class="koboSpan" id="kobo.204.1">Storage, we can use the account name as the broker identifier (as they are unique within a </span><span class="No-Break"><span class="koboSpan" id="kobo.205.1">public cloud).</span></span></p>
<p><span class="koboSpan" id="kobo.206.1">Then, we’ll inject </span><a id="_idIndexMarker602"/><span class="koboSpan" id="kobo.207.1">context into the message and proceed with publishing. </span><span class="koboSpan" id="kobo.207.2">After the message is published successfully, we can also record the information returned by the broker, such as the message ID and other details we might </span><span class="No-Break"><span class="koboSpan" id="kobo.208.1">consider useful.</span></span></p>
<p><span class="koboSpan" id="kobo.209.1">Here’s the </span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">corresponding code:</span></span></p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.211.1">producer/Controllers/SendController.cs</span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.212.1">
Stopwatch? </span><span class="koboSpan" id="kobo.212.2">duration = PublishDuration.Enabled ?
</span><span class="koboSpan" id="kobo.212.3">  Stopwatch.StartNew() : null;
using var act = StartPublishActivity();
InjectContext(message, Activity.Current);
try
{
  var receipt = await _queue.SendMessageAsync(
    BinaryData.FromObjectAsJson(message));
  act?.SetTag("messaging.message.id",
    receipt.Value.MessageId);
  RecordPublishMetrics(duration, "ok");
  ...
</span><span class="koboSpan" id="kobo.212.4">}
catch (Exception ex)
{
  act?.SetStatus(ActivityStatusCode.Error, ex.Message);
  RecordPublishMetrics(duration, "fail")
  ...
</span><span class="koboSpan" id="kobo.212.5">}</span></pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs"><span class="koboSpan" id="kobo.213.1">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs</span></a></p>
<p><span class="koboSpan" id="kobo.214.1">Here, we injected the context of </span><strong class="source-inline"><span class="koboSpan" id="kobo.215.1">Activity.Current</span></strong><span class="koboSpan" id="kobo.216.1"> with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.217.1">Inject</span></strong><span class="koboSpan" id="kobo.218.1"> method we implemented before. </span><span class="koboSpan" id="kobo.218.2">This may be useful if you want to turn off per-message activities. </span><span class="koboSpan" id="kobo.218.3">In such a case, per-message tracing will be limited, but consumer and producer calls will still be </span><a id="_idIndexMarker603"/><span class="koboSpan" id="kobo.219.1">correlated. </span><span class="koboSpan" id="kobo.219.2">We also record metrics here – stay tuned for </span><a id="_idIndexMarker604"/><span class="koboSpan" id="kobo.220.1">the details; we’re going to cover them in the </span><span class="No-Break"><span class="koboSpan" id="kobo.221.1">next section.</span></span></p>
<p><span class="koboSpan" id="kobo.222.1">Here’s the </span><strong class="source-inline"><span class="koboSpan" id="kobo.223.1">StartPublishActivity</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.224.1">method implementation:</span></span></p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.225.1">producer/Controllers/SendController.cs</span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.226.1">
var act = Source.StartActivity($"{_queue.Name} publish",
  ActivityKind.Producer);
if (act?.IsAllDataRequested == true)
  act.SetTag("messaging.system", "azqueues")
    .SetTag("messaging.operation", "publish")
    .SetTag("messaging.destination.name", _queue.Name)
    .SetTag("net.peer.name", _queue.AccountName)
}</span></pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs"><span class="koboSpan" id="kobo.227.1">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs</span></a></p>
<p><span class="koboSpan" id="kobo.228.1">The activity here has a </span><strong class="source-inline"><span class="koboSpan" id="kobo.229.1">producer</span></strong><span class="koboSpan" id="kobo.230.1"> kind, which indicates the start of an async flow. </span><span class="koboSpan" id="kobo.230.2">The name follows </span><a id="_idIndexMarker605"/><span class="koboSpan" id="kobo.231.1">OpenTelemetry semantic conventions, which </span><a id="_idIndexMarker606"/><span class="koboSpan" id="kobo.232.1">recommend using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.233.1">{queue_name} {operation}</span></strong><span class="koboSpan" id="kobo.234.1"> pattern. </span><span class="koboSpan" id="kobo.234.2">We can also cache it to avoid unnecessary </span><span class="No-Break"><span class="koboSpan" id="kobo.235.1">string formatting.</span></span></p>
<p><span class="koboSpan" id="kobo.236.1">This is it; we’ve covered producer tracing – let’s look at </span><span class="No-Break"><span class="koboSpan" id="kobo.237.1">metrics now.</span></span></p>
<h2 id="_idParaDest-183"><a id="_idTextAnchor182"/><span class="koboSpan" id="kobo.238.1">Producer metrics</span></h2>
<p><span class="koboSpan" id="kobo.239.1">Messaging-specific metrics </span><a id="_idIndexMarker607"/><span class="koboSpan" id="kobo.240.1">come as an addition to resource utilization, .NET runtime, HTTP, and other metrics you might want </span><span class="No-Break"><span class="koboSpan" id="kobo.241.1">to expose.</span></span></p>
<p><span class="koboSpan" id="kobo.242.1">To some extent, we can use HTTP metrics to monitor calls to Azure Queue Storage since they work on top of HTTP. </span><span class="koboSpan" id="kobo.242.2">This would allow us to monitor duration, success rate, and throughput for </span><a id="_idIndexMarker608"/><span class="koboSpan" id="kobo.243.1">individual HTTP calls to storage, but won’t allow us to distinguish queues within one </span><span class="No-Break"><span class="koboSpan" id="kobo.244.1">storage account.</span></span></p>
<p><span class="koboSpan" id="kobo.245.1">So, if we rely on metrics, we should record some messaging-specific ones that cover common indicators such as publish call duration, throughput, and latency for each queue </span><span class="No-Break"><span class="koboSpan" id="kobo.246.1">we use.</span></span></p>
<p><span class="koboSpan" id="kobo.247.1">We can report all of them using a duration histogram, as we saw in </span><a href="B19423_07.xhtml#_idTextAnchor115"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.248.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.249.1">, </span><em class="italic"><span class="koboSpan" id="kobo.250.1">Adding Custom Metrics</span></em><span class="koboSpan" id="kobo.251.1">. </span><span class="koboSpan" id="kobo.251.2">First, let’s initialize the duration histogram, as shown in the following </span><span class="No-Break"><span class="koboSpan" id="kobo.252.1">code snippet:</span></span></p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.253.1">producer/Controllers/SendController.cs</span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.254.1">
private static readonly Meter Meter = new("Queue.Publish");
private static readonly Histogram&lt;double&gt; PublishDuration =
  Meter.CreateHistogram&lt;double&gt;(
    "messaging.azqueues.publish.duration", ...);</span></pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs"><span class="koboSpan" id="kobo.255.1">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs</span></a></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.256.1">Meter</span></strong><span class="koboSpan" id="kobo.257.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.258.1">Histogram</span></strong><span class="koboSpan" id="kobo.259.1"> are static since we defined them in the controller. </span><span class="koboSpan" id="kobo.259.2">The controller lifetime is scoped to a request, so we keep them static to </span><span class="No-Break"><span class="koboSpan" id="kobo.260.1">stay efficient.</span></span></p>
<p><span class="koboSpan" id="kobo.261.1">As we saw in the tracing example, every time we publish a message, we’re also going to record a publish duration. </span><span class="koboSpan" id="kobo.261.2">Here’s how </span><span class="No-Break"><span class="koboSpan" id="kobo.262.1">it’s implemented:</span></span></p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.263.1">producer/Controllers/SendController.cs</span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.264.1">
public void RecordPublishMetrics(Stopwatch? </span><span class="koboSpan" id="kobo.264.2">dur,
  string status)
{
  ...
</span><span class="koboSpan" id="kobo.264.3">  TagList tags = new() {
    { "net.peer.name", _queue.AccountName },
    { "messaging.destination.name", _queue.Name },
    { "messaging.azqueue.status", status }};
  PublishDuration.Record(dur.Elapsed. </span><span class="koboSpan" id="kobo.264.4">TotalMilliseconds,
    tags);
}</span></pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs"><span class="koboSpan" id="kobo.265.1">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/producer/Controllers/SendController.cs</span></a></p>
<p><span class="koboSpan" id="kobo.266.1">Here, we used </span><a id="_idIndexMarker609"/><span class="koboSpan" id="kobo.267.1">the same attributes to describe the queue and added a custom status </span><a id="_idIndexMarker610"/><span class="koboSpan" id="kobo.268.1">attribute. </span><span class="koboSpan" id="kobo.268.2">Keep in mind that we need it to have low cardinality, so we only use </span><strong class="source-inline"><span class="koboSpan" id="kobo.269.1">ok</span></strong><span class="koboSpan" id="kobo.270.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.271.1">fail</span></strong><span class="koboSpan" id="kobo.272.1"> statuses when we call </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">this method.</span></span></p>
<p><span class="koboSpan" id="kobo.274.1">We’re done with the producer. </span><span class="koboSpan" id="kobo.274.2">Having basic tracing and metrics should give us a good starting point to diagnose and debug most of the issues and monitor overall producer health, as we’ll see in the </span><em class="italic"><span class="koboSpan" id="kobo.275.1">Performance analysis in messaging scenarios</span></em><span class="koboSpan" id="kobo.276.1"> section later. </span><span class="koboSpan" id="kobo.276.2">Let’s now explore instrumentation </span><span class="No-Break"><span class="koboSpan" id="kobo.277.1">on consumers.</span></span></p>
<h1 id="_idParaDest-184"><a id="_idTextAnchor183"/><span class="koboSpan" id="kobo.278.1">Instrumenting the consumer</span></h1>
<p><span class="koboSpan" id="kobo.279.1">While you might be able to get away without custom instrumentation on the producer, consumer instrumentation </span><span class="No-Break"><span class="koboSpan" id="kobo.280.1">is unavoidable.</span></span></p>
<p><span class="koboSpan" id="kobo.281.1">Some brokers </span><a id="_idIndexMarker611"/><span class="koboSpan" id="kobo.282.1">push messages to consumers using synchronous HTTP or RPC calls, and the existing framework instrumentation can provide the bare minimum of observability data. </span><span class="koboSpan" id="kobo.282.2">In all other cases, messaging traces and metrics are all we have to detect consumer health and </span><span class="No-Break"><span class="koboSpan" id="kobo.283.1">debug issues.</span></span></p>
<p><span class="koboSpan" id="kobo.284.1">Let’s start by tracing individual messages – recording when they arrive in the consumer and how they are processed. </span><span class="koboSpan" id="kobo.284.2">This allows us to debug issues by answering questions such as “Where is this message now?” </span><span class="koboSpan" id="kobo.284.3">or “Why did it take so long to process </span><span class="No-Break"><span class="koboSpan" id="kobo.285.1">the data?”</span></span></p>
<h2 id="_idParaDest-185"><a id="_idTextAnchor184"/><span class="koboSpan" id="kobo.286.1">Tracing consumer operations</span></h2>
<p><span class="koboSpan" id="kobo.287.1">When using Azure Queue Storage, applications request one or more messages from the queue. </span><span class="koboSpan" id="kobo.287.2">Received messages stay in the queue but become invisible to other consumers for configurable </span><a id="_idIndexMarker612"/><span class="koboSpan" id="kobo.288.1">visibility timeout. </span><span class="koboSpan" id="kobo.288.2">The application processes messages and, when done, deletes them from the queue. </span><span class="koboSpan" id="kobo.288.3">If processing fails with a transient issue, applications don’t delete messages. </span><span class="koboSpan" id="kobo.288.4">The same flow is commonly used when working with </span><span class="No-Break"><span class="koboSpan" id="kobo.289.1">AWS SQS.</span></span></p>
<p><span class="koboSpan" id="kobo.290.1">RabbitMQ- and AMQP-based messaging flows look similar, except messages can be pushed to the consumer so that the application reacts to the client library callback instead of polling </span><span class="No-Break"><span class="koboSpan" id="kobo.291.1">the queue.</span></span></p>
<p><span class="koboSpan" id="kobo.292.1">Callback-based delivery allows us to implement instrumentation in client libraries or provide a shared instrumentation library, and with a poll-based model, we essentially are forced to write at least some custom instrumentation for processing. </span><span class="koboSpan" id="kobo.292.2">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.293.1">do it.</span></span></p>
<p><span class="koboSpan" id="kobo.294.1">First, let’s instrument message processing in isolation from receiving. </span><span class="koboSpan" id="kobo.294.2">We’ll need to create an activity to track processing that will capture everything that happens there, including </span><span class="No-Break"><span class="koboSpan" id="kobo.295.1">message deletion:</span></span></p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.296.1">consumer/SingleReceiver.cs</span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.297.1">
using var act = StartProcessActivity(msg);
...
</span><span class="koboSpan" id="kobo.297.2">try
{
  await ProcessMessage(msg, token);
  await _queue.DeleteMessageAsync(msg.MessageId,
    msg.PopReceipt, token);
}
catch (Exception ex)
{
  await _queue.UpdateMessageAsync(msg.MessageId,
    msg.PopReceipt, visibilityTimeout: BackoffTimeout,
    cancellationToken: token);
  ...
</span><span class="koboSpan" id="kobo.297.3">  act?.SetStatus(ActivityStatusCode.Error, ex.Message);
}</span></pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs"><span class="koboSpan" id="kobo.298.1">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs</span></a></p>
<p><span class="koboSpan" id="kobo.299.1">Here, all the </span><a id="_idIndexMarker613"/><span class="koboSpan" id="kobo.300.1">processing logic happens in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.301.1">ProcessMessage</span></strong><span class="koboSpan" id="kobo.302.1"> method. </span><span class="koboSpan" id="kobo.302.2">When it completes successfully, we delete the message from the queue. </span><span class="koboSpan" id="kobo.302.3">Otherwise, we update its visibility to reappear in the queue after the </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">backoff timeout.</span></span></p>
<p><span class="koboSpan" id="kobo.304.1">Here’s the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.305.1">StartProcessActivity</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.306.1"> implementation:</span></span></p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.307.1">consumer/SingleReceiver.cs</span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.308.1">
PropagationContext ctx = ExtractContext(msg);
var current = new ActivityLink(Activity.Current?.Context ??
</span><span class="koboSpan" id="kobo.308.2">  default);
var act = _messageSource.StartActivity(
  $"{_queue.Name} process",
  ActivityKind.Consumer,
  ctx.ActivityContext,
  links: new[] { current });
if (act?.IsAllDataRequested == true)
  act.SetTag("net.peer.name",_queue.AccountName)
     .SetTag("messaging.system", "azqueues")
     .SetTag("messaging.operation", "process")
     .SetTag("messaging.source.name", _queue.Name)
     .SetTag("messaging.message.id", msg.MessageId);
  ...</span></pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs "><span class="koboSpan" id="kobo.309.1">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs</span></a></p>
<p><span class="koboSpan" id="kobo.310.1">Here, we extracted </span><a id="_idIndexMarker614"/><span class="koboSpan" id="kobo.311.1">the context from the message and used it as a parent of the processing activity. </span><span class="koboSpan" id="kobo.311.2">It has the </span><strong class="source-inline"><span class="koboSpan" id="kobo.312.1">consumer</span></strong><span class="koboSpan" id="kobo.313.1"> kind, which indicates the continuation of the asynchronous flow. </span><span class="koboSpan" id="kobo.313.2">We also kept </span><strong class="source-inline"><span class="koboSpan" id="kobo.314.1">Activity.Current</span></strong><span class="koboSpan" id="kobo.315.1"> as a link to preserve correlation. </span><span class="koboSpan" id="kobo.315.2">We also added </span><span class="No-Break"><span class="koboSpan" id="kobo.316.1">messaging attributes.</span></span></p>
<p><span class="koboSpan" id="kobo.317.1">Message deletion and updates are traces by HTTP or Azure Queue SDK instrumentations. </span><span class="koboSpan" id="kobo.317.2">They don’t have messaging semantics, but should give us reasonable observability. </span><span class="koboSpan" id="kobo.317.3">Corresponding activities become children of the processing one, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.318.1">Figure 11</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.319.1">.2</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.320.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer116">
<span class="koboSpan" id="kobo.321.1"><img alt="Figure 11.2 – Message trace from producer to consumer" src="image/B19423_11_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.322.1">Figure 11.2 – Message trace from producer to consumer</span></p>
<p><span class="koboSpan" id="kobo.323.1">The message was published and then we see two attempts to process it on the consumer: the first attempt failed. </span><span class="koboSpan" id="kobo.323.2">The second try was successful, and the message </span><span class="No-Break"><span class="koboSpan" id="kobo.324.1">was deleted.</span></span></p>
<p><span class="koboSpan" id="kobo.325.1">What’s missing in the preceding screenshot? </span><span class="koboSpan" id="kobo.325.2">We don’t see how and when the message was received. </span><span class="koboSpan" id="kobo.325.3">This might not be important on this trace, but look at another one in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.326.1">Figure 11</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.327.1">.3</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.328.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer117">
<span class="koboSpan" id="kobo.329.1"><img alt="Figure 11.3 – Message trace with a nine-minute gap between producer and consumer" src="image/B19423_11_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.330.1">Figure 11.3 – Message trace with a nine-minute gap between producer and consumer</span></p>
<p><span class="koboSpan" id="kobo.331.1">Here, nothing has </span><a id="_idIndexMarker615"/><span class="koboSpan" id="kobo.332.1">happened for almost nine minutes. </span><span class="koboSpan" id="kobo.332.2">Was the message received by a consumer during that time? </span><span class="koboSpan" id="kobo.332.3">Were the consumers alive? </span><span class="koboSpan" id="kobo.332.4">What were they doing? </span><span class="koboSpan" id="kobo.332.5">Were there any problems in the Azure Queue service that prevented messages from </span><span class="No-Break"><span class="koboSpan" id="kobo.333.1">being received?</span></span></p>
<p><span class="koboSpan" id="kobo.334.1">We’ll see how to answer these questions later. </span><span class="koboSpan" id="kobo.334.2">Now, let’s focus on tracing the </span><span class="No-Break"><span class="koboSpan" id="kobo.335.1">receive operation.</span></span></p>
<p><span class="koboSpan" id="kobo.336.1">The challenge with the receive operation is that the message trace context is available after the message is received and the corresponding operation is about to end. </span><span class="koboSpan" id="kobo.336.2">We could add links to message trace contexts then, but it’s currently only possible to add them at activity </span><span class="No-Break"><span class="koboSpan" id="kobo.337.1">start time.</span></span></p>
<p><span class="koboSpan" id="kobo.338.1">This is likely to change, but for now, we’ll work around it by tracing the receive-and-process iteration and adding an attribute with the received message ID so we can find all spans that touched </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">this message:</span></span></p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.340.1">consumer/SingleReceiver.cs</span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.341.1">
using var act = _receiverSource
  .StartActivity("ReceiveAndProcess");
try
{
  var response = await _queue.ReceiveMessagesAsync(1,
    ProcessingTimeout, token);
  QueueMessage[] messages = response.Value;
  if (messages.Length == 0)
  {
    ...; continue;
  }
  act?.SetTag("messaging.message.id",
    messages[0].MessageId);
  await ProcessAndSettle(messages[0], token);
  ...
</span><span class="koboSpan" id="kobo.341.2">}
catch (Exception ex)
{
  act?.SetStatus(ActivityStatusCode.Error, ex.Message);
  ...
</span><span class="koboSpan" id="kobo.341.3">}</span></pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs"><span class="koboSpan" id="kobo.342.1">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs</span></a></p>
<p><span class="koboSpan" id="kobo.343.1">Here, we receive </span><a id="_idIndexMarker616"/><span class="koboSpan" id="kobo.344.1">at most one message from the queue. </span><span class="koboSpan" id="kobo.344.2">If a message was received, we </span><span class="No-Break"><span class="koboSpan" id="kobo.345.1">process it.</span></span></p>
<p><span class="koboSpan" id="kobo.346.1">One iteration is tracked with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.347.1">ReceiveAndProcess</span></strong><span class="koboSpan" id="kobo.348.1"> activity, which becomes a parent to the receiving operation. </span><span class="koboSpan" id="kobo.348.2">The message processing activity is created in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.349.1">ProcessAndSettle</span></strong><span class="koboSpan" id="kobo.350.1"> method and links to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.351.1">ReceiveAndProcess</span></strong><span class="koboSpan" id="kobo.352.1"> activity, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.353.1">Figure 11</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.354.1">.4</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.355.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer118">
<span class="koboSpan" id="kobo.356.1"><img alt="Figure 11.4 – Link from processing to outer loop activity" src="image/B19423_11_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.357.1">Figure 11.4 – Link from processing to outer loop activity</span></p>
<p><span class="koboSpan" id="kobo.358.1">If we follow </span><a id="_idIndexMarker617"/><span class="koboSpan" id="kobo.359.1">the link, we’ll see an outer loop trace like the one shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.360.1">Figure 11</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.361.1">.5</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.362.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer119">
<span class="koboSpan" id="kobo.363.1"><img alt="Figure 11.5 – Trace representing the receive and process iteration" src="image/B19423_11_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.364.1">Figure 11.5 – Trace representing the receive and process iteration</span></p>
<p><span class="koboSpan" id="kobo.365.1">Since more and more observability backends are providing better support for links, it can be more convenient to use them in </span><span class="No-Break"><span class="koboSpan" id="kobo.366.1">your backend.</span></span></p>
<p><span class="koboSpan" id="kobo.367.1">With iteration instrumented, we can now correlate receiving and processing or see how long a full loop cycle takes. </span><span class="koboSpan" id="kobo.367.2">This can help us understand whether consumers are alive and trying to receive and </span><span class="No-Break"><span class="koboSpan" id="kobo.368.1">process something.</span></span></p>
<p><span class="koboSpan" id="kobo.369.1">We’re stamping the </span><strong class="source-inline"><span class="koboSpan" id="kobo.370.1">messaging.message.id</span></strong><span class="koboSpan" id="kobo.371.1"> attribute on all spans to simplify finding all operations related to any </span><span class="No-Break"><span class="koboSpan" id="kobo.372.1">given message.</span></span></p>
<p><span class="koboSpan" id="kobo.373.1">Now, back to the nine-minute gap we saw in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.374.1">Figure 11</span></em></span><em class="italic"><span class="koboSpan" id="kobo.375.1">.3</span></em><span class="koboSpan" id="kobo.376.1">. </span><span class="koboSpan" id="kobo.376.2">What happened there is that we got too many messages in the queue – they were produced faster than we consumed them. </span><span class="koboSpan" id="kobo.376.3">By looking at gaps in individual traces, we can suspect that message spent time in the queue, but can’t tell for sure. </span><span class="koboSpan" id="kobo.376.4">What we need is to see the rate at which messages are published, processed, and deleted. </span><span class="koboSpan" id="kobo.376.5">We should also understand how long messages spend in the queue and how big the queue is. </span><span class="koboSpan" id="kobo.376.6">Let’s see how we can record and use </span><span class="No-Break"><span class="koboSpan" id="kobo.377.1">such metrics.</span></span></p>
<h2 id="_idParaDest-186"><a id="_idTextAnchor185"/><span class="koboSpan" id="kobo.378.1">Consumer metrics</span></h2>
<p><span class="koboSpan" id="kobo.379.1">Similar to producers, we should enable common runtime and process metrics so we know the </span><a id="_idIndexMarker618"/><span class="koboSpan" id="kobo.380.1">resource utilization for consumer processes. </span><span class="koboSpan" id="kobo.380.2">We should also record the processing loop duration, which will give us the error rate </span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">and throughput.</span></span></p>
<p><span class="koboSpan" id="kobo.382.1">From a messaging perspective, we’d also want to cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.383.1">following things:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.384.1">The amount of time messages spend in the queue is a great indicator of consumer health and scale. </span><span class="koboSpan" id="kobo.384.2">When there are not enough consumers, the amount of time spent in the queue will grow and can be used to scale consumers up. </span><span class="koboSpan" id="kobo.384.3">When it decreases consistently, it could serve as a signal to scale </span><span class="No-Break"><span class="koboSpan" id="kobo.385.1">consumers down.</span></span></li>
<li><span class="koboSpan" id="kobo.386.1">The number of messages in the queue provides similar data, but in real time. </span><span class="koboSpan" id="kobo.386.2">It includes messages that have not yet been processed. </span><span class="koboSpan" id="kobo.386.3">Queue size metric can also be recorded on the producer side without ever depending on </span><span class="No-Break"><span class="koboSpan" id="kobo.387.1">the consumer.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.388.1">These metrics, or similar ones you can come up with, and their trends over time provide a great indication of </span><span class="No-Break"><span class="koboSpan" id="kobo.389.1">consumer health.</span></span></p>
<p><span class="koboSpan" id="kobo.390.1">These metrics increase if consumer performance degrades or the error rate increases. </span><span class="koboSpan" id="kobo.390.2">They won’t be helpful if consumers fail to process messages but immediately delete them from the queue, but this will manifest in high error rate. </span><span class="koboSpan" id="kobo.390.3">So, let’s go ahead and instrument our application with </span><span class="No-Break"><span class="koboSpan" id="kobo.391.1">these metrics.</span></span></p>
<h3><span class="koboSpan" id="kobo.392.1">Duration, throughput, and failure rate</span></h3>
<p><span class="koboSpan" id="kobo.393.1">We’re going to measure the processing loop duration, which includes trying to receive a message </span><a id="_idIndexMarker619"/><span class="koboSpan" id="kobo.394.1">and its processing. </span><span class="koboSpan" id="kobo.394.2">Measuring the receiving and processing duration independently would be even more precise and is something to consider in your </span><span class="No-Break"><span class="koboSpan" id="kobo.395.1">production applications.</span></span></p>
<p><span class="koboSpan" id="kobo.396.1">At the beginning of the loop, we’ll start a stopwatch to measure operation duration, and once processing completes, we’ll report it as a histogram along with queue information and the status. </span><span class="koboSpan" id="kobo.396.2">Let’s first create the </span><span class="No-Break"><span class="koboSpan" id="kobo.397.1">histogram instrument:</span></span></p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.398.1">consumer/SingleReceiver.cs</span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.399.1">
private readonly Meter _meter = new ("Queue.Receive");
private readonly Histogram&lt;double&gt; _loopDuration;
...
</span><span class="koboSpan" id="kobo.399.2">_loopDuration = _meter.CreateHistogram&lt;double&gt;(
  "messaging.azqueues.process.loop.duration", "ms",
  "Receive and processing duration.");</span></pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs"><span class="koboSpan" id="kobo.400.1">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs</span></a></p>
<p><span class="koboSpan" id="kobo.401.1">We create </span><a id="_idIndexMarker620"/><span class="koboSpan" id="kobo.402.1">meter and duration instruments here as instance variables, which we dispose of along with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.403.1">SingleReceiver</span></strong><span class="koboSpan" id="kobo.404.1"> instance. </span><span class="koboSpan" id="kobo.404.2">The receiver extends the </span><strong class="source-inline"><span class="koboSpan" id="kobo.405.1">BackgroundService</span></strong><span class="koboSpan" id="kobo.406.1"> interface and is registered in the dependency injection container as a singleton, so they are all disposed of once the application </span><span class="No-Break"><span class="koboSpan" id="kobo.407.1">shuts down.</span></span></p>
<p><span class="koboSpan" id="kobo.408.1">The processing loop instrumentation can be done in the </span><span class="No-Break"><span class="koboSpan" id="kobo.409.1">following way:</span></span></p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.410.1">consumer/SingleReceiver.cs</span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.411.1">
Stopwatch? </span><span class="koboSpan" id="kobo.411.2">duration = Stopwatch.StartNew();
try
{
  var response = await _queue.ReceiveMessagesAsync(1,
    ProcessingTimeout, token);
  QueueMessage[] messages = response.Value;
  RecordLag(messages);
  if (messages.Length == 0)
  {
    ...
</span><span class="koboSpan" id="kobo.411.3">    RecordLoopDuration(duration, "empty");
    continue;
  }
  ...
</span><span class="koboSpan" id="kobo.411.4">  await ProcessAndSettle(messages[0], token);
  RecordLoopDuration(duration, "ok");
}
catch (Exception ex)
{
  RecordLoopDuration(duration, "fail"); ...
</span><span class="koboSpan" id="kobo.411.5">}</span></pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs"><span class="koboSpan" id="kobo.412.1">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs</span></a></p>
<p><span class="koboSpan" id="kobo.413.1">Here, we </span><a id="_idIndexMarker621"/><span class="koboSpan" id="kobo.414.1">record the duration of each iteration along with the queue information and status. </span><span class="koboSpan" id="kobo.414.2">The status can have the following values: </span><strong class="source-inline"><span class="koboSpan" id="kobo.415.1">ok</span></strong><span class="koboSpan" id="kobo.416.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.417.1">fail</span></strong><span class="koboSpan" id="kobo.418.1">, or </span><strong class="source-inline"><span class="koboSpan" id="kobo.419.1">empty</span></strong><span class="koboSpan" id="kobo.420.1"> (if no messages were received). </span><span class="koboSpan" id="kobo.420.2">In real applications, you probably want to be more precise and add a few more statuses to indicate the failure reason. </span><span class="koboSpan" id="kobo.420.3">For example, it would be important to record why the receive operation failed, whether there was a serialization or validation error, processing timed out, or it failed with a terminal or </span><span class="No-Break"><span class="koboSpan" id="kobo.421.1">transient error.</span></span></p>
<p><span class="koboSpan" id="kobo.422.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.423.1">RecordLoopDuration</span></strong><span class="koboSpan" id="kobo.424.1"> method implementation is shown in </span><span class="No-Break"><span class="koboSpan" id="kobo.425.1">this snippet:</span></span></p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.426.1">consumer/SingleReceiver.cs</span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.427.1">
TagList tags = new () {
  { "net.peer.name", _queue.AccountName },
  { "messaging.source.name", _queue.Name },
  { "messaging.azqueue.status", status }};
_loopDuration.Record(duration.ElapsedMilliseconds, tags);</span></pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs"><span class="koboSpan" id="kobo.428.1">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs</span></a></p>
<p><span class="koboSpan" id="kobo.429.1">We’ll see how we can use this metric later in this chapter. </span><span class="koboSpan" id="kobo.429.2">Let’s first implement consumer lag and </span><span class="No-Break"><span class="koboSpan" id="kobo.430.1">queue size.</span></span></p>
<h3><span class="koboSpan" id="kobo.431.1">Consumer lag</span></h3>
<p><span class="koboSpan" id="kobo.432.1">In the </span><a id="_idIndexMarker622"/><span class="koboSpan" id="kobo.433.1">code sample showing metrics in the processing loop, we called into the </span><strong class="source-inline"><span class="koboSpan" id="kobo.434.1">RecordLag</span></strong><span class="koboSpan" id="kobo.435.1"> method as soon as we received a message. </span><span class="koboSpan" id="kobo.435.2">Consumer lag records the approximate time a message spent in the queue – the delta between the receive and </span><span class="No-Break"><span class="koboSpan" id="kobo.436.1">enqueue time.</span></span></p>
<p><span class="koboSpan" id="kobo.437.1">The enqueue time is recorded by the Azure Queue service and is exposed as a property on the </span><strong class="source-inline"><span class="koboSpan" id="kobo.438.1">QueueMessage</span></strong><span class="koboSpan" id="kobo.439.1"> instance. </span><span class="koboSpan" id="kobo.439.2">We can record the metric with the </span><span class="No-Break"><span class="koboSpan" id="kobo.440.1">following code:</span></span></p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.441.1">consumer/SingleReceiver.cs</span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.442.1">
_consumerLag = _meter.CreateHistogram&lt;double&gt;(
  "messaging.azqueues.consumer.lag", "s", ...);
...
</span><span class="koboSpan" id="kobo.442.2">long receivedAt = DateTimeOffset.UtcNow
  .ToUnixTimeMilliseconds();
TagList tags = new () {
  { "net.peer.name", _queue.AccountName },
  { "messaging.source.name", _queue.Name }};
foreach (var msg in messages
    .Where(m =&gt; m.InsertedOn.HasValue))
{
  long insertedOn = msg.InsertedOn!
</span><span class="koboSpan" id="kobo.442.3">    .Value.ToUnixTimeMilliseconds());
  long lag = Math.Max(1, receivedAt - insertedOn);
  _consumerLag.Record(lag/1000d, tags);
}</span></pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs"><span class="koboSpan" id="kobo.443.1">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/SingleReceiver.cs</span></a></p>
<p><span class="koboSpan" id="kobo.444.1">Here, we create a histogram that represents the lag (in seconds) and record it for every received message as the difference between the current time and the time at which the message was received by </span><span class="No-Break"><span class="koboSpan" id="kobo.445.1">the broker.</span></span></p>
<p><span class="koboSpan" id="kobo.446.1">Note that </span><a id="_idIndexMarker623"/><span class="koboSpan" id="kobo.447.1">these timestamps usually come from two different computers – the difference can be negative and is not precise due to clock skew. </span><span class="koboSpan" id="kobo.447.2">The margin of error can reach seconds but may, to some extent, be corrected within </span><span class="No-Break"><span class="koboSpan" id="kobo.448.1">your system.</span></span></p>
<p><span class="koboSpan" id="kobo.449.1">Clock skew should be expected, but sometimes things can go really wrong. </span><span class="koboSpan" id="kobo.449.2">I once was involved in investigating an incident that took our service down in one of the data centers. </span><span class="koboSpan" id="kobo.449.3">It happened because of the wrong time server configuration, which moved the clock on one of the services back a few hours. </span><span class="koboSpan" id="kobo.449.4">It broke authentication – the authentication tokens had timestamps from hours ago and were </span><span class="No-Break"><span class="koboSpan" id="kobo.450.1">considered expired.</span></span></p>
<p><span class="koboSpan" id="kobo.451.1">Despite being imprecise, consumer lag should give us an idea of how long messages spend in a queue. </span><span class="koboSpan" id="kobo.451.2">We record it every time a message is received, so it also reflects redeliveries. </span><span class="koboSpan" id="kobo.451.3">Also, we record it before we know whether processing was successful, so it does not have </span><span class="No-Break"><span class="koboSpan" id="kobo.452.1">any status.</span></span></p>
<p><span class="koboSpan" id="kobo.453.1">Before we record lag on the consumer, we first need to receive a message. </span><span class="koboSpan" id="kobo.453.2">When we see a huge lag, it’s a good signal that something is not right, but it does not tell us how many messages have not yet </span><span class="No-Break"><span class="koboSpan" id="kobo.454.1">been received.</span></span></p>
<p><span class="koboSpan" id="kobo.455.1">For example, when the load is low and the queue is empty, there might be a few invalid messages that are stuck there. </span><span class="koboSpan" id="kobo.455.2">It’s a bug, but it can be fixed during business hours. </span><span class="koboSpan" id="kobo.455.3">To detect how big the issue is, we also need to know how many messages are in the queue. </span><span class="koboSpan" id="kobo.455.4">Let’s see how to </span><span class="No-Break"><span class="koboSpan" id="kobo.456.1">implement it.</span></span></p>
<h3><span class="koboSpan" id="kobo.457.1">Queue size</span></h3>
<p><span class="koboSpan" id="kobo.458.1">Azure Queue Storage as well as Amazon SQS allow us to retrieve an approximate count of messages. </span><span class="koboSpan" id="kobo.458.2">We can register another </span><strong class="source-inline"><span class="koboSpan" id="kobo.459.1">BackgroundService</span></strong><span class="koboSpan" id="kobo.460.1"> implementation to retrieve the </span><a id="_idIndexMarker624"/><span class="koboSpan" id="kobo.461.1">count periodically. </span><span class="koboSpan" id="kobo.461.2">This can be done on the consumer </span><span class="No-Break"><span class="koboSpan" id="kobo.462.1">or producer.</span></span></p>
<p><span class="koboSpan" id="kobo.463.1">We’ll use a gauge instrument to report it, as shown in this </span><span class="No-Break"><span class="koboSpan" id="kobo.464.1">code snippet:</span></span></p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.465.1">consumer/QueueSizeReporter.cs</span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.466.1">
TagList tags = new () {
  { "net.peer.name", queue.AccountName},
  { "messaging.source.name", queue.Name}};
_queueSize = _meter.CreateObservableGauge(
  "messaging.azqueues.queue.size",
  () =&gt; new Measurement&lt;long&gt;(_currentQueueSize, tags), ...);</span></pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.467.1">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/QueueSizeReporter.cs</span></p>
<p><span class="koboSpan" id="kobo.468.1">We passed a callback that returns a </span><strong class="source-inline"><span class="koboSpan" id="kobo.469.1">_currentQueueSize</span></strong><span class="koboSpan" id="kobo.470.1"> instance variable. </span><span class="koboSpan" id="kobo.470.2">We’re going to update it every several seconds as we retrieve the size from </span><span class="No-Break"><span class="koboSpan" id="kobo.471.1">the queue:</span></span></p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.472.1">consumer/QueueSizeReporter.cs</span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.473.1">
var res = await _queue.GetPropertiesAsync(token);
_currentQueueSize = res.Value.ApproximateMessagesCount;</span></pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.474.1">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/consumer/QueueSizeReporter.cs</span></p>
<p><span class="koboSpan" id="kobo.475.1">That’s it – now we measure the queue size. </span><span class="koboSpan" id="kobo.475.2">This number alone does not tell the entire story, but if it’s significantly different from the baseline or grows fast, this is a great indication of </span><span class="No-Break"><span class="koboSpan" id="kobo.476.1">a problem.</span></span></p>
<p><span class="koboSpan" id="kobo.477.1">Once the load grows, the queue size will also go up and we may try to add more consumers or optimize them. </span><span class="koboSpan" id="kobo.477.2">One of the typical optimizations is batching – it helps reduce the </span><a id="_idIndexMarker625"/><span class="koboSpan" id="kobo.478.1">number of network calls and would utilize consumer instances better. </span><span class="koboSpan" id="kobo.478.2">Let’s see how we can </span><span class="No-Break"><span class="koboSpan" id="kobo.479.1">instrument it.</span></span></p>
<h1 id="_idParaDest-187"><a id="_idTextAnchor186"/><span class="koboSpan" id="kobo.480.1">Instrumenting batching scenarios</span></h1>
<p><span class="koboSpan" id="kobo.481.1">Instrumentation </span><a id="_idIndexMarker626"/><span class="koboSpan" id="kobo.482.1">for batching scenarios can be different depending on the use case – transport-level batching needs a slightly different approach compared to </span><span class="No-Break"><span class="koboSpan" id="kobo.483.1">batch processing.</span></span></p>
<h2 id="_idParaDest-188"><a id="_idTextAnchor187"/><span class="koboSpan" id="kobo.484.1">Batching on a transport level</span></h2>
<p><span class="koboSpan" id="kobo.485.1">Messages can be batched together to minimize the number of network calls. </span><span class="koboSpan" id="kobo.485.2">It can be used by </span><a id="_idIndexMarker627"/><span class="koboSpan" id="kobo.486.1">producers or consumers, and systems such as Kafka, Amazon SQS, or Azure Service Bus support batching on </span><span class="No-Break"><span class="koboSpan" id="kobo.487.1">both sides.</span></span></p>
<p><span class="koboSpan" id="kobo.488.1">On the consumer, when multiple messages are received together but processed independently, everything we had for single message processing </span><span class="No-Break"><span class="koboSpan" id="kobo.489.1">still applies.</span></span></p>
<p><span class="koboSpan" id="kobo.490.1">From a </span><a id="_idIndexMarker628"/><span class="koboSpan" id="kobo.491.1">tracing perspective, the only thing we’d want to change is to add attributes that record all received message identifiers and batch size on the outer </span><span class="No-Break"><span class="koboSpan" id="kobo.492.1">iteration activity.</span></span></p>
<p><span class="koboSpan" id="kobo.493.1">From the metrics side, we’d also want to measure individual message processing duration, error rate, and throughput. </span><span class="koboSpan" id="kobo.493.2">We can track them all by adding a message processing </span><span class="No-Break"><span class="koboSpan" id="kobo.494.1">duration histogram.</span></span></p>
<p><span class="koboSpan" id="kobo.495.1">When we send multiple messages in a batch, we still need to trace these messages independently . </span><span class="koboSpan" id="kobo.495.2">To do so, we’d have to create an activity per message and inject unique trace context into each message. </span><span class="koboSpan" id="kobo.495.3">The publish activity then should be linked to all the messages </span><span class="No-Break"><span class="koboSpan" id="kobo.496.1">being published.</span></span></p>
<p><span class="koboSpan" id="kobo.497.1">The main question here is when to create a per-message activity and inject context into the message. </span><span class="koboSpan" id="kobo.497.2">Essentially, the message trace context should continue the operation that created the message. </span><span class="koboSpan" id="kobo.497.3">So, if we buffer messages from different unrelated operations and then send them in a background thread, we should create message activities when the message is created. </span><span class="koboSpan" id="kobo.497.4">Then, a batch publish operation will link to independent, unrelated </span><span class="No-Break"><span class="koboSpan" id="kobo.498.1">trace contexts.</span></span></p>
<p><span class="koboSpan" id="kobo.499.1">The duration </span><a id="_idIndexMarker629"/><span class="koboSpan" id="kobo.500.1">metric for the publish operation remains the same as for the single-message case we implemented before, but we should consider adding another metric to describe the batch size and the exact number of sent messages – we won’t be able to figure it out from the </span><span class="No-Break"><span class="koboSpan" id="kobo.501.1">publish duration.</span></span></p>
<h2 id="_idParaDest-189"><a id="_idTextAnchor188"/><span class="koboSpan" id="kobo.502.1">Processing batches</span></h2>
<p><span class="koboSpan" id="kobo.503.1">In some cases, we process messages in batches, for example, when aggregating data for analytic purposes, replicating or archiving received data. </span><span class="koboSpan" id="kobo.503.2">In such cases, it’s just not possible to </span><a id="_idIndexMarker630"/><span class="koboSpan" id="kobo.504.1">separate individual messages. </span><span class="koboSpan" id="kobo.504.2">Things get even more complicated in scenarios such as routing or sharding, when a received batch is split into several new batches and sent to the </span><span class="No-Break"><span class="koboSpan" id="kobo.505.1">next destination.</span></span></p>
<p><span class="koboSpan" id="kobo.506.1">We can record relationships using links – this will allow us to tell whether (when and how many times) a message was received, and which processing operation it </span><span class="No-Break"><span class="koboSpan" id="kobo.507.1">contributed to.</span></span></p>
<p><span class="koboSpan" id="kobo.508.1">Essentially, we create a batch-processing activity with links to all messages being processed. </span><span class="koboSpan" id="kobo.508.2">Links have attributes and there we can put important message metadata, such as the delivery count, message ID, or </span><span class="No-Break"><span class="koboSpan" id="kobo.509.1">insertion time.</span></span></p>
<p><span class="koboSpan" id="kobo.510.1">From a metrics perspective, consumer lag (measured per message), queue size, and processing duration (throughput and failure rate) still apply. </span><span class="koboSpan" id="kobo.510.2">We might also want to report the batch size as </span><span class="No-Break"><span class="koboSpan" id="kobo.511.1">a histogram.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.512.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.513.1">Message and batch processing are frequently done outside of messaging client library control, by application code or integration frameworks. </span><span class="koboSpan" id="kobo.513.2">It’s rarely possible for auto-instrumentation to trace or measure processing calls. </span><span class="koboSpan" id="kobo.513.3">These scenarios vary a lot from application to application, requiring custom instrumentations tuned to specific use cases and </span><span class="No-Break"><span class="koboSpan" id="kobo.514.1">messaging systems.</span></span></p>
<p><span class="koboSpan" id="kobo.515.1">Now that we have an idea of how to instrument messaging scenarios, let’s see how we can use it </span><span class="No-Break"><span class="koboSpan" id="kobo.516.1">in practice.</span></span></p>
<h1 id="_idParaDest-190"><a id="_idTextAnchor189"/><span class="koboSpan" id="kobo.517.1">Performance analysis in messaging scenarios</span></h1>
<p><span class="koboSpan" id="kobo.518.1">We’re going to use our demo application to simulate a few common problems and use signals we </span><a id="_idIndexMarker631"/><span class="koboSpan" id="kobo.519.1">have to detect and debug issues. </span><span class="koboSpan" id="kobo.519.2">Let’s start the application with the </span><span class="No-Break"><span class="koboSpan" id="kobo.520.1">following command:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.521.1">
$ docker-compose up --build --scale consumer=3</span></pre>
<p><span class="koboSpan" id="kobo.522.1">It will run one producer and three consumers along with the </span><span class="No-Break"><span class="koboSpan" id="kobo.523.1">observability stack.</span></span></p>
<p><span class="koboSpan" id="kobo.524.1">You can now send a request to the producer at </span><strong class="source-inline"><span class="koboSpan" id="kobo.525.1">http://localhost:5051/send</span></strong><span class="koboSpan" id="kobo.526.1">, which sends one message to the queue and returns receipt information as </span><span class="No-Break"><span class="koboSpan" id="kobo.527.1">a response.</span></span></p>
<p><span class="koboSpan" id="kobo.528.1">Now you need to add some load with the tool of your choice. </span><span class="koboSpan" id="kobo.528.2">If you use </span><strong class="source-inline"><span class="koboSpan" id="kobo.529.1">bombardier</span></strong><span class="koboSpan" id="kobo.530.1">, you can do it with the </span><span class="No-Break"><span class="koboSpan" id="kobo.531.1">following command:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.532.1">
$ bombardier -c 1 -d 30m http://localhost:5051/send</span></pre>
<p><span class="koboSpan" id="kobo.533.1">It sends requests to the producer in one connection. </span><span class="koboSpan" id="kobo.533.2">You can play with a different number of connections and consumers in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.534.1">docker-compose</span></strong><span class="koboSpan" id="kobo.535.1"> command to see how the </span><span class="No-Break"><span class="koboSpan" id="kobo.536.1">metrics change.</span></span></p>
<p><span class="koboSpan" id="kobo.537.1">You might also want to install Grafana and import the dashboard from the book’s repository (https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter11/grafana-dashboard.json) to look at all metrics </span><span class="No-Break"><span class="koboSpan" id="kobo.538.1">at once.</span></span></p>
<p><span class="koboSpan" id="kobo.539.1">How do we check whether the consumers are working properly? </span><span class="koboSpan" id="kobo.539.2">We can start with consumer lag and queue size metrics. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.540.1">Figure 11</span></em></span><em class="italic"><span class="koboSpan" id="kobo.541.1">.6</span></em><span class="koboSpan" id="kobo.542.1"> shows the 95</span><span class="superscript"><span class="koboSpan" id="kobo.543.1">th</span></span><span class="koboSpan" id="kobo.544.1"> percentile for consumer lag obtained with the </span><span class="No-Break"><span class="koboSpan" id="kobo.545.1">following query:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.546.1">
histogram_quantile(0.95,
  sum(rate(messaging_azqueues_consumer_lag_seconds_bucket[1m]))
  by (le, messaging_source_name, net_peer_name)
)</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer120">
<span class="koboSpan" id="kobo.547.1"><img alt="Figure 11.6 – Consumer lag grows over time" src="image/B19423_11_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.548.1">Figure 11.6 – Consumer lag grows over time</span></p>
<p><span class="koboSpan" id="kobo.549.1">Consumer lag </span><a id="_idIndexMarker632"/><span class="koboSpan" id="kobo.550.1">grows almost to 600 seconds, and if we look at the queue size, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.551.1">Figure 11</span></em></span><em class="italic"><span class="koboSpan" id="kobo.552.1">.7</span></em><span class="koboSpan" id="kobo.553.1">, we’ll see there were up to about 11,000 messages in </span><span class="No-Break"><span class="koboSpan" id="kobo.554.1">the queue:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer121">
<span class="koboSpan" id="kobo.555.1"><img alt="Figure 11.7 – Queue size grows and then slowly goes down" src="image/B19423_11_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.556.1">Figure 11.7 – Queue size grows and then slowly goes down</span></p>
<p><span class="koboSpan" id="kobo.557.1">Here’s the query for the </span><span class="No-Break"><span class="koboSpan" id="kobo.558.1">queue size:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.559.1">
max by (net_peer_name, messaging_source_name)
(messaging_azqueues_queue_size_messages)</span></pre>
<p><span class="koboSpan" id="kobo.560.1">Consumer lag stays high for a long time until all messages are processed  at around 19:32, but we can judge by the queue size that things started to improve </span><span class="No-Break"><span class="koboSpan" id="kobo.561.1">at 19:27.</span></span></p>
<p><span class="koboSpan" id="kobo.562.1">The trend changed and the queue quickly shrunk because I stopped the application and restarted it with </span><span class="No-Break"><span class="koboSpan" id="kobo.563.1">15 consumers.</span></span></p>
<p><span class="koboSpan" id="kobo.564.1">But now we </span><a id="_idIndexMarker633"/><span class="koboSpan" id="kobo.565.1">have too many consumers and are wasting resources. </span><span class="koboSpan" id="kobo.565.2">We can check the average batch size we retrieve – if it’s consistently and noticeably lower than the configured batch size, we may slowly start decreasing the number of consumers, leaving some buffer for bursts </span><span class="No-Break"><span class="koboSpan" id="kobo.566.1">of load.</span></span></p>
<p><span class="koboSpan" id="kobo.567.1">Now, let’s stop the load and add some errors. </span><span class="koboSpan" id="kobo.567.2">Send a malformed message with </span><strong class="source-inline"><span class="koboSpan" id="kobo.568.1">http://localhost:5051/send?malformed=true</span></strong><span class="koboSpan" id="kobo.569.1">. </span><span class="koboSpan" id="kobo.569.2">We should see that queue size remains small, but consumer lag grows </span><span class="No-Break"><span class="koboSpan" id="kobo.570.1">over time.</span></span></p>
<p><span class="koboSpan" id="kobo.571.1">We can also see that despite no messages being sent, we’re receiving messages, processing them, and </span><span class="No-Break"><span class="koboSpan" id="kobo.572.1">failing repeatedly.</span></span></p>
<p><span class="koboSpan" id="kobo.573.1">For example, we can visualize it with the </span><span class="No-Break"><span class="koboSpan" id="kobo.574.1">following query:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.575.1">
sum(
 rate(messaging_azqueues_process_loop_duration_milliseconds_
    count[1m]))
by (messaging_source_name, messaging_azqueue_status)</span></pre>
<p><span class="koboSpan" id="kobo.576.1">It shows the rate of process-and-receive iterations grouped by queue name and status. </span><span class="koboSpan" id="kobo.576.2">This is shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.577.1">Figure 11</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.578.1">.8</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.579.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer122">
<span class="koboSpan" id="kobo.580.1"><img alt="Figure 11.8 – Processing rate grouped by status" src="image/B19423_11_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.581.1">Figure 11.8 – Processing rate grouped by status</span></p>
<p><span class="koboSpan" id="kobo.582.1">We can see </span><a id="_idIndexMarker634"/><span class="koboSpan" id="kobo.583.1">here that from around 20:57, we attempt to receive messages about four times per second. </span><span class="koboSpan" id="kobo.583.2">Three of these calls don’t return any messages, and in the other case, processing fails. </span><span class="koboSpan" id="kobo.583.3">There are no </span><span class="No-Break"><span class="koboSpan" id="kobo.584.1">successful iterations.</span></span></p>
<p><span class="koboSpan" id="kobo.585.1">We sent a few malformed messages, and it seems they are being processed forever – this is a bug. </span><span class="koboSpan" id="kobo.585.2">If there were more than a few such messages, they would keep consumers busy and not let them process any </span><span class="No-Break"><span class="koboSpan" id="kobo.586.1">valid messages.</span></span></p>
<p><span class="koboSpan" id="kobo.587.1">To confirm this suggestion, let’s look at the traces. </span><span class="koboSpan" id="kobo.587.2">Let’s open Jaeger at </span><strong class="source-inline"><span class="koboSpan" id="kobo.588.1">http://localhost:16686</span></strong><span class="koboSpan" id="kobo.589.1"> and filter traces with errors that come from consumers. </span><span class="koboSpan" id="kobo.589.2">One such trace is shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.590.1">Figure 11</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.591.1">.9</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.592.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer123">
<span class="koboSpan" id="kobo.593.1"><img alt="Figure 11.9 – Failed receive-and-process iteration" src="image/B19423_11_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.594.1">Figure 11.9 – Failed receive-and-process iteration</span></p>
<p><span class="koboSpan" id="kobo.595.1">Here, we see that four messages were received, and the iteration failed with an error. </span><span class="koboSpan" id="kobo.595.2">If we could </span><a id="_idIndexMarker635"/><span class="koboSpan" id="kobo.596.1">add links to this operation, we would be able to navigate to traces for each individual message. </span><span class="koboSpan" id="kobo.596.2">Instead, we have message ID stamped. </span><span class="koboSpan" id="kobo.596.3">Let’s find the trace for one of these messages using the corresponding attribute. </span><span class="koboSpan" id="kobo.596.4">The result is shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.597.1">Figure 11</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.598.1">.10</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.599.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer124">
<span class="koboSpan" id="kobo.600.1"><img alt="Figure 11.10 – Trace for one of the failed messages" src="image/B19423_11_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.601.1">Figure 11.10 – Trace for one of the failed messages</span></p>
<p><span class="koboSpan" id="kobo.602.1">This does not look great – we have 3,000 spans for just one message. </span><span class="koboSpan" id="kobo.602.2">If we open the trace and check out the </span><strong class="source-inline"><span class="koboSpan" id="kobo.603.1">messaging.azqueues.message.dequeue_count</span></strong><span class="koboSpan" id="kobo.604.1"> attribute for the latest processing spans, we’ll see the message was received more than </span><span class="No-Break"><span class="koboSpan" id="kobo.605.1">1,000 times.</span></span></p>
<p><span class="koboSpan" id="kobo.606.1">To fix the issue, we should delete messages that fail validation. </span><span class="koboSpan" id="kobo.606.2">We also make sure we do so for </span><a id="_idIndexMarker636"/><span class="koboSpan" id="kobo.607.1">any other terminal error and introduce a limit to the number of times a message is dequeued, after which the message </span><span class="No-Break"><span class="koboSpan" id="kobo.608.1">is deleted.</span></span></p>
<p><span class="koboSpan" id="kobo.609.1">We just saw a couple of problems that frequently arise in messaging scenarios (but usually in less obvious ways) and used instrumentation to detect and debug them. </span><span class="koboSpan" id="kobo.609.2">As observability vendors improve user experience for links, it will become even easier to do such investigations. </span><span class="koboSpan" id="kobo.609.3">But we already have all the means to record telemetry and correlate it in </span><span class="No-Break"><span class="koboSpan" id="kobo.610.1">messaging flows.</span></span></p>
<h1 id="_idParaDest-191"><a id="_idTextAnchor190"/><span class="koboSpan" id="kobo.611.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.612.1">In this chapter, we explored messaging instrumentation. </span><span class="koboSpan" id="kobo.612.2">We started with messaging specifics and the new challenges they bring to observability. </span><span class="koboSpan" id="kobo.612.3">We briefly looked into OpenTelemetry messaging semantic conventions and then dived into producer instrumentation. </span><span class="koboSpan" id="kobo.612.4">The producer is responsible for injecting trace context into messages and instrumenting publish operations so that it’s possible to trace each independent flow </span><span class="No-Break"><span class="koboSpan" id="kobo.613.1">on consumers.</span></span></p>
<p><span class="koboSpan" id="kobo.614.1">Then, we instrumented the consumer with metrics and traces. </span><span class="koboSpan" id="kobo.614.2">We learned how to measure consumer health using queue size and lag and explored the instrumentation options for batching scenarios. </span><span class="koboSpan" id="kobo.614.3">Finally, we saw how we can use instrumentation to detect and investigate common </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">messaging issues.</span></span></p>
<p><span class="koboSpan" id="kobo.616.1">With this, you’re prepared to instrument common messaging patterns and can start designing and tuning instrumentation for advanced </span><span class="No-Break"><span class="koboSpan" id="kobo.617.1">streaming scenarios.</span></span></p>
<p><span class="koboSpan" id="kobo.618.1">In the next chapter, we’re going to design a comprehensive observability store for databases </span><span class="No-Break"><span class="koboSpan" id="kobo.619.1">and caching.</span></span></p>
<h1 id="_idParaDest-192"><a id="_idTextAnchor191"/><span class="koboSpan" id="kobo.620.1">Questions</span></h1>
<ol>
<li><span class="koboSpan" id="kobo.621.1">How would you measure end-to-end latency for an asynchronous operation? </span><span class="koboSpan" id="kobo.621.2">For example, in a scenario when a user uploads a meme and it takes some time to process and index it before it appears in </span><span class="No-Break"><span class="koboSpan" id="kobo.622.1">search results.</span></span></li>
<li><span class="koboSpan" id="kobo.623.1">How would you report batch size as a metric? </span><span class="koboSpan" id="kobo.623.2">How it can </span><span class="No-Break"><span class="koboSpan" id="kobo.624.1">be used?</span></span></li>
<li><span class="koboSpan" id="kobo.625.1">How would you approach baggage propagation in </span><span class="No-Break"><span class="koboSpan" id="kobo.626.1">messaging scenarios</span></span></li>
</ol>
</div>
</body></html>