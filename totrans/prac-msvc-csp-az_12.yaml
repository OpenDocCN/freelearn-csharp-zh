- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Scaling Services
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展服务
- en: How fast is the service responding? Is the service limited to CPU cores or memory?
    Based on user load, when is it useful to start more server instances? If you run
    too many compute resources, or if they’re too big, you pay more than is necessary.
    If the resources you use are too small, the response time increases or the applications
    might not be available at all. With this, you lose customers, and your income
    is reduced. You should know how to find bottlenecks and know what good knobs to
    turn to scale the resources as needed.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 服务的响应速度有多快？服务是否仅限于 CPU 内核或内存？根据用户负载，何时启动更多服务器实例是有用的？如果你运行过多的计算资源，或者如果它们太大，你将支付比必要的更多费用。如果你使用的资源太小，响应时间会增加，或者应用程序可能根本不可用。这样，你会失去客户，你的收入也会减少。你应该知道如何找到瓶颈，并知道如何调整哪些好的旋钮来按需扩展资源。
- en: In [*Chapter 10*](B21217_10.xhtml#_idTextAnchor239), we created load tests to
    see how the service behaves under load, while in [*Chapter 11*](B21217_11.xhtml#_idTextAnchor263),
    we extended the service by adding telemetry data. Now, we’ll use both load tests
    and telemetry data to find out what scaling option is best.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 10 章*](B21217_10.xhtml#_idTextAnchor239) 中，我们创建了负载测试来查看服务在负载下的行为，而在 [*第
    11 章*](B21217_11.xhtml#_idTextAnchor263) 中，我们通过添加遥测数据扩展了服务。现在，我们将使用负载测试和遥测数据来找出最佳的扩展选项。
- en: In this chapter, we’ll start reducing the response time with the help of telemetry
    data before analyzing the load, which can be run with one instance. Finally, we’ll
    define rules so that we can scale out to multiple instances. To automatically
    restart instances when the service is not responding, we’ll add health checks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将借助遥测数据开始减少响应时间，在分析负载之前，这可以通过一个实例运行。最后，我们将定义规则，以便我们可以扩展到多个实例。为了在服务无响应时自动重启实例，我们将添加健康检查。
- en: 'In this chapter, you’ll learn how to do the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习以下内容：
- en: Increase performance using caching
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用缓存提高性能
- en: Simulate users with Azure Load Testing
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Azure Load Testing 模拟用户
- en: Scale up and scale out services
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩容和扩展服务
- en: Use scale rules
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用缩放规则
- en: Implement health checks
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施健康检查
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, like the previous chapters, you’ll need an Azure subscription,
    the Azure Developer CLI (`winget install Microsoft.Azd`), and Docker Desktop.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，就像前面的章节一样，你需要一个 Azure 订阅、Azure 开发者 CLI (`winget install Microsoft.Azd`)
    和 Docker Desktop。
- en: 'The code for this chapter can be found in this book’s GitHub repository: [https://github.com/PacktPublishing/Pragmatic-Microservices-with-CSharp-and-Azure](https://github.com/PacktPublishing/Pragmatic-Microservices-With-CSharp-and-Azure).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在本书的 GitHub 仓库中找到：[https://github.com/PacktPublishing/Pragmatic-Microservices-with-CSharp-and-Azure](https://github.com/PacktPublishing/Pragmatic-Microservices-With-CSharp-and-Azure)。
- en: The `ch12` folder contains the projects we’ll need for this chapter, as well
    as their output. To add the functionality from this chapter, you can start with
    the source code from the previous chapter.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '`ch12` 文件夹包含本章所需的各个项目及其输出。要添加本章的功能，你可以从上一章的源代码开始。'
- en: 'Here are the projects we’ll be implementing in this chapter:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将实现以下项目：
- en: '`Codebreaker.AppHost`: The .NET Aspire host project. This project has been
    enhanced by adding a Redis resource for caching.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Codebreaker.AppHost`: .NET Aspire 主项目。该项目通过添加 Redis 资源用于缓存而得到增强。'
- en: '`Codebreaker.ServiceDefaults`: Here, we use a common health check configuration
    for all the services.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Codebreaker.ServiceDefaults`: 在这里，我们为所有服务使用一个通用的健康检查配置。'
- en: '`Codebreaker.GameAPIs`: With this project, we implement caching games to reduce
    database access and add a custom health check.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Codebreaker.GameAPIs`: 通过这个项目，我们实现了缓存游戏以减少数据库访问，并添加了自定义的健康检查。'
- en: To learn how to publish the resources to Microsoft Azure, check out the README
    file for this chapter.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何将资源发布到 Microsoft Azure，请查看本章的 README 文件。
- en: Note
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: While working on this chapter, we created load tests with many users and changed
    the scale of the Azure Cosmos database. The duration of these tests and the number
    of virtual users you can use with them depends on the amount of money you want
    to spend. If you increase the RU/s with the database, make sure you delete the
    resources after running the tests, or at least reduce the number of RU/s again
    after running the tests. You might also skip running the tests with larger user
    numbers and just read the results.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写本章时，我们创建了具有许多用户和更改 Azure Cosmos 数据库规模的负载测试。这些测试的持续时间和您可以使用它们的虚拟用户数量取决于您愿意花费的金额。如果您增加数据库的
    RU/s，请确保在测试运行后删除资源，或者至少在测试运行后再次减少 RU/s 的数量。您还可以跳过运行具有更多用户数的测试，只需读取结果即可。
- en: Increasing performance with caches
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用缓存提高性能
- en: 'Before we analyze the application’s CPU and memory needs, let’s look at where
    easy wins are possible to return faster responses to the client. By checking telemetry
    information (as we did in the previous chapter), we can see that when using distributed
    tracing to send a game move, several requests are made to the database. *Figure
    12**.1* shows the bot sending the SetMoveAsync request:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析应用程序的 CPU 和内存需求之前，让我们看看在哪里可以轻松获得胜利，以便更快地向客户端返回响应。通过检查遥测信息（如我们在上一章中所做的那样），我们可以看到在使用分布式跟踪发送游戏移动时，会向数据库发出多个请求。*图
    12**.1* 显示了机器人发送 SetMoveAsync 请求：
- en: '![Figure 12.1 – Tracing a move set](img/B21217_12_01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.1 – 追踪移动集](img/B21217_12_01.jpg)'
- en: Figure 12.1 – Tracing a move set
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1 – 追踪移动集
- en: As shown in the preceding figure, when receiving a PATCH request, the game ID
    is used to retrieve the game from the database to verify the correctness of the
    data that’s received. After the move is calculated, the resulting game is written
    to the database. Trace information from EF Core is shown with the DATA keyword,
    along with the time needed for access.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，当接收到 PATCH 请求时，使用游戏 ID 从数据库中检索游戏以验证接收到的数据的正确性。在计算移动后，结果游戏被写入数据库。EF Core
    的跟踪信息以 DATA 关键字显示，以及访问所需的时间。
- en: Performance might be good enough, but this also depends on the database load.
    When using the SQL Server database, having many writes can reduce the read performance
    because of locks with write operations. With higher database loads, increasing
    the number of Request Units (RU) or using bigger machines (which increases the
    price) can be a solution for higher loads. A better option is to cache data. Many
    of the database reads can be replaced by reading objects from a memory cache.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 性能可能已经足够好，但这也取决于数据库负载。当使用 SQL Server 数据库时，由于写入操作导致的锁定，大量的写入可能会降低读取性能。在更高的数据库负载下，增加请求单元（RU）的数量或使用更大的机器（这会增加成本）可以是解决更高负载的一种方法。更好的选择是缓存数据。许多数据库读取可以通过从内存缓存中读取对象来替代。
- en: An initial idea might be to store the game in the memory of the process. If
    it is not there, retrieve it from the database. However, if multiple instances
    of the service are running, the client could invoke one move with server A and
    another move with server B. Because the game contains the last move number, reading
    it from the local cache could result in an older version of the game, and thus
    the request fails. One option around this would be to use sticky sessions. With
    this, one client always gets the same service instance to fulfill a request. This
    requirement can easily be avoided by using a distributed memory cache.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一个初步的想法可能是将游戏存储在进程的内存中。如果不在那里，则从数据库中检索它。然而，如果有多个服务实例正在运行，客户端可能会用服务器 A 调用一个移动，用服务器
    B 调用另一个移动。因为游戏包含最后移动的编号，从本地缓存中读取它可能会导致游戏的旧版本，因此请求失败。围绕这个问题的一个选项是使用粘性会话。这样，一个客户端总是获得同一个服务实例来满足请求。通过使用分布式内存缓存，可以轻松避免这个要求。
- en: Note
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: With a sticky session, a client always connects to the same service instance.
    The biggest disadvantage of sticky sessions is when the service goes down. Without
    sticky sessions, the client can immediately switch to another service instance,
    and no downtime is detected. With sticky sessions, all the session data is lost
    for the client. This is not the only disadvantage. What if another instance is
    started because of low performance? The new service instance only receives the
    traffic from new clients. Existing ones stick with the servers they already communicate
    with. There’s a delayed server utilization (only from new clients). With sticky
    sessions, the load can be unevenly distributed between service instances. The
    best thing to do is try to avoid them.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在粘性会话中，客户端总是连接到同一服务实例。粘性会话的最大缺点是当服务崩溃时。没有粘性会话，客户端可以立即切换到另一个服务实例，并且不会检测到任何停机时间。在有粘性会话的情况下，所有会话数据都会对客户端丢失。这并不是唯一的缺点。如果由于性能低下而启动了另一个实例怎么办？新的服务实例只会接收来自新客户端的流量。现有的客户端会继续与它们已经通信的服务器保持连接。这会导致服务器利用率延迟（仅来自新客户端）。在有粘性会话的情况下，服务实例之间的负载可能会不均匀分布。最好的做法是尽量避免使用粘性会话。
- en: When using a distributed memory cache, multiple options are available. With
    Microsoft Azure, Azure Cache for Redis can be used. This service offers Standard,
    Premium, Enterprise, and Enterprise Flash offerings based on your availability
    and memory size needs. Using Azure Cosmos DB, an integrated in-memory cache built
    into the Azure Cosmos DB gateway, can be used. One feature of this service is
    an item cache for point reads, which fulfills the purpose of reading the item
    several times while the game is running. This reduces the cost with Azure Cosmos
    DB because the RU/s needed to read from the cache are 0.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用分布式内存缓存时，有多种选项可供选择。使用Microsoft Azure时，可以使用Azure Cache for Redis。这项服务根据您的可用性和内存大小需求提供标准、高级、企业和企业闪存产品。使用Azure
    Cosmos DB时，可以采用集成在Azure Cosmos DB网关中的内存缓存。该服务的一项特性是针对点读取的项缓存，在游戏运行期间可以多次读取项，从而降低了使用Azure
    Cosmos DB的成本，因为从缓存中读取所需的RU/s为0。
- en: Here, we’ll use a Docker container for Redis that can be used in the local Docker
    environment, as well as to run the solution with Azure Container Apps.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用Redis的Docker容器，它可以在本地Docker环境中使用，也可以用于在Azure Container Apps中运行解决方案。
- en: Reading and writing from the cache
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从缓存中读取和写入
- en: 'The API of the IDistributedCache interface supports writing byte arrays and
    strings – the data needs to be sent across the network to a Redis cluster. For
    this, we’ll create methods to convert the Game class to and from bytes:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`IDistributedCache`接口的API支持写入字节数组和字符串——数据需要通过网络发送到Redis集群。为此，我们将创建将`Game`类转换为字节并从字节转换回`Game`类的方法：'
- en: Codebreaker.GameAPIs/Models/GameExtensions.cs
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Codebreaker.GameAPIs/Models/GameExtensions.cs
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `System.Text.Json` serializer supports serializing the data not only to
    JSON but also to a byte array. The `Game` class already supports serialization
    with this serializer, so no other changes need to be made to the `Game` and `Move`
    model types.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`System.Text.Json`序列化器不仅支持将数据序列化为JSON，还可以将其序列化为字节数组。`Game`类已经支持使用此序列化器进行序列化，因此不需要对`Game`和`Move`模型类型进行任何其他更改。'
- en: 'We can access the cache from the `GamesService` class:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从`GamesService`类访问缓存：
- en: Codebreaker.GameAPIs/Services/GamesService.cs
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Codebreaker.GameAPIs/Services/GamesService.cs
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: No matter what technology is used for the distributed memory cache, we can inject
    the `IDistributedCache` interface.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 无论使用什么技术实现分布式内存缓存，我们都可以注入`IDistributedCache`接口。
- en: 'To update the `Game` class with the cache, we can implement the following method:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用缓存更新`Game`类，我们可以实现以下方法：
- en: Codebreaker.GameAPIs/Services/GamesService.cs
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Codebreaker.GameAPIs/Services/GamesService.cs
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The game ID is used as a key to retrieve the game object from the cache. Invoking
    the `SetAsync` method adds the object to the cache. If the object has already
    been cached, it is updated with the new value. With an additional parameter of
    the `DistributedEntryCacheOptions` type, the object can be configured to specify
    the time the object should stay in the cache. Here, we need to use a typical time
    the user needs from one move to another. With every retrieval and update, the
    **sliding expiration** starts anew. Instead of specifying this here, we can configure
    default values.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏ID用作键，从缓存中检索游戏对象。调用`SetAsync`方法将对象添加到缓存。如果对象已经缓存，则使用新值更新。通过`DistributedEntryCacheOptions`类型的附加参数，可以配置对象以指定对象应在缓存中停留的时间。在这里，我们需要使用用户从一个动作到另一个动作所需的典型时间。每次检索和更新时，**滑动过期**重新开始。我们可以在这里指定此值，也可以配置默认值。
- en: The `UpdateGameInCacheAsync` method needs to be invoked from the `GamesService`
    class when the game (`StartGameAsync`) is created, as well as after setting the
    game move (`SetMoveAsync`).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当创建游戏（`StartGameAsync`）以及设置游戏移动（`SetMoveAsync`）后，需要从`GamesService`类调用`UpdateGameInCacheAsync`方法。
- en: 'The implementation within the `StartGameAsync` method is shown here:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`StartGameAsync`方法中的实现如下所示：'
- en: Codebreaker.GameAPIs/Services/GamesService.cs
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Codebreaker.GameAPIs/Services/GamesService.cs
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Writing to the database and the cache can be done in parallel. We don’t need
    to wait until the database write is completed to add the game object to the cache
    to return a faster answer. If the database fails, it doesn’t matter if the game
    is cached or not.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 可以并行写入数据库和缓存。我们不需要等待数据库写入完成，就可以将游戏对象添加到缓存并返回更快的答案。如果数据库失败，游戏是否缓存并不重要。
- en: 'To read the data from the cache, we need to implement `GetGameFromCacheOrDataStoreAsync`:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要从缓存中读取数据，我们需要实现`GetGameFromCacheOrDataStoreAsync`：
- en: Codebreaker.GameAPIs/Services/GamesService.cs
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Codebreaker.GameAPIs/Services/GamesService.cs
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `GetAsync` method of the cache returns a byte array of the cached data,
    which is then converted using the `ToGame` method. If the data is not available
    within the cache (the item might have been removed from the cache because too
    much memory was already allocated, or if the user was thinking about their next
    move for too long), we get the game from the database. The code in the source
    code repository includes a flag where you can switch off reading from the cache
    to easily try out not using the cache with different loads that are used to check
    the results.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存中的`GetAsync`方法返回缓存的字节数组，然后使用`ToGame`方法进行转换。如果数据在缓存中不可用（可能因为已经分配了太多内存而将项目从缓存中移除，或者如果用户思考下一步棋的时间过长），我们将从数据库中获取游戏。源代码仓库中的代码包括一个标志，您可以通过它关闭从缓存中读取，以便轻松尝试不使用缓存的不同负载，以检查结果。
- en: '`GetGameFromCacheOrDataStoreAsync` needs to be invoked from the `SetMoveAsync`
    and `GetGameAsync` methods.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`GetGameFromCacheOrDataStoreAsync`需要从`SetMoveAsync`和`GetGameAsync`方法中调用。'
- en: Configuring the Aspire Redis component
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置Aspire Redis组件
- en: 'Regarding the `game-apis` project, we need to add the **.NET Aspire StackExchange
    Redis component** to configure the DI container:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 关于`game-apis`项目，我们需要添加**.NET Aspire StackExchange Redis组件**来配置DI容器：
- en: Codebreaker.GameAPIs/ApplicationServices.cs
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Codebreaker.GameAPIs/ApplicationServices.cs
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `AddRedisDistributedCache` method uses the cache name that needs to be configured
    with the Aspire App Host project to get the connection string and configuration
    values. With this method, it’s also possible to specify the configuration values
    programmatically.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`AddRedisDistributedCache`方法使用需要与Aspire App Host项目配置的缓存名称来获取连接字符串和配置值。使用此方法，还可以以编程方式指定配置值。'
- en: 'Finally, a Docker container for the Redis resource is configured with `app-model`
    in the AppHost project:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在AppHost项目中使用`app-model`配置Redis资源的Docker容器：
- en: Codebreaker.AppHost/Program.cs
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Codebreaker.AppHost/Program.cs
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `AddRedis` method configures using the `redis` Docker image for this service.
    This needs to be configured both with `PublishAsAzureRedis` API instead of `PublishAsContainer`.
    This method configures the PaaS offering for `WithRedisCommander` adds a management
    UI for Redis to `app-model`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`AddRedis`方法配置使用`redis` Docker镜像为此服务。这需要使用`PublishAsAzureRedis` API而不是`PublishAsContainer`进行配置。此方法配置`WithRedisCommander`的PaaS提供程序，为`app-model`添加Redis的管理UI。'
- en: 'With this configuration in place, running games via the bot provides the results
    shown in *Figure 12**.2*. Even when using a low load on the local system, writing
    to SQL Server took 5.96 ms, and writing to the cache took 1.83 ms. Both were running
    in a Docker container:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在此配置下，通过机器人运行游戏提供 *图 12*.2 中所示的结果。即使在使用本地系统上的低负载时，写入 SQL Server 需要 5.96 毫秒，写入缓存需要
    1.83 毫秒。两者都在 Docker 容器中运行：
- en: '![Figure 12.2 – Set move with a distributed cache](img/B21217_12_02.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.2 – 使用分布式缓存设置移动](img/B21217_12_02.jpg)'
- en: Figure 12.2 – Set move with a distributed cache
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.2 – 使用分布式缓存设置移动
- en: Next, let’s add some load to the game-apis project to see the resource consumption.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们给 game-apis 项目添加一些负载，以查看资源消耗。
- en: Simulating users with Azure Load Testing
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Azure Load Testing 模拟用户
- en: 'In [*Chapter 10*](B21217_10.xhtml#_idTextAnchor239), we created Playwright
    tests that were used to create load tests. These Playwright tests allowed us to
    use .NET code to easily create a complete flow so that we could play a game from
    a test. Using Microsoft Azure, we can use another service to create tests and
    get integrated analysis with Azure services: Azure Load Testing.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 10 章*](B21217_10.xhtml#_idTextAnchor239) 中，我们创建了用于创建负载测试的 Playwright 测试。这些
    Playwright 测试允许我们使用 .NET 代码轻松创建一个完整的流程，以便我们可以从测试中玩游戏。使用 Microsoft Azure，我们可以使用另一个服务来创建测试，并获取与
    Azure 服务的集成分析：Azure Load Testing。
- en: Note
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: At the time of writing, the **Microsoft Playwright Testing** cloud service is
    great for testing the load of web applications. However, it doesn’t support load
    testing APIs, so we’ll use Azure Load Testing here. You can still use Azure compute
    (for example, Azure Container Instances) to run Playwright tests, but Azure Load
    Testing has a better report configuration and report functionality.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，**Microsoft Playwright 测试**云服务非常适合测试 Web 应用程序的负载。然而，它不支持 API 的负载测试，因此我们将在此使用
    Azure Load Testing。您仍然可以使用 Azure 计算（例如，Azure 容器实例）来运行 Playwright 测试，但 Azure Load
    Testing 具有更好的报告配置和报告功能。
- en: Before creating the load test, make sure you deploy the solution to Microsoft
    Azure using azd up. Check the README file for this chapter for more details about
    the different azd versions.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建负载测试之前，请确保使用 azd up 将解决方案部署到 Microsoft Azure。查看本章节的 README 文件以获取有关不同 azd
    版本更多详细信息。
- en: After creating the Azure resources, open the `game-apis` Azure Container App
    in the Azure portal and select **Application** | **Containers** from the left
    bar. The container’s resource allocation will be shown as *0.5 CPU cores* and
    *1* *Gi memory*.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Azure 资源创建后，在 Azure 门户中打开 `game-apis` Azure 容器应用，并从左侧栏选择 **应用程序** | **容器**。容器的资源分配将显示为
    *0.5 CPU 核心* 和 *1* *Gi 内存*。
- en: Now, let’s make sure the first tests use just one replica.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们确保第一次测试只使用一个副本。
- en: Scaling to one replica
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缩放到一个副本
- en: 'Scales and replicas can scale up to 300 instances. The default configuration
    is to scale from 1 to 10\. Creating a load with many users would automatically
    scale out and start multiple instances. To see what the limits of one instance
    are, change the scale to just one instance for both Min replicas and Max replicas,
    as shown in *Figure 12**.3*. Clicking Create creates a new revision of the app
    and deprovisions the existing revision afterward:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放和副本可以扩展到 300 个实例。默认配置是从 1 扩展到 10。创建带有许多用户的负载将自动扩展并启动多个实例。要查看单个实例的限制，将缩放更改为
    Min 副本和 Max 副本都仅为一个实例，如图 *图 12*.3 所示。点击创建将创建应用程序的新版本，并在之后取消预配现有版本：
- en: '![Figure 12.3 – Changing replicas with Azure Container Apps](img/B21217_12_03.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.3 – 使用 Azure 容器应用更改副本](img/B21217_12_03.jpg)'
- en: Figure 12.3 – Changing replicas with Azure Container Apps
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.3 – 使用 Azure 容器应用更改副本
- en: 'To specify scaling at deployment time, create YAML templates that specify the
    configuration for Azure Container Apps. Start a terminal with the current directory
    set to the solution and run the following command from the Azure Developer CLI
    (after you’ve initialized the solution with azd init):'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 要在部署时指定缩放，创建 YAML 模板以指定 Azure 容器应用的配置。启动一个终端，将当前目录设置为解决方案，并从 Azure 开发者 CLI 中运行以下命令（在您使用
    azd init 初始化解决方案之后）：
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This tool uses the app-model manifest to create Bicep files to deploy the Azure
    resources of app-model (in the root infra folder). The infra folder of the AppHost
    project contains YAML templates that describe every Azure Container App that’s
    been created (from projects and Docker images). See [*Chapter 6*](B21217_06.xhtml#_idTextAnchor137)
    for more details on Bicep files.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此工具使用 app-model 清单创建 Bicep 文件以部署 app-model 的 Azure 资源（在根 infra 文件夹中）。AppHost
    项目的 infra 文件夹包含描述已创建的每个 Azure 容器应用（从项目和 Docker 镜像）的 YAML 模板。参见 [*第 6 章*](B21217_06.xhtml#_idTextAnchor137)
    了解有关 Bicep 文件的更多详细信息。
- en: In the AppHost project, you’ll see that a `<app>.tmpl.yaml` file has been generated
    to specify the settings for Azure Container Apps.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AppHost 项目中，您会看到已生成一个 `<app>.tmpl.yaml` 文件，用于指定 Azure 容器应用的设置。
- en: 'By default, the minimum number of replicas is set to `1`. With bot-service,
    you can change the configuration:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，最小副本数设置为 `1`。使用 bot-service，您可以更改配置：
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: With bot-service, to reduce cost, you can define the scale from `0` to `1`.
    When the minimum instance count is set to `0`, there’s no cost for the service.
    Just be aware that it takes a few seconds to start up the service, and the first
    user accessing the service needs to wait. Because the bot is not invoked by game-playing
    users, and this service is not always needed, it can be scaled down to `0`. The
    game-apis service should always return answers fast; thus, the minimum scale should
    be set to `1`. If there’s no load on the service, there’s an idle price. With
    this, the cost of the CPU is reduced to about 10% of the normal cost, but the
    memory (the application is still loaded in memory) has the normal price. To test
    the load with exactly one replica, set the game-apis service’s minimum and maximum
    values to `1`. Later, when scaling out, we’ll increase the value of Max replicas
    again.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 bot-service，为了降低成本，可以将缩放范围定义为从 `0` 到 `1`。当最小实例计数设置为 `0` 时，服务没有成本。只需注意，启动服务需要几秒钟，第一个访问服务的用户需要等待。因为机器人不是由游戏玩家调用的，而且这个服务并不总是需要的，它可以缩小到
    `0`。game-apis 服务应该始终快速返回答案；因此，最小缩放应设置为 `1`。如果没有服务负载，会有闲置费用。这样，CPU 的成本可以降低到正常成本的约
    10%，但内存（应用程序仍然加载在内存中）的价格是正常的。为了测试精确一个副本的负载，将 game-apis 服务的最小和最大值设置为 `1`。稍后，在扩展时，我们将再次增加最大副本的数量。
- en: After changing the number of replicas in the YAML file, you can re-deploy the
    application using az up or just using az deploy.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在 YAML 文件中更改副本数量后，可以使用 az up 或仅使用 az deploy 重新部署应用程序。
- en: We also need to make sure that the database allows the requests that are needed.
    With a load test, we can expect that we’ll need more than the 400 RU/s. Before
    the first test runs, change the Azure Cosmos DB throughput to Autoscale with a
    maximum of 1,000 RU/s.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要确保数据库允许所需的请求。通过负载测试，我们可以预期我们需要超过 400 RU/s。在第一次测试运行之前，将 Azure Cosmos DB
    的吞吐量更改为自动缩放，最大值为 1,000 RU/s。
- en: Now, we are ready to create a test.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已准备好创建测试。
- en: Creating an Azure URL-based load test
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建基于 Azure URL 的负载测试
- en: To create a new load test, create the Azure Load Testing resource using the
    Azure portal. Specify a resource group name and the name of the resource.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建新的负载测试，使用 Azure 门户创建 Azure Load Testing 资源。指定资源组名称和资源名称。
- en: 'Once the resource is available, open it in the portal and select **Tests**
    | **Tests** from the left bar. Then, click **Create** after choosing **Create
    a URL-based test**. Under **Basics**, specify **Test name** and **Test description**
    values and check the **Enable advanced settings** box, as shown in *Figure 12**.4*:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦资源可用，在门户中打开它，并在左侧栏中选择 **测试** | **测试**。然后，在选择 **创建基于 URL 的测试** 后点击 **创建**。在
    **基本** 选项卡下，指定 **测试名称** 和 **测试描述** 的值，并勾选 **启用高级设置** 复选框，如图 *图 12* 所示。4*：
- en: '![Figure 12.4 – Load testing – basic settings](img/B21217_12_04.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.4 – 负载测试 – 基本设置](img/B21217_12_04.jpg)'
- en: Figure 12.4 – Load testing – basic settings
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.4 – 负载测试 – 基本设置
- en: 'With Enable advanced settings selected, a test plan consisting of up to five
    HTTP requests can be created. So, in the Test plan section, add five requests,
    as shown in *Figure 12**.5*:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 选择 **启用高级设置** 后，可以创建包含最多五个 HTTP 请求的测试计划。因此，在 **测试计划** 部分添加五个请求，如图 *图 12* 所示。5*：
- en: '![Figure 12.5 – Load testing – test plans](img/B21217_12_05.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.5 – 负载测试 – 测试计划](img/B21217_12_05.jpg)'
- en: Figure 12.5 – Load testing – test plans
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.5 – 负载测试 – 测试计划
- en: The first request is a POST request to create the game. The second is a PATCH
    request to update the game with a move. This is followed by a GET request to get
    information about the game, a PATCH request to end the game, and a DELETE request
    to delete the game.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个请求是创建游戏的 POST 请求。第二个是更新游戏移动的 PATCH 请求。接着是一个获取游戏信息的 GET 请求，一个结束游戏的 PATCH 请求，以及一个删除游戏的
    DELETE 请求。
- en: 'These requests can easily be configured with the UI, as shown in *Figure 12**.6*:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这些请求可以很容易地通过 UI 进行配置，如图 *图 12.6* 所示：
- en: '![Figure 12.6 – Load testing – adding requests](img/B21217_12_06.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.6 – 压力测试 – 添加请求](img/B21217_12_06.jpg)'
- en: Figure 12.6 – Load testing – adding requests
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.6 – 压力测试 – 添加请求
- en: Instead of getting the request information from the OpenAPI description or the
    HTTP files, you can copy the requests from the README file of this chapter to
    the Body area. The requests, including their HTTP headers, are listed. Make sure
    you use the links to your Azure Container App when specifying the URL.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从 OpenAPI 描述或 HTTP 文件中获取请求信息，也可以将此章节的 README 文件中的请求复制到 Body 区域。请求及其 HTTP
    头部信息已列出。确保在指定 URL 时使用指向你的 Azure 容器应用的链接。
- en: With the POST request, don’t just specify the body – also define the use of
    the response. With the JSON result, id is returned; this can be accessed with
    the $.id expression. Set this to the gameId variable. Response variables can be
    used with later requests – and the game ID is needed with all the following requests.
    When setting the game move, use ${gameId} to pass the game ID to the URL string
    and the HTTP body. You can check the README file for this chapter for more details
    about the values you should specify with the different requests.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在 POST 请求中，不仅指定正文，还要定义响应的使用。JSON 结果返回 id；这可以通过 $.id 表达式访问。将其设置为 gameId 变量。响应变量可以在后续请求中使用——并且所有后续请求都需要游戏
    ID。在设置游戏移动时，使用 ${gameId} 将游戏 ID 传递到 URL 字符串和 HTTP 正文。你可以查看此章节的 README 文件以获取有关不同请求应指定哪些值的更多详细信息。
- en: 'In the next dialogue, shown in *Figure 12**.7*, the load can be specified:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一对话框中，如 *图 12.7* 所示，可以指定负载：
- en: '![Figure 12.7 – Load testing – specifying the load](img/B21217_12_07.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.7 – 压力测试 – 指定负载](img/B21217_12_07.jpg)'
- en: Figure 12.7 – Load testing – specifying the load
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.7 – 压力测试 – 指定负载
- en: Here, we’ll start small with just 5 concurrent virtual users and do other tests
    with more user loads and multiple engine instances, with a ramp-up time of 0.3
    minutes. With one test engine instance, you can specify up to 250 virtual users
    and go up to 2,500 virtual users with 10 instances. The configuration also allows
    you to specify a Load pattern value, which increases the load over time. Having
    multiple test runs with different user numbers can give a good indication of what
    scaling rules should be used to increase the number of service instances.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将从 5 个并发虚拟用户开始测试，并使用更多用户负载和多个引擎实例进行其他测试，起始时间为 0.3 分钟。一个测试引擎实例可以指定多达 250
    个虚拟用户，并且使用 10 个实例可以增加到 2,500 个虚拟用户。配置还允许你指定负载模式值，该值会随着时间的推移增加负载。进行多次不同用户数量的测试运行可以很好地指示应使用哪些缩放规则来增加服务实例的数量。
- en: Be aware of the cost you can incur when testing with 2,500 virtual users and
    10 virtual machines behind the scenes. Contrary to the other resources we’ve used
    so far, with this, you can easily go over the subscription limits with the Visual
    Studio Enterprise Azure subscription or the free Azure subscription. Luckily,
    we only pay for the time the test runs and don’t need to pay for physical machines
    that are only needed for a short time.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在测试时，使用 2,500 个虚拟用户和 10 个后台虚拟机可能产生的成本。与迄今为止我们使用的其他资源不同，使用这个方法，你很容易就会超过 Visual
    Studio Enterprise Azure 订阅或免费 Azure 订阅的订阅限制。幸运的是，我们只需为测试运行的时间付费，而无需为仅需要短时间的物理机器付费。
- en: Note
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Don’t assume virtual users are the same as real users. Real users produce a
    lot less load than virtual users with Azure Load Testing. A real user needs to
    think between moves. Several seconds, if not minutes, are spent between each move.
    Virtual users just continuously invoke the APIs. With the JMeter tests that are
    used behind the scenes, the number of virtual users configures the number of threads
    to be used. How many real users you can calculate compared to virtual users depends
    on the type of application. You need to find out how long real users think on
    average with Codebreaker when monitoring the application in production. In [*Chapter
    10*](B21217_10.xhtml#_idTextAnchor239), we created custom metric data to monitor
    the time spent between moves; this is a good value to use.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 不要假设虚拟用户与真实用户相同。与Azure负载测试中的虚拟用户相比，真实用户产生的负载要少得多。真实用户在移动之间需要思考。每次移动之间可能需要几秒钟，甚至几分钟。虚拟用户只是连续调用API。在幕后使用的JMeter测试中，虚拟用户的数量配置了要使用的线程数。您可以将虚拟用户与真实用户相比计算出多少真实用户取决于应用程序的类型。您需要找出在监控生产中的应用程序时，真实用户平均思考的时间有多长。在[*第10章*](B21217_10.xhtml#_idTextAnchor239)中，我们创建了自定义指标数据来监控移动之间的时间；这是一个很好的值来使用。
- en: 'With the test criteria configuration (see *Figure 12**.8*), you can specify
    when the test should fail – for example, when the response time takes too long.
    Before doing the first test run, you can leave the test criteria empty to see
    values that are reached with low load:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试标准配置（见图*图12.8*）中，您可以指定测试何时应该失败 – 例如，当响应时间过长时。在进行第一次测试运行之前，您可以留空测试标准以查看低负载下达到的值：
- en: '![Figure 12.8 – Load testing – test criteria](img/B21217_12_08.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图12.8 – 压力测试 – 测试标准](img/B21217_12_08.jpg)'
- en: Figure 12.8 – Load testing – test criteria
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.8 – 压力测试 – 测试标准
- en: 'For the last configuration, open the Monitoring settings, as shown in *Figure
    12**.9*:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对于最后一个配置，打开监控设置，如图*图12.9*所示：
- en: '![Figure 12.9 – Specifying monitoring resources](img/B21217_12_09.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图12.9 – 指定监控资源](img/B21217_12_09.jpg)'
- en: Figure 12.9 – Specifying monitoring resources
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.9 – 指定监控资源
- en: Select the resources that are taking part in the test, such as the gameapis
    and redis Azure Container Apps, and the Azure Cosmos DB resource. You can easily
    filter the resources based on the resource group.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 选择参与测试的资源，例如游戏APIs和Redis Azure容器应用，以及Azure Cosmos DB资源。您可以根据资源组轻松过滤资源。
- en: Now, we are ready to run the test.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好运行测试。
- en: Running a load with virtual users
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用虚拟用户运行负载
- en: When creating and changing a test, after clicking Save, you need to wait until
    the JMeter script is created; otherwise, the test will fail to start. To run the
    test, click the Run button and enter a test description – for example, 5 users
    0.5 core.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建和修改测试后，点击保存后，您需要等待JMeter脚本创建完成；否则，测试将无法启动。要运行测试，请点击运行按钮并输入测试描述 – 例如，5个用户
    0.5核心。
- en: After the test is completed, you will see client-side metrics from the test
    engine and server-side metrics from the selected Azure Container Apps service.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 测试完成后，您将看到来自测试引擎的客户端指标和来自所选Azure容器应用的端点服务的服务器端指标。
- en: When I did my test run, 7,834 requests were sent (a lot more than five human
    users would do for 2 minutes), and up to 0.49 CPU cores and 354 MB of memory were
    used. The response time was below 116 milliseconds for 90% of the requests, and
    the throughput was 67.53 requests per second.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当我进行测试运行时，发送了7,834个请求（远多于五个人在2分钟内会做的请求），并且使用了高达0.49个CPU核心和354 MB的内存。90%的请求响应时间低于116毫秒，吞吐量为每秒67.53个请求。
- en: Note
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Don’t expect to get the same results with multiple runs. Many dependencies run
    these tests. What’s the network performance and latency between the different
    Azure services being used? For my tests, I created the Azure Load Testing service
    in the same Azure region where the services are running. Even in the same Azure
    region, different resources could be running in the same or different data centers.
    These differences are not an issue. Users will be located outside an Azure data
    center. What we need to know is how many users can be served from one instance
    and what settings are best for the application, such as CPU and memory resources
    (scale up) or running multiple replicas (scale out). We also need to see what
    the real bottlenecks are, and what can be controlled.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 不要期望多次运行会得到相同的结果。许多依赖项运行这些测试。使用不同 Azure 服务之间的网络性能和延迟如何？对于我的测试，我在服务运行的同一天 Azure
    区域创建了 Azure Load Testing 服务。即使在同一 Azure 区域，不同的资源也可能在相同或不同的数据中心中运行。这些差异不是问题。用户将位于
    Azure 数据中心之外。我们需要知道一个实例可以服务多少用户，以及最佳的应用程序设置是什么，例如 CPU 和内存资源（扩展）或运行多个副本（扩展）。我们还需要看到真正的瓶颈是什么，以及什么可以控制。
- en: '*Figure 12**.10* shows the response time results with every API invocation:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 12.10* 显示了每次 API 调用的响应时间结果：'
- en: '![Figure 12.10 – Response time for five virtual users](img/B21217_12_10.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.10 – 五个虚拟用户的响应时间](img/B21217_12_10.jpg)'
- en: Figure 12.10 – Response time for five virtual users
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.10 – 五个虚拟用户的响应时间
- en: With five virtual users, the response time is OK when considering all the requests.
    What might be interesting is that the delete request takes the most time to complete
    with Azure Cosmos DB.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用五个虚拟用户时，考虑到所有请求，响应时间是可以接受的。有趣的是，使用 Azure Cosmos DB，删除请求需要最长时间来完成。
- en: Five virtual users is a good start, but let’s add more load.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 五个虚拟用户是一个良好的起点，但让我们增加更多的负载。
- en: Reaching limits with a higher load
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在更高负载下达到限制
- en: To change the load of a test, you can edit it. To do so, click `25`. Click **Apply**
    and wait for the JMeter script to be created with **Notifications** in the Azure
    portal. At this point, you can start the test again.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 要更改测试的负载，您可以编辑它。为此，点击 `25`。点击 **应用** 并等待 Azure 门户中的 JMeter 脚本创建带有 **通知**。此时，您可以再次开始测试。
- en: 'With my test run, increasing the number of virtual users to 25 resulted in
    just 11.701 total requests with 98.39 requests per second. The request to create
    a game needed 289 ms with a 90% percentile. *Figure 12**.11* shows the number
    of requests per second for this test:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的测试运行中，将虚拟用户数量增加到 25 仅导致 11.701 个总请求，每秒 98.39 个请求。创建游戏的请求需要 289 毫秒，90% 分位数。*图
    12.11* 显示了此测试每秒的请求数量：
- en: '![Figure 12.11 – Requests per second](img/B21217_12_11.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.11 – 每秒请求数量](img/B21217_12_11.jpg)'
- en: Figure 12.11 – Requests per second
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.11 – 每秒请求数量
- en: 'Comparing the results to the five users’ test runs, using 25 users resulted
    in just a small increase concerning the total requests and requests per second.
    As a result, the time for creating a game increased from 96 ms to 498 ms. This
    is not a good outcome. Why did this happen? The server-side metrics didn’t reach
    Azure Container Apps limits with CPU cores and memory. The limit was not down
    to Azure Container Apps but the Azure Cosmos database, as shown in *Figure 12**.12*:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 将结果与五个用户的测试运行进行比较，使用 25 个用户仅导致总请求和每秒请求量略有增加。因此，创建游戏的时间从 96 毫秒增加到 498 毫秒。这不是一个好的结果。为什么会这样？服务器端指标没有达到
    Azure Container Apps 的 CPU 核心和内存限制。限制不是 Azure Container Apps 的，而是 Azure Cosmos
    数据库的，如图 *图 12.12* 所示：
- en: '![Figure 12.12 – Azure Cosmos DB RU consumption](img/B21217_12_12.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.12 – Azure Cosmos DB RU 消耗](img/B21217_12_12.jpg)'
- en: Figure 12.12 – Azure Cosmos DB RU consumption
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.12 – Azure Cosmos DB RU 消耗
- en: 'When running this test with Azure Cosmos DB, the RU was configured with **autoscale**
    throughput, and a maximum 1,000 RU/s limit was reached. This can be seen in the
    preceding figure. You can also dig into **Application Map** on App Insights and
    check the different Azure Cosmos DB metrics, as shown in *Figure 12**.13*:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 Azure Cosmos DB 运行此测试时，RU 被配置为 **autoscale** 吞吐量，并达到了最大 1,000 RU/s 限制。这可以在前面的图中看到。您还可以在
    App Insights 中的 **Application Map** 中深入了解，并检查不同的 Azure Cosmos DB 指标，如图 *图 12.13*
    所示。13*：
- en: '![Figure 12.13 – Requests throttled with Azure Cosmos DB](img/B21217_12_13.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.13 – 使用 Azure Cosmos DB 限制请求](img/B21217_12_13.jpg)'
- en: Figure 12.13 – Requests throttled with Azure Cosmos DB
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.13 – 使用 Azure Cosmos DB 限制请求
- en: 'As we can see, the requests have been throttled; an error code of 429 has been
    returned to the `game-apis` service. You can use **Kusto Query Language** (**KQL**)
    to query for these log messages (see [*Chapter 11*](B21217_11.xhtml#_idTextAnchor263)
    for more information on KQL):'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，请求已被限制；`game-apis`服务返回了错误代码429。您可以使用**Kusto查询语言**（**KQL**）来查询这些日志消息（有关KQL的更多信息，请参阅[*第11章*](B21217_11.xhtml#_idTextAnchor263)）：
- en: '[PRE9]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The complete log message that’s returned states “*Request rate is large. More
    Request Units may be needed, so no changes were made. Plea*[*se retry this request
    later. Lea*](http://aka.ms/cosmosdb-error-429)*rn* *more: http://aka.ms/cosmosdb-error-429.*”'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的完整日志消息为“*请求速率过高。可能需要更多请求单元，因此没有进行更改。请稍后重试此请求。更多信息：http://aka.ms/cosmosdb-error-429.*”
- en: Earlier in this chapter, we reduced the load on Azure Cosmos DB by removing
    requests from the database that were not needed by using a cache. Without this
    change, the limit would have been hit earlier.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章早期，我们通过使用缓存删除数据库中不需要的请求来减少Azure Cosmos DB的负载。如果没有这个更改，限制会更早地被触及。
- en: While error code 429 has been returned to the `game-apis` service, the result
    of the invocation was still successful because of the built-in retry configuration
    with .NET Aspire. But of course, the time needed for the request increased.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然错误代码429已返回到`game-apis`服务，但由于.NET Aspire内置的重试配置，调用结果仍然成功。但当然，请求所需的时间增加了。
- en: When creating the test, we ensured we could see the metrics data for all the
    resources participating in the test. That’s why we can see the Cosmos metrics
    with the test run and can easily fix it. Let’s use Cosmos DB to increase the RU/s.
    With a maximum value of 1,000 RU/s, the minimum RU/s is 100\. Increasing the maximum
    to 10,000 sets the minimum to 1,000\. Make sure you only change the maximum to
    higher values for a short period while running the tests, and when needed. You
    can view the expected cost in the dialogue where you scale the RU/s. Make sure
    you reduce the scale limits as you don’t need them anymore. It is possible to
    set the maximum to 1,000,000 RU/s, which sets the minimum to 100,000\. You can
    view the price range when you change this throughput before clicking the **Save**
    button. Be aware that when changing the maximum value above 10,000 RU/s, it can
    take 4 to 6 hours for this compute power to become available.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建测试时，我们确保可以看到测试中所有资源的指标数据。这就是为什么我们可以通过测试运行看到Cosmos指标，并且可以轻松修复它。让我们使用Cosmos
    DB来增加RU/s。最大值为1,000 RU/s，最小值为100。将最大值增加到10,000将最小值设置为1,000。确保您在测试运行期间仅将最大值更改为更高的值，并在需要时进行更改。您可以在调整RU/s的对话框中查看预期的成本。确保您在不再需要时减少缩放限制。可以将最大值设置为1,000,000
    RU/s，这将最小值设置为100,000。在点击**保存**按钮之前，您可以在更改此吞吐量时查看价格范围。请注意，当更改超过10,000 RU/s的最大值时，此计算能力可能需要4到6小时才能可用。
- en: With 25 virtual users, we reached the 1,000 RU/s limit. So, let’s increase it
    to 10,000 RU/s. If not that many RU/s are required with a specific number of users,
    we’ll see this in the test results and adjust the setting according to our needs
    after the test runs.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在25个虚拟用户的情况下，我们达到了1,000 RU/s的限制。因此，让我们将其增加到10,000 RU/s。如果不需要这么多RU/s来满足特定数量的用户，我们将在测试结果中看到这一点，并在测试运行后根据我们的需求调整设置。
- en: After increasing the RU/s limit and running the test again with 25 virtual users,
    Azure Cosmos DB is no longer the bottleneck. Just 12% of the RU/s are being used.
    So, let’s increase the number of virtual users to 50.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在增加RU/s限制并再次使用25个虚拟用户运行测试后，Azure Cosmos DB不再是瓶颈。只有12%的RU/s正在使用。因此，让我们将虚拟用户数量增加到50。
- en: Scaling up or scaling out services
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展服务或扩展服务
- en: Let’s run the test with 50 virtual users and compare how the performance differs
    when increasing CPU and memory, as well as increasing the number of replicas.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用50个虚拟用户运行测试，并比较在增加CPU和内存以及增加副本数量时性能如何不同。
- en: Configuring scaling up
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置扩展
- en: To scale up, we must increase the CPU and memory values.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行扩展，我们必须增加CPU和内存值。
- en: Note
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When using `azd up` to create a Container App, a *consumption-based* environment
    is created. There’s also the option to create a *workload profile* with *dedicated
    hardware*. When using dedicated hardware, you can choose the type of virtual machine
    that will be used. At the time of writing this book, the virtual machines were
    in categories D (general purpose, 4 – 32 cores, 16 – 128 GiB memory) and E (memory
    optimized 4 – 32 cores, 32 – 256 GiB memory, and GPU enabled with up to 4 GPUs).
    The type of machine also defines the available network bandwidth. Depending on
    the workload you have, there are great options available.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 `azd up` 创建容器应用时，会创建一个 *基于消费的* 环境。还有创建带有 *专用硬件* 的 *工作负载配置文件* 的选项。当使用专用硬件时，您可以选择将要使用的虚拟机类型。在撰写本书时，虚拟机分为类别
    D（通用型，4 – 32 核心，16 – 128 GiB 内存）和 E（内存优化型，4 – 32 核心，32 – 256 GiB 内存，并带有最多 4 个
    GPU）。机器类型还定义了可用的网络带宽。根据您的工作负载，有许多不错的选择。
- en: 'To change CPU and memory in the Azure portal, within the Container App, select
    **Containers** from the left bar, click the **Edit and deploy** button, select
    the container image, and click **Edit**. This will open the container editor,
    where you can select the CPU cores and memory, as shown in *Figure 12**.14*:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Azure 门户中更改 CPU 和内存，在容器应用中，从左侧栏选择 **容器**，点击 **编辑和部署** 按钮，选择容器镜像，然后点击 **编辑**。这将打开容器编辑器，您可以在其中选择
    CPU 核心和内存，如图 *图 12**.14* 所示：
- en: '![Figure 12.14 – Editing the container’s settings](img/B21217_12_14.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.14 – 编辑容器的设置](img/B21217_12_14.jpg)'
- en: Figure 12.14 – Editing the container’s settings
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.14 – 编辑容器的设置
- en: Here, we’ll change the CPU and memory values. When using the consumption-based
    environment, be aware that configurations need to map – for example, 0.5 cores
    and 1.0 Gi memory, 1.0 cores and 2.0 Gi memory, up to 2.0 cores and 4.0 Gi memory.
    In the consumption workload profile, you can have up to 4.0 cores and 8.0 Gi memory
    with a container.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将更改 CPU 和内存值。当使用基于消费的环境时，请注意配置需要映射，例如，0.5 核心 1.0 Gi 内存，1.0 核心 2.0 Gi 内存，最高到
    2.0 核心 4.0 Gi 内存。在消费工作负载配置文件中，您可以使用一个容器拥有高达 4.0 核心 8.0 Gi 内存。
- en: 'We can also configure this with the YAML template file:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用 YAML 模板文件进行配置：
- en: '[PRE10]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: CPU and memory resources are specified within the `resources` category. After
    deciding on the best configuration, specifying this with the YAML file creates
    the right size.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: CPU 和内存资源在 `resources` 类别中指定。在确定最佳配置后，使用 YAML 文件指定它将创建正确的大小。
- en: 'Running the load test for 50 users for 2 minutes shows the following results
    based on the configuration:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 对 50 个用户进行 2 分钟的负载测试，基于以下配置显示以下结果：
- en: '|  | **Total Requests** | **Throughput** | **Create** **Game Response** | **10,000
    RU/s** |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | **总请求** | **吞吐量** | **创建** **游戏响应** | **10,000 RU/s** |'
- en: '| 0.5 Cores, 1 Gi | 12,015 | 100.13/s | 491 ms | 16% |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 0.5 核心，1 Gi | 12,015 | 100.13/s | 491 ms | 16% |'
- en: '| 1 Cores, 2 Gi | 20,621 | 171.84/s | 383 ms | 24% |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 1 核心，2 Gi | 20,621 | 171.84/s | 383 ms | 24% |'
- en: '| 2 Cores, 4 Gi | 22,444 | 187.03/s | 381 ms | 26% |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 2 核心，4 Gi | 22,444 | 187.03/s | 381 ms | 26% |'
- en: Table 12.1 – Scaling up load test results
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12.1 – 扩展负载测试结果
- en: With these configurations, we can see that increasing the compute resources
    to 1 core and 2 Gi of memory makes an improvement, whereas duplicating the compute
    resources again makes just a small improvement.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些配置，我们可以看到将计算资源增加到 1 核心 2 Gi 内存会带来改进，而再次复制计算资源只会带来小幅改进。
- en: Now, let’s change the replicas.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更改副本。
- en: Configuring scaling out
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置扩展
- en: You learned how to change the number of replicas earlier in this chapter. In
    this section, we’ll change both the minimum and maximum count to the same values
    so that we can distribute the load across different instances.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 您在本章前面学习了如何更改副本数量。在本节中，我们将同时更改最小和最大计数到相同的值，这样我们就可以在不同的实例之间分配负载。
- en: 'We receive the following counts when the tests use 0.5 cores and 1 Gi of memory:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 当测试使用 0.5 核心 1 Gi 内存时，我们收到以下计数：
- en: '|  | **Total Requests** | **Throughput** | **Create** **Game Response** | **10,000
    RU/s** |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  | **总请求** | **吞吐量** | **创建** **游戏响应** | **10,000 RU/s** |'
- en: '| 1 replica | 12,015 | 100.13/s | 491 ms | 16% |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 1 副本 | 12,015 | 100.13/s | 491 ms | 16% |'
- en: '| 2 replicas | 16,291 | 135.76/s | 490 ms | 20% |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 2 副本 | 16,291 | 135.76/s | 490 ms | 20% |'
- en: '| 4 replicas | 27,704 | 230.87/s | 299 ms | 34% |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 4 副本 | 27,704 | 230.87/s | 299 ms | 34% |'
- en: Table 12.2 – Scaling out the load test results
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12.2 – 扩展负载测试结果
- en: Using two replicas with 0.5 cores and 1 Gi of memory uses the same CPU and memory
    resources as one replica with 1 core and 2 Gi of memory does. One instance with
    1 core was the better performing option, with 20,621 requests compared to 16,291
    requests. By adding more replicas, we can scale higher than what’s possible by
    just adding CPU resources.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 使用0.5核心和1 Gi的内存的两个副本与一个核心和2 Gi内存的一个副本使用的CPU和内存资源相同。一个核心的实例是更好的性能选择，有20,621个请求，而另一个有16,291个请求。通过添加更多副本，我们可以比仅添加CPU资源实现更高的缩放。
- en: A big advantage of using multiple replicas is that we can dynamically scale
    based on the load. We’ll create scale rules in the next section. Scale rules don’t
    allow us to change CPU and memory resources.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多个副本的一个大优点是我们可以根据负载动态缩放。我们将在下一节中创建缩放规则。缩放规则不允许我们更改CPU和内存资源。
- en: 'One issue you need to be aware of when scaling multiple instances is whether
    the application is designed for this. When running the Codebreaker application
    while using the in-memory games store, the implementation was built with multi-threading
    in mind, but not with multiple machines. When one user starts a game and the next
    user sets a move, the first request might access the first machine where the game
    is stored in memory, and the second request might access the second machine where
    the game to set the move is not available. The Redis cache, which offers distributed
    memory, solves this issue. The sample application available in this chapter’s
    GitHub repository includes the `DistributedMemoryGamesRepository` class, which
    can be configured with the `DataStore` configuration set to `DistributedMemory`.
    To test this on your local development environment, you can change the AppHost
    project:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在扩展多个实例时，你需要注意应用程序是否为此而设计。当在内存游戏存储中使用Codebreaker应用程序时，实现时考虑了多线程，但没有考虑多台机器。当一个用户开始游戏，而下一个用户设置移动时，第一个请求可能会访问存储在内存中的第一台机器，而第二个请求可能会访问设置移动的游戏不可用的第二台机器。提供分布式内存的Redis缓存解决了这个问题。本章GitHub仓库中提供的示例应用程序包括`DistributedMemoryGamesRepository`类，它可以配置为`DataStore`配置设置为`DistributedMemory`。为了在本地开发环境中测试此功能，你可以更改AppHost项目：
- en: Codebreaker.AppHost/Program.cs
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Codebreaker.AppHost/Program.cs
- en: '[PRE11]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'When adding the `WithReplicas` method when configuring a project, the number
    of replicas can be specified. With a value of `2`, when running the solution locally,
    the .NET Aspire dashboard (*Figure 12**.15*) shows two instances of the `game-apis`
    service running. Each service has a port number that allows the specific instance
    to be accessed. The common port number, `9400`, is the port of the proxy client
    that references both `game-apis` service instances running with port numbers `49379`
    and `49376`. The port number that’s used by the proxy is defined with the `launchsettings.json`
    file, while the port number for the instances randomly changes when a new application
    starts:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 当在配置项目时添加`WithReplicas`方法，可以指定副本的数量。当值为`2`时，在本地运行解决方案时，.NET Aspire仪表板（*图12**.15*）显示运行的两个`game-apis`服务实例。每个服务都有一个端口号，允许访问特定的实例。公共端口号`9400`是引用运行在端口号`49379`和`49376`的`game-apis`服务实例的代理客户端的端口号。代理使用的端口号由`launchsettings.json`文件定义，而实例的端口号在新的应用程序启动时随机更改：
- en: '![Figure 12.15 – Two replicas for game-apis](img/B21217_12_15.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图12.15 – game-apis的两个副本](img/B21217_12_15.jpg)'
- en: Figure 12.15 – Two replicas for game-apis
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.15 – game-apis的两个副本
- en: Now that we know about the improvements that we can make when running multiple
    replicas, let’s scale dynamically.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了在运行多个副本时可以进行的改进，让我们进行动态缩放。
- en: Scaling dynamically with scale rules
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用缩放规则动态缩放
- en: With Azure Container Apps, scale rules can be defined based on concurrent HTTP
    requests, concurrent TCP requests, or custom rules. With custom rules, scaling
    can be based on CPU, memory, or many events based on different data sources.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Azure Container Apps，可以根据并发HTTP请求、并发TCP请求或自定义规则定义缩放规则。使用自定义规则，缩放可以基于CPU、内存或基于不同数据源的许多事件。
- en: A microservice isn’t necessarily triggered based on HTTP requests. The service
    can also be triggered asynchronously, such as when a message arrives in a queue
    (for example, using Azure Storage Queue or Azure Service Bus) or when events occur
    (for example, using Azure Event Hub or Apache Kafka).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 一个微服务不一定基于HTTP请求触发。该服务也可以异步触发，例如，当队列中到达消息时（例如，使用Azure存储队列或Azure服务总线）或当事件发生时（例如，使用Azure事件中心或Apache
    Kafka）。
- en: Azure Container Apps scale rules are based on **Kubernetes Event-driven Autoscaling**
    (**KEDA**), which offers a large list of scalers. You can find the full list at
    [https://keda.sh](https://keda.sh).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 容器应用的缩放规则基于 **Kubernetes 事件驱动自动缩放**（**KEDA**），提供了一长串的缩放器。您可以在 [https://keda.sh](https://keda.sh)
    找到完整的列表。
- en: When using a KEDA scaler with the Azure Service Bus queue, you can specify how
    many messages should be in the queue when another replica should be started. What’s
    common with all the KEDA scalers is the configuration of the polling interval
    – how often the values are checked (by default, this is 30 seconds), a scaling
    algorithm to calculate the number of replicas, and a cooldown period (300 seconds)
    – the time before replicas started can be stopped again.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 Azure Service Bus 队列与 KEDA 缩放器一起使用时，您可以指定在启动另一个副本时应有多少条消息在队列中。所有 KEDA 缩放器共有的特点是轮询间隔的配置
    – 检查值的频率（默认为 30 秒），一个用于计算副本数量的缩放算法，以及冷却期（300 秒） – 在副本启动后可以再次停止副本之前的时间。
- en: In [*Chapter 15*](B21217_15.xhtml#_idTextAnchor349), we’ll use communication
    with messages and events where autoscaling will be based on event-based KEDA scalers.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 15 章*](B21217_15.xhtml#_idTextAnchor349) 中，我们将使用消息和事件进行通信，自动缩放将基于基于事件的
    KEDA 缩放器。
- en: 'As we saw when we tested the load with a fixed number of instances, the best
    option to scale the services is using the number of HTTP requests. So, let’s configure
    scaling with an HTTP rule. We can do this by using the Azure portal and the `azd
    infra synth` generated template YAML file. This is the output from the JSON content
    in the Azure portal:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用固定数量的实例测试负载时，我们看到最佳的服务扩展选项是使用 HTTP 请求的数量。因此，让我们使用 HTTP 规则配置扩展。我们可以通过使用
    Azure 门户和由 `azd infra synth` 生成的模板 YAML 文件来完成此操作。这是 Azure 门户中 JSON 内容的输出：
- en: '[PRE12]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The default HTTP rule scales with 10 concurrent requests. Based on the tests,
    let’s set this value to `30`. The number of replicas is in the range of `1` to
    `8`. What’s important to know regarding HTTP scaling is that the number of requests
    is calculated every 15 seconds. The total requests from within the last 15 seconds
    are divided by 15 so that they can be compared to the `concurrentRequests` value.
    Based on this, the number of replicas is calculated. So, if there are 140 requests
    per second, the instance count will be set to 5.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的 HTTP 规则与 10 个并发请求成比例。根据测试结果，我们将此值设置为 `30`。副本的数量在 `1` 到 `8` 之间。关于 HTTP 缩放的重要信息是，每
    15 秒计算一次请求数量。将过去 15 秒内的总请求数除以 15，以便可以将其与 `concurrentRequests` 值进行比较。据此，计算副本的数量。因此，如果每秒有
    140 个请求，实例计数将设置为 5。
- en: 'When this scale rule is applied, and the instances are active, we can configure
    a load test with a dynamic pattern configuration, as shown in *Figure 12**.16*:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用此缩放规则且实例处于活动状态时，我们可以配置具有动态模式配置的负载测试，如图 *图 12.16* 所示：
- en: '![Figure 12.16 – Step load pattern](img/B21217_12_16.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.16 – 步骤负载模式](img/B21217_12_16.jpg)'
- en: Figure 12.16 – Step load pattern
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.16 – 步骤负载模式
- en: With this step load pattern, two engine instances are used to start 200 virtual
    users. The complete test duration is 4 minutes. The ramp-up time defines how long
    it takes until the 200 virtual users are reached – and 5 ramp-up steps are used
    to increment the users with 40 increments.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在此步骤负载模式下，使用两个引擎实例启动 200 个虚拟用户。完整的测试持续时间是 4 分钟。爬坡时间定义了达到 200 个虚拟用户所需的时间 – 使用
    5 个爬坡步骤以 40 的增量增加用户。
- en: 'After the test runs are completed, you can see how the virtual users were added
    over time, as shown in *Figure 12**.17*:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 测试运行完成后，您可以看到虚拟用户随时间增加的情况，如图 *图 12.17* 所示：
- en: '![Figure 12.17 – 200 virtual users](img/B21217_12_17.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.17 – 200 虚拟用户](img/B21217_12_17.jpg)'
- en: Figure 12.17 – 200 virtual users
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.17 – 200 虚拟用户
- en: 'The number of requests per second is shown in *Figure 12**.18*:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 每秒请求数量显示在 *图 12.18* 中：
- en: '![Figure 12.18 – Requests per second](img/B21217_12_18.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.18 – 每秒请求数量](img/B21217_12_18.jpg)'
- en: Figure 12.18 – Requests per second
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.18 – 每秒请求数量
- en: '*Figure 12**.19* shows the response time with longer times starting after 2:17
    P.M. Do you have any idea what could have happened here?'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 12.19* 显示了响应时间，时间在下午 2:17 开始变长。您有什么想法吗？'
- en: '![Figure 12.19 – Response time](img/B21217_12_19.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.19 – 响应时间](img/B21217_12_19.jpg)'
- en: Figure 12.19 – Response time
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.19 – 响应时间
- en: 'The answer is that with this load, we reached the 10,000 RU/s limit using the
    Azure Cosmos database. This is shown in *Figure 12**.20*:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是，使用这种负载，我们达到了 Azure Cosmos 数据库的 10,000 RU/s 限制，如图 *图 12.20* 所示：
- en: '![Figure 12.20 – Reaching 10,000 RU/s](img/B21217_12_20.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.20 – 达到 10,000 RU/s](img/B21217_12_20.jpg)'
- en: Figure 12.20 – Reaching 10,000 RU/s
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.20 – 达到 10,000 RU/s
- en: The max RU/s were reached after 2:16 P.M. Responses with “too many requests”
    are not returned immediately when this limit is reached.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在下午 2:16 之后达到了最大 RU/s。当达到这个限制时，不会立即返回“请求过多”的响应。
- en: 'It’s also interesting to see the replica count for the scale rule we’ve created.
    This is shown in *Figure 12**.21*:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 看到我们创建的缩放规则的副本计数也很有趣。这显示在 *图 12.21* 中：
- en: '![Figure 12.21 – Replica count](img/B21217_12_21.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.21 – 副本计数](img/B21217_12_21.jpg)'
- en: Figure 12.21 – Replica count
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.21 – 副本计数
- en: 'The replica count started at 1 and increased up to 8 – the maximum configured
    replica number. *Figure 12**.19* showed another issue than the RU/s limit. Directly
    after the start, there are some peaks in the response time. This corresponds to
    the reduction in requests per second in *Figure 12**.18*. I had to dig into this
    issue after this test run. Metrics data didn’t reveal the reason, but checking
    the logs in the `ContainerAppSystemLogs_CL` table provided information about what
    the issue was. At that time, it was time to start a new replica. As indicated
    by the logs, a new replica was assigned, the image was pulled, and a container
    was created – but the *startup probe* failed, and the replica was unhealthy. Requests
    are not sent to such replicas. So, for the load we generated, we still only had
    one replica. Faulty replicas were started two times before the third one succeeded.
    This is why increasing the replica count took longer than it should have been,
    and that’s the reason for the peak in the beginning. After this, every new replica
    that was created immediately succeeded. If you have some issues with a specific
    replica, you can use the replica’s name to query the logs:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 副本计数从 1 开始，增加到 8 – 最大配置的副本数。*图 12.19* 显示了另一个问题，而不是 RU/s 限制。启动后，响应时间有一些峰值。这对应于
    *图 12.18* 中每秒请求量的减少。在这次测试运行之后，我不得不深入调查这个问题。指标数据没有揭示原因，但检查 `ContainerAppSystemLogs_CL`
    表中的日志提供了关于问题是什么的信息。当时，是时候启动一个新的副本了。根据日志，分配了新的副本，拉取了镜像，并创建了一个容器 – 但 *启动探测* 失败，副本不健康。不会向这样的副本发送请求。因此，对于我们所生成的负载，我们仍然只有一个副本。在第三个副本成功之前，有缺陷的副本启动了两次。这就是为什么增加副本计数比预期花费的时间更长，这也是为什么开始时出现峰值的原因。之后，每个新创建的副本都立即成功。如果你有特定副本的问题，你可以使用副本的名称来查询日志：
- en: '[PRE13]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Next, we’ll dive into health checks. This will help you understand startup probes.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入了解健康检查。这将帮助你理解启动探测。
- en: Implementing health checks
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现健康检查
- en: The hosting platform needs to know if the service started successfully and is
    available to serve requests. While the service is running, the hosting platform
    continuously checks the service to see if it is running or broken and needs to
    be restarted. This is what health checks are for.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 托管平台需要知道服务是否成功启动并且可以处理请求。当服务正在运行时，托管平台会持续检查服务是否正在运行或已损坏并需要重启。这就是健康检查的目的。
- en: 'With Kubernetes, three probes can be configured:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Kubernetes，可以配置三个探测：
- en: '`Startup`: Is the container ready and did it start? When this probe succeeds,
    Kubernetes switches to the other probes.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`启动`: 容器是否已准备好并且已启动？当这个探测成功时，Kubernetes 将切换到其他探测。'
- en: '`Liveness`: Did the application crash or deadlock? If this fails, the pod is
    stopped, and a new container instance is created.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`活跃性`: 应用程序是否崩溃或死锁？如果失败，则停止 pod，并创建一个新的容器实例。'
- en: '`Readiness`: Is the application ready to receive requests? If this fails, no
    requests are sent to this service instance, but the pod keeps running.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`就绪性`: 应用程序是否准备好接收请求？如果失败，则不会向此服务实例发送请求，但 pod 仍然运行。'
- en: Because Azure Container Apps is based on Kubernetes, these three probes can
    be configured with this Azure service as well.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Azure Container Apps 基于 Kubernetes，因此可以使用此 Azure 服务配置这三个探测。
- en: Adding health checks to the DI container
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将健康检查添加到 DI 容器
- en: 'Health checks can be configured with the DI container within the `AddDefaultHealthChecks`
    extension method:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 健康检查可以通过 `AddDefaultHealthChecks` 扩展方法中的 DI 容器进行配置：
- en: Codebreaker.ServiceDefaults/Extensions.cs
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Codebreaker.ServiceDefaults/Extensions.cs
- en: '[PRE14]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `AddHealthChecks` method registers the `HealthCheckService` class with the
    DI container that will be available for health checks. The `AddHealthChecks` method
    can be invoked multiple times to access `IHealthChecksBuilder`, which is used
    to register different implementations of checks. The fluently invoked `AddCheck`
    method uses a delegate to return the `HealthCheckResult.Healthy` result on invocation.
    The last parameter defines the `"live"` tag. Tags are used with middleware to
    specify with which route this health check should be used. As the name suggests,
    this tag is good for a `liveness` probe. When the service is accessible, a result
    is returned. If the service is not available, nothing is returned, and thus it
    will be restarted. The name `self` indicates that only the service itself is used
    with this health check, and external resources are only consulted with readiness
    health checks.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`AddHealthChecks` 方法将 `HealthCheckService` 类注册到 DI 容器中，该容器将可用于健康检查。`AddHealthChecks`
    方法可以多次调用以访问 `IHealthChecksBuilder`，它用于注册不同的检查实现。流畅调用的 `AddCheck` 方法使用委托在调用时返回
    `HealthCheckResult.Healthy` 结果。最后一个参数定义了 `"live"` 标签。标签与中间件一起使用，以指定应使用哪个路由进行此健康检查。正如其名所示，此标签非常适合
    `liveness` 探针。当服务可访问时，会返回一个结果。如果服务不可用，则不返回任何内容，因此它将被重新启动。名称 `self` 表示仅使用此健康检查本身，而外部资源仅在就绪健康检查中咨询。'
- en: 'On startup of the `game-apis` service, using Azure Cosmos DB, the container
    is created, and with SQL Server, database migration can occur in case the database
    schema is updated. The application is not ready to receive requests before this
    is completed. With some applications, the cache needs to be filled with reference
    data before requests are accepted. To do this, a Boolean flag must be defined
    with the database update code that is set when the update is completed. Let’s
    add a health check to the DI container configuration of `game-apis`:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `game-apis` 服务的启动时，使用 Azure Cosmos DB 创建容器，如果数据库模式已更新，则可以使用 SQL Server 进行数据库迁移。在此完成之前，应用程序尚未准备好接收请求。对于某些应用程序，在接收请求之前需要用参考数据填充缓存。为此，必须使用数据库更新代码定义一个布尔标志，该标志在更新完成后设置。让我们向
    `game-apis` 的 DI 容器配置中添加一个健康检查：
- en: Codebreaker.GameAPIs/Program.cs
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Codebreaker.GameAPIs/Program.cs
- en: '[PRE15]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: With this implementation, we return `Healthy` if the flag (the `IsDatabaseUpdateComplete`
    property) is true and `Degraded` otherwise.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在此实现中，如果标志（`IsDatabaseUpdateComplete` 属性）为真，则返回 `Healthy`，否则返回 `Degraded`。
- en: When you try this out, the database migration might be too fast to see a degraded
    result – especially if the database has already been created Adding a delay to
    the migration of the database in the `Codebreaker.GameAPIs/ApplicationServices.cs`
    file helps here as you can view different health results.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 当你尝试此操作时，数据库迁移可能太快而无法看到降级结果——特别是如果数据库已经创建的话。在 `Codebreaker.GameAPIs/ApplicationServices.cs`
    文件中将数据库迁移添加延迟有助于这里，因为你可以查看不同的健康结果。
- en: Note
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Health checks should be implemented quickly. These checks are invoked quite
    often and shouldn’t result in a big overhead. Often, these checks involve opening
    a connection or checking a flag to be set.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 健康检查应快速实现。这些检查调用频率很高，不应导致大的开销。通常，这些检查涉及打开连接或检查要设置的标志。
- en: Adding health checks with .NET Aspire components
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 .NET Aspire 组件添加健康检查
- en: All.NET Aspire components have health checks enabled – if the component supports
    health checks. Check out the documentation regarding these components to learn
    more.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 .NET Aspire 组件都启用了健康检查功能——如果组件支持健康检查。请查阅有关这些组件的文档以了解更多信息。
- en: Similar to metrics and telemetry configuration, health checks can be enabled
    and disabled with the configuration settings of the component.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 与指标和遥测配置类似，健康检查可以通过组件的配置设置启用和禁用。
- en: 'This can be done programmatically:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过编程方式完成：
- en: Codebreaker.GameAPIs/ApplicationServices.cs
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: Codebreaker.GameAPIs/ApplicationServices.cs
- en: '[PRE16]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The parameter of the .NET Aspire SQL Server EF Core `EnrichSqlServerDbContext`
    component method allows us to override the default for the component settings,
    such as metrics, tracing, and health checks.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: .NET Aspire SQL Server EF Core `EnrichSqlServerDbContext` 组件方法的参数允许我们覆盖组件设置的默认值，例如指标、跟踪和健康检查。
- en: 'We can also specify this with the following .NET Aspire configuration:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用以下 .NET Aspire 配置来指定此功能：
- en: '[PRE17]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This configuration shows the settings for the Redis and SQL Server EF Core components.
    Both components integrate with the ready probe. What’s done with these health
    checks? There’s no reading or writing to the database. Health checks should be
    fast and not put a high load on the system. The Redis component tries to open
    the connection. The SQL Server EF Core component invokes the EF Core `CanConnectAsync`
    method. What you need to be aware of is that when the idle pricing of an Azure
    Container App scales down to 1, with custom health checks, it might never be idle.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 此配置显示了 Redis 和 SQL Server EF Core 组件的设置。这两个组件都与就绪探针集成。这些健康检查做了什么？没有对数据库进行读取或写入。健康检查应该是快速的，并且不会对系统造成高负载。Redis
    组件尝试打开连接。SQL Server EF Core 组件调用 EF Core 的 `CanConnectAsync` 方法。您需要注意，当 Azure
    Container App 的空闲定价缩放到 1 时，带有自定义健康检查，它可能永远不会空闲。
- en: Using such a configuration ensures that this can be changed without the need
    to recompile the project.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此类配置确保可以在不重新编译项目的情况下进行更改。
- en: Now that we’ve implemented and configured health checks, let’s map these to
    URL requests.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经实现了和配置了健康检查，让我们将这些映射到 URL 请求。
- en: Mapping health checks
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 映射健康检查
- en: 'Mapping health links to URLs allows us to access them. The shared `Codebreaker.ServiceDefaults`
    file contains the health endpoints that have been configured with the `MapDefaultEndpoints`
    method:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 将健康链接映射到 URL 允许我们访问它们。共享的 `Codebreaker.ServiceDefaults` 文件包含使用 `MapDefaultEndpoints`
    方法配置的健康端点：
- en: Codebreaker.ServiceDefaults/Extensions.cs
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: Codebreaker.ServiceDefaults/Extensions.cs
- en: '[PRE18]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `/alive` probe link has been configured to only use the health checks with
    the `live` tag, so the health checks are used to check if the service is alive.
    This link should be configured for live probing, and the service should be restarted
    if it does not return `Healthy`.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '`/alive` 探针链接已配置为仅使用带有 `live` 标签的健康检查，因此健康检查用于检查服务是否存活。此链接应配置为实时探测，如果服务没有返回
    `Healthy`，则应重新启动服务。'
- en: 'The `/health` probe link has been configured not to restrict the health checks
    based on a tag. Here, all health checks are invoked and need to be successful.
    This link should be used for a ready probe: is the service ready to receive requests?
    If this returns `Unhealthy` or `Degraded`, the service isn’t stopped but doesn’t
    receive requests.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '`/health` 探针链接已配置为不根据标签限制健康检查。在这里，所有健康检查都会被调用，并且需要成功。此链接应用于就绪探针：服务是否准备好接收请求？如果返回
    `Unhealthy` 或 `Degraded`，则服务没有停止，但不会接收请求。'
- en: Note
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You might be wondering why .NET Aspire is not using the `/healthz` link for
    the ready probes, as it is typically used with Kubernetes. `/healthz` historically
    comes from Google’s internal practices, z-pages, so that it doesn’t get into conflicts.
    The .NET Aspire team had several iterations on deciding on the different links
    and included `/liveness` and `/readiness`, and finally ended up with `/alive`
    and `/health`.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想知道为什么 .NET Aspire 不使用 `/healthz` 链接进行就绪探针，因为它通常与 Kubernetes 一起使用。`/healthz`
    历史上来自 Google 的内部实践，z-pages，因此不会发生冲突。.NET Aspire 团队对不同的链接进行了多次迭代，包括 `/liveness`
    和 `/readiness`，最终确定为 `/alive` 和 `/health`。
- en: Now that we’ve mapped the health checks to URIs, let’s use these links.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将健康检查映射到 URI，让我们使用这些链接。
- en: Using the health checks with Azure Container Apps
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Azure Container Apps 进行健康检查
- en: You can automatically integrate the health probes after generating probe configuration
    with Azure Container Apps. You can also use these health probes with the Azure
    dashboard. However, this is not available with the first release of .NET Aspire
    and is planned for a later release. However, with a little customization, this
    can easily be done.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Azure Container Apps 生成探针配置后，您可以自动集成健康探针。您还可以使用这些健康探针与 Azure 仪表板一起使用。然而，这不在
    .NET Aspire 的第一个版本中提供，计划在以后的版本中提供。但是，通过一点定制，这可以轻松完成。
- en: 'For this to work, you need to have initialized the solution with `azd init`,
    which you did previously, before publishing the solution to Azure. Now, from the
    folder that contains the solution, create the code that will publish these projects:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使其工作，您需要使用 `azd init` 初始化解决方案，您之前已经这样做，然后在将解决方案发布到 Azure 之前。现在，从包含解决方案的文件夹中创建将发布这些项目的代码：
- en: '[PRE19]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'With this, the AppHost project contains an `infra` folder with `<app>.tmpl.yaml`
    files. Within the `gameapis.tmpl.yaml` file, specify the `probes` section:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，AppHost 项目包含一个 `infra` 文件夹，其中包含 `<app>.tmpl.yaml` 文件。在 `gameapis.tmpl.yaml`
    文件中，指定 `probes` 部分：
- en: '[PRE20]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `probes` section allows `liveness`, `readiness`, and `startup` probe types
    to be configured. The `liveness` probe is configured to invoke the `/alive` link
    and the `readiness` probe is configured to invoke the `/health` link with the
    port of the running Docker container. Azure Container Apps have default settings.
    However, as soon as you specify probing, you need to configure all probe types;
    otherwise, the probes that haven’t been configured will be disabled. Thus, when
    specifying `liveness` and `readiness` probes, the `startup` probe should be configured
    as well. This probe uses a TCP connection to connect to the service to verify
    that the connection succeeds.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '`probes` 部分允许配置 `liveness`、`readiness` 和 `startup` 探测类型。`liveness` 探测配置为调用
    `/alive` 链接，而 `readiness` 探测配置为调用带有运行 Docker 容器端口的 `/health` 链接。Azure Container
    Apps 有默认设置。然而，一旦指定了探测，就需要配置所有探测类型；否则，未配置的探测将被禁用。因此，当指定 `liveness` 和 `readiness`
    探测时，也应配置 `startup` 探测。此探测使用 TCP 连接连接到服务以验证连接是否成功。'
- en: '`initialDelaySeconds` specifies the seconds to wait until the first probe is
    done. If this fails, additional checks are done after the number of seconds specified
    by `periodSeconds`. A failure only counts until `failureTreshold` is reached.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '`initialDelaySeconds` 指定了等待直到第一次探测完成所需的秒数。如果这失败了，将在 `periodSeconds` 指定的秒数之后进行额外的检查。只有当达到
    `failureTreshold` 时，失败才会计数。'
- en: The default startup probe uses a TCP probe to check the ingress target port
    with an initial delay of 1 second, a timeout of 3 seconds, and a period of 1 second.
    With the failure threshold, this multiplies, and the app can take some time until
    it’s started successfully. When the `startup` probe has been successful once,
    only the `liveness` and `readiness` probes are used afterward.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 默认启动探测使用 TCP 探测，检查入口目标端口，初始延迟为 1 秒，超时为 3 秒，周期为 1 秒。随着失败阈值的增加，这会乘以，应用可能需要一些时间才能成功启动。一旦
    `startup` 探测成功一次，之后只使用 `liveness` 和 `readiness` 探测。
- en: 'After making this change, from the solution folder, run the following command:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 进行此更改后，从解决方案文件夹中运行以下命令：
- en: '[PRE21]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This will deploy the service to Azure and configure the health checks.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这将部署服务到 Azure 并配置健康检查。
- en: 'Open the Azure portal, navigate to Azure Container Apps, and select **Containers**
    from the left pane. You’ll see a tab called **Health probes**, as shown in *Figure
    12**.22*:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 打开 Azure 门户，导航到 Azure Container Apps，并从左侧面板中选择 **容器**。你会看到一个名为 **健康探测** 的选项卡，如图
    *图 12.22* 所示：
- en: '![Figure 12.22 – Health probes within Azure Container Apps](img/B21217_12_22.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.22 – Azure Container Apps 中的健康探测](img/B21217_12_22.jpg)'
- en: Figure 12.22 – Health probes within Azure Container Apps
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.22 – Azure Container Apps 中的健康探测
- en: 'Here, we can see the configured settings for the liveness probe in the portal.
    You can also verify the readiness and startup probes. *Figure 12**.23* shows the
    status of a container:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到门户中配置的 `liveness` 探测的设置。你还可以验证 `readiness` 和 `startup` 探测。*图 12.23*
    显示了容器的状态：
- en: '![Figure 12.23 – Container app running but not ready](img/B21217_12_23.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.23 – 容器应用正在运行但尚未就绪](img/B21217_12_23.jpg)'
- en: Figure 12.23 – Container app running but not ready
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.23 – 容器应用正在运行但尚未就绪
- en: Here, one replica is running, but this replica isn’t ready. The `readiness`
    probe didn’t return success at that time. If you configured the Redis component
    to offer health checks with .NET Aspire, you can stop this container using the
    Azure Container Apps environment. You’ll see that the `game-apis` service isn’t
    ready. Because the `game-apis` service has the component health check activated,
    an error is returned with the readiness check.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，有一个副本正在运行，但这个副本尚未就绪。当时的 `readiness` 探测没有返回成功。如果你配置了 Redis 组件使用 .NET Aspire
    提供健康检查，你可以使用 Azure Container Apps 环境停止此容器。你会看到 `game-apis` 服务尚未就绪。因为 `game-apis`
    服务启用了组件健康检查，所以健康检查返回了错误。
- en: Summary
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned how to use telemetry data and implemented a cache
    to reduce the number of database requests. You created health checks while differing
    between startup, liveness, and readiness checks. Liveness checks are used to restart
    services, while readiness checks are used to verify whether a service is ready
    to receive requests. Regarding readiness checks, you learned how to integrate
    .NET Aspire components. You also learned how to get information from load tests
    to find bottlenecks in the deployed application and to decide on the infrastructure
    you wish to use. By doing this, you learned how to configure the application so
    that it scales up when changes are made to CPU and memory, as well as how to scale
    out when running multiple replicas using scaling rules.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何使用遥测数据并实现缓存以减少数据库请求的数量。你创建了健康检查，区分了启动、存活性和就绪性检查。存活性检查用于重启服务，而就绪性检查用于验证服务是否准备好接收请求。关于就绪性检查，你学习了如何集成
    .NET Aspire 组件。你还学习了如何从负载测试中获取信息以找到已部署应用程序中的瓶颈，并决定你希望使用的架构。通过这样做，你学习了如何配置应用程序，使其在
    CPU 和内存发生变化时进行扩展，以及如何使用扩展规则在运行多个副本时进行横向扩展。
- en: 'This chapter has uncovered an important reason for using microservices: with
    scaling, great flexibility can easily be achieved.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 本章揭示了使用微服务的一个重要原因：随着规模的扩大，可以轻松实现极大的灵活性。
- en: The next chapter will act as a starting point and implement different communication
    techniques with microservices. When adding more functionality to your application,
    you need to think about doing continuous load tests on the solution in a test
    environment and monitoring the changes.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将作为一个起点，并实现与微服务不同的通信技术。当向你的应用程序添加更多功能时，你需要考虑在测试环境中对解决方案进行持续负载测试并监控变化。
- en: Further reading
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'To learn more about the topics that were discussed in this chapter, please
    refer to the following links:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于本章讨论的主题，请参阅以下链接：
- en: '*Distributed caching in ASP.NET* *Core*: [https://learn.microsoft.com/en-us/aspnet/core/performance/caching/distributed](https://learn.microsoft.com/en-us/aspnet/core/performance/caching/distributed)'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ASP.NET* *Core* 中的分布式缓存：[https://learn.microsoft.com/en-us/aspnet/core/performance/caching/distributed](https://learn.microsoft.com/en-us/aspnet/core/performance/caching/distributed)'
- en: '*Database scalability: scaling out vs scaling* *up*: [https://azure.microsoft.com/en-au/resources/cloud-computing-dictionary/scaling-out-vs-scaling-up/](https://azure.microsoft.com/en-au/resources/cloud-computing-dictionary/scaling-out-vs-scaling-up/)'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据库可伸缩性：横向扩展与纵向扩展*：[https://azure.microsoft.com/en-au/resources/cloud-computing-dictionary/scaling-out-vs-scaling-up/](https://azure.microsoft.com/en-au/resources/cloud-computing-dictionary/scaling-out-vs-scaling-up/)'
- en: '*Azure Virtual Machine* *Sizes*: [https://learn.microsoft.com/en-us/azure/virtual-machines/sizes](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes)'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Azure 虚拟机* *大小*：[https://learn.microsoft.com/en-us/azure/virtual-machines/sizes](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes)'
- en: '*Workload* *Profiles*: [https://learn.microsoft.com/en-us/azure/container-apps/workload-profiles-overview](https://learn.microsoft.com/en-us/azure/container-apps/workload-profiles-overview)'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*工作负载* *配置文件*：[https://learn.microsoft.com/en-us/azure/container-apps/workload-profiles-overview](https://learn.microsoft.com/en-us/azure/container-apps/workload-profiles-overview)'
- en: '*Container* *configuration*: [https://learn.microsoft.com/en-us/azure/container-apps/containers#configuration](https://learn.microsoft.com/en-us/azure/container-apps/containers#configuration)'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*容器* *配置*：[https://learn.microsoft.com/en-us/azure/container-apps/containers#configuration](https://learn.microsoft.com/en-us/azure/container-apps/containers#configuration)'
- en: '*Azure Container Apps YAML* *specification*: [https://learn.microsoft.com/en-us/azure/container-apps/azure-resource-manager-api-spec](https://learn.microsoft.com/en-us/azure/container-apps/azure-resource-manager-api-spec)'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Azure Container Apps YAML* *规范*：[https://learn.microsoft.com/en-us/azure/container-apps/azure-resource-manager-api-spec](https://learn.microsoft.com/en-us/azure/container-apps/azure-resource-manager-api-spec)'
- en: '*Health checks in ASP.NET* *Core*: [https://learn.microsoft.com/en-us/aspnet/core/host-and-deploy/health-checks/](https://learn.microsoft.com/en-us/aspnet/core/host-and-deploy/health-checks/)'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ASP.NET* *Core* 中的健康检查：[https://learn.microsoft.com/en-us/aspnet/core/host-and-deploy/health-checks/](https://learn.microsoft.com/en-us/aspnet/core/host-and-deploy/health-checks/)'
- en: '*Convention for /**healthz*: [https://stackoverflow.com/questions/43380939/where-does-the-convention-of-using-healthz-for-application-health-checks-come-f](https://stackoverflow.com/questions/43380939/where-does-the-convention-of-using-healthz-for-application-health-checks-come-f)'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*关于 /**healthz* 的约定：[https://stackoverflow.com/questions/43380939/where-does-the-convention-of-using-healthz-for-application-health-checks-come-f](https://stackoverflow.com/questions/43380939/where-does-the-convention-of-using-healthz-for-application-health-checks-come-f)*'
- en: 'Part 4: More communication options'
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四部分：更多通信选项
- en: In this part, the focus shifts towards leveraging various communication technologies
    and incorporating additional Azure services to enhance the application. Real-time
    messaging capabilities are implemented using SignalR to deliver instantaneous
    updates from the application to clients. gRPC is utilized for efficient binary
    communication between services, enabling seamless message exchange through queues
    and event publication. Azure services such as Azure SignalR Services, Event Hub,
    Azure Queue Storage, and Apache Kafka are integrated into the application ecosystem.
    Additionally, a detailed examination of considerations for production environments
    is provided, culminating in the deployment of the application to a Kubernetes
    cluster, specifically Azure Kubernetes Services, utilizing **Aspir8**.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在本部分，重点转向利用各种通信技术以及整合额外的 Azure 服务来增强应用程序。通过 SignalR 实现实时消息功能，将应用程序到客户端的即时更新传递。使用
    gRPC 进行服务间的有效二进制通信，通过队列和事件发布实现无缝的消息交换。Azure 服务如 Azure SignalR 服务、事件中心、Azure 队列存储和
    Apache Kafka 被集成到应用程序生态系统中。此外，还提供了针对生产环境考虑因素的详细审查，最终将应用程序部署到 Kubernetes 集群，特别是
    Azure Kubernetes 服务，利用 **Aspir8**。
- en: 'This part has the following chapters:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 13*](B21217_13.xhtml#_idTextAnchor317), *Real-time Messaging with
    SignalR*'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第13章*](B21217_13.xhtml#_idTextAnchor317)，*使用 SignalR 进行实时消息传递*'
- en: '[*Chapter 14*](B21217_14.xhtml#_idTextAnchor330), *gRPC for Binary Communication*'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第14章*](B21217_14.xhtml#_idTextAnchor330)，*二进制通信的 gRPC*'
- en: '[*Chapter 15*](B21217_15.xhtml#_idTextAnchor349)*, Asynchronous Communication
    with Messages and Events*'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第15章*](B21217_15.xhtml#_idTextAnchor349)*，使用消息和事件进行异步通信*'
- en: '[*Chapter 16*](B21217_16.xhtml#_idTextAnchor373)*, Running Applications On-Premises
    and in the Cloud*'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第16章*](B21217_16.xhtml#_idTextAnchor373)*，在本地和云端运行应用程序*'
