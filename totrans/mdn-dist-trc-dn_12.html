<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-193"><a id="_idTextAnchor192"/>12</h1>
<h1 id="_idParaDest-194"><a id="_idTextAnchor193"/>Instrumenting Database Calls</h1>
<p>In this chapter, we’re going to continue exploring instrumentation approaches for popular distributed patterns and will look into database instrumentation. We’ll use MongoDB as an example and combine it with Redis cache. We’ll add tracing and metrics instrumentation for database and cache calls and discuss how to add application context and provide observability in these composite scenarios. In addition to client-side instrumentation, we’ll see how to also scrape Redis server metrics with the OpenTelemetry Collector Finally, we’ll explore the generated telemetry and see how it helps with analyzing application performance.</p>
<p>Here’s what you’ll learn about:</p>
<ul>
<li>Tracing MongoDB operations</li>
<li>Tracing Redis cache and logical calls</li>
<li>Adding client- and server-side metrics</li>
<li>Using telemetry to analyze failures and performance</li>
</ul>
<p>By the end of this chapter, you’ll be familiar with generic database instrumentations and will be able to instrument your own applications using databases or caches and analyze their performance.</p>
<h1 id="_idParaDest-195"><a id="_idTextAnchor194"/>Technical requirements</h1>
<p>The code for this chapter is available in the book’s repository on GitHub at <a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter12">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter12</a>.</p>
<p>To run the samples and perform analysis, we’ll need the following tools:</p>
<ul>
<li>.NET SDK 7.0 or later</li>
<li>Docker and <code>docker-compose</code></li>
</ul>
<h1 id="_idParaDest-196"><a id="_idTextAnchor195"/>Instrumenting database calls</h1>
<p>Databases are<a id="_idIndexMarker637"/> used in almost every distributed application. Many databases provide advanced monitoring capabilities on the server side, which include database-specific metrics, logs, or expensive query detection and analysis tools. Client instrumentation complements it by providing observability on the client side of this communication, correlating database operations, and adding application-specific context.</p>
<p>Client instrumentation describes an application’s communication with a database ORM system, driver, or client library, which can be quite complicated performing load balancing or batching operations in the background.</p>
<p>In some cases, it could be possible to trace network-level communication between the client library and the database cluster. For example, if a database uses gRPC or HTTP protocols, the corresponding auto-instrumentation would capture transport-level spans. In this case, we would see transport-level spans as children of a logical database operation initiated by the application.</p>
<p>Here, we’re going to instrument the logical level of the MongoDB C# driver to demonstrate the principles that apply to other database instrumentations.</p>
<p class="callout-heading">Note</p>
<p class="callout">Generic instrumentation for <code>MongoDB.Driver</code> is available in the <code>MongoDB.Driver.Core.Extensions.OpenTelemetry</code> NuGet package.</p>
<p>Before we start the instrumentation, let’s check out OpenTelemetry semantic conventions for databases.</p>
<h2 id="_idParaDest-197"><a id="_idTextAnchor196"/>OpenTelemetry semantic conventions for databases</h2>
<p>The <a id="_idIndexMarker638"/>conventions are available at <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/trace/semantic_conventions/database.md">https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/trace/semantic_conventions/database.md</a>. They have an experimental status and may have changed by the time you access the link.</p>
<p>Conventions define attributes for both logical and physical calls. In our case, we are not instrumenting transport-level communication, so we will only use the ones applicable to logical operations:</p>
<ul>
<li><code>db.system</code>: This is a required attribute that tracing backends use to distinguish database spans from all others. It should match the <code>mongodb</code> string, which observability backends may use to provide database or even MongoDB-specific analysis and visualizations.</li>
<li><code>db.connection_string</code>: This is a recommended attribute. It’s also recommended to strip credentials before providing it. We’re not going to add it to our custom instrumentation. There could be cases where it’s useful to capture the connection string (without credentials) as it can help detect configuration issues or we can also log it once at start time.</li>
<li><code>db.user</code>: This is yet another recommended attribute that captures user information and is useful to detect configuration and access issues. We’re not going to capture it since we have just one user.</li>
<li><code>db.name</code>: This is a required attribute defining the database name.</li>
<li><code>db.operation</code>: This is a required attribute that captures the name of the operation being executed, which should match the MongoDB command name.</li>
<li><code>db.mongodb.collection</code>: This is a required attribute that represents the MongoDB collection name.</li>
</ul>
<p>In addition to <a id="_idIndexMarker639"/>database-specific attributes, we’re going to populate MongoDB host information with <code>net.peer.name</code> and <code>net.peer.port</code> – generic network attributes.</p>
<p>Populating network-level attributes on logical calls is not always possible or useful. For example, when a MongoDB driver is configured with multiple hosts, we don’t necessarily know which one is used for a particular command. In practice, we should use auto-instrumentation that operates on the command level, subscribing to command events with <code>IEventSubscriber</code> (as described in the MongoDB documentation at <a href="http://mongodb.github.io/mongo-csharp-driver/2.11/reference/driver_core/events">http://mongodb.github.io/mongo-csharp-driver/2.11/reference/driver_core/events</a>).</p>
<p>In addition to attributes, semantic conventions require the use of the client kind on spans and providing a low-cardinality span name that includes the operation and database name. We’re going to use the <code>{db.operation} {</code><code>db.name}.{db.mongodb.collection}</code> pattern.</p>
<p>Now that we <a id="_idIndexMarker640"/>know what information to include in spans, let’s go ahead and instrument a MongoDB operation.</p>
<h2 id="_idParaDest-198"><a id="_idTextAnchor197"/>Tracing implementation</h2>
<p>In our application, we <a id="_idIndexMarker641"/>store records in a MongoDB collection and handle all communication with the collection in a custom <code>DatabaseService</code> class.</p>
<p>Let’s start by instrumenting an operation that reads a single record from a collection:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">DatabaseService.cs</p>
<pre class="source-code">
<strong class="bold">using var act = StartMongoActivity(GetOperation);</strong>
try {
  var rec = await _records.Find(r =&gt; r.Id == id)
    .SingleOrDefaultAsync();
  ...
  return rec;
} catch (Exception ex) {
<strong class="bold">  act?.SetStatus(ActivityStatusCode.Error,</strong>
<strong class="bold">    ex.GetType().Name);</strong>
  ...
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter12/database/DatabaseService.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter12/database/DatabaseService.cs</a></p>
<p>Here, we trace the <code>Find</code> method call. We use the <code>GetOperation</code> constant as the operation name, which is set to <code>FindSingleOrDefault</code> – a synthetic name describing what we do here. If the MongoDB command throws an exception, we set the activity status to <code>error</code>.</p>
<p>Let’s look in the <code>StartMongoActivity</code> method implementation:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">DatabaseService.cs</p>
<pre class="source-code">
var act = MongoSource.StartActivity(
    $"{operation} {_dbName}.{_collectionName}",
    ActivityKind.Client);
  if (act?.IsAllDataRequested != true) return act;
  return act.SetTag("db.system", "mongodb")
    .SetTag("db.name", _dbName)
    .SetTag("db.mongodb.collection", _collectionName)
    .SetTag("db.operation", operation)
    .SetTag("net.peer.name", _host)
    .SetTag("net.peer.port", _port);</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter12/database/DatabaseService.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter12/database/DatabaseService.cs</a></p>
<p>Here, we populate<a id="_idIndexMarker642"/> the activity name, kind, and attributes from the semantic conventions mentioned previously. The host, port, database name, and collection name are populated from the MongoDB settings provided via configuration and captured at construction time.</p>
<p>A similar approach could be used for any other operation. For bulk operations, we may consider adding more context to describe individual requests in the array attribute, as shown in this code snippet:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">DatabaseService.cs</p>
<pre class="source-code">
private static void AddBulkAttributes&lt;T&gt;(
  IEnumerable&lt;WriteModel&lt;T&gt;&gt; requests, Activity? act)
{
  if (act?.IsAllDataRequested == true)
  {
    act.SetTag("db.mongodb.bulk_operations",
      requests.Select(r =&gt; r.ModelType).ToArray());
  }
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter12/database/DatabaseService.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter12/database/DatabaseService.cs</a></p>
<p>This instrumentation is very generic – it does not record anything application-specific even though it knows the type of the record. For example, we could add a record identifier as an attribute or set the status to <code>error</code> if no records were found. These are all valid things to do if you’re going to stick with specialized manual instrumentation, but it’s more common to use a shared one when possible.</p>
<p>So, how do we record application-specific context along with generic database instrumentation? One solution would be to enrich auto-collected activities as we did in <a href="B19423_05.xhtml#_idTextAnchor083"><em class="italic">Chapter 5</em></a>, <em class="italic">Configuration and </em><em class="italic">Control Plane</em>.</p>
<p>Another<a id="_idIndexMarker643"/> solution is to add another layer of logical activities around database and cache calls. Before we do this, let’s learn how to trace cache calls.</p>
<h1 id="_idParaDest-199"><a id="_idTextAnchor198"/>Tracing cache calls</h1>
<p>Caches such as Redis and <a id="_idIndexMarker644"/>Memcached are a special class of databases and are covered by database semantic conventions too. Instrumenting cache calls according to conventions is beneficial as it helps you to stay consistent across all services and to get the most out of your tracing backends in terms of visualization and analysis.</p>
<p>So, let’s instrument Redis according to database conventions and add cache-specific context. There is nothing specifically defined in OpenTelemetry for caches, so let’s design something of our own.</p>
<p class="callout-heading">Note</p>
<p class="callout">Auto-instrumentation for the <code>StackExchange.Redis</code> client is available in the <code>OpenTelemetry.Instrumentation.StackExchangeRedis</code> NuGet package.</p>
<p>When it comes to tracing, we want to know typical things: how long a call took, whether there was an error, and what operation was attempted. Cache-specific things include an indication whether an item was retrieved from the cache or the expiration strategy (if it’s conditional) for set operations.</p>
<p>Let’s go ahead and instrument a <code>Get</code> call – it looks pretty similar to the database instrumentation<a id="_idIndexMarker645"/> we saw in the previous section:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">CacheService.cs</p>
<pre class="source-code">
<strong class="bold">using var act = StartCacheActivity(GetOperationName);</strong>
try
{
  var record = await _cache.GetStringAsync(id);
<strong class="bold">  act?.SetTag("cache.hit", record != null);</strong>
  ...
}
catch (Exception ex)
{
  act?.SetStatus(ActivityStatusCode.Error,
    ex.GetType().Name);
  ...
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter12/database/CacheService.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter12/database/CacheService.cs</a></p>
<p>Here, we created an activity to trace a <code>GetString</code> call to Redis. If a record is found, we set the <code>cache.hit</code> attribute to <code>true</code>, and if an exception happens, we set the activity status to <code>error</code> and include an exception message.</p>
<p>Let’s take a look at the attributes that are set in the <code>StartCacheActivity</code> method:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">CacheService.cs</p>
<pre class="source-code">
var act = RedisSource.StartActivity(operation,
  ActivityKind.Client);
if (act?.IsAllDataRequested != true) return act;
return act.SetTag("db.operation", operation)
    .SetTag("db.system", "redis")
    .SetTagIfNotNull("db.redis.database_index", _dbIndex)
    .SetTagIfNotNull("net.peer.name", _host)
    .SetTagIfNotNull("net.peer.port", _port)
    .SetTagIfNotNull("net.sock.peer.addr", _address)
    .SetTagIfNotNull("net.sock.family", _networkFamily);</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter12/database/CacheService.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter12/database/CacheService.cs</a></p>
<p>In this snippet, we<a id="_idIndexMarker646"/> start a client activity with the name matching the operation name. We also set all the applicable database and network attributes and add a Redis-specific attribute defined by OpenTelemetry – <code>db.redis.database_index</code>. Network attributes, which describe the host, port, IP address, and network family, are populated from Redis configuration options. The <code>SetTagIfNotNull</code> method is an extension method defined in our project.</p>
<p>Here, we have the same problem as with MongoDB – Redis configuration options may include multiple servers and we don’t know which one is going to be used for a specific call. The instrumentation in the <code>OpenTelemetry.Instrumentation.StackExchangeRedis</code> package (we took a quick look at it in <a href="B19423_03.xhtml#_idTextAnchor052"><em class="italic">Chapter 3</em></a>, <em class="italic">The .NET Observability Ecosystem</em>) provides more precise information.</p>
<p>This instrumentation is very generic for the same reasons as for MongoDB – in most cases, we’d rather enrich auto-instrumentation or add another layer of application-specific spans than write a custom instrumentation. So, let’s see how we can add the context by adding <a id="_idIndexMarker647"/>another layer of instrumentation.</p>
<h2 id="_idParaDest-200"><a id="_idTextAnchor199"/>Instrumenting composite calls</h2>
<p>With MongoDB and <a id="_idIndexMarker648"/>Redis calls instrumented <a id="_idIndexMarker649"/>independently and in a generic way, it could be hard to answer questions such as “How long did it take to retrieve a record with a specific ID?” or “How long did retrieval take?” given it involved a call to the cache, a call to the database, and then another call to the cache.</p>
<p>We did not add a record identifier attribute to query on and we only know the duration of individual calls  that don’t really describe the overall operation.</p>
<p>In the following example, we’re adding an extra layer of instrumentation that traces logical operations with a record identifier:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">RecordsController.cs</p>
<pre class="source-code">
<strong class="bold">using var act = Source.StartActivity("GetRecord");</strong>
<strong class="bold">act?.SetTag("app.record.id", id);</strong>
try
{
  var recordStr = await _cache.GetRecord(id);
  if (recordStr != null) return recordStr;
<strong class="bold">  act?.SetTag("cache.hit", false);</strong>
  var record = await _database.Get(id);
  if (record != null) return await Cache(record);
}
catch (Exception ex)
{
<strong class="bold">  act?.SetStatus(ActivityStatusCode.Error,</strong>
<strong class="bold">    ex.GetType().Name);</strong>
  throw;
}
<strong class="bold">act?.SetStatus(ActivityStatusCode.Error, "not found");</strong></pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter12/database/Controllers/RecordsController.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter12/database/Controllers/RecordsController.cs</a></p>
<p>Here, we wrap the sequence of calls in the <code>GetRecord</code> activity – it has an <code>internal</code> kind and just two attributes: <code>app.record.id</code> (which captures the record identifier) and <code>cache.hit</code> (describing whether the record was retrieved from the database).</p>
<p>We also provide a <code>not found</code> status description when nothing is found and can report other known issues in the same way.</p>
<p>In the case of our demo application, the encompassing database and cache spans almost match the ASP.NET Core ones in terms of status and duration, but in practice, controller methods do many other things. The encompassing operation helps us separate all<a id="_idIndexMarker650"/> spans and logs related to record <a id="_idIndexMarker651"/>retrieval.</p>
<p>Now that we have an idea of how to approach tracing, let’s explore metrics.</p>
<h1 id="_idParaDest-201"><a id="_idTextAnchor200"/>Adding metrics</h1>
<p>With databases, it’s <a id="_idIndexMarker652"/>common to monitor connections and query execution count and duration, contention, and resource utilization in addition to technology-specific things. The MongoDB cluster reports a set of such metrics that you can receive with OpenTelemetry Collector (check it out at https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/mongodbreceiver). These metrics provide the server side of the story. We should also add client-side duration metrics. It’d help us account for connectivity issues and network latency.</p>
<p>OpenTelemetry semantic conventions only document connection metrics for now. We could record them by implementing an <code>IEventSubscriber</code> interface and listening to connection events.</p>
<p>Instead, we’re going to record the basic operation duration, which also allows us to derive the throughput and failure rate and slice and dice by operation, database, or collection name.</p>
<p>Let’s get back to the <code>Get</code> operation code and see how the metric can be added. First, we’ll create a duration <a id="_idIndexMarker653"/>histogram:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">DatabaseService.cs</p>
<pre class="source-code">
private static readonly Meter MongoMeter = new("MongoDb");
private readonly Histogram&lt;double&gt; _operationDuration;
…
public DatabaseService(IOptions&lt;MongoDbSettings&gt; settings) {
  ...
  _operationDuration = MongoMeter.CreateHistogram&lt;double&gt;(
    "db.operation.duration", "ms",
    "Database call duration");
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter12/database/DatabaseService.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter12/database/DatabaseService.cs</a></p>
<p>Now that we have a histogram, we can record the duration for each operation:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">DatabaseService.cs</p>
<pre class="source-code">
var start = _operationDuration.Enabled ?
    Stopwatch.StartNew() : null;
using var act = StartMongoActivity(GetOperation);
try
{
  var rec = await _records.Find(r =&gt; r.Id == id)
    .SingleOrDefaultAsync();
  <strong class="bold">TrackDuration(start, GetOperation);</strong>
  return rec;
}
catch (Exception ex)
{
  ...
  <strong class="bold">TrackDuration(start, GetOperation, ex);</strong>
  throw;
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter12/database/DatabaseService.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter12/database/DatabaseService.cs</a></p>
<p>Here, we call into<a id="_idIndexMarker654"/> the <code>TrackDuration</code> method and pass a stopwatch that tracks the duration, the low-cardinality operation name, and an exception (if any). Here’s the <code>TrackDuration</code> method:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">DatabaseStatus.cs</p>
<pre class="source-code">
private void TrackDuration(Stopwatch? start,
  string operation, Exception? ex = null)
{
  if (start == null) return;
  string status = ex?.GetType()?.Name ?? "ok";
<strong class="bold">  _operationDuration.Record(start.ElapsedMilliseconds,</strong>
<strong class="bold">    new TagList() {</strong>
<strong class="bold">      { "db.name", _dbName },</strong>
<strong class="bold">      { "db.mongodb.collection", _collectionName },</strong>
<strong class="bold">      { "db.system", "mongodb"},</strong>
<strong class="bold">      { "db.operation", operation },</strong>
<strong class="bold">      { "db.mongodb.status", status },</strong>
<strong class="bold">      { "net.peer.name", _host },</strong>
<strong class="bold">      { "net.peer.port", _port }});</strong>
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter12/database/DatabaseService.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter12/database/DatabaseService.cs</a></p>
<p>Here, we add all the attributes we used for tracing and a new one – <code>db.mongodb.status</code>. We use the exception type as a status to make sure that metric cardinality stays low.</p>
<p>While the idea of using the exception type looks compelling and easy, it only works when we use the same MongoDB driver in the same language across the system. Even then, statuses might change over time with driver updates. In a real production scenario, I would recommend mapping known exceptions to language-agnostic status codes. It also makes sense to test corresponding cases and check that proper error codes are captured. It’s important if your alerts are based on specific codes.</p>
<p>The duration histogram and the metrics we can derive from it at query time cover common monitoring needs (throughput, latency, and error rate). We could also use it to do capacity analysis and make better design decisions. For example, before adding a cache in front of the database, we could check the read-to-write ratio to see whether caching would be helpful.</p>
<p>With custom<a id="_idIndexMarker655"/> queries over traces, we could also estimate how frequently the same records are accessed. This would help us pick a suitable expiration strategy.</p>
<h2 id="_idParaDest-202"><a id="_idTextAnchor201"/>Recording Redis metrics</h2>
<p>In addition to <a id="_idIndexMarker656"/>common database concerns, we want to measure cache-specific things: the hit-to-miss ratio, key expiration, and the eviction rate. This helps optimize and scale the cache.</p>
<p>These metrics are reported by Redis and can be captured with the Redis receiver for OpenTelemetry Collector, available at https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/redisreceiver.</p>
<p>We can enable them with the following configuration:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">configs/otel-collector-config.yml</p>
<pre class="source-code">
receivers:
...
<strong class="bold">  redis:</strong>
<strong class="bold">    endpoint: "redis:6379"</strong>
<strong class="bold">    collection_interval: 5s</strong>
...
service:
  pipelines:
    ...
    metrics:
      receivers: [otlp, <strong class="bold">redis</strong>]
...</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter12/configs/otel-collector-config.yml">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter12/configs/otel-collector-config.yml</a></p>
<p>OpenTelemetry<a id="_idIndexMarker657"/> Collector connects to a Redis instance and scrapes available metrics from it. Redis exposes multiple metrics, including uptime and resource utilization metrics and, most importantly, counters measuring command rate, hits, misses, expirations, evictions, and average time to live. With these, we can monitor Redis’ health and see whether it’s used efficiently and where the bottlenecks are.</p>
<p>For example, a low hit-to-miss ratio could indicate that we’re not utilizing the cache well and potentially could tune caching parameters to make it more efficient. First, we should make sure caching makes sense – usually, it does when at least some items are read more frequently than they are modified. We also need the interval between reads to be relatively low.</p>
<p>If, based on the collected data, we decided to add a cache, we can optimize its configuration further by looking into other cache metrics:</p>
<ul>
<li>A high key eviction rate can tell us if we don’t have enough memory and keys are evicted before items are read. We might want to scale Redis vertically or horizontally or change the eviction policy to better match the usage pattern. For example, if we have a relatively low number of periodically accessed items, a <strong class="bold">least frequently used</strong> (<strong class="bold">LFU</strong>) policy <a id="_idIndexMarker658"/>could be more efficient <a id="_idIndexMarker659"/>than the <strong class="bold">least recently used</strong> (<strong class="bold">LRU</strong>) one.</li>
<li>If we see a low eviction but high expiration rate, it could mean that the expiration time is too low – items are read less frequently than we expected. We can try to gradually increase the expiration time or disable it and rely on eviction policy instead.</li>
</ul>
<p>In addition to server-side metrics, we’ll also add a client-side duration histogram. It allows us to record <a id="_idIndexMarker660"/>call duration distribution with command and other database-specific dimensions. The implementation is almost identical to the MongoDB duration metric. The only difference is that we’re going to add the <code>cache.hit</code> attribute to the metrics for the <code>GetString</code> operation. This could be helpful when server-side metrics are not available or there are multiple different operations we want to measure a hit ratio for independently of each other.</p>
<p>Now that we have all the database traces and metrics in place, let’s bring all the pieces together and see how we use this telemetry in practice.</p>
<h1 id="_idParaDest-203"><a id="_idTextAnchor202"/>Analyzing performance</h1>
<p>Let’s first run the<a id="_idIndexMarker661"/> demo application using the <code>$ docker-compose up --build</code> command. It will start local MongoDB and Redis instances along the application and observability stack.</p>
<p>You can create some records with a tool such as <code>curl</code>:</p>
<pre class="console">
$ curl -X POST http://localhost:5051/records \
  -H "Content-Type: application/json" \
  -d '[{"name":"foo"},{"name":"bar"},{"name":"baz"}]'</pre>
<p>It should return a list of record identifiers the service created.</p>
<p>Now, let’s look at the Jaeger trace at <code>http://localhost:16686</code>, like the one shown in <em class="italic">Figure 12</em><em class="italic">.1</em>:</p>
<div><div><img alt="Figure 12.1 – Trace showing bulk record creation" src="img/B19423_12_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.1 – Trace showing bulk record creation</p>
<p>We see a controller span (<code>Records</code>) and then <code>CreateRecords</code>, which describes a database-and-cache-encompassing operation. It’s a parent of the <code>BulkWrite</code> span, which describes a MongoDB call and three individual Redis spans – one for each record.</p>
<p>Note that the controller and the <code>CreateRecords</code> spans end before caching is complete, because <a id="_idIndexMarker662"/>we don’t wait for it. Anything that happens within the <code>SetString</code> operation would still be properly correlated despite the parent request being complete.</p>
<p>If we were to wait about 10 seconds and try to get one of the records (by calling <code>http://localhost:5051/records/{id}</code>), we’d see a trace like the one shown in <em class="italic">Figure 12</em><em class="italic">.2</em>:</p>
<div><div><img alt="Figure 12.2 – Trace showing record retrieval from the database" src="img/B19423_12_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.2 – Trace showing record retrieval from the database</p>
<p>If we get the same record within 10 seconds, we’ll see it’s returned from the cache, as shown in <em class="italic">Figure 12</em><em class="italic">.3</em>:</p>
<div><div><img alt="Figure 12.3 – Trace showing record retrieval from﻿ the cache" src="img/B19423_12_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.3 – Trace showing record retrieval from the cache</p>
<p>By looking at individual traces, we can now quickly see whether records were retrieved from the cache or the database. We can also find all operations that happened across all traces for a specific record using the <code>app.record.id</code> attribute or write ad hoc queries using the <code>cache.hit</code> flag.</p>
<p>Let’s now simulate a failure by stopping the Redis container with <code>$ docker </code><code>stop chapter12-redis-1</code>.</p>
<p>If we try to get one of the records again, the application will return the <code>500 – Internal Server Error</code> response. The trace predictably shows that the call to Redis failed with <code>RedisConnectionException</code>. We might want to change this behavior, and if<a id="_idIndexMarker663"/> the Redis call fails, retrieve the record from the database.</p>
<p>If we did this, we’d see a trace similar to the one shown in <em class="italic">Figure 12</em><em class="italic">.4</em>:</p>
<div><div><img alt="Figure 12.4 – Trace showing Redis call failures with fallback to database" src="img/B19423_12_04.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.4 – Trace showing Redis call failures with fallback to database</p>
<p>Here, calls to Redis failed, but the overall operation succeeded. You can reproduce it if you comment out the <code>throw</code> statement on line 63 in <code>CacheService.cs</code> and then rerun the application with <code>$ docker-compose </code><code>up --build</code>.</p>
<p>Let’s check what happens with metrics in this case. We can start by applying some load with <code>loadgenerator$ dotnet run -c Release --rate 50</code>. Give it a few minutes to stabilize and let’s check our application’s performance.</p>
<p>Let’s first check out the service throughput with the following query in Prometheus (at <code>http://localhost:9090</code>):</p>
<pre class="console">
sum by (http_route, http_status_code)
  (rate(http_server_duration_milliseconds_count[1m])
)</pre>
<p>As we’ll see in <em class="italic">Figure 12</em><em class="italic">.6</em>, throughput stabilizes at around 40-50 requests per second – that’s what we configured in the <code>rate</code> parameter.</p>
<p>Then, we can check the 50th percentile for latency with the following query:</p>
<pre class="console">
histogram_quantile(0.50,
  sum (rate(http_server_duration_milliseconds_bucket[1m]))
  by (le, http_route, http_method))</pre>
<p>Later, in <em class="italic">Figure 12</em><em class="italic">.7</em>, we’ll see that responses are blazing fast – the 50th percentile for latency is just <a id="_idIndexMarker664"/>a few milliseconds.</p>
<p class="callout-heading">Spoiler</p>
<p class="callout">If we checked the 95th percentile for latency, we’d notice it is much bigger, reaching 200-300 milliseconds. MongoDB shows these spikes in latency because container resources are constrained for demo purposes.</p>
<p>Let’s now check the cache hit rate. We can derive it from Redis server metrics or a client operation duration histogram. The following query uses the latter approach:</p>
<pre class="source-code">
100 *
sum by (net_peer_name) (
  rate(db_operation_duration_milliseconds_count{cache_hit="true",
          db_operation="GetString",
          db_system="redis"}[1m]))
/
sum by (net_peer_name) (
  rate(
      db_operation_duration_milliseconds_count{db_
        operation="GetString",
      db_redis_status="ok",
      db_system="redis"}[1m]))</pre>
<p>The query gets the rate of the <code>GetString</code> operation on Redis with the <code>cache.hit</code> attribute set to <code>true</code> and divides it by the overall <code>GetString</code> operation success rate. It also multiplies <a id="_idIndexMarker665"/>the ratio by 100 to calculate the hit percentage, which is around 80%, as we can see in <em class="italic">Figure 12</em><em class="italic">.5</em>:</p>
<div><div><img alt="Figure 12.5 – Redis hit rate for the GetString method" src="img/B19423_12_05.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.5 – Redis hit rate for the GetString method</p>
<p>So, the cache is used and it handles 80% of read requests. Let’s see what happens if we stop it with the <code>$ docker stop </code><code>chapter12-redis-1</code> command.</p>
<p class="callout-heading">Tip</p>
<p class="callout">With this exercise, you may find it interesting to explore the effect of recording exceptions from Redis. Once the Redis container is stopped, every call to Redis will result in an exception being recorded. In the case of our tiny application, it alone increases the telemetry volume tenfold. Check it out yourself with the following Prometheus query:</p>
<pre class="console">
sum by (container_image_name)
  (rate(container_network_io_usage_rx_bytes_total[1m]))</pre>
<p>Immediately after the Redis container is stopped (at around 14:48), the application throughput starts to decrease to less than one record per second, as shown in <em class="italic">Figure 12</em><em class="italic">.6</em>:</p>
<div><div><img alt="Figure 12.6 – Application throughput before and after the Redis container is stopped" src="img/B19423_12_06.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.6 – Application throughput before and after the Redis container is stopped</p>
<p>HTTP latency (the 50th percentile) increases<a id="_idIndexMarker666"/> from a few milliseconds to several seconds, as you can see in <em class="italic">Figure 12</em><em class="italic">.7</em>:</p>
<div><div><img alt="Figure 12.7 – Application latency 50th percentile before and after Redis container is stopped" src="img/B19423_12_07.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.7 – Application latency 50th percentile before and after Redis container is stopped</p>
<p>The spikes in HTTP<a id="_idIndexMarker667"/> latency are consistent with the MongoDB latency increase shown in <em class="italic">Figure 12</em><em class="italic">.8</em>:</p>
<div><div><img alt="Figure 12.8 – MongoDB latency (p50) in milliseconds" src="img/B19423_12_08.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.8 – MongoDB latency (p50) in milliseconds</p>
<p>Finally, we should check what happened with MongoDB throughput: since Redis no longer handles 80% of read requests, the load on the database increases and, initially, it tries to catch up, as you can see in <em class="italic">Figure 12</em><em class="italic">.9</em>:</p>
<div><div><img alt="Figure 12.9 – MongoDB throughput before and after the container is stopped" src="img/B19423_12_09.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.9 – MongoDB throughput before and after the container is stopped</p>
<p>The resources on a MongoDB container are significantly constrained and it can’t handle such a load.</p>
<p>If we check the traces, we’ll see the MongoDB call takes significantly longer and is the root cause of slow application responses and low throughput. An example of such a trace is shown in <em class="italic">Figure 12</em><em class="italic">.10</em>:</p>
<div><div><img alt="Figure 12.10 – Trace showing ﻿a long MongoDB request when Redis is stopped" src="img/B19423_12_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.10 – Trace showing a long MongoDB request when Redis is stopped</p>
<p>If you now start Redis with the <code>$ docker start chapter12-redis-1</code> command, the<a id="_idIndexMarker668"/> throughput and latency will be restored to the original values within a few minutes.</p>
<p>We did this analysis knowing the root cause, but it also works as a general approach – when service-level indicators such as latency and throughput change drastically, we should check the state and health of service dependencies. The findings here are that we need to protect the database better, for example, by adding a few more (potentially smaller) Redis instances that would handle the load if one of them goes down. We may also consider rate-limiting calls to the database on the service side, so it stays responsive, even with lower throughput.</p>
<h1 id="_idParaDest-204"><a id="_idTextAnchor203"/>Summary</h1>
<p>In this chapter, we explored database instrumentation. We started by looking into OpenTelemetry semantic conventions for databases and implemented tracing for MongoDB. Then, we added similar instrumentation for Redis and encompassing calls. We saw how to provide application-specific context on encompassing spans and record whether data was retrieved from the cache or database to improve performance analysis across traces.</p>
<p>Then, we added metrics, including client duration histograms for MongoDB and Redis along with server-side metrics for Redis that help analyze and optimize cache usage, starting with the hit ratio, which we were able to measure.</p>
<p>Finally, we simulated a Redis outage and saw how collecting telemetry makes it easy to detect and analyze what went wrong and how the outage progressed. We also found several issues in our application that make it unreliable.</p>
<p>Now you’re ready to start instrumenting database calls in your application or enrich auto-collected telemetry with additional traces and metrics.</p>
<p>This concludes our journey through instrumentation recipes. In the next chapter, we’ll talk about organizational aspects of adopting and evolving tracing and observability.</p>
<h1 id="_idParaDest-205"><a id="_idTextAnchor204"/>Questions</h1>
<ol>
<li>How would you approach instrumenting a database change feed (the event stream exposed by the database that notifies about changes to database records)? For example, an application can subscribe to a notification that the cloud provider will send when a blob is created, updated, or removed from cloud storage (which we can consider to be a database).</li>
<li>Would it make sense to record calls to Redis as events/logs instead of spans?</li>
<li>Try removing resource limitations on the MongoDB container and check what happens if we kill Redis now.</li>
</ol>
</div>


<div><h1 id="_idParaDest-206"><a id="_idTextAnchor205"/>Part 4: Implementing Distributed Tracing in Your Organization</h1>
</div>
<div><p>This part walks through the sociotechnical aspects of observability adoption – making an initial push and improving it further, developing telemetry standards within your company, and instrumenting new parts of a system in the presence of legacy services.</p>
<p>This part has the following chapters:</p>
<ul>
<li><a href="B19423_13.xhtml#_idTextAnchor206"><em class="italic">Chapter 13</em></a>, <em class="italic">Driving Change</em></li>
<li><a href="B19423_14.xhtml#_idTextAnchor220"><em class="italic">Chapter 14</em></a>, <em class="italic">Creating Your Own Conventions</em></li>
<li><a href="B19423_15.xhtml#_idTextAnchor233"><em class="italic">Chapter 15</em></a>, <em class="italic">Instrumenting Brownfield Applications</em></li>
</ul>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
</body></html>