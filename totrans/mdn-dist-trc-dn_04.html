<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-69"><a id="_idTextAnchor068"/>4</h1>
<h1 id="_idParaDest-70"><a id="_idTextAnchor069"/>Low-Level Performance Analysis with Diagnostic Tools</h1>
<p>While distributed tracing works great for microservices, it’s less useful for deep performance analysis within a process. In this chapter, we’ll explore .NET diagnostics tools that allow us to detect and debug performance issues and profile inefficient code. We’ll also learn how to perform ad hoc performance analysis and capture necessary information automatically in production.</p>
<p>In this chapter, you will learn how to do the following:</p>
<ul>
<li>Use .NET runtime counters to identify common performance problems</li>
<li>Use performance tracing to optimize inefficient code</li>
<li>Collect diagnostics in production</li>
</ul>
<p>By the end of this chapter, you will be able to debug memory leaks, identify thread pool starvation, and collect and analyze detailed performance traces with .NET diagnostics tools.</p>
<h1 id="_idParaDest-71"><a id="_idTextAnchor070"/>Technical requirements</h1>
<p>The code for this chapter is available in this book’s repository on GitHub at <a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4</a>. It consists of the following components:</p>
<ul>
<li>The <code>issues</code> application, which contains examples of performance issues</li>
<li><code>loadgenerator</code>, which is a tool that generates load to reproduce problems</li>
</ul>
<p>To run samples and perform analysis, we’ll need the following tools:</p>
<ul>
<li>.NET SDK 7.0 or later.</li>
<li>The .NET <code>dotnet-trace</code>, <code>dotnet-stack</code>, and <code>dotnet-dump</code> diagnostics tools. Please install each of them with <code>dotnet tool install –</code><code>global dotnet-&lt;tool&gt;</code>.</li>
<li>Docker and <code>docker-compose</code>.</li>
</ul>
<p>To run the samples in this chapter, start the observability stack, which consists of Jaeger, Prometheus, and the OpenTelemetry collector, with <code>docker-compose up</code>.</p>
<p>Make sure that you start the <code>dotnet run -c Release</code> from the <code>issues</code> folder. We don’t run it in Docker so that it’s easier to use diagnostics tools.</p>
<p>In the <code>OpenTelemetry.Instrumentation.Process</code> and <code>OpenTelemetry.Instrumentation.Runtime</code> NuGet packages and configured metrics for the HTTP client and ASP.NET Core. Here’s our metrics configuration:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">Program.cs</p>
<pre class="source-code">
builder.Services.AddOpenTelemetry()
  ...
  .WithMetrics(meterProviderBuilder =&gt;
      meterProviderBuilder
      .AddOtlpExporter()
<strong class="bold">          .AddProcessInstrumentation()</strong>
<strong class="bold">          .AddRuntimeInstrumentation()</strong>
          .AddHttpClientInstrumentation()
          .AddAspNetCoreInstrumentation());</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4/issues/Program.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4/issues/Program.cs</a></p>
<h1 id="_idParaDest-72"><a id="_idTextAnchor071"/>Investigating common performance problems</h1>
<p>Performance degradation<a id="_idIndexMarker221"/> is a symptom of some other issues such as race conditions, dependency<a id="_idIndexMarker222"/> slow-down, high load, or any other problem that causes your <strong class="bold">service-level indicators</strong> (<strong class="bold">SLIs</strong>) to go beyond healthy limits and miss <strong class="bold">service-level objectives</strong> (<strong class="bold">SLOs</strong>). Such issues may affect multiple, if not all, code paths and APIs, even<a id="_idIndexMarker223"/> if they’re initially limited to a specific scenario.</p>
<p>For example, when a downstream service experiences issues, it can cause throughput to drop significantly for all APIs, including those that don’t depend on that downstream service. Retries, additional connections, or threads that handle downstream calls consume more resources than usual and take them away from other requests.</p>
<p class="callout-heading">Note</p>
<p class="callout">Resource consumption alone, be it high or low, does not indicate a performance issue (or lack of it). High CPU or memory utilization can be valid if users are not affected. It could still be important to investigate when they are unusually high as it could be an early signal of a problem to come.</p>
<p>We can detect performance issues by monitoring SLIs and alerting them to violations. If you see that issues are widespread and not specific to certain scenarios, it makes sense to check the overall resource consumption for the process, such as CPU usage, memory, and thread counts, to find the bottleneck. Then, depending on the constrained resource, we may need to capture more information, such as dumps, thread stacks, detailed runtime, or library events. Let’s go through several examples of common issues and talk about their symptoms.</p>
<h2 id="_idParaDest-73"><a id="_idTextAnchor072"/>Memory leaks</h2>
<p>Memory leaks<a id="_idIndexMarker224"/> happen when an application<a id="_idIndexMarker225"/> consumes more and more memory over time. For example, if we cache objects in-memory without proper expiration and overflow logic, the application will consume more and more memory over time. Growing memory consumption triggers garbage collection, but the cache keeps references to all objects and GC cannot free them up.</p>
<p>Let’s reproduce a memory leak and go through the signals that would help us identify it and find the root cause. First, we need to run the <code>loadgenerator</code> tool:</p>
<pre class="console">
loadgenerator$ dotnet run -c Release memory-leak –-parallel 100 –-count 20000000</pre>
<p>It makes 20 million requests<a id="_idIndexMarker227"/> and then stops, but if we let it run for a long<a id="_idIndexMarker228"/> time, we’ll see throughput dropping, as shown in <em class="italic">Figure 4</em><em class="italic">.1</em>:</p>
<div><div><img alt="Figure 4.1 – Service throughput (successful requests per second)" src="img/B19423_04_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – Service throughput (successful requests per second)</p>
<p>We can see periods when throughput drops and the service stops processing requests – let’s investigate why.</p>
<p>.NET reports event counters that help us monitor the size of each GC generation. Newly allocated objects appear in <strong class="bold">generation 0</strong>; if they survive garbage collection, they get promoted to <strong class="bold">generation 1</strong>, and then to <strong class="bold">generation 2</strong>, where they stay until they’re collected or the process<a id="_idIndexMarker229"/> terminates. Large objects (that are 85 KB or bigger) appear on a <strong class="bold">large object </strong><strong class="bold">heap</strong> (<strong class="bold">LOH</strong>).</p>
<p>OpenTelemetry runtime instrumentations report generation sizes under the <code>process_runtime_dotnet_gc_heap_size_bytes</code> metric. It’s also useful to monitor the <code>process_memory_usage_bytes</code>. We can see generation 2 and physical memory consumption in <em class="italic">Figure 4</em><em class="italic">.2</em>:</p>
<div><div><img alt="Figure 4.2 – Memory consumption showing a memory leak in the application" src="img/B19423_04_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 – Memory consumption showing a memory leak in the application</p>
<p>We can see that generation 2 grows over time, along with the virtual memory. The physical memory used by the process<a id="_idIndexMarker230"/> goes up and down, which means<a id="_idIndexMarker231"/> that the OS started using disk in addition to RAM. This process is called <strong class="bold">paging</strong> or <strong class="bold">swapping</strong>, which is enabled (or disabled) at the OS level. When enabled, it may significantly affect performance since RAM is usually much faster than disk.</p>
<p>Eventually, the system will run out of physical memory and the pagefile will reach its size limit; then, the process will crash with an <code>OutOfMemoryException</code> error. This may happen earlier, depending on the environment and heap size configuration. For 32-bit processes, OOM happens when the virtual memory size reaches 4 GB as it runs out of address space. Memory limits can be configured or imposed by the application server (IIS), hosting providers, or container runtimes.</p>
<p>Kubernetes or Docker<a id="_idIndexMarker232"/> allows you to limit the virtual memory for a container. The behavior<a id="_idIndexMarker233"/> of different environments varies, but in general, the application is terminated with the <code>OutOfMemory</code> exit code after the limit is reached. It might take days, weeks, or even months for a memory leak to crash the process with <code>OutOfMemoryException</code>, so some memory leaks can stay dormant, potentially causing rare restarts and affecting only a long tail of latency distribution.</p>
<p>Memory leaks on the hot path can take the whole service down fast. When memory consumption grows quickly, garbage collection intensively tries to free up some memory, which uses the CPU and can pause managed threads.</p>
<p>We can monitor garbage collection for individual generations using .NET event counters and OpenTelemetry instrumentation, as shown in <em class="italic">Figure 4</em><em class="italic">.3</em>:</p>
<div><div><img alt="Figure 4.3 – Garbage collection rate per second for individual generations" src="img/B19423_04_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3 – Garbage collection rate per second for individual generations</p>
<p>As you can see, the generation 0 and generation 1 collections happened frequently. Looking<a id="_idIndexMarker234"/> at the consistent memory<a id="_idIndexMarker235"/> growth and the frequency of garbage collection, we can now be pretty sure we’re dealing with a memory leak. We could also collect GC events from the <code>Microsoft-Windows-DotNETRuntime</code> event provider (we’ll learn how to do this in the next section) to come to the same conclusion.</p>
<p>Let’s also check the CPU utilization (shown in <em class="italic">Figure 4</em><em class="italic">.4</em>) reported by the OpenTelemetry process instrumentation as the <code>process_cpu_time_seconds_total</code> metric, from which we can derive the utilization:</p>
<div><div><img alt="Figure 4.4 – CPU utilization during the memory leak" src="img/B19423_04_04.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.4 – CPU utilization during the memory leak</p>
<p>We can see that there are periods when both user CPU utilization and privileged (system) CPU utilization go up. These are the same periods when throughput dropped in <em class="italic">Figure 4</em><em class="italic">.1</em>. User CPU utilization is derived from the <code>System.Diagnostics.Process.UserProcessorTime</code> property, while system utilization (based on OpenTelemetry terminology) is derived from the <code>System.Diagnostics.Process.PriviledgedProcessorTime</code> property. These are the same periods when throughput dropped in <em class="italic">Figure 4</em><em class="italic">.1</em>.</p>
<p>Our investigation could have<a id="_idIndexMarker236"/> started with high latency, high error rate, a high number<a id="_idIndexMarker237"/> of process restarts, high CPU, or high memory utilization, and all of those are symptoms of the same problem – a memory leak. So, now, we need to investigate it further – let’s collect a memory dump to see what’s in there. Assuming you can reproduce the issue on a local machine, Visual Studio or JetBrains dotMemory can capture and analyze a memory dump. We will use <code>dotnet-dump</code>, which we can run on an instance<a id="_idIndexMarker238"/> experiencing problems. Check out the .NET documentation at <a href="https://learn.microsoft.com/dotnet/core/diagnostics/dotnet-dump">https://learn.microsoft.com/dotnet/core/diagnostics/dotnet-dump</a> to learn more about the tool.</p>
<p>So, let’s capture the dump using the following command:</p>
<pre class="console">
$ dotnet-dump collect -–name issues</pre>
<p>Once the dump has been collected, we can analyze it with Visual Studio, JetBrains dotMemory, or other tools that automate and simplify it. We’re going to do this the hard way with the <code>dotnet-dump</code> CLI tool:</p>
<pre class="console">
$ dotnet-dump analyze &lt;dump file name&gt;</pre>
<p>This will open<a id="_idIndexMarker239"/> a prompt where we can run <strong class="bold">SOS</strong> commands. SOS is a debugger extension that allows us to examine running processes and dumps. It can help us find out what’s on the heap.</p>
<p>We can do this with the <code>dumpheap -stat</code> command, which prints the count and total count and size of objects by their type, as shown in <em class="italic">Figure 4</em><em class="italic">.5</em>:</p>
<div><div><img alt="Figure 4.5 – Managed heap stats showing ~20 million MemoryLeakController instances" src="img/B19423_04_05.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.5 – Managed heap stats showing ~20 million MemoryLeakController instances</p>
<p>Stats are printed in ascending order, so the objects with the biggest total size appear at the end. Here, we can see that we have almost 20 million <code>MemoryLeakController</code> instances, which consume about 1.5 GB of memory. The controller instance is scoped to the request, and it seems<a id="_idIndexMarker240"/> it is not collected after the request ends. Let’s find <strong class="bold">the GC roots</strong> – objects that keep controller instances alive.</p>
<p>We need to find the address<a id="_idIndexMarker241"/> of any controller instance. We can do this using<a id="_idIndexMarker242"/> its method table – the first hex number in each table row. The method table stores type information for each object and is an internal CLR implementation detail.</p>
<p>We can find the object address for it using another SOS command:</p>
<pre class="console">
$ dumpheap -mt 00007ffe53f5d488</pre>
<p>This will print a table that contains the addresses of all <code>MemoryLeakController</code> instances. Let’s copy one of them so that we can find the GC root with it:</p>
<pre class="console">
$ gcroot -all &lt;controller-instance-address&gt;</pre>
<p><em class="italic">Figure 4</em><em class="italic">.6</em> shows the path from the GC root to the controller instance printed by the <code>gcroot</code> command:</p>
<div><div><img alt="Figure 4.6 – ProcessingQueue is keeping the controller instances alive" src="img/B19423_04_06.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.6 – ProcessingQueue is keeping the controller instances alive</p>
<p>We can see that <code>issues.ProcessingQueue</code> is holding this and other controller instances. It uses <code>ConcurrentQueue&lt;Action&gt;</code> inside. If we were to check the controller code, we’d see that we added an action that uses <code>_logger</code> – a controller instance variable that implicitly keeps controller instances alive:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">MemoryLeakController.cs</p>
<pre class="source-code">
<strong class="bold">_queue.Enqueue</strong>(() =&gt; <strong class="bold">_logger</strong>.LogInformation(
    "notification for {user}",
    new User("Foo", "leak@memory.net")));</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4/issues/Controllers/MemoryLeakController.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4/issues/Controllers/MemoryLeakController.cs</a></p>
<p>To fix this, we’d need to stop capturing<a id="_idIndexMarker243"/> the controller’s logger in action<a id="_idIndexMarker244"/> and add size limits and backpressure to the queue.</p>
<h2 id="_idParaDest-74"><a id="_idTextAnchor073"/>Thread pool starvation</h2>
<p>Thread pool starvation happens<a id="_idIndexMarker245"/> when CLR does not have enough threads<a id="_idIndexMarker246"/> in the pool to process work, which can happen at startup or when the load increases significantly. Let’s reproduce it and see how it manifests.</p>
<p>With the <strong class="bold">issues</strong> app running, add some load using the following<a id="_idIndexMarker247"/> commands to send 300 concurrent requests to the app:</p>
<pre class="console">
$ dotnet run -c Release starve ––parallel 300</pre>
<p>Now, let’s check what happens with the throughput and latency. You might not see any metrics or traces coming from the application or see stale metrics that were reported before the load started. If you try to hit any API on the issue application, such as http://localhost:5051/ok, it will time out.</p>
<p>If you check the CPU or memory for the <strong class="bold">issues</strong> process, you will see very low utilization – the process got stuck doing nothing. It lasts for a few minutes and then resolves – the service starts responding and reports metrics and traces as usual.</p>
<p>One way to understand what’s going on when a process does not report metrics and traces is to use the <code>dotnet-counters</code> tool. Check<a id="_idIndexMarker248"/> out the .NET documentation at <a href="https://learn.microsoft.com/dotnet/core/diagnostics/dotnet-counters">https://learn.microsoft.com/dotnet/core/diagnostics/dotnet-counters</a> to learn more about the tool. Now, let’s run it to see the runtime counters:</p>
<pre class="console">
$ dotnet-counters monitor --name issues</pre>
<p>It should print a table consisting of runtime counters that change over time, as shown in <em class="italic">Figure 4</em><em class="italic">.7</em>:</p>
<div><div><img alt="Figure 4.7 – The dotnet-counters output dynamically showing runtime counters" src="img/B19423_04_07.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.7 – The dotnet-counters output dynamically showing runtime counters</p>
<p>Here, we’re interested in thread pool<a id="_idIndexMarker249"/> counters. We can see 1,212 work items<a id="_idIndexMarker250"/> waiting in the thread pool queue length and that it keeps growing along with the thread count. Only a few (if any) work items are completed per second.</p>
<p>The root cause of this behavior is the following code in the controller, which blocks the thread pool threads:</p>
<pre class="source-code">
_httpClient.GetAsync("/dummy/?delay=100", token)<strong class="bold">.Wait()</strong>;</pre>
<p>So, instead of switching to another work item, the threads sit and wait for the dummy call to complete. It affects all tasks, including those that export telemetry data to the collector – they are waiting in the same queue.</p>
<p>The runtime increases the thread pool size gradually and eventually, it becomes high enough to clean up the work item queue. Check out <em class="italic">Figure 4</em><em class="italic">.8</em> to see thread pool counter dynamics:</p>
<div><div><img alt="Figure 4.8 – The thread pool threads count and queue changes before and after starvation" src="img/B19423_04_08.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.8 – The thread pool threads count and queue changes before and after starvation</p>
<p>As you can see, we have no data for the time when starvation happened. But after the thread pool queue is cleared, we start getting the data and see that the runtime adjusts the number of threads to a lower value.</p>
<p>We just saw how problems<a id="_idIndexMarker251"/> on a certain code path can affect the performance<a id="_idIndexMarker252"/> of the whole process and how we can use runtime metrics and diagnostics tools to narrow them down. Now, let’s learn how to investigate performance issues specific to certain APIs or individual requests.</p>
<h1 id="_idParaDest-75"><a id="_idTextAnchor074"/>Profiling</h1>
<p>If we analyze individual traces<a id="_idIndexMarker253"/> corresponding to thread pool starvation or memory leaks, we will not see anything special. They are fast under a small load and get slower or fail when the load increases.</p>
<p>However, some performance issues only affect certain scenarios, at least under typical load. Locks and inefficient code are examples of such operations.</p>
<p>We rarely instrument local operations with distributed tracing under the assumption that local calls are fast and exceptions have enough information for us to investigate failures.</p>
<p>But what happens when we have compute-heavy or just inefficient code in the service? If we look at distributed traces, we’ll see high latency and gaps between spans, but we wouldn’t know why it happens.</p>
<p>We know ahead of time that some operations, such as complex algorithms or I/O, can take a long time to complete or fail, so we can deliberately instrument them with tracing or just write a log record. But we rarely introduce inefficient code to the hot path intentionally; due to this, our ability to debug it with distributed tracing, metrics, or logs is limited.</p>
<p>In such cases, we need<a id="_idIndexMarker254"/> more precise signals, such as profiling. <strong class="bold">Profiling</strong> involves collecting call stacks, memory allocations, timings, and the frequency of calls. This can be done in-process using .NET profiling APIs that need the application to be configured in a certain way. Low-level performance<a id="_idIndexMarker255"/> profiling is usually done locally on a developer machine, but it used to be a popular mechanism among <strong class="bold">Application Performance Monitoring</strong> (<strong class="bold">APM</strong>) tools to collect performance data and traces.</p>
<p>In this chapter, we’re going to use a different kind of profiling, also called performance tracing, which<a id="_idIndexMarker256"/> relies on <code>System.Diagnostics.Tracing.EventSource</code>, and can be done ad hoc. <code>EventSource</code> is essentially a platform logger – CLR, libraries, and frameworks write their diagnostics to event sources, which are disabled by default, but it’s possible to enable and control them dynamically.</p>
<p>The .NET runtime and libraries events cover GC, tasks, the thread pool, the DNS, sockets, and HTTP, among other things. ASP.NET Core, Kestrel, Dependency Injection, Logging, and other libraries have their own event providers too.</p>
<p>You can listen to any provider inside the process using <code>EventListener</code> and access events<a id="_idIndexMarker257"/> and their payloads, but the true power of <code>EventSource</code> is that you can control providers from out-of-process over <code>dotnet-monitor</code> tool in <a href="B19423_02.xhtml#_idTextAnchor038"><em class="italic">Chapter 2</em></a>, <em class="italic">Native Monitoring </em><em class="italic">in .NET</em>.</p>
<p>Let’s see how performance tracing and profiling with <code>EventSource</code> can help us investigate specific issues.</p>
<h2 id="_idParaDest-76"><a id="_idTextAnchor075"/>Inefficient code</h2>
<p>Let’s run our demo application<a id="_idIndexMarker258"/> and see how inefficient code can manifest<a id="_idIndexMarker259"/> itself. Make sure the observability stack is running, then start the <strong class="bold">issues</strong> application, and then apply some load:</p>
<pre class="console">
$ dotnet run -c Release spin –-parallel 100</pre>
<p>The load generator bombards the http://localhost:5051/spin?fib=&lt;n&gt; endpoint with 100 concurrent requests. The spin endpoint calculates an <em class="italic">n</em>th Fibonacci number; as you’ll see, our Fibonacci implementation is quite inefficient.</p>
<p>Assuming we don’t know how bad<a id="_idIndexMarker260"/> this Fibonacci implementation is, let’s try<a id="_idIndexMarker261"/> to investigate why this request takes so long. Let’s open Jaeger by going to http://localhost:16686, clicking on <strong class="bold">Find traces</strong>, and checking out the latency distribution, as shown in <em class="italic">Figure 4</em><em class="italic">.9</em>:</p>
<div><div><img alt="Figure 4.9 – Latency distribution in Jaeger" src="img/B19423_04_09.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.9 – Latency distribution in Jaeger</p>
<p>We can see that almost all requests take more than 2 seconds to complete. If you click on any of the dots, Jaeger will show the corresponding trace. It should look similar to the one shown in <em class="italic">Figure 4</em><em class="italic">.10</em>:</p>
<div><div><img alt="Figure 4.10 – Long trace in Jaeger" src="img/B19423_04_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.10 – Long trace in Jaeger</p>
<p>The load application is instrumented so that we can measure client latency too. We can see that the client request took 4.5 seconds, while the server-side request took about 1.5 seconds. In a spin request, we call the dummy controller of the same application and can see corresponding client and server spans. The only thing that stands out here is that there are plenty of gaps and we don’t know what happened there.</p>
<p>If we check out the metrics, we will see high CPU and high server latency, but nothing suspicious that can help us find the root cause. So, it’s time to capture some performance traces.</p>
<p>Multiple tools can capture performance<a id="_idIndexMarker262"/> traces for the process that experiences<a id="_idIndexMarker263"/> this issue, such as PerfView on Windows, or PerfCollect on Linux.</p>
<p>We’re going to use the cross-platform <code>dotnet-trace</code> CLI tool, which you can install and use anywhere. Go ahead and run it using the following command for 10-20 seconds:</p>
<pre class="console">
$ dotnet-trace collect --name issues</pre>
<p>With this command, we’ve enabled the <code>Microsoft-DotNETCore-SampleProfiler</code> event source (among other default providers) to capture managed thread call stacks for the <code>dotnet-trace</code> tool by reading the .NET documentation at <a href="https://learn.microsoft.com/dotnet/core/diagnostics/dotnet-trace">https://learn.microsoft.com/dotnet/core/diagnostics/dotnet-trace</a>. We could also configure it to collect events from any other event source.</p>
<p>The tool saves traces to the <code>issues.exe_*.nettrace</code> file, which we can analyze with it as well:</p>
<pre class="console">
$ dotnet-trace report issues.exe_*.nettrace topN</pre>
<p>It outputs the top (5 by default) methods that have been on the stack most of the time. <em class="italic">Figure 4</em><em class="italic">.11</em> shows some sample output:</p>
<div><div><img alt="Figure 4.11 – Top five methods on the stack" src="img/B19423_04_11.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.11 – Top five methods on the stack</p>
<p>There are no details about the top line – this is due to unmanaged or dynamically generated code. But the second one is ours – the <code>MostInefficientFibonacci</code> method looks suspicious and is worth checking. It was on the call stack 29.3% of the time (exclusive percentage). Alongside nested calls, it was on the call stack 31.74% of the time (inclusive percentage). This was easy, but in more complex cases, this analysis won’t be enough, and we might want to dig even further into popular call stacks.</p>
<p>You can open the trace file with any of the performance analysis tools<a id="_idIndexMarker265"/> I mentioned previously. We’ll use SpeedScope (<a href="https://www.speedscope.app/">https://www.speedscope.app/</a>), a web-based tool.</p>
<p>First, let’s convert the trace file into <code>speedscope</code> format:</p>
<pre class="console">
dotnet-trace convert --format speedscope
  issues.exe_*.nettrace</pre>
<p>Then, we must drop the generated JSON file<a id="_idIndexMarker266"/> into SpeedScope via the browser. It will show<a id="_idIndexMarker267"/> the captured call stacks for each thread.</p>
<p>You can click through different threads. You will see that many of them are sitting and waiting for work, as shown in <em class="italic">Figure 4</em><em class="italic">.12</em>:</p>
<div><div><img alt="Figure 4.12 – The thread is waiting for work" src="img/B19423_04_12.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.12 – The thread is waiting for work</p>
<p>This explains the top line in the report – most of the time, threads are waiting in unmanaged code.</p>
<p>There is another group of threads that is working hard to calculate Fibonacci numbers, as you can see in <em class="italic">Figure 4</em><em class="italic">.13</em>:</p>
<div><div><img alt="Figure 4.13 – Call stack showing controller invocation with Fibonacci number calculation" src="img/B19423_04_13.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.13 – Call stack showing controller invocation with Fibonacci number calculation</p>
<p>As you can see, we use a recursive Fibonacci algorithm without memorization, which explains the terrible performance.</p>
<p>We could have<a id="_idIndexMarker268"/> also used the <code>dotnet-stack</code> tool, which prints<a id="_idIndexMarker269"/> managed thread stack trace snapshots.</p>
<h2 id="_idParaDest-77"><a id="_idTextAnchor076"/>Debugging locks</h2>
<p>With performance tracing, we can detect code<a id="_idIndexMarker270"/> that actively consumes<a id="_idIndexMarker271"/> CPU, but what if nothing happens – for example, if we have a lock in our code? Let’s find out.</p>
<p>Let’s start the <strong class="bold">issues</strong> app and generate some load:</p>
<pre class="console">
$dotnet run -c Release lock ––parallel 1000</pre>
<p>If we check the CPU and memory consumption, we can see that they are low and don’t grow much, the thread count doesn’t change much, the thread queue is empty, and the contention rate is low. At the same<a id="_idIndexMarker272"/> time, the throughput<a id="_idIndexMarker273"/> is low (around 60 requests per second) and the latency is big (P95 is around 3 seconds). So, the application is doing nothing, but it can’t go faster. If we check the traces, we will see a big gap with no further data.</p>
<p>This issue is specific to the lock API; if we hit another API, such as <code>http://localhost:5051/ok</code>, it responds immediately. This narrows down our search for the lock API.</p>
<p>Assuming we don’t know there is a lock there, let’s collect some performance traces again with <code>$ dotnet-trace collect --name issues</code>. If we get the <code>topN</code> stacks, as in the previous example, we won’t see anything interesting – just threads waiting for work – locking is fast; waiting for the locked resource to become available takes much longer.</p>
<p>We can dig deeper into the generated trace file to find actual stack traces on what happens in the lock controller. We’re going to use PerfView on Windows, but you can use PerfCollect on Linux, or other tools such as JetBrains dotTrace to open trace files and find stack traces.</p>
<p>Let’s open the trace file with PerfView and then click on the <code>LockController.Lock</code>, as shown in <em class="italic">Figure 4</em><em class="italic">.14</em>:</p>
<div><div><img alt="Figure 4.14 – Finding LockController stacks across all threads" src="img/B19423_04_14.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.14 – Finding LockController stacks across all threads</p>
<p>We can see that <code>LockController</code> rarely appears on the call stack, as well as its nested calls – we can tell since both the inclusive and exclusive percentages are close to 0. From this, we can conclude that whatever we’re waiting for is asynchronous; otherwise, we would see it on the call stack.</p>
<p>Now, let’s right-click<a id="_idIndexMarker274"/> on the <code>LockController</code> line and click on <code>LockController</code> stacks. Switch to the <strong class="bold">CallTree</strong> tab, as shown in <em class="italic">Figure 4</em><em class="italic">.15</em>:</p>
<div><div><img alt="Figure 4.15 – Call stack with LockController.Lock" src="img/B19423_04_15.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.15 – Call stack with LockController.Lock</p>
<p>We can see that the controller calls <code>SemaphoreSlim.WaitAsync</code> – this should be our first suspect. It would explain the low CPU, low memory usage, and no anomalies in the thread counts. It still makes clients wait and keeps client connections open.</p>
<p class="callout-heading">Note</p>
<p class="callout">We can only see the synchronous part of the call stack in <em class="italic">Figure 4</em><em class="italic">.15</em> – it does not include <code>WaitAsync</code> or anything that happens after that.</p>
<p>The analysis we’ve done here relies on luck. In real-world scenarios, this issue would be hidden among other calls. We would have multiple suspects and would need to collect more data to investigate further. Since we’re looking for asynchronous suspects, collecting task-related events with <code>dotnet-trace</code> from the <code>System.Threading.Tasks.TplEventSource</code> provider would be useful.</p>
<p>The issue is obvious if we look into the code, but it can be hidden well in real-world code, behind feature flags or third-party libraries:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">LockController.cs</p>
<pre class="source-code">
<strong class="bold">await semaphoreSlim.WaitAsync(token);</strong>
try
{
    ThreadUnsafeOperation();
    await _httpClient.GetAsync("/dummy/?delay=10", token);
}
finally
{
    <strong class="bold">semaphoreSlim.Release();</strong>
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4/issues/Controllers/LockController.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4/issues/Controllers/LockController.cs</a></p>
<p>The problem here is that we put a lock around the HTTP call to the downstream service. If we wrap only <code>ThreadUnsafeOperation</code> into a synchronous lock, we’ll see a much higher throughput of around 20K requests per second and low latency with P95 of around 20 milliseconds.</p>
<p>Performance tracing<a id="_idIndexMarker276"/> is a powerful tool that allows<a id="_idIndexMarker277"/> us to capture low-level data reported by the .NET runtime, standard, and third-party libraries. In the examples we have covered in this chapter, we run diagnostics tools ad hoc and on the same host as the service. This is reasonable when you’re reproducing issues locally or optimizing your service on the dev box. Let’s see what we can do in a more realistic case with multiple instances of services running and restricted SSH access.</p>
<h1 id="_idParaDest-78"><a id="_idTextAnchor077"/>Using diagnostics tools in production</h1>
<p>In production, we need<a id="_idIndexMarker278"/> to be able to collect some data proactively<a id="_idIndexMarker279"/> with reasonable performance and a telemetry budget so that we can analyze data afterward.</p>
<p>It’s difficult to reproduce an issue on a specific instance of a running process and collect performance traces or dumps from it in a secure and distributed application. If an issue such as a slow memory leak or a rare deadlock affects just a few instances, it might be difficult to even detect it and, when detected, the instance has already been recycled<a id="_idIndexMarker280"/> and the issue<a id="_idIndexMarker281"/> is no longer visible.</p>
<h2 id="_idParaDest-79"><a id="_idTextAnchor078"/>Continuous profiling</h2>
<p>What we’re looking<a id="_idIndexMarker282"/> for is a continuous profiler – a tool that collects sampled performance traces. It can run for short periods to minimize the performance impact of collection on each instance and send profiles to central storage, where they can be stored, correlated with distributed traces, queried, and viewed. Distributed tracing supports sampling and a profiler can use it to capture traces and profiles consistently.</p>
<p>Many observability vendors, such as Azure Monitor, New Relic, Dynatrace, and others, provide continuous profilers for .NET. For example, Azure Monitor allows us to navigate to profiles from traces, as you can see in <em class="italic">Figure 4</em><em class="italic">.16</em>:</p>
<div><div><img alt="Figure 4.16 – Navigating to a profile from a trace in Azure Monitor" src="img/B19423_04_16.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.16 – Navigating to a profile from a trace in Azure Monitor</p>
<p>We will see a long trace for the inefficient code examples we went through earlier in this chapter, but the continuous profiler was enabled and captured some of these calls. If we click on the profiler icon, we will see the call stack, similar to the one we captured with <code>dotnet-collect</code>, as shown in <em class="italic">Figure 4</em><em class="italic">.17</em>:</p>
<div><div><img alt="Figure 4.17 – Profile showing a recursive call stack with the MostInefficientFibonacci method" src="img/B19423_04_17.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.17 – Profile showing a recursive call stack with the MostInefficientFibonacci method</p>
<p>With a continuous profiler, we can debug inefficient code in a matter of seconds, assuming that the problem is reproduced frequently enough so that we can capture both distributed trace<a id="_idIndexMarker283"/> and profile for it.</p>
<h2 id="_idParaDest-80"><a id="_idTextAnchor079"/>The dotnet-monitor tool</h2>
<p>Beyond profiling<a id="_idIndexMarker284"/> individual calls, we also need to be able to capture dumps proactively and on demand. It’s possible to configure .NET to capture dumps when a process crashes, but it doesn’t always work in containers and it’s not trivial to access and transfer dumps.</p>
<p>With <code>dotnet-monitor</code>, we can capture logs, memory, and GC dumps, and collect performance traces in the same way we did with <code>dotnet</code> diagnostic tools:</p>
<ul>
<li>Performance traces from event sources can be collected with the <code>dotnet-monitor</code> <code>/trace</code> API or the <code>dotnet-trace</code> CLI tool</li>
<li>Dumps can be collected with the <code>/dump</code> API or the <code>dotnet-dump</code> tool</li>
<li>Event counters can be collected with the <code>/metrics</code> API or the <code>dotnet-counters</code> tool</li>
</ul>
<p>Check out the <code>dotnet-monitor</code> documentation to learn more about these and other HTTP APIs it provides: <a href="https://github.com/dotnet/dotnet-monitor/tree/main/documentation">https://github.com/dotnet/dotnet-monitor/tree/main/documentation</a>.</p>
<p>We can also configure triggers and rules that proactively collect traces or dumps based on CPU or memory utilization, GC frequency, and other runtime counter values. Results are uploaded to configurable external storage.</p>
<p>We looked at some features of <code>dotnet-monitor</code> in <a href="B19423_02.xhtml#_idTextAnchor038"><em class="italic">Chapter 2</em></a>, <em class="italic">Native Monitoring in .NET</em>, where we run it as a sidecar<a id="_idIndexMarker285"/> container in Docker. Similarly, you can run it as a sidecar in Kubernetes.</p>
<h1 id="_idParaDest-81"><a id="_idTextAnchor080"/>Summary</h1>
<p>Performance issues affect the user experience by decreasing service availability. Distributed tracing and common metrics allow you to narrow down the problem to a specific service, instance, API, or another combination of factors. When it’s not enough, you could increase resolution by adding more spans, but at some point, the performance impact and cost of the solution would become unreasonable.</p>
<p>.NET runtime metrics provide insights into CLR, ASP.NET Core, Kestrel, and other components. Such metrics can be collected with OpenTelemetry, <code>dotnet-counters</code>, or <code>dotnet-monitor</code>. It could be enough to root cause an issue, or just provide input on how to continue the investigation. The next step could be capturing process dumps and analyzing memory or threads’ call stacks, which can be achieved with <code>dotnet-dump</code>.</p>
<p>For problems specific to certain scenarios, performance traces provide details so that we can see what happens in the application or under the hood in third-party library code. Performance traces are collected with <code>dotnet-trace</code> or <code>dotnet-monitor</code>. By capturing performance traces, we can see detailed call stacks, get statistics on what consumes CPU, and monitor contentions and garbage collection more precisely. This is not only a great tool to investigate low-level issues but also to optimize your code.</p>
<p>Collecting low-level data in a secure, multi-instance environment is challenging. Continuous profilers can collect performance traces and other diagnostics on-demand, on some schedule, or by reacting to certain triggers. They also can take care of storing data in a central location and then visualizing and correlating it with other telemetry signals.</p>
<p>The <code>dotnet-monitor</code> tool can run as a sidecar and then provide essential features to diagnostics data proactively or on-demand and send it to external storage.</p>
<p>In this chapter, you learned how to collect diagnostics data using .NET diagnostics tools and how to use it to solve several classes of common performance issues. Applying this knowledge, along with what we learned about metrics, distributed tracing, and logs previously, should allow you to debug most distributed and local issues in your application.</p>
<p>So, now, you know everything you need to leverage auto-instrumentation and make use of telemetry created by someone else. In the next chapter, we’ll learn how to enrich auto-generated telemetry and tailor it to our needs.</p>
<h1 id="_idParaDest-82"><a id="_idTextAnchor081"/>Questions</h1>
<ol>
<li>What would you check first to understand whether an application was healthy?</li>
<li>If you were to see a major performance issue affecting multiple different scenarios, how would you investigate it?</li>
<li>What’s performance tracing and how can you leverage it?</li>
</ol>
</div>


<div><h1 id="_idParaDest-83"><a id="_idTextAnchor082"/>Part 2: Instrumenting .NET Applications</h1>
</div>
<div><p>This part provides an in-depth overview and practical guide for .NET tracing, metrics, logs, and beyond. We’ll start by learning about OpenTelemetry configuration and then dive deep into manual instrumentation, using different signals.</p>
<p>This part has the following chapters:</p>
<ul>
<li><a href="B19423_05.xhtml#_idTextAnchor083"><em class="italic">Chapter 5</em></a>, <em class="italic">Configuration and Control Plane</em></li>
<li><a href="B19423_06.xhtml#_idTextAnchor098"><em class="italic">Chapter 6</em></a>, <em class="italic">Tracing Your Code</em></li>
<li><a href="B19423_07.xhtml#_idTextAnchor115"><em class="italic">Chapter 7</em></a>, <em class="italic">Adding Custom Metrics</em></li>
<li><a href="B19423_08.xhtml#_idTextAnchor131"><em class="italic">Chapter 8</em></a>, <em class="italic">Writing Structured and Correlated Logs</em></li>
</ul>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
</body></html>