- en: '15'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '15'
- en: Instrumenting Brownfield Applications
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 仪器化棕色地带应用程序
- en: When building brand-new services and systems, it’s easy to achieve a basic level
    of observability with distributed traces, metrics, and logs using OpenTelemetry
    instrumentation libraries.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当构建全新的服务和系统时，使用OpenTelemetry仪器库很容易实现基本级别的可观察性，包括分布式跟踪、指标和日志。
- en: However, we don’t usually create applications from scratch – instead, we evolve
    existing systems that include services in different stages of their life, varying
    from experimental to legacy ones that are too risky to change.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们通常不会从头开始创建应用程序——相反，我们演进现有的系统，这些系统包括处于不同生命周期的服务，从实验性的到过于风险而无法更改的遗留服务。
- en: Such systems normally have some monitoring solutions in place, with custom correlation
    formats, telemetry schemas, logs and metrics management systems, dashboards, alerts,
    as well as documentation and processes around these tools.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的系统通常已经实施了一些监控解决方案，包括自定义的相关格式、遥测模式、日志和指标管理系统、仪表板、警报，以及围绕这些工具的文档和流程。
- en: In this chapter, we’ll explore instrumentation options for such heterogeneous
    systems, which are frequently referred to as **brownfield**. First, we’ll discuss
    instrumentation options for legacy parts of the system and then look deeper into
    context propagation and interoperating with legacy correlation formats. Finally,
    we’ll talk about existing monitoring solutions and investigate migration strategies.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨此类异构系统的仪器化选项，这些系统通常被称为**棕色地带**。首先，我们将讨论系统遗留部分的仪器化选项，然后深入探讨上下文传播和与遗留相关格式的互操作性。最后，我们将讨论现有的监控解决方案并研究迁移策略。
- en: 'You’ll learn to do the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 你将学习以下内容：
- en: Pick a reasonable level of instrumentation for legacy services
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为遗留服务选择合理的仪器化级别
- en: Leverage legacy correlation formats or propagate context transparently to enable
    end-to-end tracing
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用遗留的相关格式或透明地传播上下文，以实现端到端跟踪
- en: Forward telemetry from legacy services to new observability backends
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将遗留服务的前向遥测转发到新的可观察性后端
- en: By the end of this chapter, you will be able to implement distributed tracing
    in your brownfield application, keeping changes to legacy parts of a system to
    a minimum.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够在你自己的棕色地带应用程序中实现分布式跟踪，将系统遗留部分的更改保持在最低限度。
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code for this chapter is available in the book’s repository on GitHub at
    [https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter15](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter15).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可在GitHub上本书的仓库中找到，地址为[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter15](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter15)。
- en: 'To run samples for this chapter, we’ll need a Windows machine with the following
    tools:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行本章的示例，我们需要一台装有以下工具的Windows机器：
- en: .NET SDK 7.0 or later
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: .NET SDK 7.0或更高版本
- en: .NET SDK 4.6.2
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: .NET SDK 4.6.2
- en: Docker and `docker-compose`
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker和`docker-compose`
- en: Instrumenting legacy services
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 仪器化遗留服务
- en: The word **legacy** has a negative connotation in software development, implying
    something out of date and not exciting to work on. In this section, we will focus
    on a different aspect and define a legacy service as something that mostly successfully
    does its job but no longer evolves. Such services may still receive security updates
    or fixes for critical issues, but they don’t get new features, refactoring, or
    optimizations.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件开发中，单词**遗留**具有负面含义，意味着过时且不吸引人去工作。在本节中，我们将关注不同的方面，并将遗留服务定义为主要成功完成其工作但不再发展的东西。这些服务可能仍然会收到安全更新或针对关键问题的修复，但它们不会获得新功能、重构或优化。
- en: Maintaining such a service requires a different set of skills and fewer people
    than the evolving one, so the context of a specific system can easily get lost,
    especially after the team that was developing it moved on and now works on something
    else.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 维护此类服务需要不同的技能集和较少的人员，因此特定系统的上下文很容易丢失，尤其是在开发该系统的团队转向其他工作之后。
- en: As a result, changing such components is very risky, even when it comes to updating
    runtime or dependency versions. Any modification might wake up dormant issues,
    slightly change performance, causing new race conditions or deadlocks. The main
    problem here is that with limited resources and a lack of context, nobody might
    know how a service works, or how to investigate and fix such issues. There also
    may no longer be appropriate test infrastructure to validate changes.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，更改此类组件的风险非常高，即使是在更新运行时或依赖版本时也是如此。任何修改都可能唤醒沉睡的问题，略微改变性能，导致新的竞争条件或死锁。这里的主要问题是，由于资源有限且缺乏上下文，没有人可能知道一个服务是如何工作的，或者如何调查和修复此类问题。也可能不再有适当的测试基础设施来验证更改。
- en: From an observability standpoint, such components usually have some level of
    monitoring in place, which is likely to be sufficient for maintenance purposes.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 从可观察性的角度来看，此类组件通常已经实施了一定程度的监控，这可能是维护目的的足够。
- en: Essentially, when working on the observability of a system, we would touch legacy
    services only when it’s critical for newer parts of the system.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，当我们在系统可观察性方面工作时，我们只有在系统的新部分至关重要时才会触及旧服务。
- en: Let’s look at a couple of examples to better understand when changing legacy
    service is important and how we can minimize the risks.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看几个例子，以更好地理解何时更改旧服务很重要，以及我们如何最小化风险。
- en: Legacy service as a leaf node
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 旧服务作为叶节点
- en: 'Let’s assume we’re building new parts of the system using a few legacy services
    as a dependency, as shown in *Figure 15**.1*:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在使用几个旧服务作为依赖来构建系统的新的部分，如图*图15.1*所示。1*：
- en: '![Figure 15.1 – New services depend on legacy ones](img/B19423_15_01.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图15.1 – 新服务依赖于旧服务](img/B19423_15_01.jpg)'
- en: Figure 15.1 – New services depend on legacy ones
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.1 – 新服务依赖于旧服务
- en: For the purposes of our new observability solution, we may be able to treat
    a legacy system as a black box. We can trace client calls to the legacy components
    and measure client-side latency and other stats. Sometimes, we’ll need to know
    what happens inside the legacy component – for example, to understand client-side
    issues or work around legacy system limitations. For this, we can leverage existing
    logging and monitoring tools available in the legacy services. It could be inconvenient,
    but if it is rare, it can be a reasonable option.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了我们新的可观察性解决方案，我们可能能够将旧系统视为一个黑盒。我们可以跟踪客户端对旧组件的调用并测量客户端延迟和其他统计数据。有时，我们需要了解旧组件内部发生的事情——例如，为了理解客户端问题或绕过旧系统限制。为此，我们可以利用在旧服务中可用的现有日志和监控工具。这可能不太方便，但如果这种情况很少见，它可能是一个合理的选项。
- en: If legacy components support any correlation headers for incoming requests,
    we can populate them on the client side to correlate across different parts of
    a system. We’ll look at this in the *Propagating context* section of this chapter.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果旧组件支持任何用于传入请求的相关头，我们可以在客户端填充它们以跨系统的不同部分进行关联。我们将在本章的*传播上下文*部分探讨这一点。
- en: Another thing we may be able to do without changing a legacy system is forking
    and forwarding its telemetry to the same observability backend – we’ll take a
    closer look at this in the *Consolidating telemetry from legacy-monitoring* *tools*
    section.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一件事，我们可能在不更改旧系统的情况下做到的是分叉并将其遥测数据转发到相同的可观察性后端——我们将在*从旧监控工具中整合遥测数据*工具部分更详细地探讨这一点。
- en: Being able to correlate telemetry from new and legacy components and store it
    in the same place could be enough to debug occasional integration issues.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 能够将新组件和旧组件的遥测数据关联起来并存储在同一位置，可能就足以调试偶尔出现的集成问题。
- en: Things get more interesting if a legacy system is in the middle of our application
    – let’s see why.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果旧系统位于我们的应用程序中间，事情会更有趣——让我们看看原因。
- en: A legacy service in the middle
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 中间的旧服务
- en: 'When we refactor a distributed system, we can update downstream and upstream
    services around a legacy component, as shown in *Figure 15**.2*:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们重构一个分布式系统时，我们可以更新围绕旧组件的下游和上游服务，如图*图15.2*所示：
- en: '![Figure 15.2 – Legacy service-b is in between the newer service-a and service-c](img/B19423_15_02.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图15.2 – 旧服务-b位于较新的服务-a和service-c之间](img/B19423_15_02.jpg)'
- en: Figure 15.2 – Legacy service-b is in between the newer service-a and service-c
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.2 – 旧服务-b位于较新的服务-a和service-c之间
- en: From the tracing side, the challenge here is that the legacy component does
    not propagate W3C Trace Context. Operations that go through **legacy-service-b**
    are recorded as two traces – one started by **service-a** and another started
    by **service-c**.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从跟踪方面来看，这里的挑战是历史组件不传播W3C跟踪上下文。通过**legacy-service-b**进行的操作被记录为两个跟踪 - 一个由**service-a**启动，另一个由**service-c**启动。
- en: We need to either support legacy context propagation format in newer parts of
    the system, or update the legacy component itself to enable context propagation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在系统的较新部分支持历史上下文传播格式，或者更新历史组件本身以启用上下文传播。
- en: Before we go into the context propagation details, let’s discuss the appropriate
    level of changes we should consider applying to a service, depending on the level
    of its maturity.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入讨论上下文传播的细节之前，让我们讨论我们应该考虑应用于服务的适当更改水平，这取决于其成熟度。
- en: Choosing a reasonable level of instrumentation
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择合理的仪器水平
- en: 'Finding the right level of instrumentation for mature parts of a system depends
    on how big of a change is needed and how risky it is. Here are several things
    to consider:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为系统的成熟部分找到合适的仪器水平取决于需要多大的更改以及风险有多大。以下是一些需要考虑的事项：
- en: Where do legacy services send telemetry to? Is it the same observability backend
    that we want to use for the newer parts?
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 历史服务将遥测数据发送到何处？它是我们想要用于较新部分的相同可观察性后端吗？
- en: How critical is it for the observability of the overall system to get telemetry
    from legacy components?
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取历史组件的遥测数据对整个系统的可观察性有多重要？
- en: Do legacy services support some context propagation format? Can we interoperate
    with it from newer services?
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 历史服务支持某种上下文传播格式吗？我们能否从新服务中与之交互？
- en: Can we change some of our legacy services? How old is the .NET runtime? Do we
    have an adequate testing infrastructure? How big is the load on this service?
    How critical is the component?
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们能否更改一些我们的历史服务？.NET运行时有多老？我们是否有足够的测试基础设施？这个服务的负载有多大？该组件有多关键？
- en: Let’s go through a few solutions that may apply, depending on your answers.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们根据您的回答，探讨一些可能适用的解决方案。
- en: Not changing legacy services
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不更改历史服务
- en: When legacy parts of a system are instrumented with a vendor-specific SDK or
    agent and send telemetry to the same observability backend as we want to use for
    newer parts, we might not need to do anything – correlation might work out of
    the box or with a little context propagation adapter in newer parts of the system.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当系统的历史部分使用特定供应商的SDK或代理进行仪器化并向我们想要用于较新部分的相同可观察性后端发送遥测数据时，我们可能不需要做任何事情 - 关联可能默认或在新系统的较新部分中通过一点上下文传播适配器工作。
- en: Your vendor might have a migration plan and documentation explaining how to
    make services, using their old SDK and OpenTelemetry-based solution, produce consistent
    telemetry.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 您的供应商可能有一个迁移计划和相关文档，解释如何使用他们的旧SDK和基于OpenTelemetry的解决方案，使服务产生一致的遥测数据。
- en: Another case when doing nothing is a good option is when our legacy components
    are mostly isolated and either work side by side with newer parts or are leaf
    nodes, as shown in *Figure 15**.1*. Then, we can usually develop and debug new
    components without data from legacy services.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个不需要采取任何行动的情况是，当我们的历史组件大部分是隔离的，要么与较新部分并行工作，要么是叶子节点，如图*图15**.1*所示。在这种情况下，我们通常可以在没有来自历史服务的数据的情况下开发和调试新组件。
- en: We could also be able to tolerate having broken traces, especially if they don’t
    affect critical flows and we’re going to retire legacy services soon.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可能能够容忍存在损坏的跟踪，特别是如果它们不影响关键流程，并且我们打算很快退役历史服务。
- en: Doing nothing is the best, but if it’s problematic for overall observability,
    the next discreet option is passing context though a legacy system.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 不采取任何行动可能是最好的选择，但如果这对整体可观察性造成问题，下一个可行的选项是通过历史系统传递上下文。
- en: Propagating context only
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 仅传播上下文
- en: If newer parts communicate with legacy services back and forth and we can’t
    make trace context propagation work, it can prevent us from tracing critical operations
    through a system. The least invasive change we can do then is to transparently
    propagate trace context through a legacy service.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果较新部分与历史服务双向通信，而我们无法使跟踪上下文传播工作，这可能会阻止我们通过系统跟踪关键操作。那时我们可以做的最小侵入性更改是透明地通过历史服务传播跟踪上下文。
- en: When such a service receives a request, we would read the trace context in W3C
    (B3, or another format) and then pass it through, without any modification to
    all downstream services.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当此类服务收到请求时，我们会读取W3C（B3或另一种格式）中的跟踪上下文，然后将其无修改地传递给所有下游服务。
- en: This way, legacy services will not appear on traces, but we will have consistent
    end-to-end traces for the newer parts.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，旧服务将不会出现在跟踪中，但我们将拥有一致的全端到端跟踪。
- en: We can possibly go further and stamp trace context on the legacy telemetry to
    simplify debugging.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能可以更进一步，在旧遥测数据上打上跟踪上下文，以简化调试。
- en: If transparent context propagation is still not enough and we need to have telemetry
    from all services in one place, the next option to consider is forking legacy
    telemetry and sending it to the new observability backend.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果透明的上下文传播仍然不够，并且我们需要将所有服务的遥测数据集中在一个地方，下一个要考虑的选项是分叉旧遥测数据并将其发送到新的可观察性后端。
- en: Forwarding legacy telemetry to the new observability backend
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将旧遥测数据转发到新的可观察性后端
- en: Debugging issues across different observability backends and log management
    tools can be challenging, even when data is correlated.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同的可观察性后端和日志管理工具之间调试问题可能具有挑战性，即使数据是相关的。
- en: To improve it, we may be able to intercept telemetry from the legacy system
    on the way to its backend or enable continuous export from that backend to the
    new one used by the rest of the system.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改进它，我们可能能够在旧系统前往其后端的过程中拦截遥测数据，或者启用从该后端到系统其他部分使用的新后端的不间断导出。
- en: Forwarding may require configuration changes on the legacy system, and even
    if such changes are small, there is still a risk of slowing down the telemetry
    pipeline and causing an incident for the legacy service.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 转发可能需要在旧系统中进行配置更改，即使这些更改很小，仍然存在减缓遥测管道并导致旧服务发生事故的风险。
- en: The younger and the more flexible the system is, the more changes we can consider,
    and the most invasive one is onboarding a legacy system onto OpenTelemetry and
    enabling network instrumentations.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 系统越年轻、越灵活，我们可以考虑的更改就越多，最具侵入性的是将旧系统加入OpenTelemetry并启用网络仪器。
- en: Adding network-level instrumentation
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加网络级仪器
- en: It’s likely that legacy telemetry is not consistent with distributed traces
    coming from new services. We may be able to transform it, or can sometimes tolerate
    the difference, but we may as well consider enabling minimalistic distributed
    tracing in legacy services. This will take care of context propagation and produce
    consistent telemetry with the rest of the system.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能旧遥测数据与来自新服务的分布式跟踪不一致。我们可能能够转换它，或者有时可以容忍这种差异，但我们也应考虑在旧服务中启用最小化分布式跟踪。这将处理上下文传播，并产生与系统其他部分一致的遥测数据。
- en: With this approach, we’ll pump new telemetry from legacy services to the new
    backend and keep all existing instrumentations and pipelines running to avoid
    breaking existing reports, dashboards, and alerts.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 采用这种方法，我们将从旧服务向新后端泵送新的遥测数据，并保持所有现有仪器和管道运行，以避免破坏现有的报告、仪表板和警报。
- en: Something to be aware of here is that OpenTelemetry works on .NET 4.6.2 or newer
    versions of .NET. While instrumentations for IIS, classic ASP.NET, and OWIN are
    available in the **contrib** repository (at [https://github.com/open-telemetry/opentelemetry-dotnet-contrib](https://github.com/open-telemetry/opentelemetry-dotnet-contrib)),
    such instrumentations do not get as much love as newer ones.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的一点是，OpenTelemetry在.NET 4.6.2或更新的.NET版本上运行。虽然IIS、经典ASP.NET和OWIN的仪器在**contrib**存储库（在[https://github.com/open-telemetry/opentelemetry-dotnet-contrib](https://github.com/open-telemetry/opentelemetry-dotnet-contrib)）中可用，但这些仪器并不像新仪器那样受到关注。
- en: You might also hit some edge cases with `Activity.Current` when using IIS –
    it can get lost during hopping between managed and native threads.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用IIS时，您可能会遇到一些与`Activity.Current`相关的边缘情况——它可能在在托管线程和本地线程之间跳转时丢失。
- en: Onboarding existing services to OpenTelemetry while keeping old tools working
    can be a first step in a migration project, which eventually sunsets legacy monitoring
    solutions.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在保持旧工具运行的同时将现有服务加入OpenTelemetry，可以是迁移项目的一个第一步，最终将淘汰旧监控解决方案。
- en: This is a viable solution for any mature service and should be considered unless
    the service is on a retirement path already. However, if it’s not an option, we
    can still combine and evolve other approaches mentioned here. Let’s now look at
    the practical side and see how we can do it.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对任何成熟服务都可行的解决方案，除非该服务已经在退休路径上，否则应该考虑。然而，如果这不是一个选项，我们仍然可以结合并演变这里提到的其他方法。现在让我们看看实际的一面，看看我们如何能实现它。
- en: Propagating context
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 传播上下文
- en: The first goal for context propagation is to enable end-to-end distributed tracing
    for new services, even when they communicate through legacy ones, as shown in
    *Figure 15**.2*. As a stretch goal, we can also try to correlate telemetry from
    new and legacy parts.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文传播的第一个目标是实现新服务的端到端分布式跟踪，即使它们通过遗留系统进行通信，如图*15.2*所示。作为一个挑战目标，我们还可以尝试关联新和遗留部分的数据。
- en: The solution that would work in most cases involves enabling context propagation
    in legacy services. Depending on how legacy services are implemented, this change
    can be significant and risky. So, before we do it, let’s check whether we can
    avoid it.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，该解决方案涉及在遗留服务中启用上下文传播。根据遗留服务的实现方式，这种更改可能是重大且风险较高的。因此，在我们这样做之前，让我们检查是否可以避免它。
- en: Leveraging existing correlation formats
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用现有的关联格式
- en: Our legacy services might already propagate context, just in a different format.
    One popular approach is to pass a correlation ID that serves the same purpose
    as a trace ID in the W3C Trace Context standard, identifying a logical end-to-end
    operation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遗留的服务可能已经传播了上下文，只是格式不同。一种流行的方法是传递一个关联ID，它具有与W3C Trace Context标准中的跟踪ID相同的作用，标识一个逻辑的端到端操作。
- en: While correlation ID is not compatible with trace context out of the box, it
    may be possible to translate one to another.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然关联ID与跟踪上下文不兼容，但可能可以将一个转换为另一个。
- en: 'In a simple case, correlation ID is just a string, and then we just need to
    pass it to the legacy service in a header. Then, we can expect it to propagate
    it as is to downstream calls, as shown in *Figure 15**.3*:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在简单的情况下，关联ID只是一个字符串，然后我们只需将其传递到遗留服务的一个头中。然后，我们可以期望它按原样传播到下游调用，如图*15.3*所示：
- en: '![Figure 15.3 – Passing the W3C Trace ID via a legacy correlation header](img/B19423_15_03.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图15.3 – 通过遗留关联头传递W3C跟踪ID](img/B19423_15_03.jpg)'
- en: Figure 15.3 – Passing the W3C Trace ID via a legacy correlation header
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.3 – 通过遗留关联头传递W3C跟踪ID
- en: Here, `correlation-id` header along with `traceparent`, `correlation-id` up,
    ignoring the unknown `traceparent`, and passes it over to `traceparent` and `correlation-id`
    values. It only has `correlation-id`, so it uses it to continue the trace started
    by **service-a**.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`correlation-id`头与`traceparent`、`correlation-id`一起，忽略未知的`traceparent`，并将其传递给`traceparent`和`correlation-id`值。它只有`correlation-id`，所以它使用它来继续由**service-a**启动的跟踪。
- en: 'Let’s implement it with a custom OpenTelemetry context propagator, starting
    with the injection side, as shown in the following code snippet:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个自定义的OpenTelemetry上下文传播器来实现它，从注入方面开始，如下面的代码片段所示：
- en: CorrelationIdPropagator.cs
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: CorrelationIdPropagator.cs
- en: '[PRE0]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/Brownfield.OpenTelemetry.Common/CorrelationIdPropagator.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/Brownfield.OpenTelemetry.Common/CorrelationIdPropagator.cs)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/Brownfield.OpenTelemetry.Common/CorrelationIdPropagator.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/Brownfield.OpenTelemetry.Common/CorrelationIdPropagator.cs)'
- en: Here, we check whether the activity context is valid and set `TraceId` as a
    string on the `correlation-id` header. We’re setting this propagator up to run
    after the `TraceContextPropagator` implementation available in OpenTelemetry,
    so there is no need to take care of Trace Context headers here.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们检查活动上下文是否有效，并在`correlation-id`头中将`TraceId`设置为字符串。我们设置这个传播器在OpenTelemetry中可用的`TraceContextPropagator`实现之后运行，因此在这里不需要处理跟踪上下文头。
- en: 'And here’s the extraction code:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是提取代码：
- en: CorrelationIdPropagator.cs
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: CorrelationIdPropagator.cs
- en: '[PRE1]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/Brownfield.OpenTelemetry.Common/CorrelationIdPropagator.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/Brownfield.OpenTelemetry.Common/CorrelationIdPropagator.cs)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/Brownfield.OpenTelemetry.Common/CorrelationIdPropagator.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/Brownfield.OpenTelemetry.Common/CorrelationIdPropagator.cs)'
- en: The custom extraction we implemented here runs after trace context extraction,
    so if there was a valid `traceparent` header in the incoming request, then `context.ActivityContext`
    is populated by the time the `Extract` method is called. Here, we give priority
    to W3C Trace Context and ignore the `correlation-id` value.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里实现的自定义提取在追踪上下文提取之后运行，因此如果传入请求中有一个有效的`traceparent`头，那么在调用`Extract`方法时`context.ActivityContext`就会被填充。在这里，我们优先考虑W3C追踪上下文并忽略`correlation-id`值。
- en: If `context.ActivityContext` is not populated, we retrieve the `correlation-id`
    value and try to translate it to a trace ID. If we can do it, then we create a
    new `ActivityContext` instance, using `correlation-id` as a trace ID and a fake
    parent span ID.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`context.ActivityContext`没有被填充，我们检索`correlation-id`值并尝试将其转换为追踪ID。如果我们能这样做，那么我们创建一个新的`ActivityContext`实例，使用`correlation-id`作为追踪ID和一个假的父跨度ID。
- en: 'Here’s the implementation of the `TryGetTraceId` method:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这是`TryGetTraceId`方法的实现：
- en: CorrelationIdPropagator.cs
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: CorrelationIdPropagator.cs
- en: '[PRE2]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/Brownfield.OpenTelemetry.Common/CorrelationIdPropagator.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/Brownfield.OpenTelemetry.Common/CorrelationIdPropagator.cs)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/Brownfield.OpenTelemetry.Common/CorrelationIdPropagator.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/Brownfield.OpenTelemetry.Common/CorrelationIdPropagator.cs)'
- en: In this snippet, we support a variety of possible `correlation-id` formats –
    we remove dashes if it’s a GUID, and pad or trim it if the length is not right.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个片段中，我们支持多种可能的`correlation-id`格式——如果是GUID，则删除连字符；如果长度不正确，则填充或修剪它。
- en: Note
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In a more complicated case, we may need to do other transformations during context
    extraction and injection. For example, when a legacy system requires a GUID, we
    can add dashes. Alternatively, if it wants a `base64`-encoded string, we can decode
    and encode the trace ID.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在更复杂的情况下，我们可能需要在上下文提取和注入期间进行其他转换。例如，当遗留系统需要一个GUID时，我们可以添加连字符。或者，如果它需要一个`base64`编码的字符串，我们可以解码并重新编码追踪ID。
- en: Let’s now check out the traces we get with this approach.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在检查我们使用这种方法得到的追踪记录。
- en: First, run new parts of the system with the `$ docker-compose up --build` command.
    It starts with **service-a**, **service-c**, and the observability stack.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用`$ docker-compose up --build`命令运行系统的新的部分。它从**service-a**、**service-c**和可观察性堆栈开始。
- en: 'We also need to start **legacy-service-b**, which is the .NET Framework 4.6.2
    application running on Windows. You can start it with your IDE or the following
    command:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要启动**legacy-service-b**，这是一个运行在Windows上的.NET Framework 4.6.2应用程序。您可以使用您的IDE或以下命令启动它：
- en: '[PRE3]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, hit the following URL in your browser: http://localhost:5051/a?to=c.
    This will send a request to **service-a**, which will call **service-c** through
    **legacy-service-b**.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在您的浏览器中点击以下URL：http://localhost:5051/a?to=c。这将向**service-a**发送一个请求，该请求将通过**legacy-service-b**调用**service-c**。
- en: 'Now, let’s open Jaeger at http://localhost:16686 and find the trace from **service-a**,
    which should look like the one shown in *Figure 15**.4*:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们打开Jaeger，在http://localhost:16686上，找到来自**service-a**的追踪，它应该看起来像*图15.4*中所示的那样：
- en: '![Figure 15.4 – An end-to-end trace covering service-a and service-c](img/B19423_15_04.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图15.4 – 覆盖服务-a和service-c的端到端追踪](img/B19423_15_04.jpg)'
- en: Figure 15.4 – An end-to-end trace covering service-a and service-c
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.4 – 覆盖服务-a和service-c的端到端追踪
- en: As you can see, there is no `5050`) belongs to **legacy-service-b**.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，没有`5050`)属于**legacy-service-b**。
- en: There is just one trace, but it still looks broken – spans are correlated, but
    parent-child relationships between the client span on **service-a** and the server
    span on **service-c** are lost.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 只有一条追踪记录，但它看起来仍然损坏了——跨度是相关的，但是客户端跨度在**service-a**和服务器跨度在**service-c**之间的父子关系丢失了。
- en: Still, it’s an improvement. Let’s now disable the `correlation-id` support on
    `Compatibility__SupportLegacyCorrelation` environment variable in `docker-compose.yml`
    to `false` on both services and restarting the docker compose application. Then,
    we’ll see two independent traces for **service-a** and **service-c**, so even
    the correlation will be lost.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，这仍然是一个改进。现在，让我们在 `docker-compose.yml` 中的 `Compatibility__SupportLegacyCorrelation`
    环境变量上禁用 `correlation-id` 支持，将其在两个服务中都设置为 `false`，然后重启 docker compose 应用程序。然后，我们将看到针对
    **service-a** 和 **service-c** 的两个独立跟踪，因此关联也将丢失。
- en: Note
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: By relying on the existing context propagation format and implementing a custom
    propagation adapter, we can usually record end-to-end traces for new services
    without any modification to the legacy ones.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 通过依赖现有的上下文传播格式并实现自定义传播适配器，我们通常可以在不修改旧服务的情况下为新服务记录端到端跟踪。
- en: Can we also correlate telemetry from the legacy and new services? Usually, legacy
    services stamp their version of `correlation-id` on all logs. If that’s the case,
    we can search using the trace ID across all telemetry but may need to map the
    trace ID to the correlation ID and back, in the same way we did with the propagator.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否也可以关联旧服务和新服务的遥测数据？通常，旧服务会在所有日志上盖章其版本的 `correlation-id`。如果是这样，我们可以使用跟踪 ID
    在所有遥测数据中搜索，但可能需要将跟踪 ID 映射到关联 ID，然后再映射回来，就像我们处理传播者一样。
- en: However, what if we didn’t have custom correlation implemented in a legacy service
    or were not able to implement an adapter? We’d need to modify the legacy service
    to enable context propagation – let’s see how it can be done.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们没有在旧服务中实现自定义关联，或者无法实现适配器，该怎么办呢？我们需要修改旧服务以启用上下文传播——让我们看看如何实现。
- en: Passing context through a legacy service
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过旧服务传递上下文
- en: Essentially, if there is no existing context propagation mechanism, we can implement
    one. To minimize changes to legacy systems, we can propagate context transparently,
    without modifying it.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，如果没有现有的上下文传播机制，我们可以实现一个。为了最小化对旧系统的修改，我们可以透明地传播上下文，而不需要显式修改它。
- en: We need to intercept incoming and outgoing requests to extract and inject trace
    context, and we also need a way to pass the context inside the process.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要拦截入站和出站请求以提取和注入跟踪上下文，我们还需要一种在进程内传递上下文的方法。
- en: The implementation of this approach, especially the interception, depends on
    the technologies, libraries, and patterns used in a specific legacy service.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的实现，特别是拦截，取决于特定旧服务中使用的科技、库和模式。
- en: Incoming request interception can be achieved with some middleware or request
    filter. If IIS is used, it can be also done in a custom HTTP telemetry module,
    but then we cannot fully rely on ambient context propagation due to managed-to-native
    thread hops.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 入站请求拦截可以通过某些中间件或请求过滤器实现。如果使用 IIS，也可以在自定义 HTTP 遥测模块中完成，但那时由于托管到本地线程的跳跃，我们无法完全依赖环境上下文传播。
- en: Passing context within a process can be usually achieved with `AsyncLocal` on
    .NET 4.6+ or `LogicalCallContext` on .NET 4.5 – this way, it will be contained
    in the new code and won’t require plumbing context explicitly.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在进程内传递上下文通常可以通过 .NET 4.6+ 上的 `AsyncLocal` 或 .NET 4.5 上的 `LogicalCallContext`
    实现——这样，它将包含在新代码中，并且不需要显式地处理上下文。
- en: 'In our demo system, **legacy-service-b** is a self-hosted OWIN application,
    and we can implement context extraction in the OWIN middleware:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的演示系统中，**legacy-service-b** 是一个自托管的 OWIN 应用程序，我们可以在 OWIN 中间件中实现上下文提取：
- en: PassThroughMiddleware.cs
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: PassThroughMiddleware.cs
- en: '[PRE4]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/legacy-service-b/PassThrough/PassThroughMiddleware.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/legacy-service-b/PassThrough/PassThroughMiddleware.cs)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/legacy-service-b/PassThrough/PassThroughMiddleware.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/legacy-service-b/PassThrough/PassThroughMiddleware.cs)'
- en: First, we declare a static `AsyncLocal` value that holds trace context, represented
    with a simple dictionary.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们声明一个静态的 `AsyncLocal` 值，它包含跟踪上下文，用简单的字典表示。
- en: In the middleware `Invoke` method, we read `traceparent` along with the `tracestate`
    and `baggage` headers (which are omitted for brevity). We populate them in the
    trace context dictionary. Depending on your needs, you can always limit supported
    context fields to `traceparent` only and optimize the code further.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在中间件的`Invoke`方法中，我们读取`traceparent`、`tracestate`和`baggage`头（为了简洁起见省略了）。我们在跟踪上下文字典中填充它们。根据您的需求，您始终可以限制支持的上下文字段仅为`traceparent`，并进一步优化代码。
- en: Then, we populate the context dictionary on the `_currentContext` field, which
    we can then access through the public `CurrentContext` static property.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在`_currentContext`字段上填充上下文字典，然后我们可以通过公共的`CurrentContext`静态属性访问它。
- en: The last thing we do here is to invoke the next middleware, which we wrap with
    a logger scope containing the context dictionary. This allows us to populate trace
    context on all logs coming from **legacy-service-b**, thus correlating them with
    telemetry coming from new services.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里做的最后一件事是调用下一个中间件，我们将它包装在一个包含上下文字典的日志作用域中。这允许我们在来自**legacy-service-b**的所有日志中填充跟踪上下文，从而将它们与来自新服务的遥测数据相关联。
- en: In practice, legacy applications rarely use `ILogger`, but logging libraries
    usually have some other mechanism to populate ambient context on log records.
    Depending on the library, you may be able to access and populate `CurrentContext`
    with little change to the logging configuration code.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，遗留应用程序很少使用`ILogger`，但日志库通常有一些其他机制来在日志记录中填充环境上下文。根据库的不同，您可能能够通过稍微修改日志配置代码来访问和填充`CurrentContext`。
- en: Getting back to context propagation, we now need to inject the `CurrentContext`
    value into the outgoing requests.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 回到上下文传播，我们现在需要将`CurrentContext`值注入到发出的请求中。
- en: In the case of HTTP and when .NET `HttpClient` is used, we can do it with custom
    `DelegatingHandler` implementation. It will be more tedious with `WebRequest`
    usage spread across the application code when there are no helper methods that
    create them consistently.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在HTTP的情况下，当使用.NET `HttpClient`时，我们可以通过自定义`DelegatingHandler`实现来完成。如果没有创建它们的辅助方法，且在应用程序代码中广泛使用`WebRequest`时，这将会更加繁琐。
- en: 'The handler implementation is shown in the following code snippet:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 处理器实现如下代码片段所示：
- en: PassThroughHandler.cs
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: PassThroughHandler.cs
- en: '[PRE5]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/legacy-service-b/PassThrough/PassThroughMiddleware.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/legacy-service-b/PassThrough/PassThroughMiddleware.cs)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/legacy-service-b/PassThrough/PassThroughMiddleware.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/legacy-service-b/PassThrough/PassThroughMiddleware.cs)'
- en: Here, we just inject all fields from `CurrentContext` on outgoing request headers
    and then invoke the next handler. That’s it.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只是将`CurrentContext`中的所有字段注入到发出的请求头中，然后调用下一个处理器。就是这样。
- en: Note
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Starting with the `System.Diagnostics.DiagnosticSource` package version 6.0.0,
    .NET provides a `DistributedContextPropagator` base class along with several implementations,
    including W3C trace context and a pass-through propagator. It can be useful if
    you can add a dependency on a newish `DiagnosticSource` package, or when configuring
    propagation for native distributed tracing instrumentations in ASP.NET Core and
    `HttpClient`. In the case of our legacy service, extraction and injection alone
    are trivial, so adding a new dependency is not really justified.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 从`System.Diagnostics.DiagnosticSource`包版本6.0.0开始，.NET提供了一个`DistributedContextPropagator`基类以及几个实现，包括W3C跟踪上下文和透传传播器。如果您可以添加对新`DiagnosticSource`包的依赖，或者在配置ASP.NET
    Core和`HttpClient`中的原生分布式跟踪工具的传播时，这可能很有用。在我们的遗留服务中，提取和注入本身是微不足道的，因此添加新的依赖并不真正合理。
- en: 'Now, we can run the application again and check the traces:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以再次运行应用程序并检查跟踪：
- en: 'Start new services with `$ docker-compose up --build` and then **legacy-service-b**
    with the following command:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令启动新服务并构建：`$ docker-compose up --build`，然后启动**legacy-service-b**：
- en: '[PRE6]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then call **service-a** with http://localhost:5051/a?to=c again and open Jaeger.
    We should see a trace like the one in *Figure 15**.5*:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后再次使用http://localhost:5051/a?to=c调用**service-a**并打开Jaeger。我们应该看到像*Figure 15*.5*中的那样的跟踪：
- en: '![Figure 15.5 – An end-to-end trace with transparent service-b](img/B19423_15_05.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 15.5 – An end-to-end trace with transparent service-b](img/B19423_15_05.jpg)'
- en: Figure 15.5 – An end-to-end trace with transparent service-b
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 15.5 – An end-to-end trace with transparent service-b
- en: Here, we have correlation and causation – the client span on **service-a** is
    a direct parent of the server span on **service-c**. However, **service-b** is
    nowhere to be seen, as it does not actively participate in the tracing.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有关联和因果关系——**service-a**上的客户端跨度是**service-c**上服务器跨度的直接父级。然而，**service-b**却无处可寻，因为它没有积极参与跟踪。
- en: Now, we have a couple of options to pass context through the legacy system,
    but we can be creative and come up with more options specific to our application
    – for example, we can stamp legacy correlation or request IDs on the new telemetry,
    or log them and then post-process telemetry to correlate broken traces.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有几种方法可以通过遗留系统传递上下文，但我们可以发挥创意，为我们的应用程序提出更多特定的选项——例如，我们可以在新的遥测中添加遗留相关性或请求ID，或者记录它们，然后后处理遥测以关联损坏的跟踪。
- en: With these options, we should be able to achieve at least some level of correlation.
    Let’s now check how we can forward telemetry from legacy services to the new observability
    backends.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些选项，我们应该能够实现至少某种程度的关联。现在让我们检查如何将遗留服务中的遥测转发到新的可观察性后端。
- en: Consolidating telemetry from legacy monitoring tools
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 整合遗留监控工具的遥测数据
- en: One of the biggest benefits a good observability solution can provide is low
    cognitive load when debugging an application and reading through telemetry. Even
    perfectly correlated and high-quality telemetry is very hard to use if it’s spread
    across multiple tools and can’t be visualized and analyzed together.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的可观察性解决方案可以提供的最大好处之一是在调试应用程序和阅读遥测数据时的低认知负荷。即使关联完美且遥测数据质量高，如果它们分散在多个工具中且无法一起可视化和分析，也非常难以使用。
- en: When re-instrumenting legacy services with OpenTelemetry is not an option, we
    should check whether it’s possible to forward existing data from legacy services
    to a new observability backend.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用OpenTelemetry重新对遗留服务进行仪器化不是一个选项时，我们应该检查是否有可能将遗留服务中的现有数据转发到新的可观察性后端。
- en: As with context propagation, we can be creative and should start by leveraging
    existing solutions. For example, old .NET systems usually report and consume Windows
    performance counters and send logs to EventLog, or store them on the hard drive.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 与上下文传播一样，我们可以发挥创意，并应首先利用现有解决方案。例如，旧的.NET系统通常报告和消费Windows性能计数器，并将日志发送到EventLog，或者将它们存储在硬盘上。
- en: The OpenTelemetry Collector provides support for such cases via receivers, available
    in the contrib repository (at https://github.com/open-telemetry/opentelemetry-collector-contrib).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: OpenTelemetry Collector通过接收器提供对这种案例的支持，这些接收器位于contrib存储库中（在https://github.com/open-telemetry/opentelemetry-collector-contrib）。
- en: 'For example, we can configure a file receiver with the following snippet:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以使用以下片段配置一个文件接收器：
- en: otel-collector-config.yml
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: otel-collector-config.yml
- en: '[PRE7]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/configs/otel-collector-config.yml](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/configs/otel-collector-config.yml)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/configs/otel-collector-config.yml](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter15/configs/otel-collector-config.yml)'
- en: Here, we configure the collector receiver and specify the log file location
    and name pattern. We also configure mapping and transformation rules for individual
    properties in log records. In this example, we only map timestamp and log level,
    but if log records are structured, it’s possible to parse other properties using
    similar operators.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们配置收集器接收器并指定日志文件的位置和名称模式。我们还为日志记录中的单个属性配置映射和转换规则。在这个例子中，我们只映射时间戳和日志级别，但如果日志记录是结构化的，则可以使用类似的运算符解析其他属性。
- en: We can also rely on our backend to grok unstructured log records or parse records
    at a query time if we rarely need the data.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们很少需要数据，我们也可以依赖我们的后端来解析非结构化日志记录或在查询时解析记录。
- en: 'Here’s an example of collector output with a parsed log record, which, depending
    on your collector configuration, can send logs to the new observability backend:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个带有解析日志记录的收集器输出的示例，根据您的收集器配置，可以将日志发送到新的可观察性后端：
- en: '[PRE8]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you can see, we could also configure the receiver to parse the `traceparent`
    value populated in the log scopes to record `Trace ID` and `Span ID` for the proper
    correlation.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们还可以配置接收器解析在日志作用域中填充的`traceparent`值，以记录`Trace ID`和`Span ID`以进行适当的关联。
- en: 'You can reproduce it by running **legacy-service-b** with the following command
    and sending some requests to it directly, or via **service-a**:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下命令运行**legacy-service-b**并直接向其发送一些请求，或者通过**service-a**来重现它：
- en: '[PRE9]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: A collector can be helpful in sidecar mode, forwarding data available on the
    machine where legacy service instances are running, and collecting performance
    counters or logs. It can also pretend to be our old backend and receive Zipkin
    or Jaeger spans, Prometheus metrics, and vendor-specific signals, such as Splunk
    metrics and logs.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 收集器在边车模式下可以很有帮助，将运行遗产服务实例的机器上的可用数据转发，并收集性能计数器或日志。它还可以假装是我们的旧后端，接收Zipkin或Jaeger跨度、Prometheus指标和供应商特定的信号，例如Splunk指标和日志。
- en: We can write custom receivers and leverage collector transformation processors
    to produce consistent telemetry whenever possible.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以编写自定义接收器并利用收集器转换处理器，尽可能产生一致的遥测数据。
- en: In addition to the endless possibilities a OpenTelemetry Collector can provide,
    we should check whether the observability vendor we use for legacy services allows
    continuous export for collected telemetry, which would allow us to get the data
    without changing anything on the legacy system.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 除了OpenTelemetry收集器可以提供的无限可能性，我们还应该检查我们用于遗产服务的可观察性供应商是否允许对收集的遥测数据进行连续导出，这将使我们能够在不更改遗产系统的情况下获取数据。
- en: Summary
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored tracing in brownfield applications, where some
    of the services can be hard to change and onboard onto a full-fledged observability
    solution with OpenTelemetry.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了棕色字段应用程序中的跟踪，其中一些服务可能难以更改，并加入OpenTelemetry的完整可观察性解决方案。
- en: We discussed possible levels of instrumentation for such services and found
    several cases when we can avoid changing old components altogether. Then, we went
    through the changes we can apply, starting with minimalistic transparent context
    propagation and going all the way to onboarding onto OpenTelemetry.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了此类服务的可能仪器化级别，并发现了一些我们可以完全避免更改旧组件的情况。然后，我们讨论了我们可以应用的变化，从最小化的透明上下文传播开始，一直到最后加入OpenTelemetry。
- en: Finally, we applied some of these options in practice, enabling end-to-end correlation
    through a legacy service and forwarding file logs to the OpenTelemetry Collector.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在实践中应用了一些这些选项，通过遗产服务实现了端到端关联，并将文件日志转发到OpenTelemetry收集器。
- en: Now, you should be ready to come up with the strategy for your own legacy components
    and have the building blocks to implement it.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你应该准备好为你的遗产组件制定策略，并拥有实施它的构建块。
- en: This chapter concludes our journey into distributed tracing and observability
    on .NET – I hope you enjoyed it! The observability area is evolving fast, but
    now you have a foundational knowledge to design and implement your systems with
    observability in mind, evolve them by relying on relevant telemetry data, and
    operate them with more confidence, knowing what telemetry represents and how it’s
    collected. Now, it’s time to apply your knowledge or create something new based
    on it.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束了我们对.NET上分布式跟踪和可观察性的探索之旅——希望您喜欢！可观察性领域发展迅速，但现在您已经有了设计和管理具有可观察性意识系统的基本知识，通过依赖相关遥测数据来演进它们，并更有信心地操作它们，了解遥测数据代表什么以及它是如何收集的。现在，是时候应用你的知识或基于它创建新事物了。
- en: Questions
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: How would you approach instrumenting an existing service that is a critical
    part of most user scenarios in your system? This service is mature and is rarely
    changed, but there are no plans to retire it any time soon.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将如何处理对现有服务的仪器化，该服务是您系统中大多数用户场景的关键部分？这个服务已经成熟，很少改变，但近期内没有计划将其退役。
- en: What can go wrong when we add OpenTelemetry to a legacy service?
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们将OpenTelemetry添加到遗产服务时，可能会出什么问题？
- en: When implementing transparent context propagation, can we leverage the `Activity`
    class instead of adding our own context primitive and the `AsyncLocal` field?
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在实现透明上下文传播时，我们能否利用`Activity`类而不是添加我们自己的上下文原语和`AsyncLocal`字段？
- en: Assessments
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: Chapter 1 – Observability Needs of Modern Applications
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章 - 现代应用程序的可观察性需求
- en: You can think about a span as a structured event with a strict but extensible
    schema, allowing you to track any interesting operation. Spans have trace context
    that describes the relationships between them. They also have a name, start time,
    end time, status, and a property bag, with attributes to represent operation details.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以将Span视为具有严格但可扩展模式的结构化事件，允许您跟踪任何有趣的操作。Span具有描述它们之间关系的跟踪上下文。它们还有一个名称、开始时间、结束时间、状态和一个属性包，其中包含表示操作详细信息的属性。
- en: Complex and distributed operations need multiple spans that describe at least
    each incoming and outgoing request. A group of such correlated spans that share
    the same `trace-id` is called a trace.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂和分布式操作需要多个Span来描述至少每个传入和传出的请求。一组具有相同`trace-id`的关联Span称为跟踪。
- en: Spans (also known as Activities in .NET) are created by many libraries and applications.
    To enable correlation, we need to propagate context within the process and between
    processes.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Span（在.NET中也称为活动）由许多库和应用创建。为了启用关联，我们需要在进程内和进程间传播上下文。
- en: In .NET, we use `Activity.Current` to propagate context within the process.
    This is a current span that flows with an execution context in synchronous or
    asynchronous calls. Whenever a new activity is started, it uses `Activity.Current`
    as its parent and then becomes current itself.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在.NET中，我们使用`Activity.Current`在进程内传播上下文。这是一个与执行上下文同步或异步调用一起流动的当前Span。每当启动一个新的活动时，它使用`Activity.Current`作为其父项，然后成为当前项。
- en: To propagate the trace context between the processes, we pass it over the wire
    to the next service. W3C Trace Context is a standard propagation format for the
    HTTP protocol, but some services use the B3 format.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 要在进程间传播跟踪上下文，我们将它通过线路传递给下一个服务。W3C跟踪上下文是HTTP协议的标准传播格式，但某些服务使用B3格式。
- en: 'There is no single answer to this question, but here’re some general considerations
    on how you can leverage a combination of signals coming from your service:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于这个问题没有唯一的答案，但以下是一些关于如何利用来自您服务的信号组合的一般考虑：
- en: Check whether the problem is widespread and affects more than this user and
    request. Is your service healthy overall? Is it specific to the API path the user
    hits, region, partition, feature flag, or new service version? Your observability
    backend might be able to assist with it
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查问题是否普遍且影响超过此用户和请求。您的服务整体是否健康？是否特定于用户触发的API路径、区域、分区、功能标志或新服务版本？您的可观察性后端可能能够帮助解决这个问题。
- en: If the problem is not widespread, find traces for problematic requests using
    trace context if it is known, or filtering by known attributes. If you see gaps
    in traces, retrieve logs for this operation. If that’s not enough, use profiling
    to investigate further. Consider adding more telemetry.
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果问题不是普遍的，如果已知，使用跟踪上下文查找有问题的请求，或者通过已知属性进行过滤。如果您在跟踪中看到差距，检索此操作的日志。如果这还不够，使用分析进一步调查。考虑添加更多遥测数据。
- en: For widespread issues, you might find the root cause of the problem by identifying
    specific attributes correlated with the reported problem.
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于普遍问题，您可能可以通过识别与报告问题相关的特定属性来找到问题的根本原因。
- en: Otherwise, narrow down the issue layer by layer. Are dependencies working fine?
    Is there something new upstream? Any changes in the load?
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，逐层缩小问题范围。依赖项是否运行正常？上游是否有新情况？负载是否有变化？
- en: If issues are not specific to any combination of attributes, check the dependency
    health and resource utilization. Check the crash and restart count, CPU load,
    memory utilization, extensive garbage collection, I/O, and network bottlenecks.
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果问题不是特定于任何属性组合，请检查依赖项健康状态和资源利用率。检查崩溃和重启次数、CPU负载、内存利用率、大量垃圾回收、I/O和网络瓶颈。
- en: Chapter 2 – Native Monitoring in .NET
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章 – .NET的本地监控
- en: 'Use `Activity.Current?.Id` on the page. For example, like this: `<``p>traceparent:
    <code>@System.Diagnostics.Activity.Current?.Id</code></p>`.'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '在页面上使用`Activity.Current?.Id`。例如，如下所示：`<p>traceparent: <code>@System.Diagnostics.Activity.Current?.Id</code></p>`。'
- en: If we have `dotnet-monitor` running as a sidecar, we can connect to its instance
    corresponding to the problematic service instance, check the metrics and logs,
    and create dumps. We could even configure `dotnet-monitor` to trigger a dump collection
    based on certain events or resource consumption thresholds.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们以sidecar的形式运行`dotnet-monitor`，我们可以连接到对应于有问题的服务实例的实例，检查指标和日志，并创建转储。我们甚至可以配置`dotnet-monitor`，使其基于某些事件或资源消耗阈值触发转储收集。
- en: If we don’t have `dotnet-monitor`, but can access service instances, we can
    install `dotnet-monitor` there and get diagnostics information from the running
    process.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们没有 `dotnet-monitor`，但可以访问服务实例，我们可以在那里安装 `dotnet-monitor` 并从运行进程获取诊断信息。
- en: If instances are healthy, but the problem is somewhere inside the telemetry
    pipeline, troubleshooting steps would depend on the tools we use. For example,
    with Jaeger we can check logs; the Prometheus UI shows connectivity with targets;
    the OpenTelemetry collector provides logs and metrics for self-diagnostics.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如果实例运行正常，但问题出在遥测管道内部，则故障排除步骤将取决于我们使用的工具。例如，使用 Jaeger 我们可以检查日志；Prometheus UI
    显示与目标的连接性；OpenTelemetry 收集器提供日志和指标以进行自我诊断。
- en: 'Query:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查询：
- en: '[PRE10]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The query sums up the request rates across all running service instances, grouping
    it by service name and `http_route` (which represents the API route).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 查询汇总了所有运行服务实例的请求速率，按服务名称和 `http_route`（表示 API 路由）进行分组。
- en: The rate function `(rate(http_server_duration_ms_count)` first calculates the
    rate per second, then averages the rate over one minute.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 速率函数 `(rate(http_server_duration_ms_count)` 首先计算每秒的速率，然后平均一分钟的速率。
- en: Search the traces with the URL and method filter in Jaeger. For uploads, it
    would be `http. url=http://storage:5050/memes/<name> http.method=PUT`. To find
    downloads, we would use `http.url=http://storage:5050/memes/<name> http. method=GET`.
    However, this isn’t convenient and we should consider adding the meme name as
    an attribute on all spans.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Jaeger 的 URL 和方法过滤器搜索跟踪。对于上传，将是 `http.url=http://storage:5050/memes/<name>
    http.method=PUT`。要查找下载，我们将使用 `http.url=http://storage:5050/memes/<name> http.method=GET`。然而，这并不方便，我们应该考虑将
    meme 名称作为所有跨度的一个属性。
- en: Chapter 3 – The .NET Observability Ecosystem
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章 – .NET 可观测性生态系统
- en: Check the registry ([https://opentelemetry.io/registry/](https://opentelemetry.io/registry/))
    and OpenTelemetry .NET repo. If you don’t see your library in any of them, search
    in issues and PRs. It’s also a good idea to search whether anything is available
    in the library GitHub repo or documentation.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查注册表 ([https://opentelemetry.io/registry/](https://opentelemetry.io/registry/))
    和 OpenTelemetry .NET 仓库。如果您在它们中看不到您的库，请在问题和 PR 中搜索。在库的 GitHub 仓库或文档中搜索是否有任何可用的内容也是一个好主意。
- en: 'When you find an instrumentation, there are several things to check for:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 当您找到仪器时，有几个方面需要检查：
- en: '**Version and stability**: Beta instrumentations could still have a high quality
    and be battle-tested but do not guarantee API or telemetry stability'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**版本和稳定性**：Beta 测试的仪器可能仍然具有高质量且经过实战检验，但不能保证 API 或遥测的稳定性'
- en: '**Performance and thread safety**: Understanding the mechanism behind instrumentation
    is important to identify possible limitations and issues in advance'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能和线程安全**：理解仪器背后的机制对于提前识别可能的限制和问题很重要'
- en: The most common way to instrument libraries and frameworks is `ActivitySource`—
    it’s the .NET analog of OpenTelemetry Tracer, which can start activities. You
    can configure OpenTelemetry to listen to a source by its name. You might also
    see instrumentations using `DiagnosticSource`—it’s an older and less structured
    mechanism available in .NET.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仪器化库和框架最常见的方式是 `ActivitySource`——它是 OpenTelemetry Tracer 的 .NET 等价物，可以启动活动。您可以通过名称配置
    OpenTelemetry 以监听源。您也可能看到使用 `DiagnosticSource` 的仪器化——它是在 .NET 中可用的一种较旧且结构较松散的机制。
- en: It’s also common to leverage hooks provided by libraries that can be global
    or applied to specific instances of the client.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 利用库提供的钩子也很常见，这些钩子可以是全局的，也可以应用于客户端的特定实例。
- en: Service meshes can trace requests to and from service mesh sidecars and provide
    insights into retries, service discovery, or load balancing. If they handle communication
    with cloud service, remote database, or queue, they can instrument corresponding
    communication. Service meshes can propagate the context from one application to
    another but cannot propagate it within the service from incoming to outgoing calls.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 服务网格可以跟踪服务网格边车之间的请求，并提供关于重试、服务发现或负载均衡的见解。如果它们处理与云服务、远程数据库或队列的通信，它们可以检测相应的通信。服务网格可以从一个应用程序传播上下文到另一个应用程序，但不能在服务内部从传入调用传播到传出调用。
- en: Chapter 4 – Low-Level Performance Analysis with Diagnostic Tools
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章 – 使用诊断工具进行低级性能分析
- en: If your service defines SLIs, check them first and see whether they are within
    the boundaries defined by your SLOs. In other words, check the key metrics that
    measure your user experience and see whether they are within healthy limits. For
    REST API-based services, it is usually the throughput of successful requests and
    latency grouped by API and other things that are important in your application.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您的服务定义了服务级别指标（SLIs），请首先检查它们，看看它们是否在您的服务级别目标（SLOs）定义的边界内。换句话说，检查衡量您用户体验的关键指标，看看它们是否在健康范围内。对于基于REST
    API的服务，通常是通过API和其他对您的应用程序重要的事情来分组成功的请求吞吐量和延迟。
- en: Resource consumption metrics could be correlated to user experience, but do
    not determine it. They (and other metrics that describe the internals of your
    service) can help you understand why the user experience has degraded and can
    predict future issues with some level of confidence.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 资源消耗指标可能与用户体验相关联，但并不决定它。它们（以及描述您服务内部结构的其他指标）可以帮助您了解用户体验为何下降，并可以在一定程度上预测未来的问题。
- en: 'First, we should try to find which service is responsible: check upstream and
    downstream services for whether the load on your service is normal and properly
    distributed across instances. Check whether dependencies are healthy using their
    server-side metrics when possible.'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们应该尝试找出哪个服务负责：检查上游和下游服务，看看您的服务的负载是否正常并且合理地分布在实例之间。当可能时，使用它们的服务器端指标检查依赖项是否健康。
- en: If we can narrow down the issue to a specific service, we can check whether
    the issue is specific to a certain instance or group of instances, or whether
    instances are restarting a lot. For affected instances, we can check their resource
    utilization patterns for memory, CPU, GC frequency, threads, contentions, or anything
    that looks unusually high or low. Then, we can capture a dump from the problematic
    instance(s) to analyze memory and thread stacks.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们可以将问题缩小到特定的服务，我们可以检查问题是否特定于某个实例或一组实例，或者实例是否频繁重启。对于受影响的实例，我们可以检查它们的资源利用率模式，包括内存、CPU、GC频率、线程、竞争，或任何看起来异常高或低的内容。然后，我们可以从有问题的实例中捕获转储以分析内存和线程栈。
- en: Performance tracing (also known as profiling or just tracing) is a technique
    that allows us to capture detailed diagnostics about application behavior and
    code – call stacks, GC, contention, network events, or anything else that .NET
    or third-party libraries want to expose. Such events are off by default but can
    be enabled and controlled inside the process and out-of-process. Tools such as
    `dotnet-trace`, `dotnet-monitor`, PerfView, PerfCollect, JetBrains dotTrace, Visual
    Studio, and continuous profilers can collect and visualize them. Performance tracing
    can be used to investigate functional and performance issues or optimize your
    code.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 性能追踪（也称为性能分析或简称为追踪）是一种技术，它允许我们捕获关于应用程序行为和代码的详细诊断信息——调用栈、垃圾回收、竞争、网络事件，或.NET或第三方库想要暴露的任何其他内容。这些事件默认是关闭的，但可以在进程内和进程外启用和控制。例如，`dotnet-trace`、`dotnet-monitor`、PerfView、PerfCollect、JetBrains
    dotTrace、Visual Studio和连续性能分析工具可以收集和可视化这些信息。性能追踪可用于调查功能和性能问题或优化您的代码。
- en: Chapter 5 – Configuration and Control Plane
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章 - 配置和控制平面
- en: We’d need tail-based sampling that’s applied after span or trace ends and we
    know the duration or if there were any failures. Tail-based sampling can’t be
    done inside the process since we have distributed multi-instance applications,
    but we can use a tail-based sampling processor in the OpenTelemetry Collector
    that buffers traces and then samples them based on latency, or status codes.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要基于尾部的采样，在跨度或追踪结束后应用，并且我们知道持续时间或是否有任何失败。由于我们拥有分布式多实例应用程序，基于尾部的采样无法在进程内进行，但我们可以使用OpenTelemetry
    Collector中的基于尾部的采样处理器，该处理器缓冲追踪并基于延迟或状态码进行采样。
- en: If we only capture suspicious traces, we will not have a baseline anymore –
    we won’t be able to use traces to observe normal system behavior, build analytics,
    and so on. So, we should additionally capture a percentage or rate of random traces
    – if we mark them somehow, we can analyze them separately from problematic traces
    to create unbiased analytics.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只捕获可疑的追踪，我们将不再有基线——我们将无法使用追踪来观察正常系统行为、构建分析等。因此，我们应该额外捕获一定比例或速率的随机追踪——如果我们以某种方式标记它们，我们可以将它们与有问题的追踪分开来创建无偏的分析。
- en: It’s always a good idea to rate-limit all traces, so we don’t overload the telemetry
    pipeline with traffic bursts.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 限制所有追踪的速率总是一个好主意，这样我们就不至于因为流量突增而超载遥测管道。
- en: In addition to sampling configuration on the OpenTelemetry Collector, we should
    consider configuring probability sampling on individual .NET services – depending
    on this, we would allocate an appropriate number of resources for Collector and
    also balance the performance impact of the instrumentation.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在 OpenTelemetry Collector 上进行采样配置之外，我们还应该考虑在单个 .NET 服务上配置概率采样——根据这一点，我们将为
    Collector 分配适当数量的资源，并平衡仪表化的性能影响。
- en: Let’s record a try number using the OpenTelemetry `http.resend_count` attribute
    that should be set on each HTTP span that represents a retry or redirect. We can
    use the `EnrichWithHttpRequestMessage` hook on the HTTP client instrumentation
    to intercept the outgoing request and its activity, but where would we get the
    retry number from?
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用 OpenTelemetry 的 `http.resend_count` 属性来记录尝试次数，该属性应设置在每个表示重试或重定向的 HTTP
    span 上。我们可以使用 HTTP 客户端仪表化的 `EnrichWithHttpRequestMessage` 钩子来拦截出站请求及其活动，但我们从哪里获取重试次数呢？
- en: 'Well, we can maintain it in our retry handler (if you use Polly, you could
    use `Context` instead) and pass it to the hook via `HttpRequestMessage.Options`.
    So, the final solution could look like this:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我们可以在我们的重试处理器中维护它（如果你使用 Polly，你可以使用 `Context` 代替）并通过 `HttpRequestMessage.Options`
    将其传递给钩子。所以，最终的解决方案可能看起来像这样：
- en: Program.cs
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Program.cs
- en: '[PRE12]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter5/memes/frontend/Program.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter5/memes/frontend/Program.cs)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter5/memes/frontend/Program.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter5/memes/frontend/Program.cs)'
- en: RetryHandler.cs
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: RetryHandler.cs
- en: '[PRE13]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter5/memes/frontend/RetryHandler.cs
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter5/memes/frontend/RetryHandler.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter5/memes/frontend/RetryHandler.cs)'
- en: 'Let’s check out the OpenTelemetry Collector documentation for tail-based sampling
    at https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/tailsamplingprocessor/README.md.
    We need to declare and configure the `tail_sampling` processor and add it to the
    pipeline. Here’s a sample configuration:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们查看 OpenTelemetry Collector 的基于尾部采样的文档，网址为 https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/tailsamplingprocessor/README.md。我们需要声明和配置
    `tail_sampling` 处理器并将其添加到管道中。以下是一个示例配置：
- en: otel-collector-config.yml
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: otel-collector-config.yml
- en: '[PRE14]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter5/memes/configs/otel-collector-config.yml](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter5/memes/configs/otel-collector-config.yml)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter5/memes/configs/otel-collector-config.yml](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter5/memes/configs/otel-collector-config.yml)'
- en: You can check your current rate of recorded spans using the `rate(otelcol_receiver_
    accepted_spans[1m]`) query in Prometheus and monitor the exported rate with the
    `rate(otelcol_exporter_sent_spans[1m])` query.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 Prometheus 中的 `rate(otelcol_receiver_ accepted_spans[1m])` 查询检查当前记录的 span
    的速率，并使用 `rate(otelcol_exporter_sent_spans[1m])` 查询监控导出的速率。
- en: Chapter 6 – Tracing Your Code
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 6 章 – 跟踪您的代码
- en: When setting up OpenTelemetry, you can enable `ActivitySource` by calling into
    the `TracerProviderBuilder.AddSource` method and passing the source name. OpenTelemetry
    will then create an `ActivityListener` – a low-level .NET API that listens to
    `ActivitySource` instances. The listener samples activities using the callback
    provided by OpenTelemetry and notifies OpenTelemetry when activities start or
    end.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在设置 OpenTelemetry 时，您可以通过调用 `TracerProviderBuilder.AddSource` 方法并传递源名称来启用 `ActivitySource`。然后，OpenTelemetry
    将创建一个 `ActivityListener` —— 这是一个低级别的 .NET API，它监听 `ActivitySource` 实例。监听器使用 OpenTelemetry
    提供的回调来采样活动，并在活动开始或结束时通知 OpenTelemetry。
- en: Activity (or span) events can be used to represent something that happens at
    a point in time or is too short to be a span and does not need individual context.
    At the same time, events must happen in the scope of some activity and are recorded
    along with it. Activity events stay in memory until the activity is garbage-collected
    and their number is limited on the exporter side.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 活动或 span 事件可以用来表示在某个时间点发生的事情，或者太短以至于不能成为 span，并且不需要单独的上下文。同时，事件必须在某些活动的范围内发生，并与其一起记录。活动事件将保留在内存中，直到活动被垃圾回收，并且它们在导出器侧的数量有限。
- en: Logs are usually a better alternative to `Activity` events as they are not necessarily
    tied to specific activity, sampling, or exporter limitations. OpenTelemetry treats
    events and logs similarly. Events expressed as log records are structured and
    can follow specific semantic conventions.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 日志通常比`Activity`事件更好，因为它们不一定与特定的活动、采样或导出器限制相关。OpenTelemetry将事件和日志视为类似。表示为日志记录的事件是结构化的，并可以遵循特定的语义约定。
- en: Links provide another way to correlate spans with cover scenarios when the span
    has multiple parents or is related in some way to several other spans at once.
    Without links, spans can only have one parent and multiple children and can’t
    be related to spans in other traces.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当跨度有多个父级或与多个其他跨度同时相关时，链接提供了另一种关联跨度与覆盖场景的方法。没有链接，跨度只能有一个父级和多个子级，并且不能与其他跟踪中的跨度相关。
- en: Links are used in messaging scenarios to express receiving or processing multiple
    independent messages at once. When we process multiple messages, we need to extract
    the trace context and create an `ActivityLink` from each of them. Then, we can
    pass a collection of these links to the `ActivitySource.StartActivity` method.
    We can’t change these links after the corresponding `Activity` starts. Observability
    backends support (or don’t support) links in different ways and we might need
    to adjust the instrumentation based on the backend capabilities.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在消息场景中，链接被用来表示同时接收或处理多个独立消息。当我们处理多个消息时，我们需要从每个消息中提取跟踪上下文并创建一个`ActivityLink`。然后，我们可以将这些链接的集合传递给`ActivitySource.StartActivity`方法。一旦相应的`Activity`开始，我们就不能更改这些链接。可观察性后端以不同的方式支持（或不支持）链接，我们可能需要根据后端能力调整仪器。
- en: Chapter 7 – Adding Custom Metrics
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章 – 添加自定义度量
- en: We should first decide what we need the metric for. For example, if we need
    it to rank memes in search results or to calculate ad hits, we should separate
    it from telemetry. Assuming we store the meme download counter in a database for
    business logic purposes, we could also stamp it on traces or events as an attribute
    when the counter is updated.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先应该决定我们需要度量做什么。例如，如果我们需要它来对搜索结果中的meme进行排名或计算广告点击，我们应该将其与遥测分开。假设我们为了业务逻辑目的将meme下载计数器存储在数据库中，我们也可以在计数器更新时将其作为属性标记在跟踪或事件上。
- en: From a telemetry-only standpoint, metric per meme would have high cardinality
    as we probably have millions of memes in the system and thousands active per minute.
    With some additional logic (for example, if we can ignore rarely accessed memes),
    we might even be able to introduce a metric with a meme name as an attribute.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 从仅遥测的角度来看，每个meme的度量会有很高的基数，因为我们可能系统中有成千上万的meme，每分钟有数千个活跃的。通过一些额外的逻辑（例如，如果我们能忽略很少访问的meme），我们甚至可能能够引入一个以meme名称作为属性的度量。
- en: I would start with traces and aggregate spans by meme name in a rich query.
    Even if traces are sampled, I can still calculate the estimated number of downloads,
    compare it between memes, and see trends.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我会从跟踪开始，通过丰富的查询按meme名称聚合跨度。即使跟踪被采样，我仍然可以计算估计的下载次数，比较不同meme之间的差异，并查看趋势。
- en: 'Usually, both, but it depends: we need incoming HTTP request traces to investigate
    individual failures and delays and know what normal request flow looks like under
    different conditions. Do we need metrics as well? Probably yes. At a high scale,
    we sample traces aggressively but likely need more precise data than estimated
    counts. Another problem is that even if we don’t sample or don’t mind rough estimates,
    querying over all spans during the time window can be expensive and long – it
    might need to process millions of records. If we build dashboards and alerts on
    this data, we want queries to be fast and cheap. Even if they are used for ad
    hoc analysis during incidents, we still want queries to be fast.'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通常，两者都需要，但这取决于：我们需要传入的HTTP请求跟踪来调查个别故障和延迟，并了解在不同条件下正常请求流的样子。我们还需要度量吗？可能需要。在高度缩放的情况下，我们积极采样跟踪，但可能需要比估计计数更精确的数据。另一个问题是，即使我们不采样或不在乎粗略估计，在时间窗口内查询所有跨度可能既昂贵又耗时——可能需要处理数百万条记录。如果我们在这个数据上构建仪表板和警报，我们希望查询既快又便宜。即使它们用于事件期间的临时分析，我们仍然希望查询速度快。
- en: So, the answer depends on the observability backend, what it is optimized for,
    and its pricing model, but collecting both gives us a good starting point.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，答案取决于可观察性后端，它优化了什么，以及其定价模型，但收集两者为我们提供了一个良好的起点。
- en: For the number of active instances, we can report `ObservableUpDownCounter`
    with resource attributes that include instance information. The counter would
    always report `1` so that the sum of values across all instances at any given
    time will represent the number of active processes. This is how Kubernetes does
    it with `kube_node_info` or `kube_pod_info` metrics (check out [https://github.com/kubernetes/kube-state-metrics](https://github.com/kubernetes/kube-state-metrics)
    for more information).
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于活动实例的数量，我们可以通过包含实例信息的资源属性来报告 `ObservableUpDownCounter`。计数器将始终报告 `1`，因此在任何给定时间所有实例的值之和将代表活动进程的数量。这就是Kubernetes使用
    `kube_node_info` 或 `kube_pod_info` 指标（有关更多信息，请参阅[https://github.com/kubernetes/kube-state-metrics](https://github.com/kubernetes/kube-state-metrics)）的方式。
- en: Uptime can be reported in multiple ways – for example, as a gauge containing
    static start time (see `kube_node_created` or `kube_pod_start_time`) or as a resource
    attribute.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 运行时间可以通过多种方式报告——例如，作为一个包含静态启动时间的仪表（参见 `kube_node_created` 或 `kube_pod_start_time`）或作为资源属性。
- en: Make sure to check whether your environment already emits anything similar or
    whether OpenTelemetry semantic conventions define a common way to report the metric
    you’re interested in.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 确保检查您的环境是否已经发出类似的内容，或者OpenTelemetry语义约定是否定义了一种报告您感兴趣指标的通用方式。
- en: Chapter 8 – Writing Structured and Correlated Logs
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章 - 编写结构化和关联日志
- en: 'The code uses string interpolation instead of semantic logging. A log message
    is formatted right away, so the `ILogger.Log` method is called underneath with
    the `"hello world: 43, bar"` string, without any indication that there are two
    arguments with specific names and values.'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '代码使用字符串插值而不是语义日志。日志消息立即格式化，因此 `ILogger.Log` 方法在下面被调用，使用 `"hello world: 43,
    bar"` 字符串，没有任何指示存在两个具有特定名称和值的参数。'
- en: If the `Information` level is disabled, string interpolation happens anyway,
    serializing all arguments and calculating just the message to be dropped.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如果禁用了 `Information` 级别，字符串插值仍然会发生，序列化所有参数，仅计算要丢弃的消息。
- en: 'This code should be changed to `logger.LogInformation("hello world: {foo},
    {bar}", 42, "bar")`.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '此代码应更改为 `logger.LogInformation("hello world: {foo}, {bar}", 42, "bar")`。'
- en: 'We need to make sure that the usage report is built using log record properties
    that don’t change:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要确保使用不改变的日志记录属性构建使用报告：
- en: A log message would change a lot when new arguments are added or code is refactored.
  id: totrans-252
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当添加新参数或重构代码时，日志消息会有很大变化。
- en: The logging category is usually based on a namespace, which might change during
    refactoring. We can consider passing categories explicitly as strings instead
    of a generic type parameter, but the better choice would be to make sure the report
    does not rely on logging categories. We can use event names or IDs – they have
    to be specified explicitly; we just need to make sure they are unique and don’t
    change. One approach would be to declare them in a separate file and document
    that the usage reports rely on them.
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录分类通常基于命名空间，这可能在重构期间发生变化。我们可以考虑明确以字符串形式传递分类，而不是通用类型参数，但更好的选择是确保报告不依赖于记录分类。我们可以使用事件名称或ID——它们必须明确指定；我们只需要确保它们是唯一的且不会改变。一种方法是在单独的文件中声明它们，并记录使用报告依赖于它们。
- en: Traces and logs describing HTTP requests contain similar information. Logs are
    more verbose, since we’d usually have human-readable text and need two records
    for one request (before and after it), with duplicated trace context and other
    scopes.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 描述HTTP请求的跟踪和日志包含类似的信息。日志更详细，因为我们通常会有可读性强的文本，并且需要为单个请求（在请求之前和之后）记录两条记录，带有重复的跟踪上下文和其他范围。
- en: If your application records all HTTP traces, there is no need to enable HTTP
    logging as well. If traces are sampled, there is a trade-off between the cost
    of capturing all telemetry and your ability to investigate rare issues. Many applications
    don’t really need to capture all telemetry to efficiently investigate problems.
    For them, collecting sampled traces without HTTP logs would be the best option.
    If you have to investigate rare issues, one option would be to increase the sampling
    rate for traces. Recording HTTP logs instead is another option that comes with
    an additional cost to collect, store, retrieve, and analyze logs.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的应用程序记录了所有HTTP跟踪，就没有必要启用HTTP日志记录。如果跟踪被采样，捕获所有遥测数据的成本与调查罕见问题的能力之间存在权衡。许多应用程序实际上不需要捕获所有遥测数据来有效地调查问题。对于他们来说，收集带有HTTP日志的采样跟踪将是最佳选择。如果你必须调查罕见的问题，一个选项是增加跟踪的采样率。记录HTTP日志是另一种选择，但这会带来额外的成本来收集、存储、检索和分析日志。
- en: Chapter 9 – Best Practices
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章 – 最佳实践
- en: HTTP traces, potentially combined with some application-specific attributes,
    can help answer most questions about tiny RESTful service behavior. We can aggregate
    metrics from traces using OpenTelemetry Collector or at query time on the backend.
    We still need metrics for resource utilization though. The right questions to
    ask here are how much this solution costs us and whether there is the potential
    to reduce costs with sampling and how much we must spend to keep alerts running
    based on queries over traces. If it’s a lot, then we should look into adding metrics.
    So, the answer is – yes, but it can be more cost-efficient to add other signals.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: HTTP跟踪，可能结合一些特定于应用程序的属性，可以帮助回答关于小型RESTful服务行为的大部分问题。我们可以使用OpenTelemetry Collector或后端查询时间从跟踪中聚合指标。尽管如此，我们仍然需要资源利用的指标。这里要问的正确问题是这个解决方案给我们带来了多少成本，以及是否有通过采样降低成本的可能性以及我们必须花费多少来保持基于跟踪查询的警报运行。如果是很多，那么我们应该考虑添加指标。所以，答案是——是的，但添加其他信号可能更经济高效。
- en: In an application under heavy load, every bug will happen again and again. No
    matter how small of a sampling rate we choose, we’ll record at least some occurrences
    of such an issue. A high sampling rate would likely have some performance impact,
    but more importantly, it’ll be very expensive to store all these traces. So, a
    small sampling rate should be the first choice.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在高负载的应用程序中，每个错误都会反复发生。无论我们选择多小的采样率，我们都会记录至少一些此类问题的发生。高采样率可能会对性能产生一些影响，但更重要的是，存储所有这些跟踪数据将非常昂贵。因此，小采样率应该是首选。
- en: Socket communication can be very frequent, so instrumenting every request with
    a span can create a huge overhead. A good starting point would be to identify
    how long a typical session lasts, and if it’s within seconds or minutes, instrument
    a session with a span. Small requests can be recorded with metrics on a service
    side, or sometimes with logs/events.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 套接字通信可能非常频繁，因此为每个请求都添加一个跨度可能会产生巨大的开销。一个好的起点是确定典型会话的持续时间，如果它在秒或分钟内，就可以使用跨度来记录会话。小请求可以通过服务端的指标记录，或者有时通过日志/事件记录。
- en: OpenTelemetry general and RPC semantic conventions should cover the necessary
    network attributes to represent the client and server and describe a request.
    We can also apply suitable RPC metrics to track duration and throughput.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: OpenTelemetry的一般和RPC语义约定应该涵盖表示客户端和服务器以及描述请求所需的必要网络属性。我们还可以应用合适的RPC指标来跟踪持续时间和吞吐量。
- en: Chapter 10 – Tracing Network Calls
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章 – 跟踪网络调用
- en: Reusing existing instrumentation should be the first choice, especially if you
    don’t have a lot of experience in both tracing and the gRPC stack. As you saw
    throughout this chapter, there are multiple details related to retries, the order
    of execution, and other tiny details that are hard to account for.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复使用现有的跟踪工具应该是首选，尤其是如果你在跟踪和gRPC堆栈方面没有太多经验。正如你在本章中看到的，与重试、执行顺序和其他难以考虑的微小细节相关的多个细节。
- en: Custom gRPC instrumentation makes sense if existing instrumentation does not
    satisfy your needs. For example, in our streaming experiments, we could optimize
    two layers of instrumentation (individual messages and gRPC calls) by merging
    them into one. We could also correlate requests, responses, and span events better
    if we knew the message types in the interceptor.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 如果现有的跟踪工具不能满足你的需求，自定义gRPC跟踪是有意义的。例如，在我们的流实验中，我们可以通过将它们合并为一个来优化两层跟踪（单个消息和gRPC调用）。如果我们知道拦截器中的消息类型，我们还可以更好地关联请求、响应和跨度事件。
- en: Note that even custom instrumentations benefit from following semantic conventions
    and relying on common tooling and documentation.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，即使是自定义的监控工具也受益于遵循语义约定并依赖于常见的工具和文档。
- en: In such an application, we should expect to see a very long span that describes
    a connection between the client and server. If we sample spans, we should customize
    the sampler to ensure we capture this span. Alternatively, we can just drop it
    and instead capture events that describe anything important that happens with
    the encompassing connection.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这样的应用程序中，我们应该预期看到一个非常长的跨度，描述客户端和服务器之间的连接。如果我们采样跨度，我们应该定制采样器以确保我们捕获这个跨度。或者，我们也可以简单地丢弃它，转而捕获描述包含连接中发生的任何重要事件的事件。
- en: 'Then, we should think about how/whether to trace individual messages. If they
    are very small and fast, tracing them individually could be too expensive because
    of a couple of concerns:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们应该考虑如何/是否跟踪单个消息。如果它们非常小且速度快，由于几个担忧，单独跟踪它们可能太昂贵：
- en: The first concern is message size. Trace context can be propagated frugally
    with the binary format, but still would require at least 26 bytes. You can be
    creative and come up with even more frugal format, propagating the message index
    instead of the span ID over the wire. The easiest solution would be to propagate
    context only for sampled-in messages and rely on metrics and events to see the
    overall picture.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个关注点是消息大小。跟踪上下文可以用二进制格式节俭地传播，但仍然至少需要26个字节。你可以发挥创意，提出更节俭的格式，通过在网络上传播消息索引而不是跨度ID。最简单的解决方案是只为采样进入的消息传播上下文，并依赖于指标和事件来了解整体情况。
- en: The second concern is performance overhead. If your processing is very fast,
    tracing it might be too expensive. Sampling can help offset some of these costs,
    but you probably don’t need to trace individual messages. Logs and events might
    give you the right level of observability, and you can correlate them with a message
    identifier.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个关注点是性能开销。如果你的处理非常快，跟踪它可能太昂贵了。采样可以帮助抵消一些这些成本，但你可能不需要跟踪单个消息。日志和事件可能提供适当的可观察性，并且你可以将它们与消息标识符相关联。
- en: If your message processing is complex and involves other network calls, you’d
    benefit from mitigating these concerns and tracing individual messages.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的消息处理复杂且涉及其他网络调用，你将受益于减轻这些担忧并跟踪单个消息。
- en: Chapter 11 – Instrumenting Messaging Scenarios
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章 – 监控消息场景
- en: The most difficult part is finding operations that are important to measure.
    In our example, it’s the time between when the meme is uploaded and when it became
    available for other users.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最困难的部分是找到重要的操作来进行测量。在我们的例子中，是模因上传和可供其他用户访问之间的时间。
- en: We can emit a couple of events to capture these two timestamps along with the
    meme identifier and any other context. Then, we can find the delta by joining
    events on the meme identifier.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以发出几个事件来捕获这两个时间戳以及模因标识符和任何其他上下文。然后，我们可以通过在模因标识符上连接事件来找到delta。
- en: Another option is to record the timestamp of when the meme was published along
    with the meme metadata and pass it around our system. Then, we can report delta
    as a metric or an event.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选项是记录模因发布的时间戳以及模因元数据，并在我们的系统中传递。然后，我们可以将delta作为指标或事件进行报告。
- en: When using batching, it’s usually interesting to know the number of messages
    in a batch and the payload size. By tuning these numbers, we can reduce network
    overhead, so having them readily available in the telemetry can be very useful.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当使用批处理时，通常想知道批次中的消息数量和有效载荷大小。通过调整这些数字，我们可以减少网络开销，因此，在遥测中随时可用这些信息非常有用。
- en: 'The key question is what instrument to use: a counter or histogram (a gauge
    would not fit here).'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 关键问题是使用哪种监控工具：计数器或直方图（仪表在这里不适用）。
- en: We can count messages and batches with two metrics. The ratio between them would
    give us the average batch size.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用两个指标来计数消息和批次。它们之间的比率将给出平均批次大小。
- en: We can also record the number of messages in a batch and the payload size as
    histograms. This would give us a distribution in addition to average numbers.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以记录批次中的消息数量和有效载荷大小作为直方图。这将给我们一个分布，除了平均数之外。
- en: I was tempted to record the batch size as an attribute on existing metrics but
    decided against it. In a general case, it’s a high-cardinality attribute, which
    is also hard to visualize in Prometheus; it would make more sense as a separate
    metric.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾想将批处理大小记录为现有指标的一个属性，但最终决定不这么做。在一般情况下，它是一个高基数属性，在Prometheus中可视化也比较困难；作为单独的指标可能更有意义。
- en: Baggage represents application-specific context-propagated services. If you
    have a need to propagate it across messaging systems, it can be injected into
    each message with the OpenTelemetry propagator similar to trace context.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 行李代表应用程序特定的上下文传播服务。如果你需要跨消息系统传播它，可以使用OpenTelemetry传播器将其注入到每个消息中，类似于跟踪上下文。
- en: Baggage usually does not need to flow to the messaging system, but it may be
    hard to prevent it. Attached to every message, it might create a significant overhead
    in terms of the payload size, so make sure to account for it and be ready to make
    trade-offs.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 行李通常不需要流向消息系统，但可能难以防止。附加到每个消息上，它可能在有效载荷大小方面造成显著的开销，所以请确保考虑到这一点，并准备好做出权衡。
- en: On the consumption side, things get more interesting. If messages are processed
    independently, make sure to restore baggage from the message when processing it.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在消费端，事情变得更有趣。如果消息是独立处理的，确保在处理消息时从消息中恢复行李信息。
- en: For batch processing, there is no single answer. Merging baggage from multiple
    messages may or may not make sense in your application.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 对于批处理，没有单一的答案。从多个消息中合并行李在你的应用程序中可能或可能没有意义。
- en: If you want to stamp baggage information on your telemetry, one option could
    be to record known baggage values on link attributes along with message-specific
    information.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在你的遥测信息上标记行李信息，一个选项可能是记录已知行李值在链接属性中，以及与消息特定的信息一起。
- en: Chapter 12 – Instrumenting Database Calls
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章 – 仪器化数据库调用
- en: The concept of a database change feed is similar to messaging, and we can apply
    the same approach we used in the previous chapter for it. The key question is
    how to propagate context and correlate operations that change the record and process
    the notification.
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据库变更流的概念与消息类似，我们可以应用之前章节中使用的相同方法。关键问题是如何传播上下文以及关联更改记录的操作和处理通知。
- en: One solution would be to add a record identifier attribute and use it to find
    all the operations related to a specific record. When multiple operations concurrently
    modify the same record, it will generate multiple notifications and we won’t be
    able to map producer operations to notification processing with the record ID.
    There might be additional notification identifiers we can use, such as record
    ETags. But in general cases, correlating operations that modify data and ones
    that process corresponding notifications would mean we have to add a trace context
    to the record and modify it on every operation.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 一个解决方案可能是添加一个记录标识符属性，并使用它来查找与特定记录相关的所有操作。当多个操作同时修改同一记录时，它将生成多个通知，我们无法使用记录ID将生产者操作映射到通知处理。可能还有其他通知标识符可以使用，例如记录ETags。但在一般情况下，关联修改数据的操作和处理相应通知的操作意味着我们必须将跟踪上下文添加到记录中，并在每次操作中对其进行修改。
- en: The answer depends on how your tracing backend treats events and how mature,
    robust, and reliable the cache configuration and infrastructure are.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案取决于你的追踪后端如何处理事件，以及缓存配置和基础设施的成熟度、健壮性和可靠性。
- en: 'Arguments for using events would be the following:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 使用事件的论点如下：
- en: Spans/activities have a slightly bigger performance overhead than events. Events
    also could be smaller in terms of telemetry volume.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与事件相比，跨度/活动有稍微大的性能开销。在遥测量方面，事件也可能更小。
- en: We don’t need the precise Redis duration for each operation since we have logical
    layer activity tracing composite calls and Redis metrics.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不需要每个操作的精确Redis持续时间，因为我们有逻辑层活动跟踪复合调用和Redis指标。
- en: 'The status of individual Redis calls is not very important: a set operation
    is even done in a fire-and-forget manner. It only matters when the failure rate
    increases significantly, but we’d see it in the metrics.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个Redis调用的状态并不非常重要：一个设置操作甚至是以火速忘掉的方式完成的。只有当失败率显著增加时，这才有关系，但我们会从指标中看到它。
- en: The argument to use spans is that it’s more common and convenient because tracing
    backends do a much better job at visualizing spans and performing any automated
    analysis on them.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用跨度（spans）的论点是，它更常见且方便，因为追踪后端在可视化跨度以及对其执行任何自动化分析方面做得更好。
- en: To remove limits, remove the `deploy` section under the `mongo` container in
    `docker-compose.yml`. If you run the application and kill Redis, you’ll see that
    MongoDB can easily handle the load and throughput changes, which might mean that
    Redis is not necessary in an application with such a small load.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 要移除限制，请从`docker-compose.yml`中的`mongo`容器下删除`deploy`部分。如果你运行应用程序并杀死Redis，你会看到MongoDB可以轻松处理负载和吞吐量变化，这可能意味着在如此小的负载的应用程序中Redis不是必需的。
- en: Chapter 13 – Driving Change
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13章 – 推动变革
- en: 'Using a single backend for all signals has certain advantages. It should be
    easier to navigate between signals: for example, get all logs correlated with
    the trace, query events, and traces together with additional context, and jump
    from metrics to trace with exemplars. So, using a single backend would reduce
    cognitive load and minimize duplication in backend-related configuration and tooling.'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用单个后端处理所有信号有一定的优势。在信号之间导航应该更容易：例如，获取所有与跟踪相关的日志，查询事件和跟踪以及额外的上下文，以及使用示例从指标跳转到跟踪。因此，使用单个后端将减少认知负荷并最小化与后端相关的配置和工具的重复。
- en: Using multiple backends can help reduce costs. For example, it’s usually possible
    to store logs in a cheaper log management system, assuming you already have everything
    up and running for logs and metrics. But these backends don’t always support traces
    well. Adding a new backend for traces and events only would make total sense.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多个后端可以帮助降低成本。例如，假设你已经为日志和指标设置了所有必要的基础设施，通常可以将日志存储在更便宜的日志管理系统。但是，这些后端并不总是很好地支持跟踪。仅添加用于跟踪和事件的新的后端才真正有意义。
- en: Tools such as Grafana may be able to provide a common UX on top of different
    backends to mitigate some of the disadvantages.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 工具如Grafana可能能够在不同后端之上提供统一的UX，以减轻一些不利因素。
- en: 'There are a few things that we need to do:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要做几件事情：
- en: '**Lock down the context propagation format**: Using W3C Baggage spec is a good
    default choice unless you already have something in place. It should be documented
    and, ideally, implemented and configured in internal common libraries shared by
    all services in your application.'
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**锁定上下文传播格式**：使用W3C Baggage规范是一个好的默认选择，除非你已经有现成的方案。它应该被文档化，理想情况下，在内部共享库中实现和配置，该库由你的应用程序中的所有服务共享。'
- en: '**Documenting key naming patterns**: Make sure to use namespaces and define
    the root one for your system. It’ll help filter everything else out. Document
    several common properties you want to put there – we want to make sure people
    use them and don’t come up with something custom. Adding helper methods to populate
    them would also be great.'
  id: totrans-300
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**记录关键命名模式**：确保使用命名空间并定义系统的根命名空间。这将有助于过滤其他所有内容。记录几个你想要放入那里的常见属性——我们希望确保人们使用它们，而不是想出一些自定义的东西。添加填充这些属性的辅助方法也会很棒。'
- en: '**Use common artifacts**: If you want to stamp baggage on telemetry, customize
    propagation, or just unify baggage keys, make sure to ship common internal libraries
    with these features.'
  id: totrans-301
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用通用工件**：如果你想对遥测进行标记、定制传播或只是统一工件键，确保提供具有这些功能的通用内部库。'
- en: When adding a cache, we’re probably trying to reduce the load on a database
    and optimize the service response time. We should already have observability of
    service and database calls and can see whether the cache would help.
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当添加缓存时，我们可能试图减少数据库的负载并优化服务响应时间。我们应该已经对服务和数据库调用有可观察性，并可以看到缓存是否有所帮助。
- en: If we roll this feature out gradually and conditionally, we need to be able
    to filter and compare telemetry based on feature flags, so we need to make sure
    they’re recorded.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们逐步且条件性地推出这个功能，我们需要能够根据功能标志过滤和比较遥测数据，因此我们需要确保它们被记录。
- en: Finally, we should make sure we have telemetry around the cache that will help
    us understand how it works, and why it did not work if it fails. Adding this telemetry
    along with feature code will have the biggest positive impact during development,
    testing, and initial iterations.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们应该确保我们围绕缓存有遥测数据，这将帮助我们理解其工作原理，以及为什么它失败时没有工作。在开发、测试和初始迭代期间，添加这些遥测数据以及功能代码将产生最大的积极影响。
- en: Chapter 14 – Creating Your Own Conventions
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第14章 – 创建你自己的约定
- en: A possible solution is to define and document the stability level for attributes.
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一种可能的解决方案是定义和记录属性的稳定性级别。
- en: For example, new conventions are always added at the alpha stability level.
    Once it’s fully implemented and deployed, and you’re mostly happy with the outcome,
    the convention can be graduated to beta.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，新约定总是在 alpha 稳定性级别添加。一旦完全实施并部署，并且你对结果大多满意，该约定就可以升级到 beta 版。
- en: Conventions should stay in beta until someone tries to use them for alerts,
    reports, or dashboards. If it works fine, or after feedback is addressed, the
    convention becomes stable. After that, it cannot be changed in a breaking manner.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 约定应该在 beta 版中保持，直到有人尝试使用它们进行警报、报告或仪表板。如果运行良好，或者反馈得到解决后，约定变为稳定。之后，它不能以破坏性的方式更改。
- en: It should be possible to validate actual telemetry to some extent.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该能够在某种程度上验证实际的遥测数据。
- en: For example, it should be possible to write a test processor (an in-process
    one or a custom collector component) that identifies specific spans, events, or
    metrics that should follow the convention and checks whether the conventions are
    applied consistently. This test processor could warn about issues found, flag
    unknown attributes, notify when expected signals were not received, and so on.
    It should be possible to run it as a part of integration testing in the CI pipeline.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，应该能够编写一个测试处理器（一个进程内处理器或自定义收集器组件），该处理器可以识别应该遵循约定的特定跨度、事件或指标，并检查这些约定是否得到一致的应用。这个测试处理器可以警告发现的问题，标记未知属性，在未收到预期信号时通知，等等。应该能够将其作为
    CI 管道中集成测试的一部分运行。
- en: Another approach is to just do a regular audit on a random subset of production
    telemetry, which could also be automated.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是对生产遥测的随机子集进行常规审计，这也可以自动化。
- en: Chapter 15 – Instrumenting Brownfield Applications
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 15 章 – 仪器化棕色地带应用程序
- en: Such a service is a good candidate for migration to OpenTelemetry – since we
    still update it, there is probably a reasonable test infrastructure and the context
    within the team to prevent and mitigate failures. As a first option, we should
    consider adding OpenTelemetry with network instrumentation and then gradually
    migrating existing tools and processes onto the new observability solution, while
    evolving an OpenTelemetry-based approach.
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这样的服务是迁移到 OpenTelemetry 的良好候选者——因为我们仍在更新它，所以可能有一个合理的测试基础设施和团队内部的环境来防止和减轻故障。作为一个首选方案，我们应该考虑添加带有网络仪器的
    OpenTelemetry，然后逐步将现有工具和流程迁移到新的可观察性解决方案，同时发展基于 OpenTelemetry 的方法。
- en: We can control the costs of this approach with sampling, enabling and moving
    only essential pieces onto OpenTelemetry. At some point, when we can rely on the
    new observability solution, we can remove corresponding legacy reporting.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过采样来控制这种方法的成本，只将必要的部分移动到 OpenTelemetry。在某个时候，当我们能够依赖新的可观察性解决方案时，我们可以移除相应的旧版报告。
- en: It’s likely that the .NET runtime version that the legacy service runs on is
    older than .NET 4.6.2, and then it’s impossible to use OpenTelemetry. Even if
    a newer version of .NET Framework is used, adding new dependencies, such as `System.Diagnostics.DiagnosticSource`
    and the different `Microsoft.Extensions` packages that OpenTelemetry brings transitively,
    can cause runtime problems due to version conflicts.
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 很可能，旧版服务运行的 .NET 运行时版本早于 .NET 4.6.2，那么使用 OpenTelemetry 就是不可能的。即使使用较新的 .NET Framework
    版本，添加新的依赖项，如 `System.Diagnostics.DiagnosticSource` 和 OpenTelemetry 带来的不同 `Microsoft.Extensions`
    包，也可能由于版本冲突而导致运行时问题。
- en: Other risks come from small changes and shifts in how an application works and
    its performance, waking up or amplifying dormant issues such as race conditions,
    deadlocks, or thread pool starvation.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 其他风险来自于应用程序的工作方式和性能中的微小变化和转变，唤醒或放大沉睡的问题，如竞态条件、死锁或线程池饥饿。
- en: If you can add newer versions of `System.Diagnostics.DiagnosticSource` as a
    dependency, then using `Activity` is an option.
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你可以将 `System.Diagnostics.DiagnosticSource` 的新版本作为依赖项添加，那么使用 `Activity` 就是一个选择。
- en: Note that the `Activity` class is available in .NET, starting with the `DiagnosticSource`
    package version 4.4.0 and .NET Core 3.0; however, it went through a lot of changes.
    Most of the functionality we covered in this book, including W3C Trace Context,
    was not available in the initial versions.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`Activity` 类从 .NET 的 `DiagnosticSource` 包版本 4.4.0 和 .NET Core 3.0 开始可用；然而，它经历了很多变化。本书中涵盖的大多数功能，包括
    W3C 跟踪上下文，在初始版本中都是不可用的。
- en: With newer `DiagnosticSource` versions, by using `Activity`, we would modify
    trace context – instead of passing `traceparent` as is, we would create server
    and client spans and then pass an ancestor of the original `traceparent` to the
    downstream service. If the legacy service does not report spans to the common
    observability backend, we’ll see correlated traces, but with missing parent-child
    relationships, as we saw in *Figure 15.4*.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 使用较新的`DiagnosticSource`版本，通过使用`Activity`，我们会修改跟踪上下文——而不是原样传递`traceparent`，我们会创建服务器和客户端跨度，然后将原始`traceparent`的祖先传递给下游服务。如果遗留服务没有向公共可观察性后端报告跨度，我们将看到相关的跟踪信息，但会缺少父子关系，正如我们在*图15.4*中看到的。
- en: So, we need to have full-fledged distributed tracing implemented or, if no traces
    are reported, pass context through as is, without using `Activity` for it.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要实现完整的分布式跟踪，或者如果没有跟踪信息被报告，就原样传递上下文，而不使用`Activity`。
