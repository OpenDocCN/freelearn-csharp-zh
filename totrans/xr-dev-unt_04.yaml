- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AR Development in Unity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will immerse ourselves in the fascinating realm of AR development,
    from creating our first AR project in Unity to launching our first AR scene on
    a device or simulator. We will present to you numerous AR toolkits and plugins
    that Unity offers, and guide you in understanding their unique functionalities.
  prefs: []
  type: TYPE_NORMAL
- en: In a step-by-step manner, we will walk through the process of establishing an
    AR project in Unity, ensuring it is primed for smooth deployment onto any AR-supportive
    device.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the AR landscape
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up an AR project in Unity using AR Foundation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing AR experiences directly in Unity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying AR experiences onto mobile devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we dive into the practicality of the Unity Editor, it’s important to
    ensure your computer system is up to the task. *Unity 2021.3 LTS*, or a more recent
    version, is required to walk through the exercises that we’ll explore in this
    book. Check your hardware compatibility by comparing it with the system requirements
    provided on the Unity website at [https://docs.unity3d.com/Manual/system-requirements.html](https://docs.unity3d.com/Manual/system-requirements.html).
  prefs: []
  type: TYPE_NORMAL
- en: As we’ll be exploring AR development in this chapter, we will need either an
    Android or iOS device capable of supporting ARKit or ARCore. Review whether your
    device meets these requirements at [https://developers.google.com/ar/devices](https://developers.google.com/ar/devices).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the AR landscape
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we start our exploration of AR, it’s crucial to first understand the foundational
    elements that enable this technology. How is it that our everyday devices can
    so effortlessly intertwine our physical reality with the digital? What mechanisms
    allow your device to sense, interpret, and interact with the world around it?
    And, perhaps most intriguingly, how can a simple screen transform into a doorway
    to an enhanced reality?
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we aim to unpack the complex principles and mechanisms of AR,
    distilling them into a comprehensible format.
  prefs: []
  type: TYPE_NORMAL
- en: If terms such as AR, MR, and VR still seem opaque or interchangeable to you,
    consider revisiting [*Chapter 1*](B20869_01.xhtml#_idTextAnchor000) for clarification.
    For now, our focus remains on AR, which transforms our world by superimposing
    it with elements of the digital domain. Let’s look at the different types of experiences
    that AR offers.
  prefs: []
  type: TYPE_NORMAL
- en: What types of AR experiences exist?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The AR landscape is diverse, with experiences typically presenting themselves
    through one of several mediums: handheld mobile AR, AR glasses, or other types
    of AR such as projection-based AR or spatial AR. Each form has its unique characteristics,
    and their utilization depends on the context and the level of immersion desired.
    Let’s learn more about these:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Handheld mobile AR**: Handheld mobile AR is perhaps the most widespread form
    of AR due to the ubiquity of smartphones. Imagine this type of AR as a window
    into an enriched reality. Through the screen of their smartphone or tablet, a
    user witnesses a mingling of the digital and the real. This overlay of digital
    content on a live camera feed breathes life into an otherwise static physical
    world. A prime example of this is the popular game *Pokémon Go*, where the user
    hunts for digital creatures that seem to inhabit our own world.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AR glasses**: AR glasses, on the other hand, provide a more immersive and
    hands-free AR experience. When the user dons these glasses, they step directly
    into an augmented world without the need for a separate device. Thanks to transparent
    displays, sensors, and cameras integrated into the glasses, digital information
    is seamlessly woven into the user’s field of view. The implications of this technology
    are far-reaching, with potential applications in industries ranging from manufacturing
    and healthcare to entertainment. An example of a game designed for AR glasses
    is a new iteration of the Pokémon Go game, aptly named *Pokémon Go AR+*. Pokémon
    Go AR+ revolutionizes the original concept of the game, taking full advantage
    of AR glasses’ capabilities. When wearing AR glasses, players are immersed in
    the Pokémon world more deeply. They can see Pokémon in their real-world surroundings
    as if they were actually there. Pokéstops and Gyms are visible in real-world locations,
    and players can interact with them directly. For example, if a Pokémon appears,
    the player can reach out to touch it and initiate a catch sequence. The game also
    allows for real-time battle simulations with other players using the AR environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Though both handheld mobile AR and AR glasses act as pathways to AR, they differ
    significantly in their form factor, user experience, and level of immersion. Mobile
    AR serves as a portable gateway to an augmented world, accessible through a user’s
    smartphone or tablet. On the other hand, AR glasses offer a fully immersive experience
    where the augmented world is in direct view. Given the prevalence of smartphones,
    this book will primarily focus on AR development for Android and iOS handheld
    devices.
  prefs: []
  type: TYPE_NORMAL
- en: This does not mean, however, that AR glasses lack potential or usage. The focus
    on handheld devices mainly reflects their current wider use and accessibility.
    Despite this, AR glasses present significant opportunities and a level of immersion
    that handheld devices can’t match. Even though the book focuses on handheld AR,
    the fundamental principles of AR development it covers, such as understanding
    the 3D space, user interaction, and user experience design, are largely applicable
    to AR glasses. Readers interested in AR glasses development can still gain valuable
    insights, though they might need to supplement their learning with additional
    resources specifically focused on AR glasses technologies.
  prefs: []
  type: TYPE_NORMAL
- en: '**Projection-based AR**: Projection-based AR casts digital imagery onto real-world
    surfaces. A notable application of projection-based AR is found in the automotive
    industry, where AR has started to make a significant impact. Information such
    as navigation, speed, and other essential data can be projected onto the vehicle’s
    windshield, providing real-time visual cues to the driver without the need to
    take their eyes off the road.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spatial AR**: Spatial AR uses holographic displays to create an illusion
    of virtual objects cohabitating in our physical environment. Holographic displays
    could be likened to digital mirages, using a combination of light projection and
    optics to create three-dimensional virtual objects that appear to float in space.
    This form of AR doesn’t necessitate additional devices or wearables, enabling
    the user to interact with the holograms as if they were physically present.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having explored the types of AR experiences, let’s now delve into the techniques
    and concepts that enable the overlay of virtual objects in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: What are marker-based and markerless AR?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AR is underpinned by a set of foundational principles that allow virtual objects
    to be accurately positioned and realistically interacted with in our physical
    world. These principles involve various technologies and techniques, such as **marker-based
    AR** and **markerless AR**.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now learn more about each of these technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Marker-based AR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Marker-based AR depends on specific markers or targets to initiate the display
    of AR content. These markers can take numerous forms, such as physical objects
    with identifiable patterns, QR codes, or images. When the AR system detects the
    marker, it overlays digital content onto it.
  prefs: []
  type: TYPE_NORMAL
- en: 'When developing AR applications, there are several types of markers that can
    be used to trigger the display of AR content. Here’s an overview of some commonly
    utilized markers in AR:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image markers**: These are distinct visual patterns or images that serve
    as the trigger for AR content. When an AR system detects these markers through
    the camera, it overlays the corresponding digital content onto them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**QR codes**: Quick response (QR) codes, which are two-dimensional barcodes
    containing specific information, can also serve as AR markers. When the camera
    or a QR code scanning library identifies these codes, it can trigger the display
    of certain AR content or interactions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3D object markers**: These markers involve using specific physical objects
    as triggers for AR content. The AR system identifies the object’s shape and features
    and uses this data to overlay the digital content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Location-based markers**: These markers use the user’s geolocation data to
    trigger AR content. By leveraging GPS or other location-tracking technologies,
    the AR system can overlay digital content that is relevant to the user’s current
    location.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code-based markers**: These are custom-designed patterns or symbols that
    can be recognized by the AR system and used as triggers. These markers are created
    and decoded using specific algorithms, providing a high degree of flexibility
    and customization for AR experiences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The type of marker to use depends on the specific requirements of your AR application.
    Factors such as the desired user experience, tracking accuracy, and ease of marker
    recognition will guide marker selection.
  prefs: []
  type: TYPE_NORMAL
- en: While many AR experiences use markers, not all do, as you will learn in the
    next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Markerless AR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Markerless AR, also known as **position-based AR** or **simultaneous localization
    and mapping** (**SLAM**)-based AR, doesn’t rely on predetermined markers or visual
    cues. It employs onboard sensors and complex algorithms to overlay digital information
    onto the physical world. Sensors can include **global positioning systems** (**GPS**),
    accelerometers, and cameras to ascertain the user’s location and orientation in
    the physical environment. With an understanding of the user’s location, these
    AR systems overlay virtual content onto the physical surroundings based on geographical
    coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s delve into various options for implementing markerless AR and explore
    their real-world applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Geolocation-based AR**: This approach primarily uses GPS and is suitable
    for placing AR objects on a larger scale in outdoor environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An example of this is Niantic’s game Pokémon Go. Using the device’s GPS, the
    game places virtual Pokémon creatures in real-world locations, allowing players
    to find and catch them.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Wi-Fi positioning system** (**WPS**): This method determines the device’s
    location based on the strength and origin of Wi-Fi signals. It’s especially relevant
    for indoor AR experiences where GPS may be less effective. For instance, *Indoor
    Atlas* offers a platform for indoor navigation that combines magnetic information
    and Wi-Fi signals. This has been used to enhance AR experiences in shopping malls,
    guiding the user to specific stores or attractions with digital markers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bluetooth** and **ultra-wideband** (**UWB**): These positioning techniques
    are geared for micro-location experiences, providing high precision in smaller
    spaces such as rooms or exhibits. A practical application can be seen in museums
    and galleries, where Bluetooth beacons are used in AR apps. These apps then serve
    multimedia content to visitors based on their proximity to specific artworks or
    exhibits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SLAM**: SLAM is a more advanced technique that creates a digital map of the
    environment while tracking the user’s location. This technique involves complex
    algorithms and uses the device’s camera and other sensors. Imagine you’re in a
    dark room and you light up a flashlight. As you move the flashlight around, you
    start to see and remember where different things are, such as chairs or tables.
    Over time, you build a map in your head of the whole room. SLAM does something
    similar. It’s most suitable for applications that require accurate object placement
    and interaction in smaller spaces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An example of SLAM is IKEA’s app, *IKEA Place*. On the app, the user can select
    a piece of furniture from IKEA’s catalog and the app overlays a 3D model of it
    onto the camera view, allowing the user to see how the item would look in their
    home.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Depth sensing**: This involves the use of advanced sensors such as **time-of-flight**
    (**ToF**) or **light detection and ranging** (**LIDAR**) sensors to capture the
    depth information of the surrounding environment. This method allows for more
    accurate placement and occlusion of virtual objects, where digital objects can
    correctly appear behind real-world objects based on depth information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An example of this is Apple’s *ARKit 4.0* platform, which incorporates the **Depth
    API** that leverages the LIDAR scanner available on some iPad and iPhone models.
    Depth API enables more realistic AR experiences. The *Complete Anatomy* app uses
    ARKit’s depth sensing to place a detailed 3D human anatomy model into the real
    world, allowing the user to explore and interact with it as if it were physically
    present. Because of depth sensing, this model won’t accidentally appear halfway
    inside your sofa but will stand correctly beside it.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Machine learning and AI**: Recent advances in AI and machine learning have
    opened new possibilities for markerless AR. Machine learning models can be trained
    to recognize different types of environments and objects, providing context for
    more intelligent and interactive AR experiences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An example of this is Google’s *ARCore* platform, which uses machine learning
    to recognize and augment specific objects or types of objects. Google’s *AR Animals*
    feature uses ARCore to let the user search for an animal on Google and then view
    a 3D model of the animal in their space via AR.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Markerless AR offers immense possibilities for creating immersive and interactive
    AR experiences. Whether used in gaming, interior design, education, or a myriad
    of other applications, it has the potential to revolutionize how we interact with
    the digital world.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding AR input types for interaction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Stepping into the realm of AR, one swiftly realizes that it’s not just about
    the mesmerizing blend of physical and virtual realities that one can see. It’s
    equally about how one can interact with these layered digital augmentations, a
    dimension defined by AR inputs. These inputs — modes by which the user interacts
    with the AR content — serve as a linchpin that shapes the overall AR experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we venture further into this discussion, let’s shine a spotlight on various
    AR input types and how they breathe life into real-world applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Touch input**: Touch input is a fundamental AR interaction. Simply put, the
    user can engage with the digital overlay through touch gestures on their AR device
    screen, be it a smartphone or a tablet. Touch input in AR includes not just tapping,
    but also other gestures such as swiping, pinching, and dragging. The specific
    gestures that can be used will depend on how the AR application is programmed.
    For example, a pinch gesture might be used to zoom in or out on an AR object,
    a swipe might rotate the object, and a drag could move the object around in the
    AR scene. The goal is to make the interaction with the AR elements as intuitive
    and natural as possible. *Snapchat lenses* provide a classic example of touch
    inputs at work. The user can animate the AR filters or induce changes by merely
    tapping different screen areas.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Device motion**: Device motion is another pivotal AR input. By harnessing
    the data from onboard accelerometers, gyroscopes, and magnetometers, AR applications
    can interpret the orientation and movement of the device as an input. This input
    type proves particularly useful for AR experiences that involve maneuvering through
    an environment or controlling virtual elements. A case in point is the game Pokémon
    Go, where the player can simulate the act of *throwing* Pokéballs by swinging
    their device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Voice commands**: Voice commands infuse AR applications with hands-free and
    accessibility-friendly interaction. *Google Glass*, an AR eyewear device, employs
    voice commands as one of its core input methods. The user can simply say *Okay,
    Glass* followed by a command such as *get directions* or *take a picture* to interact
    with the device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Eye tracking**: Eye tracking is typically employed in advanced AR glasses.
    By tracking the user’s eye movements, these systems allow the user to interact
    with AR content just by looking at it. *North Focals* AR glasses are a good example
    of this technology in action. The user can steer a small, virtual cursor just
    by moving their eyes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hand tracking** and **gesture recognition**: Hand tracking and gesture recognition
    can be used in highly immersive AR systems to interpret and track hand movements,
    enabling the user to touch and interact with virtual objects directly. Microsoft’s
    *HoloLens 2* is an example of this and allows the user to manipulate holograms
    with their hands, pinch to resize them, or tap them for interaction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Physical controllers**: Physical controllers can range from handheld devices
    to wearable tech such as gloves, which provide tactile feedback and precise control
    in specific AR applications. For instance, the *Magic Leap One* AR headset is
    accompanied by a handheld controller, immersing the user into the AR experience
    by allowing them to interact with virtual content in a more nuanced manner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we have seen, AR inputs can significantly impact the immersive quotient of
    AR experiences. The choice of input methods hinges on the specific nature of the
    AR application. Therefore, understanding and implementing the most suitable input
    method can enhance an AR system’s realism and usability manifold.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Since this book primarily focuses on handheld mobile AR devices, to ensure that
    readers can easily reproduce all the projects presented within, we have chosen
    to restrict our scope to touch inputs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next subsection, we will explore the AR toolkits available in Unity that
    can be utilized to implement the techniques we have just discussed.
  prefs: []
  type: TYPE_NORMAL
- en: Popular AR toolkits for Unity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we delve into Unity’s suite of AR toolkits, providing an overview
    for both budding and experienced developers seeking to navigate through their
    AR development journey. Each toolkit offers unique capabilities that aid developers
    in crafting compelling AR experiences. Let’s look at some of these now:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vuforia**: Vuforia is a widely adopted AR platform, providing a blend of
    computer vision capabilities that accommodate both marker-based and markerless
    AR experiences. Its extensive feature set includes image tracking, object recognition,
    and target recognition, with wide-ranging platform support. An additional notable
    feature is Vuforia’s cloud recognition, allowing developers to house a multitude
    of target images remotely, further expanding the AR experience’s potential.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ARKit**: Within Apple’s playground, ARKit is the optimal toolkit for experiences
    targeting iOS devices. It is crafted specifically to complement the iOS ecosystem,
    offering developers a suite of advanced features such as world tracking, face
    tracking, and scene understanding. These elements collectively serve to enrich
    the user’s AR experience. ARKit predominantly uses Swift and Objective-C, Apple’s
    proprietary programming languages. However, when integrated with Unity through
    the AR Foundation package, developers can leverage C#, enabling a more accessible
    and familiar programming environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ARCore**: Google’s ARCore is the Android equivalent of ARKit, tailored for
    the world’s most popular mobile operating system. ARCore equips developers with
    features such as environmental understanding, motion tracking, and light estimation,
    all of which are essential elements for crafting realistic AR experiences. Primarily,
    ARCore uses Java for native development. But, similar to ARKit’s integration,
    ARCore can be incorporated into Unity projects via the AR Foundation package,
    allowing developers to use C#.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AR Foundation**: AR Foundation stands as Unity’s high-level API package for
    constructing AR applications, unifying ARKit and ARCore’s capabilities. This ingenious
    package enables a single, streamlined workflow for creating cross-platform AR
    experiences, eradicating the need to write separate code bases for iOS and Android.
    It’s akin to owning a universal cookbook rather than individual recipe books for
    each cuisine. With AR Foundation, developers can leverage C#, a widely used, versatile
    programming language, making the process of crafting AR applications more efficient
    and intuitive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you were to use ARCore and ARKit separately, you would need to write separate
    sets of code for each platform. You would need to use ARCore to build your AR
    app on Android, and then rewrite the code using ARKit to make it work on iOS.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: However, by using AR Foundation to develop AR apps, you only need to write a
    single set of code that works on both iOS and Android. AR Foundation provides
    a unified API that is compatible with both ARCore and ARKit, meaning you don’t
    need to write different code for each platform. It simplifies the development
    process and saves a lot of time and effort.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll focus on creating AR applications using AR Foundation.
  prefs: []
  type: TYPE_NORMAL
- en: However, it’s important to remember that the use of ARCore or ARKit could offer
    unique advantages in certain scenarios, thanks to platform-specific features exclusive
    to each. For example, ARKit brings to the table LIDAR support from *version 3.5*
    onward. This feature utilizes the LIDAR scanner integrated into select iPhone
    and iPad models, offering refined scene understanding and precise depth estimation.
    Another ARKit exclusive is the *Motion Capture* feature from *ARKit 3* onward,
    enabling developers to record human movement and apply it to a 3D character model,
    effectively transforming the device into a motion capture studio.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, ARCore also has a unique set of features for Android devices.
    One such feature is the Depth API, which generates depth maps using a single RGB
    camera. While ARKit also has depth sensing capabilities, ARCore’s Depth API can
    function on a broader range of devices, even those without a dedicated depth sensor.
    Another distinctive feature of ARCore is *Augmented Images*, which allows the
    application to track and augment images at fixed locations, offering the potential
    for interaction with posters, murals, and similar items.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The examples mentioned earlier in this section may not be applicable when you
    are reading this book. Since ARKit, ARCore, and AR Foundation are constantly evolving,
    it is important to consult the latest documentation ([https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@5.0/manual/index.html](https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@5.0/manual/index.html))
    for the most up-to-date and accurate information.
  prefs: []
  type: TYPE_NORMAL
- en: Now, it is finally time to begin our first Unity AR project.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up an AR project in Unity using AR Foundation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will learn how to set up a simple AR project in Unity using
    AR Foundation. You will learn how you can place simple objects such as a cube,
    add plane detection functionalities, and implement touch inputs and anchors into
    your AR scenes.
  prefs: []
  type: TYPE_NORMAL
- en: Before using AR Foundation in our first AR application, however, we must first
    understand the architecture of this package.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding AR Foundation’s architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll delve into the exciting world of Unity’s AR Foundation,
    a package that empowers you to create AR experiences across various platforms.
    Whether you aim to create applications for Android, Apple, or HoloLens, AR Foundation
    simplifies the process remarkably. Its extensive capabilities range from plane
    detection, image and object tracking, and face and body tracking, to point clouds
    and more. *Figure 4**.1* breaks down the architecture of AR Foundation into a
    hierarchy of components that seamlessly work together to offer a consistent AR
    experience across different devices.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – AR Foundation’s architecture](img/B20869_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – AR Foundation’s architecture
  prefs: []
  type: TYPE_NORMAL
- en: The **AR App** represents the application where developers craft their AR experiences.
    It is directly linked with the **Managers**, which serve as the primary interfaces
    for specific AR functionalities.
  prefs: []
  type: TYPE_NORMAL
- en: The **AR PlaneManager** is crucial for detecting and tracking real-world planes,
    such as floors or walls. It provides a consistent means to engage with the physical
    environment around a user, feeding the app with data about surfaces in the user’s
    surroundings.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the **ARRaycast Manager** plays an essential role in understanding
    user interactions within this AR space. It casts rays into the AR scene, determining
    where these rays intersect with real-world surfaces. This is particularly vital
    when users intend to place virtual objects on these surfaces or wish to interact
    with virtual elements in relation to the real world.
  prefs: []
  type: TYPE_NORMAL
- en: Delving deeper, these managers interact with **Subsystems**, abstract layers
    that communicate with the actual platform-specific modules. The **XRPlane Subsystem**
    standardizes data related to plane detection, ensuring that plane-related events
    and data are uniform, irrespective of whether it’s ARKit or ARCore doing the underlying
    work. The **XRRaycast Subsystem** offers a consistent interface for raycasting,
    abstracting the nuances of each platform’s approach.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, the **Providers** represent the platform-specific SDKs that power AR
    on each device type. ARKit SDK is Apple’s contribution for its iOS devices, with
    specialized subsystems for plane detection and raycasting. On the other side,
    ARCore SDK is Google’s solution for Android, mirroring the functionalities offered
    by ARKit but tailored for the Android ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, AR Foundation offers XR developers a unified framework by allowing
    them to focus on their application’s AR experience without worrying about platform-specific
    intricacies. Through its layered architecture, it ensures that applications remain
    consistent and high-quality, whether deployed on an iPhone or an Android device.
  prefs: []
  type: TYPE_NORMAL
- en: With this understanding, it is finally time to create our first AR project in
    Unity.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an AR project with Unity’s AR template
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Creating an AR project in Unity is quite straightforward. Follow these steps
    to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your target AR platform — Android or iOS — you’ll need to navigate
    first to the `Installs` folder in the Unity Hub, click on the **Settings** icon
    adjacent to your version, select **Add Modules**, and install the appropriate
    build support.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To show your project in the AR Unity template, you’d follow the subsequent steps.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the Unity Hub’s project window, click on the **New project** button in the
    top-right corner.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose the **AR** template and give your project a unique name of your choice,
    as shown in *Figure 4**.2*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure 4.2 – \uFEFFThe AR template available in the Unity Hub](img/B20869_04_02.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – The AR template available in the Unity Hub
  prefs: []
  type: TYPE_NORMAL
- en: Why choose the AR template?
  prefs: []
  type: TYPE_NORMAL
- en: 'In Unity’s world, the AR template is like a bountiful garden, already seeded
    with pre-installed packages such as AR foundation, ARKit Face Tracking, ARKit
    XR-, ARCore XR-, Magic Leap XR-, and the OpenXR plugins, not to mention a sample
    scene. However, you might observe that this lush garden contains plants that aren’t
    necessary for your target landscape: Android or iOS. To maintain a cleaner garden,
    you can selectively pick the seeds you need – the OpenXR plugin, AR Foundation,
    Input System and the ARCore XR plugin (for Android) or ARKit XR plugin (for iOS)
    – from Unity’s own nursery, the package manager, and plant them in a regular 3D
    scene.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, click on **Create Project**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the scene loads in the Unity Editor, it should look like what is shown
    in *Figure 4**.3*.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.3 – \uFEFFUnity’s AR SampleScene](img/B20869_04_03.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Unity’s AR SampleScene
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t see the scene shown in *Figure 4**.3* in the Unity Editor, you
    can open it manually by selecting **Assets** | **ExampleAssets** | **SampleScene**.
  prefs: []
  type: TYPE_NORMAL
- en: The newly opend scene contains **Directional Light**, **AR Session Origin**,
    and **AR Session** GameObjects. If you wish to bring these objects into a new
    scene, you can conjure an **AR Session** GameObject by right-clicking in the hierarchy
    and choosing **XR** | **AR Session**. The **AR Session Origin** GameObject can
    be summoned in a similar fashion by selecting **XR** | **AR Session Origin**.
    That’s the initial magic performed.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: At the time of this book’s publication, **AR Session Origin** and **AR Session**
    represent the default GameObjects integrated into the *AR Foundation*’s template.
    Yet, given the dynamic nature of *AR Foundation*, continuous updates and revisions
    are anticipated. There are indications that **AR Session Origin** is slated to
    evolve into *XR Origin* (*Mobile AR*) in upcoming versions.
  prefs: []
  type: TYPE_NORMAL
- en: Should you encounter such changes, don’t fret. The essence of this chapter remains
    intact and can be easily navigated. We recommend accessing our *GitHub* repository
    to clone the specific project versions we’ve worked with, ensuring compatibility
    with Unity. Often, transitioning from outdated GameObjects to their newer counterparts
    is seamless, requiring minimal, if any, adjustments to the existing logic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Stay adaptable and remember: the core concepts and foundations presented here
    remain your guide, even as the tools evolve.'
  prefs: []
  type: TYPE_NORMAL
- en: Before we start inspecting these GameObjects, we first need to adjust some project
    settings of our AR scene.
  prefs: []
  type: TYPE_NORMAL
- en: Changing the project settings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To define which providers we want to target with our AR scene, go to **XR Plug-in
    Management** under **Edit** | **Project Settings**. Depending on the platform
    we’re targeting, be it an Android or an iOS device, we activate the corresponding
    ARCore or ARKit checkbox. If you seek to embrace both platforms, enable both checkboxes.
    This will install the provider packages into your project.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t see Android, iOS, or both tabs, it’s likely that the *Android*
    or *iOS Build Support* module hasn’t been integrated into your Editor. Let’s see
    how we can fix this problem.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4**.4* shows the installed Unity Editor with the Build Support modules
    marked in the Unity Hub.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – The installed Unity Editor with the Build Support modules marked](img/B20869_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – The installed Unity Editor with the Build Support modules marked
  prefs: []
  type: TYPE_NORMAL
- en: The displayed image indicates an absence of the *iOS Build Support* module in
    the Unity Editor. To adjust this, go to the **Installs** tab in the Unity Hub.
    Click the **Settings** icon in the top-right corner of your Unity Editor installation.
    Then, select **Add Modules**, enable the checkboxes for **iOS Build Support**
    or **Android Build Support** – or both, if you will – and install them. Once the
    installation is finished, go back to your project’s **XR Plug-in** **Management**
    window.
  prefs: []
  type: TYPE_NORMAL
- en: Now, you should find the **ARCore** and **ARKit** tabs under **XR** **Plug-in
    Management**.
  prefs: []
  type: TYPE_NORMAL
- en: Now, select the **ARCore** tab to explore fields related to the configuration
    and settings of the ARCore packages in your Unity project. *Figure 4**.5* shows
    the **ARCore** tab with its default settings.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – The ARCore tab with its default settings](img/B20869_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – The ARCore tab with its default settings
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go through the function of each field:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Requirement**: This parameter dictates the necessity of ARCore for your application.
    If set to **Required**, it establishes ARCore as an integral component for your
    target device. Conversely, when marked as **Optional**, ARCore is positioned as
    an additional feature, supplementing the application’s capabilities when present
    but not essential for the application’s core functionality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Depth**: This is the tool that gives your device the power of depth perception
    within its environment. Useful for occlusion and other advanced AR effects, it
    lets you toggle depth estimation in ARCore. If you set **Depth** to **Required**,
    your AR application will need to have depth estimation capabilities to function.
    On the other hand, selecting **Optional** means your AR application is capable
    of utilizing depth estimation features if the device supports it, but it doesn’t
    hamper the basic functioning if such features are absent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ignore Gradle Version** checkbox: This is your control over the Gradle build
    system, an essential tool for building Android apps. When unchecked, Unity adheres
    to the specified Gradle version for your project. However, if checked, Unity gains
    autonomy, ignoring the specified version and choosing to operate on its default
    version.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While the default settings – having both the **Requirement** and **Depth** fields
    set to **Required** and the **Ignore Gradle Version** checkbox unchecked – suit
    most projects, there are exceptions. For instance, if you’re creating an AR application
    designed for a wide variety of devices with varying capabilities, or if your application’s
    main functionalities do not heavily rely on ARCore’s depth perception, you might
    want to set the **Depth** field to **Optional**. Furthermore, if you’re working
    on a project that requires a specific Gradle version or has conflicts with newer
    versions, checking the **Ignore Gradle Version** box would be necessary. However,
    for this chapter, we can keep the default settings.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we turn our attention to the **ARKit** tab. Just like its ARCore counterpart,
    these fields are connected to the configuration and settings of the ARKit packages
    in your Unity project. *Figure 4**.6* shows the **ARKit** tab with its default
    settings.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – The ARKit tab with its default settings](img/B20869_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – The ARKit tab with its default settings
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s decode the roles of all the fields shown in *Figure 4**.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Requirement**: Acting in a manner similar to ARCore’s **Requirement** field,
    this indicator announces whether ARKit is deemed vital for your project.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Face Tracking**: Acting as a switch, this checkbox enables or disables the
    face tracking functionality in ARKit. When activated, it allows tracking and recognizing
    a range of facial features and expressions using the device’s front-facing camera.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the moment, we’ll abide by the default settings – the **Requirement** field
    standing firm at **Required** and the **Face Tracking** checkbox remaining in
    the off position, as it’s not currently needed.
  prefs: []
  type: TYPE_NORMAL
- en: In the forthcoming sections, we’re going to delve into the GameObjects that
    accompany the **AR** template. We’ll dive into their roles and demystify why they
    hold such significance. Let’s begin with the AR Session GameObject.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the AR Session GameObject
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **AR Session** GameObject manages the lifecycle of your AR application.
    To understand it, click on it in the scene hierarchy and navigate to the inspector
    window. As shown in *Figure 4**.7*, the **AR Session** GameObject contains the
    **AR Session** and the **AR Input** **Manager** components.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – The Inspector window when the AR Session GameObject is selected](img/B20869_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – The Inspector window when the AR Session GameObject is selected
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s understand each of the fields shown in *Figure 4**.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Attempt Update** checkbox: This checkbox, when marked, is your instruction
    to the AR session to be vigilant about keeping the device’s pose and tracking
    state up-to-date with every frame and every tick of the clock. Checking the box
    commands the AR session to fetch fresh tracking data diligently, ensuring that
    even when the device’s tracking wavers temporarily, the AR elements maintain their
    alignment with the real world. Imagine an AR hoop shooting game. In this game,
    you see a virtual hoop overlaid onto your real-world environment through your
    device’s screen. The objective is to shoot virtual balls into this hoop from various
    angles and distances. For this to function well, it’s vital that the tracking
    data of your device is continuously updated. That’s where the **Attempt Update**
    checkbox comes in.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activating **Attempt Update** commands the game to consistently refresh the
    tracking state, securing the harmony of the virtual objects with the real-world
    environment, notwithstanding transient tracking losses.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Match Frame Rate** checkbox: This, when marked, means that you’re asking
    the AR session to walk in step with the device’s camera frame rate. The AR session
    adjusts its rhythm to mirror the camera’s, culminating in a harmonious visual
    experience. Consider an AR app that lets the user envision virtual furniture within
    their actual space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A ticked **Match Frame Rate** checkbox ensures the AR session’s frame rate keeps
    time with the camera’s, curbing visual discord between the actual space and the
    virtual furniture, thereby enabling a precise assessment of aesthetics.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Tracking Mode** drop-down menu: This is the key to defining the tracking
    quality of your AR session. The selected mode determines the AR system’s awareness
    of, and response to, the device’s motion and position in the real world. Picture
    an AR navigation app that superimposes virtual arrows over the physical world
    to guide the user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this navigation app, opting for **Rotation and Position Tracking** imparts
    full tracking capabilities, ensuring the virtual arrows trace the contours of
    the real world accurately, guiding the user along their path with precision.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For our current exploration, we’ll stick with the default settings – both the
    **Attempt Update** and **Match Frame Rate** checkboxes are ticked and **Tracking
    Mode** is set to **Position And Rotation**. In the future, your choice of these
    settings should reflect your project’s goals and the intended user experience.
    To discover the best blend for your needs, we encourage you to experiment with
    different settings and configurations.
  prefs: []
  type: TYPE_NORMAL
- en: The second component under the AR Session umbrella is the **AR Input** **Manager**
    component.
  prefs: []
  type: TYPE_NORMAL
- en: The AR Input Manager component translates the user’s interactions with the AR
    scene into meaningful input. It perceives and processes a variety of user inputs
    such as taps, touches, and gestures. It’s the invisible hand enabling interactivity
    in your AR application, allowing placement of objects or interactions with virtual
    elements. For instance, imagine an AR game where the user neutralizes virtual
    targets with a tap. The **AR Input Manager script** reads the tap on the screen
    and furnishes the requisite information to unleash the shooting action in the
    game.
  prefs: []
  type: TYPE_NORMAL
- en: Now, our focus shifts to the AR Session Origin GameObject.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the AR Session Origin GameObject
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **AR Session Origin** GameObject is the primary element controlling the
    synchronization between the physical world and virtual objects projected in AR.
    It comprises various components, such as AR Session Origin, AR Plane Manager,
    AR Anchor Manager, AR Raycast Manager, and Anchor Creator, as we can see in *Figure
    4**.8*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – The Inspector when AR Session Origin is selected](img/B20869_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – The Inspector when AR Session Origin is selected
  prefs: []
  type: TYPE_NORMAL
- en: For the initiation phase of our AR development, we only require the AR Session
    Origin component. Hence, you may disable all other components for the time being.
  prefs: []
  type: TYPE_NORMAL
- en: Think of AR Session Origin as the pivot point of your AR experience. It provides
    a basis for anchoring virtual objects within the AR environment. Let’s visualize
    an AR app allowing the user to place virtual furniture within their home. Here,
    the AR Session Origin component arranges and alignes the virtual furniture correctly
    within the user’s physical space.
  prefs: []
  type: TYPE_NORMAL
- en: AR Session Origin ensures the alignment of the user’s environment with the position
    and orientation of the displayed AR scene. For instance, when a user places a
    virtual chair, this script maintains the chair’s positioning and orientation,
    no matter the user’s movement within the room. This is why this component is essential
    in creating an authentic AR experience.
  prefs: []
  type: TYPE_NORMAL
- en: For blending the virtual objects with the real-world footage, the AR session
    necessitates the presence of a camera. In the case of an existing main camera
    in your scene, you can safely remove it. AR Session Origin incorporates its own
    child **AR Camera**. This camera, devoid of any skybox for a monochromatic background,
    comes pre-equipped with necessary components, such as **Camera**, **Tracked Pose
    Driver**, **AR Camera Manager**, and **AR Camera Background**, as shown in *Figure
    4**.9*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – The Inspector when AR Session Origin’s child AR Camera is selected](img/B20869_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – The Inspector when AR Session Origin’s child AR Camera is selected
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now learn more about each of these:'
  prefs: []
  type: TYPE_NORMAL
- en: Like a real-world camera, the **Camera** component captures and displays a portion
    of the game scene to the player – essentially, what you see on your device’s screen
    when playing an AR game. It can be positioned and rotated to capture different
    views within the game scene.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **AR Camera Manager** component controls the phone camera settings, such
    as **Facing Direction**, **Light Estimation**, and **Auto Focus**. **Auto Focus**
    makes the camera automatically adjust its focus to keep AR objects sharp, just
    like autofocus in a regular camera. **Light Estimation** allows you to choose
    how the AR system estimates real-world lighting conditions to make virtual objects
    look more realistic. Options range from not estimating lighting at all (**None**)
    to estimating various aspects of lighting, such as **Ambient Light Intensity**
    and **Color**, and the main light source’s **Direction** and **Intensity**. **Facing
    Direction** decides the direction in which the AR Camera is facing. The **World**
    setting is typically for the rear-facing camera (seeing the environment), and
    **User** is for the front-facing camera (selfie mode). **None** disables this
    setting. For now, we can leave these settings as they are.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Tracked Pose Driver** component takes information about the device’s physical
    position and orientation, collectively referred to as its pose, and uses that
    to set the position, rotation, and scale of the camera within the Unity game scene.
    This is what’s referred to as the camera’s **Transform**. This ensures that the
    AR scene remains in harmony with the phone’s movements. To facilitate this, the
    Tracked Pose Driver provides several configurable options. **Device** lets you
    select the type of device being tracked. **Pose Source** allows you to choose
    the part of the device that provides the tracking data. The **Tracking Type**
    drop-down menu indicates what type of movement (rotation, position, or both) is
    tracked.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Update Type** dictates when the tracking information updates, whether during
    the regular update cycle, before rendering the next frame, or both. If the **Use
    Relative Transform** checkbox is ticked, the tracking data is based on the device’s
    position and rotation relative to its initial state.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Finally, the **Use Pose Provider** field can be enabled to utilize an external
    source for tracking data, which is potentially useful for specialized tracking
    systems. Together, these settings give **Tracked Pose Driver** the flexibility
    to handle a wide range of AR scenarios.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The **AR Camera Background** component controls how the real-world view captured
    by your device’s camera is displayed in your AR scene. The **Use Custom Material**
    checkbox allows you to apply a custom material, for example, to add special visual
    effects to the real-world view. If left unchecked, the real-world view is displayed
    as is, without any additional effects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The AR Session Origin GameObject performs the critical task of converting AR
    session space into Unity’s world space. Given the unique coordinate system employed
    in AR (referred to as AR session space), this conversion is vital for accurately
    positioning the GameObjects relative to the AR Camera.
  prefs: []
  type: TYPE_NORMAL
- en: With this information about the most important GameObjects of our AR scene in
    mind, it is time to place a simple object in our AR scene.
  prefs: []
  type: TYPE_NORMAL
- en: Placing a simple cube into the AR scene
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To begin testing our AR functionality, we need a 3D object in our scene to visualize
    the AR effect. For this purpose, you can create a simple cube object. Right-click
    in your project hierarchy and choose `0`,`0`,`3`) and apply a rotation of (`30`,`0`,`0`),
    as shown in *Figure 4**.10*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – The Transform values of the cube](img/B20869_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – The Transform values of the cube
  prefs: []
  type: TYPE_NORMAL
- en: These specific coordinates and rotations are only examples and can be altered
    according to your requirements.
  prefs: []
  type: TYPE_NORMAL
- en: The key objective is to position the object somewhere in the direction of the
    AR Camera and at a distinct distance. This placement will help verify whether
    the AR system is correctly overlaying virtual content onto the real world, as
    it provides a benchmark for alignment with your physical surroundings. It also
    allows you to gauge the depth perception capabilities of your AR system.
  prefs: []
  type: TYPE_NORMAL
- en: To validate whether the cube will be correctly displayed in the AR view, you
    can either select the AR Camera in the hierarchy – this will cause a camera view
    to appear in the bottom-right corner of the **Scene** window – or switch to the
    **Game** window, as shown in *Figure 4**.11*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – The Unity project showing how to validate whether the cube
    will be correctly displayed in the AR view](img/B20869_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – The Unity project showing how to validate whether the cube will
    be correctly displayed in the AR view
  prefs: []
  type: TYPE_NORMAL
- en: 'In an AR application, AR Session Origin typically corresponds to the starting
    location of your device’s camera when the AR experience begins. However, positioning
    a virtual object directly at **Session Origin** can indeed cause some issues.
    Here are a few potential problems that could arise:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Inaccurate scaling**: If you were to place a virtual cube directly at **Session
    Origin**, it could appear disproportionately large or small compared to real-world
    objects. It’s like holding a real-life object very close to your eyes – even a
    small object can appear large.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interference with tracking**: **Session Origin** serves as a reference point
    for tracking the device’s movements in the real world. If you place a virtual
    object at this exact point, the AR system might struggle to track both the object
    and the device’s movements accurately.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unpleasant user experience**: If a virtual object is positioned directly
    at **Session Origin**, it might appear too close to the user or even obstruct
    the user’s view of the rest of the AR scene, creating a less enjoyable experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: To avoid these issues, it’s generally recommended to position virtual objects
    at a reasonable distance from **Session Origin**. The precise location would depend
    on the specific requirements of your AR experience. For instance, if you’re creating
    an AR furniture placement app, you might position virtual furniture based on the
    detected real-world surfaces, such as floors and tables. By doing so, you ensure
    that the virtual furniture appears at an appropriate scale, doesn’t interfere
    with tracking, and provides a pleasant user experience by appearing in the correct
    place in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: After placing your cube, you can proceed with testing the scene by following
    the instructions outlined in the *Deploying AR experiences onto mobile devices*
    section of this chapter. As it stands, your AR application is simple, featuring
    a static object positioned in front of the user. However, we’re aiming for a more
    interactive experience, so we’ll delve into the anchor and plane detection functionalities
    in the upcoming section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing plane detection in AR Foundation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will make use of AR Foundation’s **plane detection** capabilities.
    AR Foundation’s plane detection forms the crux of numerous practical applications
    by providing a cornerstone understanding of real-world environments. Its ability
    to identify and comprehend flat surfaces enables digital objects to interact meaningfully
    and realistically with physical spaces, enhancing the overall augmented reality
    experience.
  prefs: []
  type: TYPE_NORMAL
- en: In the retail and e-commerce sectors, plane detection is fundamental to accurate
    product visualization. It ensures a virtual couch is placed right in front of
    your living room wall and not in a random position in your room.
  prefs: []
  type: TYPE_NORMAL
- en: This technology also underpins the immersive experiences in AR gaming. By identifying
    real-world surfaces, games such as Pokémon Go can accurately place virtual elements,
    enhancing the thrill of the chase. Other games use this technology to create worlds
    on your coffee table, maintaining the illusion by ensuring that virtual objects
    interact convincingly with the physical planes.
  prefs: []
  type: TYPE_NORMAL
- en: To make use of plane detection, enable the **AR Plane Manager** component you
    disabled in the *Exploring the AR Session Origin GameObject* section. Then, you
    can also delete the cube as we don’t need it anymore. Now, select **AR Session
    Origin** and inspect the **AR Plane Manager** component in the hierarchy.
  prefs: []
  type: TYPE_NORMAL
- en: The **AR Plane Manager** component in Unity’s AR Foundation is responsible for
    detecting and tracking real-world surfaces, or planes, via your device’s camera.
    It utilizes AR technology to understand the physical environment, creating digital
    representations that can interact with virtual objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a breakdown of what **AR Plane** **Manager** does:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Plane Prefab**: This is essentially the digital template or blueprint used
    to represent detected planes. When **AR Plane Manager** identifies a flat surface
    (a plane) in the real world, it creates a **Plane Prefab** instance in the AR
    space. Each prefab instance corresponds to a specific real-world plane, providing
    a surface on which you can place or interact with virtual objects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s say, for example, you’re creating an AR game where virtual cats roam around.
    The **Plane Prefab** instance could be a flat surface that the cats can walk on.
    If your living room floor is detected as a plane, a **Plane Prefab** instance
    will be created, providing a surface for your cats to frolic on.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Detection Mode**: This setting determines the orientation of planes that
    **AR Plane Manager** should detect. Here are some components within **Detection
    Mode** that would be useful for you:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Everything**: This setting combines the detection of horizontal and vertical
    planes by **AR Plane Manager**. If you’re creating an AR furniture placement app,
    you might use this setting. The app could then detect both the floor (a horizontal
    plane, for placing a chair) and the walls (vertical planes, for hanging pictures).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Horizontal**: **AR Plane Manager** will detect only flat surfaces oriented
    horizontally, such as floors and tabletops. For instance, if you’re creating an
    AR game where characters run around on the floor, you’d likely use this mode.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vertical**: Conversely, this setting will detect only vertically oriented
    surfaces, such as walls and doors. This might be used in an AR interior design
    app that allows the user to place virtual paintings or wallpapers on their walls.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: As we want to detect horizontal and vertical planes, the drop-down selection
    can remain at **Everything**.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you will notice that the AR template already has an **ARPlane** prefab
    assigned. This prefab is what you will see on top of the detected surfaces. As
    this prefab is already configured, you can use the **ARPlane** prefab. However,
    you can also create your custom-made plane prefab. This can be done by right-clicking
    in the hierarchy window and selecting **XR** | **AR** **Default Plane.**
  prefs: []
  type: TYPE_NORMAL
- en: '**AR Default Plane GameObject** in Unity’s AR Foundation is composed of various
    components, each of which performs a specific function within the overall system.
    Here’s a breakdown of what each of these components does:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AR Plane**: This script controls the essential behavior of a plane in the
    AR environment. Two notable fields in this script are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Destroy on Removal** checkbox: If checked, when a detected plane is removed
    or no longer needed, the corresponding **Plane** GameObject is destroyed or removed
    from the Unity scene to free up resources. Imagine this to be like cleaning up
    toys when you’re done playing with them to make space for other activities.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vertex Changed Threshold**: This field controls how sensitive the plane is
    to changes in its detected shape. If the real-world surface changes, the plane’s
    mesh vertices will need to be updated. This threshold determines how much change
    is needed before an update occurs. It’s like deciding when to adjust a puzzle
    because the pieces have moved – too often can be unnecessary, yet too seldom and
    the picture may not make sense.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AR Plane Mesh Visualizer**: This script is responsible for rendering the
    visual representation of a detected plane. Imagine you’re using an AR app on your
    phone to preview a piece of furniture in your room before buying it. When you
    point your phone’s camera at the floor, the app detects a flat surface – a plane.
    **AR Plane Mesh Visualizer** then creates a visible grid or pattern overlay on
    your phone screen that represents this detected plane.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tracking State Visibility** dropdown (**None**, **Limited**, **Tracking**):
    This setting determines when the plane should be visible based on the tracking
    quality. For instance, you might only want to see the plane when it’s fully tracked
    (good quality), or also when tracking is limited (lower quality). It’s like deciding
    when to display a picture – if it’s blurry, you might prefer to wait for it to
    load until it’s clear.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hide Subsumed** checkbox: If checked, when one detected plane is subsumed
    by another (i.e., entirely overlapped by a larger plane), the smaller plane is
    hidden. This can make the AR scene less cluttered and more efficient. Think of
    this to be like removing smaller rugs when you put down a larger one that covers
    them completely.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mesh Collider**: This component allows virtual objects to physically interact
    with the plane as if it were a solid surface. For example, if you drop a virtual
    ball onto the plane, **Mesh Collider** ensures that the ball bounces back rather
    than falling through the surface.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mesh Filter** and **Mesh Renderer**: These work together to create and display
    the 3D mesh for the plane. **Mesh Filter** generates the shape of the plane (the
    mesh), while **Mesh Renderer** applies materials and textures and renders it on
    the screen. They’re like a sculptor and a painter respectively, working together
    to create a lifelike statue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Line Renderer**: This component is often used to draw the border around the
    detected plane, helping the user to see the extent of the plane. It’s like drawing
    a chalk outline around an area to indicate its boundaries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these components work in harmony to detect, represent, and interact with
    real-world surfaces in your AR applications, ensuring a seamless blend of virtual
    objects with the physical environment. You can keep the default settings for our
    scene.
  prefs: []
  type: TYPE_NORMAL
- en: After creating `Prefabs`. Next, drag `Prefabs` folder. Doing so will automatically
    create a prefab out of it. With **AR Session Origin** now selected, drag the **AR
    Default Plane** prefab to the appropriate **Plane Prefab** field of the **AR Plane
    Manager** component within the Inspector, as illustrated in *Figure 4**.12*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – The AR Default Plane prefab assigned to the appropriate Plane
    Prefab field of the AR Plane Manager component](img/B20869_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – The AR Default Plane prefab assigned to the appropriate Plane
    Prefab field of the AR Plane Manager component
  prefs: []
  type: TYPE_NORMAL
- en: Now, your application would be able to detect a plane in your environment and
    place the plane prefab of your choice on top of it.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4**.13* shows the default plane prefab of Unity’s AR template.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – The plane prefab of Unity’s AR template detecting the wooden
    floor of an apartment](img/B20869_04_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 – The plane prefab of Unity’s AR template detecting the wooden floor
    of an apartment
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, with AR Foundation, it is straightforward to incorporate plane
    detection into a project and to project patterns onto detected surfaces. In the
    upcoming section, we will merge these concepts with the use of anchors and touch
    inputs. This combination will enable us to place an object onto a detected surface
    simply by tapping on the screen.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing touch inputs and anchors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To get started on working with touch inputs and anchors, enable the **AR Anchor
    Manager**, **AR Raycast Manager**, and **Anchor Creator** components in the Inspector
    when **AR Session Origin** is selected. These components are all we need to get
    the touch input and anchors to work. Let’s break each of them down:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AR Anchor Manager** script: **AR Anchor Manager** operates as the custodian
    of object stability in your AR space. When you designate a position for a virtual
    object within your AR scene, **Anchor Manager** ensures the object remains tethered
    to that specific real-world location, irrespective of changes in device perspective.
    It is akin to an overseer, maintaining each virtual object in its correct placement
    relative to the real-world coordinates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AR Raycast Manager** script: Acting as the tactile sense of your AR application,
    **AR Raycast Manager** emits invisible rays from your device into the scene. When
    these rays encounter a surface, they return data about its position and orientation.
    This process is key to interactions such as placing a virtual object on a real-world
    surface, as it enables your application to perceive and understand the topography
    of the environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anchor Creator** script: This script functions as an efficient tool, facilitating
    the creation and placement of new anchors within your scene. Given that anchors
    are pivotal to securing virtual objects in a consistent real-world location, **Anchor
    Creator** simplifies the process of generating these anchor points. This can range
    from situating a new virtual object to immobilizing an object in transit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though all components have a **Prefab** field, we just need to assign a
    prefab to the **AR Anchor Manager** component. This prefab will be spawned when
    we tap on the screen.
  prefs: []
  type: TYPE_NORMAL
- en: When you tap on the screen, **AR Raycast Manager** determines where in the real
    world you’ve indicated by casting a ray from your screen touch point into the
    AR scene. If this ray hits a detected surface, **AR Anchor Manager** creates an
    anchor at this real-world location and ties the prefab to it. This process ensures
    the stable positioning of your virtual object in the AR scene.
  prefs: []
  type: TYPE_NORMAL
- en: The prefab attached to **AR Anchor Manager** essentially acts as a template
    for the object you want to instantiate (or create) in the AR environment whenever
    the screen is tapped. This is why it’s essential to assign a prefab to **AR**
    **Anchor Manager**.
  prefs: []
  type: TYPE_NORMAL
- en: The other components do not require a prefab because they are not directly responsible
    for creating objects in your AR scene. **AR Raycast Manager** deals with detecting
    the real-world surfaces you are interacting with, and **Anchor Creator** facilitates
    the creation and placement of the anchors themselves. Neither of these tasks necessitates
    the creation of a new object from a prefab.
  prefs: []
  type: TYPE_NORMAL
- en: For demonstration purposes, we use a simple capsule primitive. Simply right-click
    in the hierarchy and select `0.2`,`0.2`,`0.2`). Now, create a new folder in the
    `Project` folder called `Prefabs` (right-click + **Create** | **Folder**) and
    drag the capsule into this folder. This will automatically create a prefab of
    the capsule that you can drag into the **Anchor Prefab** field of the **AR Anchor**
    **Manager** component.
  prefs: []
  type: TYPE_NORMAL
- en: Now, your app should spawn the capsule on a touch input. *Figure 4**.14* shows
    the deployed application.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.14 – The deployed application](img/B20869_04_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 – The deployed application
  prefs: []
  type: TYPE_NORMAL
- en: The preceding image illustrates the instantiated capsule prefabs following screen
    taps. These capsules are positioned atop the detected surface, confirming the
    effective operation of the AR systems. Specifically, **AR Raycast Manager** accurately
    casts rays from the screen touch point onto the detected surface, and in response,
    **AR Anchor Manager** successfully creates anchors, anchoring the prefabs at the
    indicated locations. Everything should now be working perfectly, so it’s time
    to build the scene onto your device. Depending on whether you’re using an Android
    or iOS device, you can navigate to the relevant subsection in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Testing AR experiences directly in Unity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As of *AR Foundation 5.0*, developers can conveniently test AR scenes right
    in the Unity Editor using the **XR Simulation** feature, without the constant
    need to deploy on mobile devices. By the time you read this book, this feature
    might already be pre-installed. Let’s quickly check that by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to **Edit** | **Project Settings**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose **XR** **Plug-in Management**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Look for the **XR Simulation** option under **Plug-in Providers** and enable
    it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you don’t see the **XR Simulation** option, you’ll need to manually install
    *AR Foundation 5.0* or a newer version. The next section explains how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: Installing AR Foundation 5.0 or later versions and related packages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you edit your project manifest, you control which package versions Unity
    loads into your project. There are two ways to edit your project manifest: add
    a package by name in **Package Manager**, or manually edit the project manifest
    file. Let’s do it in **Package Manager**:'
  prefs: []
  type: TYPE_NORMAL
- en: Select **Window** | **Package Manager** to open the **Package** **Manager**
    window.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the small `com.unity.xr.arfoundation`. This will automatically add
    the most recent version available. At the time of writing, this is version **5.1.0**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you want to use a specific version, you can type in your desired version
    in the **Version (****optional)** field.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Continue by updating the `com.unity.xr.openxr`. This action will import the
    latest version of the **OpenXR** plugin.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you’re upgrading from *AR Foundation 4* to a newer version, uninstall both
    the `com.unity.xr.arkit-face-tracking` and `com.unity.xr.arsubsystems`. If they
    appear in the search results, proceed with uninstallation; otherwise, they are
    not present in your project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, depending on your targeted mobile device platform, it’s important to update
    either the *ARCore* or *ARKit* packages. Navigate to the `com.unity.xr.arcore`
    and ensure its version aligns with that of *AR Foundation*. For iOS platforms,
    input `com.unity.xr.arkit`, making sure its version matches *AR Foundation*’s.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Always remember to refer to the *AR Foundation* documentation for the most
    recent version details and Editor compatibility: [https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@5.0/manual/project-setup/install-arfoundation.html](https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@5.0/manual/project-setup/install-arfoundation.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Now, if you revisit **XR Plug-in Management** via the aforementioned quick check,
    the **XR Simulation** option should be visible in the **Windows, Mac, Linux settings**
    tab, as shown in *Figure 4**.15*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15 – The Windows, Mac, Linux settings tab of the XR Plug-in Management
    window with the XR Simulation checkbox enabled](img/B20869_04_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 – The Windows, Mac, Linux settings tab of the XR Plug-in Management
    window with the XR Simulation checkbox enabled
  prefs: []
  type: TYPE_NORMAL
- en: With this feature activated, you’re primed to choose an environment and test
    the AR scene within Unity.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing an environment and testing the scene
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To test an AR scene within Unity, you’ll need an environment that emulates the
    real-world setting. To find the right simulation setting, go to **Window** | **XR**
    | **AR** **Foundation** | **XR Environment**. In the middle of the **XR Environment**
    interface, there’s an **Environment** dropdown. Initially, your project will offer
    just one environment option. While it’s possible to add more by importing sample
    environments, **DefaultSimulationEnvironment** is usually adequate for most testing
    needs. Simply choose this option. Once you’ve made your selection, hit the play
    button to activate play mode and start the simulation. You’ll now be able to view
    your scene in real time, as depicted in *Figure 4**.16*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.16 – XR Simulation at runtime](img/B20869_04_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.16 – XR Simulation at runtime
  prefs: []
  type: TYPE_NORMAL
- en: Pressing the play button transfers you to the **Game** mode of **XR Simulation**,
    presenting the simulated AR scene (highlighted as *1* in *Figure 4**.16*). In
    this mode, you can adjust the viewpoint as needed, witnessing features such as
    plane detection in real time. Once satisfied with the viewpoint, you can shift
    from **Game** mode to **Simulator** mode (highlighted as *2* in *Figure 4**.16*)
    using the dropdown in the top-left corner. It’s important to switch to **Simulator**
    mode to access the **XR Simulation** functionalities, such as touch inputs. Observing
    the scene shown as *2* in *Figure 4**.16*, you’ll notice key features such as
    the distinct white point pattern, indicating the detected plane, and white capsules,
    marking the spots where mouse clicks occurred.
  prefs: []
  type: TYPE_NORMAL
- en: However, it’s crucial to be aware of the constraints surrounding **XR Simulation**.
    Some elements might seem smaller than expected, and the resolution may not be
    the sharpest. Although **XR Simulation** provides a convenient testing avenue,
    it’s not a complete substitute for deploying the AR scene on an actual device.
    Consider it a tool for iterative testing until you’re confident about deploying
    it onto your target mobile device.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s proceed to launch the scene on our smartphone and observe the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying AR experiences onto mobile devices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It’s now time to launch your AR experiences onto smartphones or tablets. Primarily,
    you have two paths to accomplish this: deployment onto an Android or an iOS device.'
  prefs: []
  type: TYPE_NORMAL
- en: For solo projects, where the AR application is meant for personal use, you may
    opt to deploy onto just Android or iOS, depending on your device’s operating system.
    However, for larger-scale projects that involve several users – be it academic,
    industrial, or any other group, irrespective of its size – it’s advisable to deploy
    and test the AR app on both Android and iOS platforms.
  prefs: []
  type: TYPE_NORMAL
- en: This strategy has multiple benefits. First, if your application gains momentum
    or its usage expands, it would already be compatible with both major platforms,
    eliminating the need for time-consuming porting later on. Second, making your
    app accessible on both platforms from the outset can draw in more users, and possibly
    attract increased funding or support.
  prefs: []
  type: TYPE_NORMAL
- en: Another key advantage to this is the cross-platform compatibility offered by
    Unity. This enables you to maintain a singular code base for both platforms, simplifying
    the management and updating process for your application. Any modifications made
    need to be done in one location and then deployed across both platforms.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll delve into the steps required to deploy your AR scene
    onto an Android device.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying onto Android
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section outlines the procedure to deploy your AR scene onto an Android
    device. The initial part of the process involves enabling some settings on your
    phone to prepare it for testing. Here’s a step-by-step guide:'
  prefs: []
  type: TYPE_NORMAL
- en: Confirm that your device is compatible with ARCore. ARCore is essential for
    AR Foundation to work correctly. You can find a list of supported devices at [https://developers.google.com/ar/devices](https://developers.google.com/ar/devices).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install ARCore, which AR Fo[undation uses to enable AR capabilities on Android
    devices. ARCo](https://play.google.com/store/apps/details?id=com.google.ar.core)re
    can be downloaded from the Google Play Store at [https://play.google.com/store/apps/details?id=com.google.ar.core](https://play.google.com/store/apps/details?id=com.google.ar.core).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Activate **Developer Options**. To do this, open **Settings** on your Android
    device, scroll down, and select **About Phone**. Find **Build number** and tap
    it seven times until a message appears stating **You are now** **a developer!**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upon returning to the main **Settings** menu, you should now see an option called
    **Developer Options**. If it’s not present, perform an online search to find out
    how to enable developer mode for your specific device. Though the method described
    in the previous step is the most common, the variety of Android devices available
    might require slightly different steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With **Developer Options** enabled, turn on **USB Debugging**. This will allow
    you to transfer your AR scene to your Android device via a USB cable. Navigate
    to **Settings** | **Developer options**, scroll down to **USB Debugging**, and
    switch it on. Acknowledge any pop-up prompts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Depending on your Android version, you might need to allow the installation
    of apps from unknown sources:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For Android versions 7 (Nougat) and earlier: Navigate to **Settings** | **Security
    Settings** and then check the box next to **Unknown Sources** to allow the installation
    of apps from sources other than the Google Play Store.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For Android versions 8 (Oreo) and above: Select **Settings** |**Apps & Notifications**
    | **Special App Access** | **Install unknown apps** and activate **Unknown sources**.
    You will see a list of apps that you can grant permission to install from unknown
    sources. This is where you select the *File Manager* app, as you’re using it to
    download the unknown app from Unity.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Link your Android device to your computer using a USB cable. You can typically
    use your device’s charging cable for this. A prompt will appear on your Android
    device asking for permission to allow USB debugging from your computer. Confirm
    it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With your Android device properly prepared for testing AR scenes, you can now
    proceed to deploy your Unity AR scene. This involves adjusting several parameters
    in the Unity Editor’s **Build Settings** and **Player Settings**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a step-by-step guide on how to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: Select **File** | **Build Settings** | **Android**, then click the **Switch
    Platform** button. Now, your **Build Settings** should look something like what
    is illustrated in *Figure 4**.17*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.17 – Unity’s Build Settings configuration for Android](img/B20869_04_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.17 – Unity’s Build Settings configuration for Android
  prefs: []
  type: TYPE_NORMAL
- en: Next, click on the `Android 7.0 Nougat (API level 24)` or above. This is crucial,
    as ARCore requires at least Android 7.0 to function properly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remaining in the `com.company_name.application_name`. This pattern is a widely
    adopted convention for naming application packages in Android and is used to ensure
    unique identification for each application on the Google Play Store.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Return to `Builds`. Upon selecting this folder, Unity will construct the scene
    within the newly created `Builds` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is how you can set up your Android device for deploying AR scenes onto
    it. In the next section, you will learn how you can deploy your AR scene onto
    an iOS device, such as an iPhone or iPad.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying onto iOS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we delve into the process of deploying an AR scene onto an iOS device,
    it’s important to discuss certain hardware prerequisites. Regrettably, if you’re
    using a Windows PC and an iOS device, it’s not as straightforward as deploying
    an AR scene made in Unity. The reason for this is that Apple, in its characteristic
    style, requires the use of *Xcode*, its proprietary development environment, as
    an intermediary step. This is only available on Mac devices, not Windows or Linux.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you don’t possess a Mac, there are still ways to deploy your AR scene onto
    an iOS device. Here are a few alternatives:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Borrowing a Mac*: The simplest solution to gain access to Xcode and deploy
    your app onto an iOS device is to borrow a Mac from a friend or coworker. It’s
    also worth checking whether local libraries, universities, or co-working spaces
    offer public access to Macs. For commercial or academic projects, it’s highly
    recommended to invest in a Mac for testing your AR app on iOS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Using a virtual machine*: Another no-cost alternative is to establish a macOS
    environment on your non-Apple PC. However, Apple neither endorses nor advises
    this method due to potential legal issues and stability concerns. Therefore, we
    won’t elaborate further or recommend it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Employing a Unity plugin*: Fortunately, a widely used Unity plugin enables
    deployment of an AR scene onto your iOS device with relatively less hassle. Navigate
    to `iOS Project Builder for Windows` by Pierre-Marie Baty. Though this plugin
    costs $50, it is a much cheaper alternative than buying a Mac. After purchasing
    the plugin, import it into your AR scene and configure everything correctly by
    following the plugin’s documentation ([https://www.pmbaty.com/iosbuildenv/documentation/unity.html](https://www.pmbaty.com/iosbuildenv/documentation/unity.html)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this book, we focus exclusively on deploying AR applications onto iOS devices
    using a Mac for running Unity and Xcode. This is due to potential inconsistencies
    and maintenance concerns with other aforementioned methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you initiate the deployment setup, ensure that your Mac and iOS devices
    have the necessary software and settings. The following steps detail this preparatory
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: Make sure the latest software versions are installed on your MacOS and iOS devices.
    Check for updates by navigating to **Settings** | **General** | **Software Update**
    on each device and install any that are available.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Confirm that your iOS device supports ARKit, which is crucial for the correct
    functioning of AR Foundation. You can check compatibility at [https://developer.apple.com/documentation/arkit/](https://developer.apple.com/documentation/arkit/).
    Generally, any device running on iPadOS 11 or iOS 11 and later versions are compatible.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will need an Apple ID for the following steps. If you don’t have one, you
    can create it at [https://appleid.apple.com/account](https://appleid.apple.com/account).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the **Xcode** software onto your Mac from Apple’s developer website
    at [https://developer.apple.com/xcode/](https://developer.apple.com/xcode/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enable **Developer Mode** on your iOS device by going to **Settings** | **Privacy
    & Security** | **Developer Mode**, activate **Developer Mode**, and then restart
    your device. If you don’t find the **Developer Mode** option, connect your iOS
    device to a Mac using a cable. Open **Xcode**, then navigate to **Window** | **Devices
    and Simulator**. If your device isn’t listed in the left pane, ensure you trust
    the computer on your device by acknowledging the prompt that appears after you
    connect your device to the Mac. Subsequently, you can enable **Developer Mode**
    on your iOS device.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Having set up your Mac and iOS devices correctly, let’s now proceed with how
    to deploy your AR scene onto your iOS device. Each time you want to deploy your
    AR scene onto your iOS device, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Use a USB cable to connect your iOS device to your Mac.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Within your Unity project, navigate to **File** | **Build Settings** and select
    **iOS** from **Platform options**. Click the **Switch** **Platform** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check the **Development Build** option in **Build Settings** for iOS. This enables
    you to deploy the app for testing purposes onto your iOS device. This step is
    crucial to avoid the annual subscription cost of an Apple Developer account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Deploying apps onto an iOS device with a free Apple Developer account has certain
    limitations. You can only deploy up to three apps onto your device at once, and
    they need to be redeployed every 7 days due to the expiration of the free provisioning
    profile. For industrial or academic purposes, we recommend subscribing to a paid
    Developer account after thorough testing using the **Development** **Build** function.
  prefs: []
  type: TYPE_NORMAL
- en: Remain in `com.company_name.application_name`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Return to `Builds` and select it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Xcode** will open with the build, displaying an error message due to the
    need for a signing certificate. To create this, click on the error message, navigate
    to the **Signing and Capabilities** tab, and select the checkbox. In the **Team**
    drop-down menu, select **New Team**, and create a new team consisting solely of
    yourself. Now, select this newly-created team from the drop-down menu. Ensure
    that the information in the **Bundle Identifier** field matches your Unity Project
    found in **Edit** | **Project Settings** | **Player**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While in **Xcode**, click on the **Any iOS Device** menu and select your specific
    iOS device as the output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Play** button on the top left of **Xcode** and wait for a message
    indicating **Build succeeded**. Your AR application should now be on your iOS
    device. However, you won’t be able to open it until you trust the developer (in
    this case, yourself). Navigate to **Settings** | **General** | **VPN & Device
    Management** on your iOS device, tap **Developer App certificate** under your
    **Apple ID**, and then tap **Trust (Your** **Apple ID)**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On your iOS device’s home screen, click the icon of your AR app. Grant the necessary
    permissions, such as camera access. Congratulations, you’ve successfully deployed
    your AR app onto your iOS device!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You now know how to deploy your AR experiences onto both Android and iOS devices.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s review what we have learned so far in this chapter before moving on to
    creating interactive XR experiences.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ve delved into the complexities and intricacies that surround
    AR glasses, exploring why these devices face numerous physical and technological
    challenges before they can be fully embraced by the public at large. We’ve examined
    the critical choice between marker-based and markerless approaches in your AR
    application, and we’ve discussed how this seemingly simple decision can significantly
    influence not only the development journey of your application but also its accessibility,
    versatility, and user engagement.
  prefs: []
  type: TYPE_NORMAL
- en: Through the exploration and installation of Unity’s AR Foundation package, you
    are now empowered to create simple AR experiences of your own, and ready to deploy
    them across an extensive array of handheld, AR-compatible mobile devices.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve also discovered that the deployment of an AR scene onto iOS devices can
    be a complex and time-intensive task, largely due to the various restrictions
    imposed by Apple’s ecosystem compared to the Android ecosystem. However, these
    constraints should not deter you from pursuing a cross-platform approach for your
    AR apps. By aiming for deployment across both operating systems, you ensure greater
    accessibility and increase the potential for reaching a broader audience.
  prefs: []
  type: TYPE_NORMAL
- en: By understanding the diverse aspects of AR development through Unity, you are
    well on your way to creating immersive AR experiences that truly stand out. In
    the next chapter, you will learn how you can use C# scripting and other techniques
    to add complex logic to your VR applications.
  prefs: []
  type: TYPE_NORMAL
