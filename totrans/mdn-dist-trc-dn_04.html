<html><head></head><body>
<div id="_idContainer072">
<h1 class="chapter-number" id="_idParaDest-69"><a id="_idTextAnchor068"/><span class="koboSpan" id="kobo.1.1">4</span></h1>
<h1 id="_idParaDest-70"><a id="_idTextAnchor069"/><span class="koboSpan" id="kobo.2.1">Low-Level Performance Analysis with Diagnostic Tools</span></h1>
<p><span class="koboSpan" id="kobo.3.1">While distributed tracing works great for microservices, it’s less useful for deep performance analysis within a process. </span><span class="koboSpan" id="kobo.3.2">In this chapter, we’ll explore .NET diagnostics tools that allow us to detect and debug performance issues and profile inefficient code. </span><span class="koboSpan" id="kobo.3.3">We’ll also learn how to perform ad hoc performance analysis and capture necessary information automatically </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">in production.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">In this chapter, you will learn how to do </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.7.1">Use .NET runtime counters to identify common </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">performance problems</span></span></li>
<li><span class="koboSpan" id="kobo.9.1">Use performance tracing to optimize </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">inefficient code</span></span></li>
<li><span class="koboSpan" id="kobo.11.1">Collect diagnostics </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">in production</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.13.1">By the end of this chapter, you will be able to debug memory leaks, identify thread pool starvation, and collect and analyze detailed performance traces with .NET </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">diagnostics tools.</span></span></p>
<h1 id="_idParaDest-71"><a id="_idTextAnchor070"/><span class="koboSpan" id="kobo.15.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.16.1">The code for this chapter is available in this book’s repository on GitHub at </span><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4"><span class="koboSpan" id="kobo.17.1">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4</span></a><span class="koboSpan" id="kobo.18.1">. </span><span class="koboSpan" id="kobo.18.2">It consists of the </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">following components:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.20.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.21.1">issues</span></strong><span class="koboSpan" id="kobo.22.1"> application, which contains examples of </span><span class="No-Break"><span class="koboSpan" id="kobo.23.1">performance issues</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.24.1">loadgenerator</span></strong><span class="koboSpan" id="kobo.25.1">, which is a tool that generates load to </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">reproduce problems</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.27.1">To run samples and perform analysis, we’ll need the </span><span class="No-Break"><span class="koboSpan" id="kobo.28.1">following tools:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.29.1">.NET SDK 7.0 </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">or later.</span></span></li>
<li><span class="koboSpan" id="kobo.31.1">The .NET </span><strong class="source-inline"><span class="koboSpan" id="kobo.32.1">dotnet-trace</span></strong><span class="koboSpan" id="kobo.33.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.34.1">dotnet-stack</span></strong><span class="koboSpan" id="kobo.35.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.36.1">dotnet-dump</span></strong><span class="koboSpan" id="kobo.37.1"> diagnostics tools. </span><span class="koboSpan" id="kobo.37.2">Please install each of them with </span><strong class="source-inline"><span class="koboSpan" id="kobo.38.1">dotnet tool install –</span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.39.1">global dotnet-&lt;tool&gt;</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.41.1">Docker </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.43.1">docker-compose</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.45.1">To run the samples in this chapter, start the observability stack, which consists of Jaeger, Prometheus, and the OpenTelemetry collector, with </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.46.1">docker-compose up</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.47.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.48.1">Make sure that you start the </span><strong class="bold"><span class="koboSpan" id="kobo.49.1">issues</span></strong><span class="koboSpan" id="kobo.50.1"> app in the Release configuration – for example, by calling </span><strong class="source-inline"><span class="koboSpan" id="kobo.51.1">dotnet run -c Release</span></strong><span class="koboSpan" id="kobo.52.1"> from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.53.1">issues</span></strong><span class="koboSpan" id="kobo.54.1"> folder. </span><span class="koboSpan" id="kobo.54.2">We don’t run it in Docker so that it’s easier to use </span><span class="No-Break"><span class="koboSpan" id="kobo.55.1">diagnostics tools.</span></span></p>
<p><span class="koboSpan" id="kobo.56.1">In the </span><strong class="bold"><span class="koboSpan" id="kobo.57.1">issues</span></strong><span class="koboSpan" id="kobo.58.1"> app, we enabled the .NET runtime and process counters with the help of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.59.1">OpenTelemetry.Instrumentation.Process</span></strong><span class="koboSpan" id="kobo.60.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.61.1">OpenTelemetry.Instrumentation.Runtime</span></strong><span class="koboSpan" id="kobo.62.1"> NuGet packages and configured metrics for the HTTP client and ASP.NET Core. </span><span class="koboSpan" id="kobo.62.2">Here’s our </span><span class="No-Break"><span class="koboSpan" id="kobo.63.1">metrics configuration:</span></span></p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.64.1">Program.cs</span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.65.1">
builder.Services.AddOpenTelemetry()
  ...
</span><span class="koboSpan" id="kobo.65.2">  .WithMetrics(meterProviderBuilder =&gt;
      meterProviderBuilder
      .AddOtlpExporter()
</span><strong class="bold"><span class="koboSpan" id="kobo.66.1">          .AddProcessInstrumentation()</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.67.1">          .AddRuntimeInstrumentation()</span></strong><span class="koboSpan" id="kobo.68.1">
          .AddHttpClientInstrumentation()
          .AddAspNetCoreInstrumentation());</span></pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4/issues/Program.cs"><span class="koboSpan" id="kobo.69.1">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4/issues/Program.cs</span></a></p>
<h1 id="_idParaDest-72"><a id="_idTextAnchor071"/><span class="koboSpan" id="kobo.70.1">Investigating common performance problems</span></h1>
<p><span class="koboSpan" id="kobo.71.1">Performance degradation</span><a id="_idIndexMarker221"/><span class="koboSpan" id="kobo.72.1"> is a symptom of some other issues such as race conditions, dependency</span><a id="_idIndexMarker222"/><span class="koboSpan" id="kobo.73.1"> slow-down, high load, or any other problem that causes your </span><strong class="bold"><span class="koboSpan" id="kobo.74.1">service-level indicators</span></strong><span class="koboSpan" id="kobo.75.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.76.1">SLIs</span></strong><span class="koboSpan" id="kobo.77.1">) to go beyond healthy limits and miss </span><strong class="bold"><span class="koboSpan" id="kobo.78.1">service-level objectives</span></strong><span class="koboSpan" id="kobo.79.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.80.1">SLOs</span></strong><span class="koboSpan" id="kobo.81.1">). </span><span class="koboSpan" id="kobo.81.2">Such issues may affect multiple, if not all, code paths and APIs, even</span><a id="_idIndexMarker223"/><span class="koboSpan" id="kobo.82.1"> if they’re initially limited to a </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">specific scenario.</span></span></p>
<p><span class="koboSpan" id="kobo.84.1">For example, when a downstream service experiences issues, it can cause throughput to drop significantly for all APIs, including those that don’t depend on that downstream service. </span><span class="koboSpan" id="kobo.84.2">Retries, additional connections, or threads that handle downstream calls consume more resources than usual and take them away from </span><span class="No-Break"><span class="koboSpan" id="kobo.85.1">other requests.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.86.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.87.1">Resource consumption alone, be it high or low, does not indicate a performance issue (or lack of it). </span><span class="koboSpan" id="kobo.87.2">High CPU or memory utilization can be valid if users are not affected. </span><span class="koboSpan" id="kobo.87.3">It could still be important to investigate when they are unusually high as it could be an early signal of a problem </span><span class="No-Break"><span class="koboSpan" id="kobo.88.1">to come.</span></span></p>
<p><span class="koboSpan" id="kobo.89.1">We can detect performance issues by monitoring SLIs and alerting them to violations. </span><span class="koboSpan" id="kobo.89.2">If you see that issues are widespread and not specific to certain scenarios, it makes sense to check the overall resource consumption for the process, such as CPU usage, memory, and thread counts, to find the bottleneck. </span><span class="koboSpan" id="kobo.89.3">Then, depending on the constrained resource, we may need to capture more information, such as dumps, thread stacks, detailed runtime, or library events. </span><span class="koboSpan" id="kobo.89.4">Let’s go through several examples of common issues and talk about </span><span class="No-Break"><span class="koboSpan" id="kobo.90.1">their symptoms.</span></span></p>
<h2 id="_idParaDest-73"><a id="_idTextAnchor072"/><span class="koboSpan" id="kobo.91.1">Memory leaks</span></h2>
<p><span class="koboSpan" id="kobo.92.1">Memory leaks</span><a id="_idIndexMarker224"/><span class="koboSpan" id="kobo.93.1"> happen when an application</span><a id="_idIndexMarker225"/><span class="koboSpan" id="kobo.94.1"> consumes more and more memory over time. </span><span class="koboSpan" id="kobo.94.2">For example, if we cache objects in-memory without proper expiration and overflow logic, the application will consume more and more memory over time. </span><span class="koboSpan" id="kobo.94.3">Growing memory consumption triggers garbage collection, but the cache keeps references to all objects and GC cannot free </span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">them up.</span></span></p>
<p><span class="koboSpan" id="kobo.96.1">Let’s reproduce a memory leak and go through the signals that would help us identify it and find the root cause. </span><span class="koboSpan" id="kobo.96.2">First, we need to run the </span><strong class="bold"><span class="koboSpan" id="kobo.97.1">issues</span></strong><span class="koboSpan" id="kobo.98.1"> app and then add some load</span><a id="_idIndexMarker226"/><span class="koboSpan" id="kobo.99.1"> using the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.100.1">loadgenerator</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.101.1"> tool:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.102.1">
loadgenerator$ dotnet run -c Release memory-leak –-parallel 100 –-count 20000000</span></pre>
<p><span class="koboSpan" id="kobo.103.1">It makes 20 million requests</span><a id="_idIndexMarker227"/><span class="koboSpan" id="kobo.104.1"> and then stops, but if we let it run for a long</span><a id="_idIndexMarker228"/><span class="koboSpan" id="kobo.105.1"> time, we’ll see throughput dropping, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.106.1">Figure 4</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.107.1">.1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.108.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer055">
<span class="koboSpan" id="kobo.109.1"><img alt="Figure 4.1 – Service throughput (successful requests per second)" src="image/B19423_04_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.110.1">Figure 4.1 – Service throughput (successful requests per second)</span></p>
<p><span class="koboSpan" id="kobo.111.1">We can see periods when throughput drops and the service stops processing requests – let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.112.1">investigate why.</span></span></p>
<p><span class="koboSpan" id="kobo.113.1">.NET reports event counters that help us monitor the size of each GC generation. </span><span class="koboSpan" id="kobo.113.2">Newly allocated objects appear in </span><strong class="bold"><span class="koboSpan" id="kobo.114.1">generation 0</span></strong><span class="koboSpan" id="kobo.115.1">; if they survive garbage collection, they get promoted to </span><strong class="bold"><span class="koboSpan" id="kobo.116.1">generation 1</span></strong><span class="koboSpan" id="kobo.117.1">, and then to </span><strong class="bold"><span class="koboSpan" id="kobo.118.1">generation 2</span></strong><span class="koboSpan" id="kobo.119.1">, where they stay until they’re collected or the process</span><a id="_idIndexMarker229"/><span class="koboSpan" id="kobo.120.1"> terminates. </span><span class="koboSpan" id="kobo.120.2">Large objects (that are 85 KB or bigger) appear on a </span><strong class="bold"><span class="koboSpan" id="kobo.121.1">large object </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.122.1">heap</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.123.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.124.1">LOH</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.125.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.126.1">OpenTelemetry runtime instrumentations report generation sizes under the </span><strong class="source-inline"><span class="koboSpan" id="kobo.127.1">process_runtime_dotnet_gc_heap_size_bytes</span></strong><span class="koboSpan" id="kobo.128.1"> metric. </span><span class="koboSpan" id="kobo.128.2">It’s also useful to monitor the </span><strong class="bold"><span class="koboSpan" id="kobo.129.1">physical</span></strong><span class="koboSpan" id="kobo.130.1"> memory usage reported by OpenTelemetry process instrumentation as </span><strong class="source-inline"><span class="koboSpan" id="kobo.131.1">process_memory_usage_bytes</span></strong><span class="koboSpan" id="kobo.132.1">. </span><span class="koboSpan" id="kobo.132.2">We can see generation 2 and physical memory consumption in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.133.1">Figure 4</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.134.1">.2</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer056">
<span class="koboSpan" id="kobo.136.1"><img alt="Figure 4.2 – Memory consumption showing a memory leak in the application" src="image/B19423_04_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.137.1">Figure 4.2 – Memory consumption showing a memory leak in the application</span></p>
<p><span class="koboSpan" id="kobo.138.1">We can see that generation 2 grows over time, along with the virtual memory. </span><span class="koboSpan" id="kobo.138.2">The physical memory used by the process</span><a id="_idIndexMarker230"/><span class="koboSpan" id="kobo.139.1"> goes up and down, which means</span><a id="_idIndexMarker231"/><span class="koboSpan" id="kobo.140.1"> that the OS started using disk in addition to RAM. </span><span class="koboSpan" id="kobo.140.2">This process is called </span><strong class="bold"><span class="koboSpan" id="kobo.141.1">paging</span></strong><span class="koboSpan" id="kobo.142.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.143.1">swapping</span></strong><span class="koboSpan" id="kobo.144.1">, which is enabled (or disabled) at the OS level. </span><span class="koboSpan" id="kobo.144.2">When enabled, it may significantly affect performance since RAM is usually much faster </span><span class="No-Break"><span class="koboSpan" id="kobo.145.1">than disk.</span></span></p>
<p><span class="koboSpan" id="kobo.146.1">Eventually, the system will run out of physical memory and the pagefile will reach its size limit; then, the process will crash with an </span><strong class="source-inline"><span class="koboSpan" id="kobo.147.1">OutOfMemoryException</span></strong><span class="koboSpan" id="kobo.148.1"> error. </span><span class="koboSpan" id="kobo.148.2">This may happen earlier, depending on the environment and heap size configuration. </span><span class="koboSpan" id="kobo.148.3">For 32-bit processes, OOM happens when the virtual memory size reaches 4 GB as it runs out of address space. </span><span class="koboSpan" id="kobo.148.4">Memory limits can be configured or imposed by the application server (IIS), hosting providers, or </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">container runtimes.</span></span></p>
<p><span class="koboSpan" id="kobo.150.1">Kubernetes or Docker</span><a id="_idIndexMarker232"/><span class="koboSpan" id="kobo.151.1"> allows you to limit the virtual memory for a container. </span><span class="koboSpan" id="kobo.151.2">The behavior</span><a id="_idIndexMarker233"/><span class="koboSpan" id="kobo.152.1"> of different environments varies, but in general, the application is terminated with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.153.1">OutOfMemory</span></strong><span class="koboSpan" id="kobo.154.1"> exit code after the limit is reached. </span><span class="koboSpan" id="kobo.154.2">It might take days, weeks, or even months for a memory leak to crash the process with </span><strong class="source-inline"><span class="koboSpan" id="kobo.155.1">OutOfMemoryException</span></strong><span class="koboSpan" id="kobo.156.1">, so some memory leaks can stay dormant, potentially causing rare restarts and affecting only a long tail of </span><span class="No-Break"><span class="koboSpan" id="kobo.157.1">latency distribution.</span></span></p>
<p><span class="koboSpan" id="kobo.158.1">Memory leaks on the hot path can take the whole service down fast. </span><span class="koboSpan" id="kobo.158.2">When memory consumption grows quickly, garbage collection intensively tries to free up some memory, which uses the CPU and can pause </span><span class="No-Break"><span class="koboSpan" id="kobo.159.1">managed threads.</span></span></p>
<p><span class="koboSpan" id="kobo.160.1">We can monitor garbage collection for individual generations using .NET event counters and OpenTelemetry instrumentation, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.161.1">Figure 4</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.162.1">.3</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.163.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer057">
<span class="koboSpan" id="kobo.164.1"><img alt="Figure 4.3 – Garbage collection rate per second for individual generations" src="image/B19423_04_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.165.1">Figure 4.3 – Garbage collection rate per second for individual generations</span></p>
<p><span class="koboSpan" id="kobo.166.1">As you can see, the generation 0 and generation 1 collections happened frequently. </span><span class="koboSpan" id="kobo.166.2">Looking</span><a id="_idIndexMarker234"/><span class="koboSpan" id="kobo.167.1"> at the consistent memory</span><a id="_idIndexMarker235"/><span class="koboSpan" id="kobo.168.1"> growth and the frequency of garbage collection, we can now be pretty sure we’re dealing with a memory leak. </span><span class="koboSpan" id="kobo.168.2">We could also collect GC events from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.169.1">Microsoft-Windows-DotNETRuntime</span></strong><span class="koboSpan" id="kobo.170.1"> event provider (we’ll learn how to do this in the next section) to come to the </span><span class="No-Break"><span class="koboSpan" id="kobo.171.1">same conclusion.</span></span></p>
<p><span class="koboSpan" id="kobo.172.1">Let’s also check the CPU utilization (shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.173.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.174.1">.4</span></em><span class="koboSpan" id="kobo.175.1">) reported by the OpenTelemetry process instrumentation as the </span><strong class="source-inline"><span class="koboSpan" id="kobo.176.1">process_cpu_time_seconds_total</span></strong><span class="koboSpan" id="kobo.177.1"> metric, from which we can derive </span><span class="No-Break"><span class="koboSpan" id="kobo.178.1">the utilization:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer058">
<span class="koboSpan" id="kobo.179.1"><img alt="Figure 4.4 – CPU utilization during the memory leak" src="image/B19423_04_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.180.1">Figure 4.4 – CPU utilization during the memory leak</span></p>
<p><span class="koboSpan" id="kobo.181.1">We can see that there are periods when both user CPU utilization and privileged (system) CPU utilization go up. </span><span class="koboSpan" id="kobo.181.2">These are the same periods when throughput dropped in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.182.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.183.1">.1</span></em><span class="koboSpan" id="kobo.184.1">. </span><span class="koboSpan" id="kobo.184.2">User CPU utilization is derived from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.185.1">System.Diagnostics.Process.UserProcessorTime</span></strong><span class="koboSpan" id="kobo.186.1"> property, while system utilization (based on OpenTelemetry terminology) is derived from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.187.1">System.Diagnostics.Process.PriviledgedProcessorTime</span></strong><span class="koboSpan" id="kobo.188.1"> property. </span><span class="koboSpan" id="kobo.188.2">These are the same periods when throughput dropped in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.189.1">Figure 4</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.190.1">.1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.192.1">Our investigation could have</span><a id="_idIndexMarker236"/><span class="koboSpan" id="kobo.193.1"> started with high latency, high error rate, a high number</span><a id="_idIndexMarker237"/><span class="koboSpan" id="kobo.194.1"> of process restarts, high CPU, or high memory utilization, and all of those are symptoms of the same problem – a memory leak. </span><span class="koboSpan" id="kobo.194.2">So, now, we need to investigate it further – let’s collect a memory dump to see what’s in there. </span><span class="koboSpan" id="kobo.194.3">Assuming you can reproduce the issue on a local machine, Visual Studio or JetBrains dotMemory can capture and analyze a memory dump. </span><span class="koboSpan" id="kobo.194.4">We will use </span><strong class="source-inline"><span class="koboSpan" id="kobo.195.1">dotnet-dump</span></strong><span class="koboSpan" id="kobo.196.1">, which we can run on an instance</span><a id="_idIndexMarker238"/><span class="koboSpan" id="kobo.197.1"> experiencing problems. </span><span class="koboSpan" id="kobo.197.2">Check out the .NET documentation at </span><a href="https://learn.microsoft.com/dotnet/core/diagnostics/dotnet-dump"><span class="koboSpan" id="kobo.198.1">https://learn.microsoft.com/dotnet/core/diagnostics/dotnet-dump</span></a><span class="koboSpan" id="kobo.199.1"> to learn more about </span><span class="No-Break"><span class="koboSpan" id="kobo.200.1">the tool.</span></span></p>
<p><span class="koboSpan" id="kobo.201.1">So, let’s capture the dump using the </span><span class="No-Break"><span class="koboSpan" id="kobo.202.1">following command:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.203.1">
$ dotnet-dump collect -–name issues</span></pre>
<p><span class="koboSpan" id="kobo.204.1">Once the dump has been collected, we can analyze it with Visual Studio, JetBrains dotMemory, or other tools that automate and simplify it. </span><span class="koboSpan" id="kobo.204.2">We’re going to do this the hard way with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.205.1">dotnet-dump</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.206.1">CLI tool:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.207.1">
$ dotnet-dump analyze &lt;dump file name&gt;</span></pre>
<p><span class="koboSpan" id="kobo.208.1">This will open</span><a id="_idIndexMarker239"/><span class="koboSpan" id="kobo.209.1"> a prompt where we can run </span><strong class="bold"><span class="koboSpan" id="kobo.210.1">SOS</span></strong><span class="koboSpan" id="kobo.211.1"> commands. </span><span class="koboSpan" id="kobo.211.2">SOS is a debugger extension that allows us to examine running processes and dumps. </span><span class="koboSpan" id="kobo.211.3">It can help us find out what’s on </span><span class="No-Break"><span class="koboSpan" id="kobo.212.1">the heap.</span></span></p>
<p><span class="koboSpan" id="kobo.213.1">We can do this with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.214.1">dumpheap -stat</span></strong><span class="koboSpan" id="kobo.215.1"> command, which prints the count and total count and size of objects by their type, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.216.1">Figure 4</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.217.1">.5</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer059">
<span class="koboSpan" id="kobo.219.1"><img alt="Figure 4.5 – Managed heap stats showing ~20 million MemoryLeakController instances" src="image/B19423_04_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.220.1">Figure 4.5 – Managed heap stats showing ~20 million MemoryLeakController instances</span></p>
<p><span class="koboSpan" id="kobo.221.1">Stats are printed in ascending order, so the objects with the biggest total size appear at the end. </span><span class="koboSpan" id="kobo.221.2">Here, we can see that we have almost 20 million </span><strong class="source-inline"><span class="koboSpan" id="kobo.222.1">MemoryLeakController</span></strong><span class="koboSpan" id="kobo.223.1"> instances, which consume about 1.5 GB of memory. </span><span class="koboSpan" id="kobo.223.2">The controller instance is scoped to the request, and it seems</span><a id="_idIndexMarker240"/><span class="koboSpan" id="kobo.224.1"> it is not collected after the request ends. </span><span class="koboSpan" id="kobo.224.2">Let’s find </span><strong class="bold"><span class="koboSpan" id="kobo.225.1">the GC roots</span></strong><span class="koboSpan" id="kobo.226.1"> – objects that keep controller </span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">instances alive.</span></span></p>
<p><span class="koboSpan" id="kobo.228.1">We need to find the address</span><a id="_idIndexMarker241"/><span class="koboSpan" id="kobo.229.1"> of any controller instance. </span><span class="koboSpan" id="kobo.229.2">We can do this using</span><a id="_idIndexMarker242"/><span class="koboSpan" id="kobo.230.1"> its method table – the first hex number in each table row. </span><span class="koboSpan" id="kobo.230.2">The method table stores type information for each object and is an internal CLR </span><span class="No-Break"><span class="koboSpan" id="kobo.231.1">implementation detail.</span></span></p>
<p><span class="koboSpan" id="kobo.232.1">We can find the object address for it using another </span><span class="No-Break"><span class="koboSpan" id="kobo.233.1">SOS command:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.234.1">
$ dumpheap -mt 00007ffe53f5d488</span></pre>
<p><span class="koboSpan" id="kobo.235.1">This will print a table that contains the addresses of all </span><strong class="source-inline"><span class="koboSpan" id="kobo.236.1">MemoryLeakController</span></strong><span class="koboSpan" id="kobo.237.1"> instances. </span><span class="koboSpan" id="kobo.237.2">Let’s copy one of them so that we can find the GC root </span><span class="No-Break"><span class="koboSpan" id="kobo.238.1">with it:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.239.1">
$ gcroot -all &lt;controller-instance-address&gt;</span></pre>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.240.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.241.1">.6</span></em><span class="koboSpan" id="kobo.242.1"> shows the path from the GC root to the controller instance printed by the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.243.1">gcroot</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.244.1"> command:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer060">
<span class="koboSpan" id="kobo.245.1"><img alt="Figure 4.6 – ProcessingQueue is keeping the controller instances alive" src="image/B19423_04_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.246.1">Figure 4.6 – ProcessingQueue is keeping the controller instances alive</span></p>
<p><span class="koboSpan" id="kobo.247.1">We can see that </span><strong class="source-inline"><span class="koboSpan" id="kobo.248.1">issues.ProcessingQueue</span></strong><span class="koboSpan" id="kobo.249.1"> is holding this and other controller instances. </span><span class="koboSpan" id="kobo.249.2">It uses </span><strong class="source-inline"><span class="koboSpan" id="kobo.250.1">ConcurrentQueue&lt;Action&gt;</span></strong><span class="koboSpan" id="kobo.251.1"> inside. </span><span class="koboSpan" id="kobo.251.2">If we were to check the controller code, we’d see that we added an action that uses </span><strong class="source-inline"><span class="koboSpan" id="kobo.252.1">_logger</span></strong><span class="koboSpan" id="kobo.253.1"> – a controller instance variable that implicitly keeps controller </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">instances alive:</span></span></p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.255.1">MemoryLeakController.cs</span></p>
<pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.256.1">_queue.Enqueue</span></strong><span class="koboSpan" id="kobo.257.1">(() =&gt; </span><strong class="bold"><span class="koboSpan" id="kobo.258.1">_logger</span></strong><span class="koboSpan" id="kobo.259.1">.LogInformation(
    "notification for {user}",
    new User("Foo", "leak@memory.net")));</span></pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4/issues/Controllers/MemoryLeakController.cs"><span class="koboSpan" id="kobo.260.1">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4/issues/Controllers/MemoryLeakController.cs</span></a></p>
<p><span class="koboSpan" id="kobo.261.1">To fix this, we’d need to stop capturing</span><a id="_idIndexMarker243"/><span class="koboSpan" id="kobo.262.1"> the controller’s logger in action</span><a id="_idIndexMarker244"/><span class="koboSpan" id="kobo.263.1"> and add size limits and backpressure to </span><span class="No-Break"><span class="koboSpan" id="kobo.264.1">the queue.</span></span></p>
<h2 id="_idParaDest-74"><a id="_idTextAnchor073"/><span class="koboSpan" id="kobo.265.1">Thread pool starvation</span></h2>
<p><span class="koboSpan" id="kobo.266.1">Thread pool starvation happens</span><a id="_idIndexMarker245"/><span class="koboSpan" id="kobo.267.1"> when CLR does not have enough threads</span><a id="_idIndexMarker246"/><span class="koboSpan" id="kobo.268.1"> in the pool to process work, which can happen at startup or when the load increases significantly. </span><span class="koboSpan" id="kobo.268.2">Let’s reproduce it and see how </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">it manifests.</span></span></p>
<p><span class="koboSpan" id="kobo.270.1">With the </span><strong class="bold"><span class="koboSpan" id="kobo.271.1">issues</span></strong><span class="koboSpan" id="kobo.272.1"> app running, add some load using the following</span><a id="_idIndexMarker247"/><span class="koboSpan" id="kobo.273.1"> commands to send 300 concurrent requests to </span><span class="No-Break"><span class="koboSpan" id="kobo.274.1">the app:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.275.1">
$ dotnet run -c Release starve ––parallel 300</span></pre>
<p><span class="koboSpan" id="kobo.276.1">Now, let’s check what happens with the throughput and latency. </span><span class="koboSpan" id="kobo.276.2">You might not see any metrics or traces coming from the application or see stale metrics that were reported before the load started. </span><span class="koboSpan" id="kobo.276.3">If you try to hit any API on the issue application, such as http://localhost:5051/ok, it will </span><span class="No-Break"><span class="koboSpan" id="kobo.277.1">time out.</span></span></p>
<p><span class="koboSpan" id="kobo.278.1">If you check the CPU or memory for the </span><strong class="bold"><span class="koboSpan" id="kobo.279.1">issues</span></strong><span class="koboSpan" id="kobo.280.1"> process, you will see very low utilization – the process got stuck doing nothing. </span><span class="koboSpan" id="kobo.280.2">It lasts for a few minutes and then resolves – the service starts responding and reports metrics and traces </span><span class="No-Break"><span class="koboSpan" id="kobo.281.1">as usual.</span></span></p>
<p><span class="koboSpan" id="kobo.282.1">One way to understand what’s going on when a process does not report metrics and traces is to use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.283.1">dotnet-counters</span></strong><span class="koboSpan" id="kobo.284.1"> tool. </span><span class="koboSpan" id="kobo.284.2">Check</span><a id="_idIndexMarker248"/><span class="koboSpan" id="kobo.285.1"> out the .NET documentation at </span><a href="https://learn.microsoft.com/dotnet/core/diagnostics/dotnet-counters"><span class="koboSpan" id="kobo.286.1">https://learn.microsoft.com/dotnet/core/diagnostics/dotnet-counters</span></a><span class="koboSpan" id="kobo.287.1"> to learn more about the tool. </span><span class="koboSpan" id="kobo.287.2">Now, let’s run it to see the </span><span class="No-Break"><span class="koboSpan" id="kobo.288.1">runtime counters:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.289.1">
$ dotnet-counters monitor --name issues</span></pre>
<p><span class="koboSpan" id="kobo.290.1">It should print a table consisting of runtime counters that change over time, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.291.1">Figure 4</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.292.1">.7</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.293.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer061">
<span class="koboSpan" id="kobo.294.1"><img alt="Figure 4.7 – The dotnet-counters output dynamically showing runtime counters" src="image/B19423_04_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.295.1">Figure 4.7 – The dotnet-counters output dynamically showing runtime counters</span></p>
<p><span class="koboSpan" id="kobo.296.1">Here, we’re interested in thread pool</span><a id="_idIndexMarker249"/><span class="koboSpan" id="kobo.297.1"> counters. </span><span class="koboSpan" id="kobo.297.2">We can see 1,212 work items</span><a id="_idIndexMarker250"/><span class="koboSpan" id="kobo.298.1"> waiting in the thread pool queue length and that it keeps growing along with the thread count. </span><span class="koboSpan" id="kobo.298.2">Only a few (if any) work items are completed </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">per second.</span></span></p>
<p><span class="koboSpan" id="kobo.300.1">The root cause of this behavior is the following code in the controller, which blocks the thread </span><span class="No-Break"><span class="koboSpan" id="kobo.301.1">pool threads:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.302.1">
_httpClient.GetAsync("/dummy/?delay=100", token)</span><strong class="bold"><span class="koboSpan" id="kobo.303.1">.Wait()</span></strong><span class="koboSpan" id="kobo.304.1">;</span></pre>
<p><span class="koboSpan" id="kobo.305.1">So, instead of switching to another work item, the threads sit and wait for the dummy call to complete. </span><span class="koboSpan" id="kobo.305.2">It affects all tasks, including those that export telemetry data to the collector – they are waiting in the </span><span class="No-Break"><span class="koboSpan" id="kobo.306.1">same queue.</span></span></p>
<p><span class="koboSpan" id="kobo.307.1">The runtime increases the thread pool size gradually and eventually, it becomes high enough to clean up the work item queue. </span><span class="koboSpan" id="kobo.307.2">Check out </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.308.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.309.1">.8</span></em><span class="koboSpan" id="kobo.310.1"> to see thread pool </span><span class="No-Break"><span class="koboSpan" id="kobo.311.1">counter dynamics:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer062">
<span class="koboSpan" id="kobo.312.1"><img alt="Figure 4.8 – The thread pool threads count and queue changes before and after starvation" src="image/B19423_04_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.313.1">Figure 4.8 – The thread pool threads count and queue changes before and after starvation</span></p>
<p><span class="koboSpan" id="kobo.314.1">As you can see, we have no data for the time when starvation happened. </span><span class="koboSpan" id="kobo.314.2">But after the thread pool queue is cleared, we start getting the data and see that the runtime adjusts the number of threads to a </span><span class="No-Break"><span class="koboSpan" id="kobo.315.1">lower value.</span></span></p>
<p><span class="koboSpan" id="kobo.316.1">We just saw how problems</span><a id="_idIndexMarker251"/><span class="koboSpan" id="kobo.317.1"> on a certain code path can affect the performance</span><a id="_idIndexMarker252"/><span class="koboSpan" id="kobo.318.1"> of the whole process and how we can use runtime metrics and diagnostics tools to narrow them down. </span><span class="koboSpan" id="kobo.318.2">Now, let’s learn how to investigate performance issues specific to certain APIs or </span><span class="No-Break"><span class="koboSpan" id="kobo.319.1">individual requests.</span></span></p>
<h1 id="_idParaDest-75"><a id="_idTextAnchor074"/><span class="koboSpan" id="kobo.320.1">Profiling</span></h1>
<p><span class="koboSpan" id="kobo.321.1">If we analyze individual traces</span><a id="_idIndexMarker253"/><span class="koboSpan" id="kobo.322.1"> corresponding to thread pool starvation or memory leaks, we will not see anything special. </span><span class="koboSpan" id="kobo.322.2">They are fast under a small load and get slower or fail when the </span><span class="No-Break"><span class="koboSpan" id="kobo.323.1">load increases.</span></span></p>
<p><span class="koboSpan" id="kobo.324.1">However, some performance issues only affect certain scenarios, at least under typical load. </span><span class="koboSpan" id="kobo.324.2">Locks and inefficient code are examples of </span><span class="No-Break"><span class="koboSpan" id="kobo.325.1">such operations.</span></span></p>
<p><span class="koboSpan" id="kobo.326.1">We rarely instrument local operations with distributed tracing under the assumption that local calls are fast and exceptions have enough information for us to </span><span class="No-Break"><span class="koboSpan" id="kobo.327.1">investigate failures.</span></span></p>
<p><span class="koboSpan" id="kobo.328.1">But what happens when we have compute-heavy or just inefficient code in the service? </span><span class="koboSpan" id="kobo.328.2">If we look at distributed traces, we’ll see high latency and gaps between spans, but we wouldn’t know why </span><span class="No-Break"><span class="koboSpan" id="kobo.329.1">it happens.</span></span></p>
<p><span class="koboSpan" id="kobo.330.1">We know ahead of time that some operations, such as complex algorithms or I/O, can take a long time to complete or fail, so we can deliberately instrument them with tracing or just write a log record. </span><span class="koboSpan" id="kobo.330.2">But we rarely introduce inefficient code to the hot path intentionally; due to this, our ability to debug it with distributed tracing, metrics, or logs </span><span class="No-Break"><span class="koboSpan" id="kobo.331.1">is limited.</span></span></p>
<p><span class="koboSpan" id="kobo.332.1">In such cases, we need</span><a id="_idIndexMarker254"/><span class="koboSpan" id="kobo.333.1"> more precise signals, such as profiling. </span><strong class="bold"><span class="koboSpan" id="kobo.334.1">Profiling</span></strong><span class="koboSpan" id="kobo.335.1"> involves collecting call stacks, memory allocations, timings, and the frequency of calls. </span><span class="koboSpan" id="kobo.335.2">This can be done in-process using .NET profiling APIs that need the application to be configured in a certain way. </span><span class="koboSpan" id="kobo.335.3">Low-level performance</span><a id="_idIndexMarker255"/><span class="koboSpan" id="kobo.336.1"> profiling is usually done locally on a developer machine, but it used to be a popular mechanism among </span><strong class="bold"><span class="koboSpan" id="kobo.337.1">Application Performance Monitoring</span></strong><span class="koboSpan" id="kobo.338.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.339.1">APM</span></strong><span class="koboSpan" id="kobo.340.1">) tools to collect performance data </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">and traces.</span></span></p>
<p><span class="koboSpan" id="kobo.342.1">In this chapter, we’re going to use a different kind of profiling, also called performance tracing, which</span><a id="_idIndexMarker256"/><span class="koboSpan" id="kobo.343.1"> relies on </span><strong class="source-inline"><span class="koboSpan" id="kobo.344.1">System.Diagnostics.Tracing.EventSource</span></strong><span class="koboSpan" id="kobo.345.1">, and can be done ad hoc. </span><strong class="source-inline"><span class="koboSpan" id="kobo.346.1">EventSource</span></strong><span class="koboSpan" id="kobo.347.1"> is essentially a platform logger – CLR, libraries, and frameworks write their diagnostics to event sources, which are disabled by default, but it’s possible to enable and control </span><span class="No-Break"><span class="koboSpan" id="kobo.348.1">them dynamically.</span></span></p>
<p><span class="koboSpan" id="kobo.349.1">The .NET runtime and libraries events cover GC, tasks, the thread pool, the DNS, sockets, and HTTP, among other things. </span><span class="koboSpan" id="kobo.349.2">ASP.NET Core, Kestrel, Dependency Injection, Logging, and other libraries have their own event </span><span class="No-Break"><span class="koboSpan" id="kobo.350.1">providers too.</span></span></p>
<p><span class="koboSpan" id="kobo.351.1">You can listen to any provider inside the process using </span><strong class="source-inline"><span class="koboSpan" id="kobo.352.1">EventListener</span></strong><span class="koboSpan" id="kobo.353.1"> and access events</span><a id="_idIndexMarker257"/><span class="koboSpan" id="kobo.354.1"> and their payloads, but the true power of </span><strong class="source-inline"><span class="koboSpan" id="kobo.355.1">EventSource</span></strong><span class="koboSpan" id="kobo.356.1"> is that you can control providers from out-of-process over </span><strong class="bold"><span class="koboSpan" id="kobo.357.1">EventPipe</span></strong><span class="koboSpan" id="kobo.358.1"> – the runtime component that allows us to communicate with .NET applications. </span><span class="koboSpan" id="kobo.358.2">We saw it in action when we gathered event counters and collected verbose logs with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.359.1">dotnet-monitor</span></strong><span class="koboSpan" id="kobo.360.1"> tool in </span><a href="B19423_02.xhtml#_idTextAnchor038"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.361.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.362.1">, </span><em class="italic"><span class="koboSpan" id="kobo.363.1">Native Monitoring </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.364.1">in .NET</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.365.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.366.1">Let’s see how performance tracing and profiling with </span><strong class="source-inline"><span class="koboSpan" id="kobo.367.1">EventSource</span></strong><span class="koboSpan" id="kobo.368.1"> can help us investigate </span><span class="No-Break"><span class="koboSpan" id="kobo.369.1">specific issues.</span></span></p>
<h2 id="_idParaDest-76"><a id="_idTextAnchor075"/><span class="koboSpan" id="kobo.370.1">Inefficient code</span></h2>
<p><span class="koboSpan" id="kobo.371.1">Let’s run our demo application</span><a id="_idIndexMarker258"/><span class="koboSpan" id="kobo.372.1"> and see how inefficient code can manifest</span><a id="_idIndexMarker259"/><span class="koboSpan" id="kobo.373.1"> itself. </span><span class="koboSpan" id="kobo.373.2">Make sure the observability stack is running, then start the </span><strong class="bold"><span class="koboSpan" id="kobo.374.1">issues</span></strong><span class="koboSpan" id="kobo.375.1"> application, and then apply </span><span class="No-Break"><span class="koboSpan" id="kobo.376.1">some load:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.377.1">
$ dotnet run -c Release spin –-parallel 100</span></pre>
<p><span class="koboSpan" id="kobo.378.1">The load generator bombards the http://localhost:5051/spin?fib=&lt;n&gt; endpoint with 100 concurrent requests. </span><span class="koboSpan" id="kobo.378.2">The spin endpoint calculates an </span><em class="italic"><span class="koboSpan" id="kobo.379.1">n</span></em><span class="koboSpan" id="kobo.380.1">th Fibonacci number; as you’ll see, our Fibonacci implementation is </span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">quite inefficient.</span></span></p>
<p><span class="koboSpan" id="kobo.382.1">Assuming we don’t know how bad</span><a id="_idIndexMarker260"/><span class="koboSpan" id="kobo.383.1"> this Fibonacci implementation is, let’s try</span><a id="_idIndexMarker261"/><span class="koboSpan" id="kobo.384.1"> to investigate why this request takes so long. </span><span class="koboSpan" id="kobo.384.2">Let’s open Jaeger by going to http://localhost:16686, clicking on </span><strong class="bold"><span class="koboSpan" id="kobo.385.1">Find traces</span></strong><span class="koboSpan" id="kobo.386.1">, and checking out the latency distribution, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.387.1">Figure 4</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.388.1">.9</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.389.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer063">
<span class="koboSpan" id="kobo.390.1"><img alt="Figure 4.9 – Latency distribution in Jaeger" src="image/B19423_04_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.391.1">Figure 4.9 – Latency distribution in Jaeger</span></p>
<p><span class="koboSpan" id="kobo.392.1">We can see that almost all requests take more than 2 seconds to complete. </span><span class="koboSpan" id="kobo.392.2">If you click on any of the dots, Jaeger will show the corresponding trace. </span><span class="koboSpan" id="kobo.392.3">It should look similar to the one shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.393.1">Figure 4</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.394.1">.10</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.395.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer064">
<span class="koboSpan" id="kobo.396.1"><img alt="Figure 4.10 – Long trace in Jaeger" src="image/B19423_04_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.397.1">Figure 4.10 – Long trace in Jaeger</span></p>
<p><span class="koboSpan" id="kobo.398.1">The load application is instrumented so that we can measure client latency too. </span><span class="koboSpan" id="kobo.398.2">We can see that the client request took 4.5 seconds, while the server-side request took about 1.5 seconds. </span><span class="koboSpan" id="kobo.398.3">In a spin request, we call the dummy controller of the same application and can see corresponding client and server spans. </span><span class="koboSpan" id="kobo.398.4">The only thing that stands out here is that there are plenty of gaps and we don’t know what </span><span class="No-Break"><span class="koboSpan" id="kobo.399.1">happened there.</span></span></p>
<p><span class="koboSpan" id="kobo.400.1">If we check out the metrics, we will see high CPU and high server latency, but nothing suspicious that can help us find the root cause. </span><span class="koboSpan" id="kobo.400.2">So, it’s time to capture some </span><span class="No-Break"><span class="koboSpan" id="kobo.401.1">performance traces.</span></span></p>
<p><span class="koboSpan" id="kobo.402.1">Multiple tools can capture performance</span><a id="_idIndexMarker262"/><span class="koboSpan" id="kobo.403.1"> traces for the process that experiences</span><a id="_idIndexMarker263"/><span class="koboSpan" id="kobo.404.1"> this issue, such as PerfView on Windows, or PerfCollect </span><span class="No-Break"><span class="koboSpan" id="kobo.405.1">on Linux.</span></span></p>
<p><span class="koboSpan" id="kobo.406.1">We’re going to use the cross-platform </span><strong class="source-inline"><span class="koboSpan" id="kobo.407.1">dotnet-trace</span></strong><span class="koboSpan" id="kobo.408.1"> CLI tool, which you can install and use anywhere. </span><span class="koboSpan" id="kobo.408.2">Go ahead and run it using the following command for </span><span class="No-Break"><span class="koboSpan" id="kobo.409.1">10-20 seconds:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.410.1">
$ dotnet-trace collect --name issues</span></pre>
<p><span class="koboSpan" id="kobo.411.1">With this command, we’ve enabled the </span><strong class="source-inline"><span class="koboSpan" id="kobo.412.1">Microsoft-DotNETCore-SampleProfiler</span></strong><span class="koboSpan" id="kobo.413.1"> event source (among other default providers) to capture managed thread call stacks for the </span><strong class="bold"><span class="koboSpan" id="kobo.414.1">issues</span></strong><span class="koboSpan" id="kobo.415.1"> application. </span><span class="koboSpan" id="kobo.415.2">You can find out more</span><a id="_idIndexMarker264"/><span class="koboSpan" id="kobo.416.1"> about the </span><strong class="source-inline"><span class="koboSpan" id="kobo.417.1">dotnet-trace</span></strong><span class="koboSpan" id="kobo.418.1"> tool by reading the .NET documentation at </span><a href="https://learn.microsoft.com/dotnet/core/diagnostics/dotnet-trace"><span class="koboSpan" id="kobo.419.1">https://learn.microsoft.com/dotnet/core/diagnostics/dotnet-trace</span></a><span class="koboSpan" id="kobo.420.1">. </span><span class="koboSpan" id="kobo.420.2">We could also configure it to collect events from any other </span><span class="No-Break"><span class="koboSpan" id="kobo.421.1">event source.</span></span></p>
<p><span class="koboSpan" id="kobo.422.1">The tool saves traces to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.423.1">issues.exe_*.nettrace</span></strong><span class="koboSpan" id="kobo.424.1"> file, which we can analyze with it </span><span class="No-Break"><span class="koboSpan" id="kobo.425.1">as well:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.426.1">
$ dotnet-trace report issues.exe_*.nettrace topN</span></pre>
<p><span class="koboSpan" id="kobo.427.1">It outputs the top (5 by default) methods that have been on the stack most of the time. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.428.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.429.1">.11</span></em><span class="koboSpan" id="kobo.430.1"> shows some </span><span class="No-Break"><span class="koboSpan" id="kobo.431.1">sample output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer065">
<span class="koboSpan" id="kobo.432.1"><img alt="Figure 4.11 – Top five methods on the stack" src="image/B19423_04_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.433.1">Figure 4.11 – Top five methods on the stack</span></p>
<p><span class="koboSpan" id="kobo.434.1">There are no details about the top line – this is due to unmanaged or dynamically generated code. </span><span class="koboSpan" id="kobo.434.2">But the second one is ours – the </span><strong class="source-inline"><span class="koboSpan" id="kobo.435.1">MostInefficientFibonacci</span></strong><span class="koboSpan" id="kobo.436.1"> method looks suspicious and is worth checking. </span><span class="koboSpan" id="kobo.436.2">It was on the call stack 29.3% of the time (exclusive percentage). </span><span class="koboSpan" id="kobo.436.3">Alongside nested calls, it was on the call stack 31.74% of the time (inclusive percentage). </span><span class="koboSpan" id="kobo.436.4">This was easy, but in more complex cases, this analysis won’t be enough, and we might want to dig even further into popular </span><span class="No-Break"><span class="koboSpan" id="kobo.437.1">call stacks.</span></span></p>
<p><span class="koboSpan" id="kobo.438.1">You can open the trace file with any of the performance analysis tools</span><a id="_idIndexMarker265"/><span class="koboSpan" id="kobo.439.1"> I mentioned previously. </span><span class="koboSpan" id="kobo.439.2">We’ll use SpeedScope (</span><a href="https://www.speedscope.app/"><span class="koboSpan" id="kobo.440.1">https://www.speedscope.app/</span></a><span class="koboSpan" id="kobo.441.1">), a </span><span class="No-Break"><span class="koboSpan" id="kobo.442.1">web-based tool.</span></span></p>
<p><span class="koboSpan" id="kobo.443.1">First, let’s convert the trace file into </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.444.1">speedscope</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.445.1"> format:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.446.1">
dotnet-trace convert --format speedscope
  issues.exe_*.nettrace</span></pre>
<p><span class="koboSpan" id="kobo.447.1">Then, we must drop the generated JSON file</span><a id="_idIndexMarker266"/><span class="koboSpan" id="kobo.448.1"> into SpeedScope via the browser. </span><span class="koboSpan" id="kobo.448.2">It will show</span><a id="_idIndexMarker267"/><span class="koboSpan" id="kobo.449.1"> the captured call stacks for </span><span class="No-Break"><span class="koboSpan" id="kobo.450.1">each thread.</span></span></p>
<p><span class="koboSpan" id="kobo.451.1">You can click through different threads. </span><span class="koboSpan" id="kobo.451.2">You will see that many of them are sitting and waiting for work, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.452.1">Figure 4</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.453.1">.12</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.454.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer066">
<span class="koboSpan" id="kobo.455.1"><img alt="Figure 4.12 – The thread is waiting for work" src="image/B19423_04_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.456.1">Figure 4.12 – The thread is waiting for work</span></p>
<p><span class="koboSpan" id="kobo.457.1">This explains the top line in the report – most of the time, threads are waiting in </span><span class="No-Break"><span class="koboSpan" id="kobo.458.1">unmanaged code.</span></span></p>
<p><span class="koboSpan" id="kobo.459.1">There is another group of threads that is working hard to calculate Fibonacci numbers, as you can see in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.460.1">Figure 4</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.461.1">.13</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.462.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer067">
<span class="koboSpan" id="kobo.463.1"><img alt="Figure 4.13 – Call stack showing controller invocation with Fibonacci number calculation" src="image/B19423_04_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.464.1">Figure 4.13 – Call stack showing controller invocation with Fibonacci number calculation</span></p>
<p><span class="koboSpan" id="kobo.465.1">As you can see, we use a recursive Fibonacci algorithm without memorization, which explains the </span><span class="No-Break"><span class="koboSpan" id="kobo.466.1">terrible performance.</span></span></p>
<p><span class="koboSpan" id="kobo.467.1">We could have</span><a id="_idIndexMarker268"/><span class="koboSpan" id="kobo.468.1"> also used the </span><strong class="source-inline"><span class="koboSpan" id="kobo.469.1">dotnet-stack</span></strong><span class="koboSpan" id="kobo.470.1"> tool, which prints</span><a id="_idIndexMarker269"/><span class="koboSpan" id="kobo.471.1"> managed thread stack </span><span class="No-Break"><span class="koboSpan" id="kobo.472.1">trace snapshots.</span></span></p>
<h2 id="_idParaDest-77"><a id="_idTextAnchor076"/><span class="koboSpan" id="kobo.473.1">Debugging locks</span></h2>
<p><span class="koboSpan" id="kobo.474.1">With performance tracing, we can detect code</span><a id="_idIndexMarker270"/><span class="koboSpan" id="kobo.475.1"> that actively consumes</span><a id="_idIndexMarker271"/><span class="koboSpan" id="kobo.476.1"> CPU, but what if nothing happens – for example, if we have a lock in our code? </span><span class="koboSpan" id="kobo.476.2">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.477.1">find out.</span></span></p>
<p><span class="koboSpan" id="kobo.478.1">Let’s start the </span><strong class="bold"><span class="koboSpan" id="kobo.479.1">issues</span></strong><span class="koboSpan" id="kobo.480.1"> app and generate </span><span class="No-Break"><span class="koboSpan" id="kobo.481.1">some load:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.482.1">
$dotnet run -c Release lock ––parallel 1000</span></pre>
<p><span class="koboSpan" id="kobo.483.1">If we check the CPU and memory consumption, we can see that they are low and don’t grow much, the thread count doesn’t change much, the thread queue is empty, and the contention rate is low. </span><span class="koboSpan" id="kobo.483.2">At the same</span><a id="_idIndexMarker272"/><span class="koboSpan" id="kobo.484.1"> time, the throughput</span><a id="_idIndexMarker273"/><span class="koboSpan" id="kobo.485.1"> is low (around 60 requests per second) and the latency is big (P95 is around 3 seconds). </span><span class="koboSpan" id="kobo.485.2">So, the application is doing nothing, but it can’t go faster. </span><span class="koboSpan" id="kobo.485.3">If we check the traces, we will see a big gap with no </span><span class="No-Break"><span class="koboSpan" id="kobo.486.1">further data.</span></span></p>
<p><span class="koboSpan" id="kobo.487.1">This issue is specific to the lock API; if we hit another API, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.488.1">http://localhost:5051/ok</span></strong><span class="koboSpan" id="kobo.489.1">, it responds immediately. </span><span class="koboSpan" id="kobo.489.2">This narrows down our search for the </span><span class="No-Break"><span class="koboSpan" id="kobo.490.1">lock API.</span></span></p>
<p><span class="koboSpan" id="kobo.491.1">Assuming we don’t know there is a lock there, let’s collect some performance traces again with </span><strong class="source-inline"><span class="koboSpan" id="kobo.492.1">$ dotnet-trace collect --name issues</span></strong><span class="koboSpan" id="kobo.493.1">. </span><span class="koboSpan" id="kobo.493.2">If we get the </span><strong class="source-inline"><span class="koboSpan" id="kobo.494.1">topN</span></strong><span class="koboSpan" id="kobo.495.1"> stacks, as in the previous example, we won’t see anything interesting – just threads waiting for work – locking is fast; waiting for the locked resource to become available takes </span><span class="No-Break"><span class="koboSpan" id="kobo.496.1">much longer.</span></span></p>
<p><span class="koboSpan" id="kobo.497.1">We can dig deeper into the generated trace file to find actual stack traces on what happens in the lock controller. </span><span class="koboSpan" id="kobo.497.2">We’re going to use PerfView on Windows, but you can use PerfCollect on Linux, or other tools such as JetBrains dotTrace to open trace files and find </span><span class="No-Break"><span class="koboSpan" id="kobo.498.1">stack traces.</span></span></p>
<p><span class="koboSpan" id="kobo.499.1">Let’s open the trace file with PerfView and then click on the </span><strong class="bold"><span class="koboSpan" id="kobo.500.1">Thread Time</span></strong><span class="koboSpan" id="kobo.501.1"> option – it will open a new window. </span><span class="koboSpan" id="kobo.501.2">Let’s fold all the threads and search for </span><strong class="source-inline"><span class="koboSpan" id="kobo.502.1">LockController.Lock</span></strong><span class="koboSpan" id="kobo.503.1">, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.504.1">Figure 4</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.505.1">.14</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.506.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer068">
<span class="koboSpan" id="kobo.507.1"><img alt="Figure 4.14 – Finding LockController stacks across all threads" src="image/B19423_04_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.508.1">Figure 4.14 – Finding LockController stacks across all threads</span></p>
<p><span class="koboSpan" id="kobo.509.1">We can see that </span><strong class="source-inline"><span class="koboSpan" id="kobo.510.1">LockController</span></strong><span class="koboSpan" id="kobo.511.1"> rarely appears on the call stack, as well as its nested calls – we can tell since both the inclusive and exclusive percentages are close to 0. </span><span class="koboSpan" id="kobo.511.2">From this, we can conclude that whatever we’re waiting for is asynchronous; otherwise, we would see it on the </span><span class="No-Break"><span class="koboSpan" id="kobo.512.1">call stack.</span></span></p>
<p><span class="koboSpan" id="kobo.513.1">Now, let’s right-click</span><a id="_idIndexMarker274"/><span class="koboSpan" id="kobo.514.1"> on the </span><strong class="source-inline"><span class="koboSpan" id="kobo.515.1">LockController</span></strong><span class="koboSpan" id="kobo.516.1"> line and click on </span><strong class="bold"><span class="koboSpan" id="kobo.517.1">Drill-Down</span></strong><span class="koboSpan" id="kobo.518.1">. </span><span class="koboSpan" id="kobo.518.2">It will open yet another</span><a id="_idIndexMarker275"/><span class="koboSpan" id="kobo.519.1"> window focused on </span><strong class="source-inline"><span class="koboSpan" id="kobo.520.1">LockController</span></strong><span class="koboSpan" id="kobo.521.1"> stacks. </span><span class="koboSpan" id="kobo.521.2">Switch to the </span><strong class="bold"><span class="koboSpan" id="kobo.522.1">CallTree</span></strong><span class="koboSpan" id="kobo.523.1"> tab, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.524.1">Figure 4</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.525.1">.15</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.526.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer069">
<span class="koboSpan" id="kobo.527.1"><img alt="Figure 4.15 – Call stack with LockController.Lock" src="image/B19423_04_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.528.1">Figure 4.15 – Call stack with LockController.Lock</span></p>
<p><span class="koboSpan" id="kobo.529.1">We can see that the controller calls </span><strong class="source-inline"><span class="koboSpan" id="kobo.530.1">SemaphoreSlim.WaitAsync</span></strong><span class="koboSpan" id="kobo.531.1"> – this should be our first suspect. </span><span class="koboSpan" id="kobo.531.2">It would explain the low CPU, low memory usage, and no anomalies in the thread counts. </span><span class="koboSpan" id="kobo.531.3">It still makes clients wait and keeps client </span><span class="No-Break"><span class="koboSpan" id="kobo.532.1">connections open.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.533.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.534.1">We can only see the synchronous part of the call stack in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.535.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.536.1">.15</span></em><span class="koboSpan" id="kobo.537.1"> – it does not include </span><strong class="source-inline"><span class="koboSpan" id="kobo.538.1">WaitAsync</span></strong><span class="koboSpan" id="kobo.539.1"> or anything that happens </span><span class="No-Break"><span class="koboSpan" id="kobo.540.1">after that.</span></span></p>
<p><span class="koboSpan" id="kobo.541.1">The analysis we’ve done here relies on luck. </span><span class="koboSpan" id="kobo.541.2">In real-world scenarios, this issue would be hidden among other calls. </span><span class="koboSpan" id="kobo.541.3">We would have multiple suspects and would need to collect more data to investigate further. </span><span class="koboSpan" id="kobo.541.4">Since we’re looking for asynchronous suspects, collecting task-related events with </span><strong class="source-inline"><span class="koboSpan" id="kobo.542.1">dotnet-trace</span></strong><span class="koboSpan" id="kobo.543.1"> from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.544.1">System.Threading.Tasks.TplEventSource</span></strong><span class="koboSpan" id="kobo.545.1"> provider would </span><span class="No-Break"><span class="koboSpan" id="kobo.546.1">be useful.</span></span></p>
<p><span class="koboSpan" id="kobo.547.1">The issue is obvious if we look into the code, but it can be hidden well in real-world code, behind feature flags or </span><span class="No-Break"><span class="koboSpan" id="kobo.548.1">third-party libraries:</span></span></p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.549.1">LockController.cs</span></p>
<pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.550.1">await semaphoreSlim.WaitAsync(token);</span></strong><span class="koboSpan" id="kobo.551.1">
try
{
    ThreadUnsafeOperation();
    await _httpClient.GetAsync("/dummy/?delay=10", token);
}
finally
{
    </span><strong class="bold"><span class="koboSpan" id="kobo.552.1">semaphoreSlim.Release();</span></strong><span class="koboSpan" id="kobo.553.1">
}</span></pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4/issues/Controllers/LockController.cs"><span class="koboSpan" id="kobo.554.1">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4/issues/Controllers/LockController.cs</span></a></p>
<p><span class="koboSpan" id="kobo.555.1">The problem here is that we put a lock around the HTTP call to the downstream service. </span><span class="koboSpan" id="kobo.555.2">If we wrap only </span><strong class="source-inline"><span class="koboSpan" id="kobo.556.1">ThreadUnsafeOperation</span></strong><span class="koboSpan" id="kobo.557.1"> into a synchronous lock, we’ll see a much higher throughput of around 20K requests per second and low latency with P95 of around </span><span class="No-Break"><span class="koboSpan" id="kobo.558.1">20 milliseconds.</span></span></p>
<p><span class="koboSpan" id="kobo.559.1">Performance tracing</span><a id="_idIndexMarker276"/><span class="koboSpan" id="kobo.560.1"> is a powerful tool that allows</span><a id="_idIndexMarker277"/><span class="koboSpan" id="kobo.561.1"> us to capture low-level data reported by the .NET runtime, standard, and third-party libraries. </span><span class="koboSpan" id="kobo.561.2">In the examples we have covered in this chapter, we run diagnostics tools ad hoc and on the same host as the service. </span><span class="koboSpan" id="kobo.561.3">This is reasonable when you’re reproducing issues locally or optimizing your service on the dev box. </span><span class="koboSpan" id="kobo.561.4">Let’s see what we can do in a more realistic case with multiple instances of services running and restricted </span><span class="No-Break"><span class="koboSpan" id="kobo.562.1">SSH access.</span></span></p>
<h1 id="_idParaDest-78"><a id="_idTextAnchor077"/><span class="koboSpan" id="kobo.563.1">Using diagnostics tools in production</span></h1>
<p><span class="koboSpan" id="kobo.564.1">In production, we need</span><a id="_idIndexMarker278"/><span class="koboSpan" id="kobo.565.1"> to be able to collect some data proactively</span><a id="_idIndexMarker279"/><span class="koboSpan" id="kobo.566.1"> with reasonable performance and a telemetry budget so that we can analyze </span><span class="No-Break"><span class="koboSpan" id="kobo.567.1">data afterward.</span></span></p>
<p><span class="koboSpan" id="kobo.568.1">It’s difficult to reproduce an issue on a specific instance of a running process and collect performance traces or dumps from it in a secure and distributed application. </span><span class="koboSpan" id="kobo.568.2">If an issue such as a slow memory leak or a rare deadlock affects just a few instances, it might be difficult to even detect it and, when detected, the instance has already been recycled</span><a id="_idIndexMarker280"/><span class="koboSpan" id="kobo.569.1"> and the issue</span><a id="_idIndexMarker281"/><span class="koboSpan" id="kobo.570.1"> is no </span><span class="No-Break"><span class="koboSpan" id="kobo.571.1">longer visible.</span></span></p>
<h2 id="_idParaDest-79"><a id="_idTextAnchor078"/><span class="koboSpan" id="kobo.572.1">Continuous profiling</span></h2>
<p><span class="koboSpan" id="kobo.573.1">What we’re looking</span><a id="_idIndexMarker282"/><span class="koboSpan" id="kobo.574.1"> for is a continuous profiler – a tool that collects sampled performance traces. </span><span class="koboSpan" id="kobo.574.2">It can run for short periods to minimize the performance impact of collection on each instance and send profiles to central storage, where they can be stored, correlated with distributed traces, queried, and viewed. </span><span class="koboSpan" id="kobo.574.3">Distributed tracing supports sampling and a profiler can use it to capture traces and </span><span class="No-Break"><span class="koboSpan" id="kobo.575.1">profiles consistently.</span></span></p>
<p><span class="koboSpan" id="kobo.576.1">Many observability vendors, such as Azure Monitor, New Relic, Dynatrace, and others, provide continuous profilers for .NET. </span><span class="koboSpan" id="kobo.576.2">For example, Azure Monitor allows us to navigate to profiles from traces, as you can see in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.577.1">Figure 4</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.578.1">.16</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.579.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer070">
<span class="koboSpan" id="kobo.580.1"><img alt="Figure 4.16 – Navigating to a profile from a trace in Azure Monitor" src="image/B19423_04_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.581.1">Figure 4.16 – Navigating to a profile from a trace in Azure Monitor</span></p>
<p><span class="koboSpan" id="kobo.582.1">We will see a long trace for the inefficient code examples we went through earlier in this chapter, but the continuous profiler was enabled and captured some of these calls. </span><span class="koboSpan" id="kobo.582.2">If we click on the profiler icon, we will see the call stack, similar to the one we captured with </span><strong class="source-inline"><span class="koboSpan" id="kobo.583.1">dotnet-collect</span></strong><span class="koboSpan" id="kobo.584.1">, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.585.1">Figure 4</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.586.1">.17</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.587.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer071">
<span class="koboSpan" id="kobo.588.1"><img alt="Figure 4.17 – Profile showing a recursive call stack with the MostInefficientFibonacci method" src="image/B19423_04_17.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.589.1">Figure 4.17 – Profile showing a recursive call stack with the MostInefficientFibonacci method</span></p>
<p><span class="koboSpan" id="kobo.590.1">With a continuous profiler, we can debug inefficient code in a matter of seconds, assuming that the problem is reproduced frequently enough so that we can capture both distributed trace</span><a id="_idIndexMarker283"/><span class="koboSpan" id="kobo.591.1"> and profile </span><span class="No-Break"><span class="koboSpan" id="kobo.592.1">for it.</span></span></p>
<h2 id="_idParaDest-80"><a id="_idTextAnchor079"/><span class="koboSpan" id="kobo.593.1">The dotnet-monitor tool</span></h2>
<p><span class="koboSpan" id="kobo.594.1">Beyond profiling</span><a id="_idIndexMarker284"/><span class="koboSpan" id="kobo.595.1"> individual calls, we also need to be able to capture dumps proactively and on demand. </span><span class="koboSpan" id="kobo.595.2">It’s possible to configure .NET to capture dumps when a process crashes, but it doesn’t always work in containers and it’s not trivial to access and </span><span class="No-Break"><span class="koboSpan" id="kobo.596.1">transfer dumps.</span></span></p>
<p><span class="koboSpan" id="kobo.597.1">With </span><strong class="source-inline"><span class="koboSpan" id="kobo.598.1">dotnet-monitor</span></strong><span class="koboSpan" id="kobo.599.1">, we can capture logs, memory, and GC dumps, and collect performance traces in the same way we did with </span><strong class="source-inline"><span class="koboSpan" id="kobo.600.1">dotnet</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.601.1">diagnostic tools:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.602.1">Performance traces from event sources can be collected with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.603.1">dotnet-monitor</span></strong> <strong class="source-inline"><span class="koboSpan" id="kobo.604.1">/trace</span></strong><span class="koboSpan" id="kobo.605.1"> API or the </span><strong class="source-inline"><span class="koboSpan" id="kobo.606.1">dotnet-trace</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.607.1">CLI tool</span></span></li>
<li><span class="koboSpan" id="kobo.608.1">Dumps can be collected with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.609.1">/dump</span></strong><span class="koboSpan" id="kobo.610.1"> API or the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.611.1">dotnet-dump</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.612.1"> tool</span></span></li>
<li><span class="koboSpan" id="kobo.613.1">Event counters can be collected with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.614.1">/metrics</span></strong><span class="koboSpan" id="kobo.615.1"> API or the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.616.1">dotnet-counters</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.617.1"> tool</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.618.1">Check out the </span><strong class="source-inline"><span class="koboSpan" id="kobo.619.1">dotnet-monitor</span></strong><span class="koboSpan" id="kobo.620.1"> documentation to learn more about these and other HTTP APIs it </span><span class="No-Break"><span class="koboSpan" id="kobo.621.1">provides: </span></span><a href="https://github.com/dotnet/dotnet-monitor/tree/main/documentation"><span class="No-Break"><span class="koboSpan" id="kobo.622.1">https://github.com/dotnet/dotnet-monitor/tree/main/documentation</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.623.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.624.1">We can also configure triggers and rules that proactively collect traces or dumps based on CPU or memory utilization, GC frequency, and other runtime counter values. </span><span class="koboSpan" id="kobo.624.2">Results are uploaded to configurable </span><span class="No-Break"><span class="koboSpan" id="kobo.625.1">external storage.</span></span></p>
<p><span class="koboSpan" id="kobo.626.1">We looked at some features of </span><strong class="source-inline"><span class="koboSpan" id="kobo.627.1">dotnet-monitor</span></strong><span class="koboSpan" id="kobo.628.1"> in </span><a href="B19423_02.xhtml#_idTextAnchor038"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.629.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.630.1">, </span><em class="italic"><span class="koboSpan" id="kobo.631.1">Native Monitoring in .NET</span></em><span class="koboSpan" id="kobo.632.1">, where we run it as a sidecar</span><a id="_idIndexMarker285"/><span class="koboSpan" id="kobo.633.1"> container in Docker. </span><span class="koboSpan" id="kobo.633.2">Similarly, you can run it as a sidecar </span><span class="No-Break"><span class="koboSpan" id="kobo.634.1">in Kubernetes.</span></span></p>
<h1 id="_idParaDest-81"><a id="_idTextAnchor080"/><span class="koboSpan" id="kobo.635.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.636.1">Performance issues affect the user experience by decreasing service availability. </span><span class="koboSpan" id="kobo.636.2">Distributed tracing and common metrics allow you to narrow down the problem to a specific service, instance, API, or another combination of factors. </span><span class="koboSpan" id="kobo.636.3">When it’s not enough, you could increase resolution by adding more spans, but at some point, the performance impact and cost of the solution would </span><span class="No-Break"><span class="koboSpan" id="kobo.637.1">become unreasonable.</span></span></p>
<p><span class="koboSpan" id="kobo.638.1">.NET runtime metrics provide insights into CLR, ASP.NET Core, Kestrel, and other components. </span><span class="koboSpan" id="kobo.638.2">Such metrics can be collected with OpenTelemetry, </span><strong class="source-inline"><span class="koboSpan" id="kobo.639.1">dotnet-counters</span></strong><span class="koboSpan" id="kobo.640.1">, or </span><strong class="source-inline"><span class="koboSpan" id="kobo.641.1">dotnet-monitor</span></strong><span class="koboSpan" id="kobo.642.1">. </span><span class="koboSpan" id="kobo.642.2">It could be enough to root cause an issue, or just provide input on how to continue the investigation. </span><span class="koboSpan" id="kobo.642.3">The next step could be capturing process dumps and analyzing memory or threads’ call stacks, which can be achieved </span><span class="No-Break"><span class="koboSpan" id="kobo.643.1">with </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.644.1">dotnet-dump</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.645.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.646.1">For problems specific to certain scenarios, performance traces provide details so that we can see what happens in the application or under the hood in third-party library code. </span><span class="koboSpan" id="kobo.646.2">Performance traces are collected with </span><strong class="source-inline"><span class="koboSpan" id="kobo.647.1">dotnet-trace</span></strong><span class="koboSpan" id="kobo.648.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.649.1">dotnet-monitor</span></strong><span class="koboSpan" id="kobo.650.1">. </span><span class="koboSpan" id="kobo.650.2">By capturing performance traces, we can see detailed call stacks, get statistics on what consumes CPU, and monitor contentions and garbage collection more precisely. </span><span class="koboSpan" id="kobo.650.3">This is not only a great tool to investigate low-level issues but also to optimize </span><span class="No-Break"><span class="koboSpan" id="kobo.651.1">your code.</span></span></p>
<p><span class="koboSpan" id="kobo.652.1">Collecting low-level data in a secure, multi-instance environment is challenging. </span><span class="koboSpan" id="kobo.652.2">Continuous profilers can collect performance traces and other diagnostics on-demand, on some schedule, or by reacting to certain triggers. </span><span class="koboSpan" id="kobo.652.3">They also can take care of storing data in a central location and then visualizing and correlating it with other </span><span class="No-Break"><span class="koboSpan" id="kobo.653.1">telemetry signals.</span></span></p>
<p><span class="koboSpan" id="kobo.654.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.655.1">dotnet-monitor</span></strong><span class="koboSpan" id="kobo.656.1"> tool can run as a sidecar and then provide essential features to diagnostics data proactively or on-demand and send it to </span><span class="No-Break"><span class="koboSpan" id="kobo.657.1">external storage.</span></span></p>
<p><span class="koboSpan" id="kobo.658.1">In this chapter, you learned how to collect diagnostics data using .NET diagnostics tools and how to use it to solve several classes of common performance issues. </span><span class="koboSpan" id="kobo.658.2">Applying this knowledge, along with what we learned about metrics, distributed tracing, and logs previously, should allow you to debug most distributed and local issues in </span><span class="No-Break"><span class="koboSpan" id="kobo.659.1">your application.</span></span></p>
<p><span class="koboSpan" id="kobo.660.1">So, now, you know everything you need to leverage auto-instrumentation and make use of telemetry created by someone else. </span><span class="koboSpan" id="kobo.660.2">In the next chapter, we’ll learn how to enrich auto-generated telemetry and tailor it to </span><span class="No-Break"><span class="koboSpan" id="kobo.661.1">our needs.</span></span></p>
<h1 id="_idParaDest-82"><a id="_idTextAnchor081"/><span class="koboSpan" id="kobo.662.1">Questions</span></h1>
<ol>
<li><span class="koboSpan" id="kobo.663.1">What would you check first to understand whether an application </span><span class="No-Break"><span class="koboSpan" id="kobo.664.1">was healthy?</span></span></li>
<li><span class="koboSpan" id="kobo.665.1">If you were to see a major performance issue affecting multiple different scenarios, how would you </span><span class="No-Break"><span class="koboSpan" id="kobo.666.1">investigate it?</span></span></li>
<li><span class="koboSpan" id="kobo.667.1">What’s performance tracing and how can you </span><span class="No-Break"><span class="koboSpan" id="kobo.668.1">leverage it?</span></span></li>
</ol>
</div>


<div class="Content" id="_idContainer073">
<h1 id="_idParaDest-83"><a id="_idTextAnchor082"/><span class="koboSpan" id="kobo.1.1">Part 2: Instrumenting .NET Applications</span></h1>
</div>
<div id="_idContainer074">
<p><span class="koboSpan" id="kobo.2.1">This part provides an in-depth overview and practical guide for .NET tracing, metrics, logs, and beyond. </span><span class="koboSpan" id="kobo.2.2">We’ll start by learning about OpenTelemetry configuration and then dive deep into manual instrumentation, using </span><span class="No-Break"><span class="koboSpan" id="kobo.3.1">different signals.</span></span></p>
<p><span class="koboSpan" id="kobo.4.1">This part has the </span><span class="No-Break"><span class="koboSpan" id="kobo.5.1">following chapters:</span></span></p>
<ul>
<li><a href="B19423_05.xhtml#_idTextAnchor083"><em class="italic"><span class="koboSpan" id="kobo.6.1">Chapter 5</span></em></a><span class="koboSpan" id="kobo.7.1">, </span><em class="italic"><span class="koboSpan" id="kobo.8.1">Configuration and Control Plane</span></em></li>
<li><a href="B19423_06.xhtml#_idTextAnchor098"><em class="italic"><span class="koboSpan" id="kobo.9.1">Chapter 6</span></em></a><span class="koboSpan" id="kobo.10.1">, </span><em class="italic"><span class="koboSpan" id="kobo.11.1">Tracing Your Code</span></em></li>
<li><a href="B19423_07.xhtml#_idTextAnchor115"><em class="italic"><span class="koboSpan" id="kobo.12.1">Chapter 7</span></em></a><span class="koboSpan" id="kobo.13.1">, </span><em class="italic"><span class="koboSpan" id="kobo.14.1">Adding Custom Metrics</span></em></li>
<li><a href="B19423_08.xhtml#_idTextAnchor131"><em class="italic"><span class="koboSpan" id="kobo.15.1">Chapter 8</span></em></a><span class="koboSpan" id="kobo.16.1">, </span><em class="italic"><span class="koboSpan" id="kobo.17.1">Writing Structured and Correlated Logs</span></em></li>
</ul>
</div>
<div>
<div id="_idContainer075">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer076">
</div>
</div>
<div>
<div id="_idContainer077">
</div>
</div>
</body></html>