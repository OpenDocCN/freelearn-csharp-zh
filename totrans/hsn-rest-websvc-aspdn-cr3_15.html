<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">The Containerization of Services</h1>
                </header>
            
            <article>
                
<p class="mce-root">The previous chapter focused on several advanced topics to do with building web services using ASP.NET Core. This chapter offers a quick introduction to containers and how they can be useful for running your application locally in a sandbox environment. This chapter is not designed to cover everything about containers; rather, it's more of a brief introduction to them. We will learn how to run the catalog service on containers. </p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>An introduction to containers</li>
<li>How to run the catalog service on Docker</li>
<li>An overview of .NET Core Docker images</li>
<li>Optimizing Docker images</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An introduction to containers</h1>
                </header>
            
            <article>
                
<p>Nowadays, distributed systems form the base of every application. In turn, the foundation of distributed systems is containers. The goal of containerization is to run resources in an isolated environment. Containers define boundaries and the separation of concerns between the components of a distributed system. This separation of concerns is a way for us to reuse containers by providing parameterized configurations. Another important feature of web services and web applications is <em>scalability</em>. It should be easy to scale up your containers and create new instances.</p>
<p>Docker is a tool that is designed to create, run, and deploy applications using containers. Docker is also referred to as a platform that promotes this technology. Over the last few years, Docker has become somewhat of a buzzword, and it has been adopted by a considerable number of companies, start-ups, and open source projects. Various projects are <span>associated with Docker.</span></p>
<p>To begin, let's talk about <strong>Moby </strong>(<a href="https://github.com/moby/moby">https://github.com/moby/moby</a>), which is an open source project created by Docker. As a large number of people and communities started to contribute to the project, Docker decided to develop <span>Moby. All contributions to the Moby project, which can be considered as a sort of research and development department of Docker, are open source.</span></p>
<p>Moby is a framework that is used to build specific container systems. It provides a library of components that are the fundamentals of a container system:</p>
<ul>
<li>OS and container runtime</li>
<li>Orchestration</li>
<li>Infrastructure management</li>
<li>Networking</li>
<li>Storage</li>
<li>Security</li>
<li>Build</li>
<li>Image distribution</li>
</ul>
<p>It also provides the tools that are necessary to build up these components to create a runnable artifact for each platform and architecture. </p>
<p><span>If we are looking for more downstream solutions, we have the Docker <strong>Community Edition</strong> (<strong>CE</strong>) and Docker <strong>Enterprise Edition</strong> (<strong>EE</strong>) versions, which are products that use the Moby project. Docker CE is used by small teams and developers to build their system, while business customers use Docker EE. Both are recommended solutions allowing us to use containerization in development and enterprise environments.</span></p>
<div class="packt_infobox"><span>This chapter will use Docker CE to <span>containerize</span> the</span> catalog service<span>. Docker CE and Docker EE are available on the Docker website: <a href="https://www.docker.com/">https://www.docker.com/</a>. You can download and install the Community Edition by following the steps provided at <a href="https://www.docker.com/get-started">https://www.docker.com/get-started</a>.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Docker terminology</h1>
                </header>
            
            <article>
                
<p>Before setting up our service to use Docker, let's take a quick look at the terminology behind this technology.</p>
<p>Let's begin by defining a container image, which is the list of all dependencies and artifacts needed to create a container instance. The term <em>container image</em> should not be confused with the term <em>container instance</em>, which refers to a single instance of a container image<em>.</em></p>
<p><span>A core par</span>t of a container image is the Dockerfile. The Dockerfile provides the instructions <span>to build a container. For example, in the case of a container that runs a .NET Core solution, it gives the commands to restore the packages and build the solution. Docker also provides a way to tag the container images and group them all in collections known as</span> <strong>repositories</strong><span>. Repositories are usually covered b</span>y a registry, which allows access to a specific repository. The reposito<span>ries can be public or private, depending on what they are used for. For example, a private company can use a private repository to provide different versions of containers to internal teams. This centralized way of thinking is powerful, and it gives us a way to reuse containers. The main example of a public repository is </span><a href="https://hub.docker.com/">https://hub.docker.com/</a><span>, which is the world's most extensive library that provides container images:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c3f3196b-bd1e-44b6-b602-8c37057cdda5.png" style=""/></div>
<p><span>The preceding diagram describes </span>a typical interaction between the components we described in this section. The client usually provides some Docker commands that are parsed and executed in the <strong>Docker HOST</strong>. The <strong>Docker HOST</strong><em> </em>contains both of the <em>containers</em> we are running on the local machine and the images used by those containers. The images are usually taken by a Docker public or private registry, which is generally to the Docker Hub website or a private company repository.</p>
<p>Another essential term to do with Docker is the compose tool. The compose tool is used for defining and running a multi-container application or service. A<span> composer is usually combined with a definition file in a different format, such as a YAML file that defines the structure of </span><span>the</span><span> </span><span>container group.</span></p>
<div class="packt_infobox">The compose tool usually runs using the <kbd>docker-compose</kbd> CLI command. In this chapter, we will use the <kbd>docker-compose</kbd> command to get our service running in the local environment.</div>
<p>An essential part of a container system is the <em>orchestrator</em>. An <em>orchestrator</em> simplifies the use of a multi-container system. Moreover, orchestrators are usually applied to complex systems. Examples of container orchestrators include <span>Kubernetes, Azure Service Fabric, and Docker Swarm.</span></p>
<div class="packt_infobox">This book will not cover the use of orchestrators. This chapter gives a high-level overview of the capabilities of Docker and, more generally, containerization. More complex topics to do with Docker require DevOps and system engineering skills.</div>
<p class="mce-root">Let's move on to look at the power of containerization and how to use it to build our applications and services quickly. The next section will apply container principles to the catalog service built in the previous chapter. The following section will also use container technologies extensively to get our examples up and running.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using Docker to run the catalog service</h1>
                </header>
            
            <article>
                
<p>This section explains how to combine our catalog service with Docker to get it running locally. Let's start by examining the systems behind the catalog service:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/70b28f16-f147-46c6-9efd-3a732844e21c.png"/></div>
<p>As we can see from the preceding diagram, the web service part runs over the <kbd>microsoft/dotnet</kbd> Docker image, and the data source part runs over the Microsoft SQL Server instance using the <kbd>microsoft/mssql-server-linux</kbd> Docker image (we already dealt with the containerization of MSSQL in <a href="84b281bd-11a2-4703-81ae-ca080e2a267a.xhtml">Chapter 8</a>,<span> </span><em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">Building the Data Access Layer</span></span></em>). Both of the images are downloaded from the public Microsoft repository already present in Docker Hub; let's take a look at how to use <kbd>docker-compose</kbd> to define the whole infrastructure of the service.</p>
<p>First of all, let's create a <kbd>docker-compose.yml</kbd> file with the following content in the root folder of the project:</p>
<pre><span>version</span><span>: </span><span>"3.7"<br/></span><span>services</span><span>:<br/></span><span>  </span><span>catalog_api</span><span>:<br/></span><span>    </span><span>container_name</span><span>: </span>catalog_api<br/>    <span>build</span><span>:<br/></span><span>      </span><span>context</span><span>: </span>.<br/>      <span>dockerfile</span><span>: </span>containers/api/Dockerfile<br/>    <span>env_file</span><span>:<br/></span><span>      - </span>containers/api/api.env<br/>    <span>networks</span><span>:<br/></span><span>      - </span>my_network<br/>    <span>ports</span><span>:<br/></span><span>      - </span>5000:5000<br/>      <span>- </span>5001:5001<br/>    <span>depends_on</span><span>:<br/></span><span>      - </span>catalog_db<br/><br/>  <span>catalog_db</span><span>:<br/></span><span>    </span><span>image</span><span>: </span>microsoft/mssql-server-linux<br/>    <span>container_name</span><span>: </span>catalog_db<br/>    <span>ports</span><span>:<br/></span><span>      - </span>1433:1433<br/>    <span>env_file</span><span>:<br/></span><span>      - </span>containers/db/db.env<br/>    <span>networks</span><span>:<br/></span><span>      - </span>my_network<br/><br/><span>networks</span><span>:<br/></span><span>  </span><span>my_network</span><span>:<br/></span><span>    </span><span>driver</span><span>: </span>bridge</pre>
<p>The preceding file uses the YAML syntax to define two containers. The first is the <kbd>catalog_api</kbd> container: it will be used to host the core part of the service built on top of the ASP.NET Core framework. The second container is <kbd>catalog_db</kbd>, which uses a <kbd>microsoft/mssql-server-linux</kbd> image (the same that we used in <a href="84b281bd-11a2-4703-81ae-ca080e2a267a.xhtml">Chapter 8</a><span>, <em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">Building the Data Access Layer</span></span></em></span>) to set up the MSSQL database.</p>
<p>Furthermore, we should proceed by creating the following folder structure in the root of the project:</p>
<pre><strong>mkdir containers</strong><br/><strong>mkdir containers/api</strong><br/><strong>mkdir containers/db</strong></pre>
<p>The preceding folders will contain the files related to the API and the database containers specified in the <kbd>docker-compose.yml</kbd> file. Let's continue by examining the definition of the <kbd>catalog_api</kbd> container:</p>
<pre><span>  </span><span>catalog_api</span><span>:<br/></span><span>    </span><span>container_name</span><span>: </span>catalog_api<br/><strong>    <span>build</span><span>: <br/></span><span>      </span><span>context</span><span>: </span>.</strong><br/><strong>      <span>dockerfile</span><span>: </span>containers/api/Dockerfile</strong></pre>
<p>The preceding snippet of code specifies the current folder as the build context and the <kbd>containers/api/Dockerfile</kbd> file to build the Docker image. It also refers to an environment variables file by using the following syntax:   </p>
<pre>...<br/><span>  env_file</span><span>:<br/></span><span>      - </span>containers/api/api.env<br/>...</pre>
<p>Finally, it declares the container under a network called <kbd>my_network</kbd> and exposes port <kbd>5000</kbd> to the hosting system using the <kbd>ports:</kbd> directive. </p>
<p>In the same way, the <kbd>catalog_db</kbd> container declares the same network defined for the <kbd>catalog_api</kbd> container, and it specifies a different environment variables file using the same approach as seen previously.</p>
<p>At the end of the <kbd>docker-compose.yml</kbd> file, there is the definition of <kbd>my_network</kbd>, which uses a bridge driver. The bridge driver is the default option for the network. Two containers under the same bridge network can share <span>traffic.</span></p>
<div class="packt_infobox">For more information about the types of drivers provided out of the box, refer to the following link: <a href="https://docs.docker.com/network/">https://docs.docker.com/network/</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining environment variables</h1>
                </header>
            
            <article>
                
<p>Docker provides a lot of ways to specify the environment variables of a container. In the <kbd>docker-compose.yml</kbd> file specified earlier; we use the <kbd>env_file</kbd> approach. Furthermore, we can proceed by creating the <kbd>api/api.env</kbd> file in the corresponding path specified in the definition of the <kbd>catalog_api</kbd> container:</p>
<pre>ASPNETCORE_URLS=http://*:5000;https://*:5001<br/>ASPNETCORE_ENVIRONMENT=Integration</pre>
<p><span>The file syntax </span><span>expects each line in</span><span> </span><span>the </span><span>file to be</span><span> in the </span><span>following </span><span>format: <kbd>VAR=VAL</kbd>. In the preceding case, we are defining the environment variables used by ASP.NET Core to run the service: the <kbd>ASPNETCORE_URLS</kbd> variable specifies the URLs used by the web service and </span><kbd>ASPNETCORE_ENVIRONMENT</kbd> specifies the environment name used by the application. In the same way, we should also proceed by defining the <kbd>db/db.env</kbd> file in the corresponding folder:</p>
<div>
<pre><span>SA_PASSWORD</span><span>=</span><span>P@ssw0rd<br/></span><span>ACCEPT_EULA</span><span>=</span><span>"Y"</span></pre></div>
<p>In this case, the file defines the corresponding variables for the SQL Server container: <kbd>SA_PASSWORD</kbd> specifies the system administrator account password, the <kbd>ACCEPT_EULA</kbd> needed by the startup process of SQL Server.</p>
<p>In addition to this, <kbd>docker-compose</kbd> <span>supports declaring default environment variables in the <kbd>.env</kbd> file. The file must be placed in the same directory as the <kbd>docker-compose.yml</kbd> file. The file contains some simple rules for defining environment variables. Let's create a new <kbd>.env</kbd> file in the root folder of the catalog service directory:</span></p>
<pre>COMPOSE_PROJECT_NAME=store</pre>
<p>The <kbd>COMPOSE_PROJECT_NAME</kbd> variable is a reserved variable of the <kbd><span>docker-compose</span></kbd> command provided by Docker. It specifies the project name to use to run the containers. Therefore, both the <kbd>catalog_api</kbd> and <kbd>catalog_db</kbd> containers will run under the same project, called <kbd>store</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining the Dockerfile</h1>
                </header>
            
            <article>
                
<p>Let's proceed by describing the Dockerfile of our compose project. As mentioned previously, the Dockerfile is <span>a simple text file that contains the commands a user could call to assemble an image. Let's examine a possible definition of the Dockerfile contained in the <kbd>containers/api/</kbd> folder:</span></p>
<pre>FROM mcr.microsoft.com/dotnet/core/sdk:3.1.100<br/>COPY . /app<br/>WORKDIR /app<br/>RUN dotnet restore<br/>RUN dotnet build<br/>RUN dotnet tool install --global dotnet-ef<br/>ENV PATH="${PATH}:/root/.dotnet/tools"<br/>RUN chmod +x containers/api/entrypoint.sh<br/>CMD /bin/bash containers/api/entrypoint.sh</pre>
<p>This code defines specific steps to build the Docker image. The <kbd>FROM</kbd> directive refers to the base image to use during the build process. This directive is mandatory, and it must be the first instruction of the file. The <kbd>COPY</kbd> directive copies the project into the <kbd>/app</kbd> folder, and the <kbd>WORKDIR</kbd> command sets the <kbd>/app</kbd> folder as the default working directory. After that, the build script proceeds by executing the <kbd>dotnet restore</kbd> and <kbd>dotnet build</kbd> commands. Finally, the Dockerfile adds the <kbd>/root/.dotnet/tools</kbd> path inside the <kbd>PATH</kbd> variable and executes the <kbd>containers/api/entrypoint.sh</kbd> Bash file, which has the following content:</p>
<pre>#!/bin/bash<br/>set -e<br/>run_cmd="dotnet run --verbose --project ./src/Catalog.API/Catalog.API.csproj"<br/>until dotnet-ef database update --verbose --project ./src/Catalog.API/Catalog.API.csproj ; do<br/>&gt;&amp;2 echo "SQL Server is starting up"<br/>sleep 1<br/>done<br/>&gt;&amp;2 echo "SQL Server is up - executing command"<br/>exec $run_cmd</pre>
<p>The <kbd>entrypoint.sh</kbd> <span>entry point </span>file is stored on the same level as the Dockerfile. It runs the main project of the <span>catalog service</span><span> </span><span>by performing the</span> <kbd>dotnet run</kbd> <span>command and, once the database container is ready, it proceeds by executing the</span> <kbd>dotnet ef database update</kbd> <span>command to create the database schema.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Executing the docker-compose command</h1>
                </header>
            
            <article>
                
<p>To run the catalog service locally, we must complete the compose process from the CLI using the <kbd>docker-compose</kbd> command. It is possible to get an overview of the commands by running <kbd>docker-compose --help</kbd>.</p>
<p>The main commands related to the composition of a multi-container application are as follows:</p>
<ul>
<li><kbd>docker-compose build</kbd>: This builds the services. It executes the build using the Dockerfile associated with the images.</li>
<li><kbd>docker-compose images</kbd>: This lists the current container images.</li>
<li><kbd>docker-compose up</kbd>: This creates and runs the containers.</li>
<li><kbd>docker-compose config</kbd>: This validates and views the <kbd>docker-compose.yml</kbd> file.</li>
</ul>
<p>We can proceed by running the catalog service using the following command:</p>
<pre><strong>docker-compose up --build</strong></pre>
<p>By specifying the <kbd>--build</kbd> flag, it is possible to trigger the build before running the containers. Once the build is made, we can just run the <kbd>docker-compose up</kbd> command until our code changes and we need to rebuild the project.</p>
<p>Although we are now able to run our service using containers, the build and run process is not optimized: we are copying all of the files in the solution into the container; on top of that, we are running the container using the whole SDK of .NET Core, which is not needed if we want to run the project. In the next section, we will see how to optimize the containerization process to be more lightweight.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimizing Docker images</h1>
                </header>
            
            <article>
                
<p class="mce-root">Microsoft provides different Docker images to run ASP.NET Core, and, in general, .NET Core applications using Docker. It is essential to understand that a container that executes an ASP.NET Core service doesn't need to provide the SDK as well.</p>
<p>This section presents an overview of the different Docker images available on Docker Hub and how to optimize our deployment using proper Docker images.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An overview of different Docker images</h1>
                </header>
            
            <article>
                
<p><span>Microsoft provides various images, depending on what it is you're trying to achieve with your application. </span>Let's take a look at the different Docker images supplied in the <kbd>microsoft/dotnet</kbd> repository in Docker Hub (<a href="https://hub.docker.com/r/microsoft/dotnet/">https://hub.docker.com/u/microsoft/</a>):</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td><strong>Image</strong></td>
<td><strong>Description</strong></td>
<td><strong>Size</strong></td>
</tr>
<tr>
<td><kbd>mcr.microsoft.com/dotnet/core/sdk:3.1</kbd></td>
<td>This image contains the whole SDK of .NET Core. It provides all of the tools to develop, run, and build your application. It is possible to use development commands such as <kbd>dotnet run</kbd>, <kbd>dotnet ef</kbd>, and the whole set of commands provided by the SDK.</td>
<td>690 MB</td>
</tr>
<tr>
<td><kbd>mcr.microsoft.com/dotnet/core/runtime:3.1</kbd></td>
<td>This image contains the .NET Core runtime. It provides a way to run .NET Core applications, such as console applications. Since the image contains only the runtime, it is not possible to build the application. The image exposes only the runtime CLI command, <kbd>dotnet</kbd>.</td>
<td>190 MB</td>
</tr>
<tr>
<td><kbd>mcr.microsoft.com/dotnet/core/aspnet:3.1</kbd></td>
<td>This image contains the .NET Core runtime and the ASP.NET Core runtime. It is possible to execute both the .NET Core application and the ASP.NET Core application. As a runtime image, it is not possible to build the application.</td>
<td>205 MB</td>
</tr>
<tr>
<td><kbd>mcr.microsoft.com/dotnet/core/runtime-deps:3.1</kbd></td>
<td>This image is a very light one. It contains only the lower-level dependencies (<a href="https://github.com/dotnet/core/blob/master/Documentation/prereqs.md">https://github.com/dotnet/core/blob/master/Documentation/prereqs.md</a>) that .NET Core needs to run. It doesn't contain the .NET Core runtime or the ASP.NET Core runtime. It is designed for self-hosted applications.</td>
<td>110 MB</td>
</tr>
</tbody>
</table>
<div class="packt_infobox"><span>Docker images are usually available in three modes: <em>debian:stretch-slim</em>, <em>ubuntu:bionic</em>, and <em>alpine</em>, depending on the OS the image runs on. By default, images run on the Debian<em> </em></span>OS<span>. It is possible, however, to use another OS, such as <em>Alpine</em>, to save some storage. For example, Alpine-based images reduce the size of the <kbd>aspnetcore-runtime</kbd> image from ~260 MB to ~160 MB.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multi-stage builds on the catalog service</h1>
                </header>
            
            <article>
                
<p>Let's apply the concept of a <em>multi-stage</em> build to the previously defined Docker image. Multi-stage builds are a new feature that requires Docker 17.05 or higher. Multi-stage builds are useful for anyone who has struggled to optimize Dockerfiles while keeping them easy to read and maintain.</p>
<p>Let's explore how it is possible to apply the multi-stage build process by taking a look at the catalog service Dockerfile:</p>
<pre><span>FROM </span>microsoft<span>/</span>dotnet<br/><span>COPY </span>. <span>/</span>app<br/><span>WORKDIR /</span>app<br/><span>RUN </span>dotnet restore<br/><span>RUN </span>dotnet build<br/><span>RUN </span>chmod <span>+</span>x .<span>/</span>entrypoint.sh<br/><span>CMD /</span>bin<span>/</span>bash .<span>/</span>entrypoint.sh</pre>
<p>The previously defined file can be changed in the following way:</p>
<pre><span>FROM </span>mcr.microsoft.com<span>/</span>dotnet<span>/</span>core<span>/</span>aspnet:<span>3.1 </span><span>AS </span>base<br/><span>WORKDIR /</span>app<br/><br/><span>FROM </span>mcr.microsoft.com<span>/</span>dotnet<span>/</span>core<span>/</span>sdk:<span>3.1 </span><span>AS </span>build<br/><span>WORKDIR /</span>project<br/><span>COPY </span>[<span>"src/Catalog.API/Catalog.API.csproj"</span>, <span>"src/Catalog.API/"</span>]<br/><span>COPY </span>. .<br/><span>WORKDIR </span><span>"/project/src/Catalog.API"<br/></span><span>RUN </span>dotnet build <span>"Catalog.API.csproj" </span><span>-</span>c Release <span>-</span>o <span>/</span>app<span>/</span>build<br/><br/><span>FROM </span>build <span>AS </span>publish<br/><span>RUN </span>dotnet publish <span>"Catalog.API.csproj" </span><span>-</span>c Release <span>-</span>o <span>/</span>app<span>/</span>publish<br/><br/><span>FROM </span>base <span>AS </span>final<br/><span>WORKDIR /</span>app<br/><span>COPY --</span>from=publish <span>/</span>app<span>/</span>publish .<br/><span>ENTRYPOINT </span>[<span>"dotnet"</span>, <span>"Catalog.API.dll"</span>]</pre>
<p>As you can see, the Dockerfile now executes three different steps (the first two steps are described together because they use the same Docker image):</p>
<ul>
<li>The <kbd>builder</kbd> step uses the <kbd><span>mcr.microsoft.com/dotnet/core/sdk</span></kbd> image to copy the files and trigger the build of the project using the <kbd>dotnet build</kbd> command.</li>
<li>The <kbd>publish</kbd> step uses the same image to trigger the <kbd>dotnet publish</kbd> command for the <kbd>Catalog.API</kbd> project.</li>
<li>The <kbd>final</kbd> step, as the name suggests, executes the published package in the runtime environment using the <kbd>mcr.microsoft.com/dotnet/core/aspnet:3.1</kbd> image. Finally, it runs the service using the <kbd>ENTRYPOINT</kbd> command.</li>
</ul>
<p>It is important to note that this change optimizes the resulting image produced by the Dockerfile. After this change, we don't need the <kbd>entrypoint.sh</kbd> file anymore because the Dockerfile directly triggers the execution of the service using the <kbd>dotnet Catalog.API.dll</kbd> command.</p>
<p>Using a multi-stage build approach, we should also notice that we cannot trigger the execution of the database migrations because the runtime Docker image doesn't use the <strong>Entity Framework Core</strong> <span>(<strong>EF Core</strong>) </span>tools. Consequently, we need to find another way to trigger the migrations. One possible option is to unleash the migrations from the <kbd>Startup</kbd> file of our service. Furthermore, EF Core provides a way to apply migrations at runtime by using the following syntax in the <kbd>Configure</kbd> method:</p>
<pre><strong>using Polly;<br/>using Microsoft.Data.SqlClient;</strong><br/>...<br/>    <span>public class </span><span>Startup<br/></span><span>    </span>{<br/>        ...<br/>        public void Configure(IApplicationBuilder app, IWebHostEnvironment env)<br/>        {<br/>            if (env.IsDevelopment()) app.UseDeveloperExceptionPage();<br/><br/>           <strong> ExecuteMigrations(app, env);</strong><br/>            ...<br/>        }<br/><br/>        <strong>private void ExecuteMigrations(IApplicationBuilder app, <br/>         IWebHostEnvironment env)</strong><br/>        {<br/>            if (env.EnvironmentName == "Testing") return;<br/>            <br/>            var retry = Policy.Handle&lt;SqlException&gt;()<br/>                .WaitAndRetry(new TimeSpan[]<br/>                {<br/>                    TimeSpan.FromSeconds(2),<br/>                    TimeSpan.FromSeconds(6),<br/>                    TimeSpan.FromSeconds(12)<br/>                });<br/>            <br/>            retry.Execute(() =&gt; <br/>                <strong>app.ApplicationServices.GetService&lt;CatalogContext&gt;().Database.Migrate()</strong>);<br/>        }<br/>    }<br/>}</pre>
<p>The preceding code ensures the execution of the database migrations through the execution of the <kbd>app.ApplicationServices.GetService&lt;CatalogContext&gt;().Database.Migrate()</kbd> instruction. Because of the startup times of the <kbd>msssql</kbd> container, we need to implement a retry policy by handling <kbd>SqlException</kbd> and retrying with an exponential time approach. The preceding implementation uses <kbd>Polly</kbd> to define and execute a retry policy. Furthermore, we need to add the <kbd>Polly</kbd> dependency by executing the following command in the <kbd>Catalog.API</kbd> project:</p>
<pre><strong>dotnet add package Polly</strong></pre>
<p>Retry policies are really useful in the distributed systems world, to successfully handle failure. <kbd>Polly</kbd> and the implementation of resilience policies in a web service will be discussed in the next chapter as part of communication over HTTP between multiple services.</p>
<div class="packt_infobox">In a real-world application, it is quite unusual to execute migrations during the deployment phase of the service. The database schema rarely changes, and it is essential to separate the implementations related to the service with the changes made in the database schema. For demonstration reasons, we are executing the migrations of the database in every deployment by overwriting the database changes to take the most straightforward approach.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we provided a quick overview of the capabilities of Docker and how to use it to improve isolation, maintainability, and service reliability. We looked at the different Docker images supplied by Microsoft and how to use them combined with a multi-step build approach.</p>
<p>The topics covered in this chapter provide an easy way to run our service locally, in an isolated environment, and spread the same environment configuration into staging and production environments.</p>
<p>In the next chapter, we are going to improve our knowledge of how to share information between multiple services. We will also explore some patterns to do with multi-service systems. The concepts related to Docker that we explored in this chapter will also be used in the next chapter to provide a smooth deployment experience.</p>


            </article>

            
        </section>
    </body></html>