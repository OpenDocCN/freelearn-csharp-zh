- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building Advanced XR Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now, you have explored various parts of XR development, crafting immersive
    and interactive XR applications suited for a wide array of use cases. As you venture
    further into mastering XR development, it’s essential to not only be proficient
    at creating basic to intermediate XR applications but also master advanced techniques
    that elevate the commercial viability and influence of your XR offerings.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is designed to familiarize you with crucial, high-level XR methods,
    all through a hands-on approach. You will dive into hand-tracking integration,
    use eye- and head-tracking for complex interactions, and discover how to establish
    a multiplayer server to create an engaging multiplayer XR experience.
  prefs: []
  type: TYPE_NORMAL
- en: The content of this chapter might sound intimidating but don’t worry – we’re
    here to guide you every step of the way as you incorporate these sophisticated
    XR strategies into various scenes. Leveraging the solid XR foundation you’ve built
    from earlier chapters, you’ll find these advanced techniques more intuitive than
    anticipated.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of the XR equipment you possess, this chapter promises a wealth of
    knowledge, with all techniques being adaptable to different setups. We’ll delve
    deeper into this in the *Technical* *requirements* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter includes the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Adding hand-tracking to XR experiences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interacting with objects in XR experiences via eye- or head-tracking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a VR multiplayer application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Navigating the hand-tracking, eye-tracking, and multiplayer aspects of this
    chapter requires understanding both shared and unique technical prerequisites.
    For a seamless development experience, we recommend acquainting yourself with
    all listed requirements. Though this chapter delves into advanced topics and may
    initially seem daunting compared to earlier sections, you can be sure that all
    tutorials in this chapter are designed for straightforward execution – even if
    you don’t have a VR headset at hand.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s address the overarching technical requirements. To follow along
    with the tutorials in this chapter, you’ll need Unity *2021.3 LTS* or a newer
    version. Validate your hardware’s suitability by comparing it to the system requirements
    described on Unity’s website: [https://docs.unity3d.com/Manual/system-requirements.html](https://docs.unity3d.com/Manual/system-requirements.html).
    Depending on your VR headset’s specifications, ensure that your setup supports
    *Windows*/*Linux*/*Mac* or *Android* *Build Support*.'
  prefs: []
  type: TYPE_NORMAL
- en: Most contemporary VR headsets, particularly those with inside-out camera tracking,
    incorporate hand-tracking capabilities. Examples include the Meta Quest series,
    HTC Vive Cosmos Elite, PlayStation VR2, Pico Neo 2 Eye, the Lynx-R1, Valve Index,
    HP Reverb G2, Varjo VR-3, and the soon-to-be-released Apple Vision Pro. Always
    refer to the technical specifications of your VR headset to find out whether it
    supports hand-tracking.
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t have access to a headset that supports hand-tracking, you can still
    follow this chapter as the XR Device Simulator can replicate the hand-tracking
    features of a VR headset flawlessly.
  prefs: []
  type: TYPE_NORMAL
- en: Eye-tracking in VR is an evolving field, and interestingly, you can delve into
    the eye-tracking tutorials of this chapter’s eye- and head-gaze-tracking section
    regardless of your VR headset’s specifications. Even if you’re working solely
    with the XR Device Simulator without a physical VR headset, you’ll find our tutorial
    accessible. While we won’t spoil all the details now, it’s worth noting that the
    XR Interaction Toolkit provides innovative solutions for situations where a VR
    headset lacks standard eye-tracking capabilities. As of the time of this book’s
    publication, VR headsets that offer eye-tracking include the PlayStation VR2,
    HP Reverb G2 Omnicept Edition, Pico Neo 3 Pro Eye, HTC Vive Pro Eye, Pico Neo
    2 Eye, and Meta Quest Pro. Furthermore, the upcoming Apple Vision Pro is anticipated
    to feature eye-tracking. This list might not cover all available options by the
    time you read this book, so always check the specifications of your VR headset
    to confirm eye-tracking support.
  prefs: []
  type: TYPE_NORMAL
- en: Now that your hardware is all set up, let’s start exploring hand-tracking using
    the XR Interaction Toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: Adding hand-tracking to XR experiences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will learn how to add a hand-tracking functionality to
    your XR experiences. Before creating a new Unity project, however, you must understand
    the technical concept of hand-tracking and how far it can enrich an XR experience
    compared to regular controllers.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding hand-tracking and potential use cases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At its core, **hand-tracking** in VR refers to the technological capability
    of directly detecting, capturing, and interpreting the nuanced movements and positioning
    of a user’s bare hands and fingers within a virtual environment.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike controller-based tracking, which relies on external devices to mediate
    and translate user inputs into VR actions such as an Xbox controller, hand-tracking
    operates without intermediary hardware, offering a direct mapping of real-world
    hand gestures and movements into the virtual realm. This approach leverages sophisticated
    sensors, cameras, and algorithms to construct a real-time, dynamic model of the
    user’s hand. From grabbing objects to casting spells with finger gestures, hand-tracking
    provides a vast array of potential interactions. It facilitates complex and subtle
    interactions that are hard to replicate with traditional controllers, permitting
    more organic and intuitive interactions within VR.
  prefs: []
  type: TYPE_NORMAL
- en: For a VR headset to harness the potential of hand-tracking, it should be equipped
    with high-resolution sensors and cameras capable of capturing detailed movements,
    down to the subtle motions of individual fingers. These cameras often need to
    be oriented in a manner that provides a wide field of view to consistently track
    hands as they move.
  prefs: []
  type: TYPE_NORMAL
- en: Hand-tracking requires real-time interpretation of complex hand and finger movements,
    necessitating robust processing power. The VR headset should have an onboard processor
    or be connected to a machine that can handle these computations without latency.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond hardware, the headset’s software must be designed or adaptable to recognize
    and interpret hand movements effectively. This includes having algorithms capable
    of differentiating between intentional gestures and inadvertent hand motions.
  prefs: []
  type: TYPE_NORMAL
- en: Building on these foundational requirements, the next section will guide you
    through setting up our Unity project to effectively utilize and enable hand-tracking,
    unlocking a richer, more immersive experience for users of your XR experiences.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing hand-tracking with the XR Interaction Toolkit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To kick off our process of adding hand-tracking capabilities to an XR project,
    we must perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to Unity Hub and create a new project by navigating to the `AdvancedXRTechniques`,
    and clicking the **Create** **project** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the **Scene Hierarchy** window, you will see that your scene only contains
    **Main Camera** and **Directional Light**. You can delete **Main Camera** as it
    is not needed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import `com.unity.xr.interaction.toolkit`, and hitting *Enter*. The toolkit
    should now be automatically added to your project. Staying in the **Package Manager**
    window, navigate to the **Samples** tab of the newly added **XR Interaction Toolkit**
    package and import **Starter Assets**, **XR Device Simulator**, and **Hands Interaction
    Demo** by clicking the **Import** button next to each of them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To enable hand-tracking in our scene, we don’t only need `com.unity.xr.hands`,
    and hitting *Enter*. Once the package has been added to your project, navigate
    to the **Samples** tab of the **XR Hands** package in the **Package Manager**
    window and click the **Import** button next to the **HandVisualizer** sample.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, we would typically drag and drop the `XR Interaction Hands Setup` into
    the search bar of the `0`,`0`,`0`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is time to set up **XR Plug-in Management** correctly to enable hand-tracking.
    Navigate to **Edit** | **Project Settings** | **XR Plug-in Management** and select
    the **OpenXR** checkbox on either the **Windows/Mac/Linux** tab or **Android**,
    depending on the needs of your VR headset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the **OpenXR** plugin has been installed, navigate to its subtab underneath
    the **XR Plug-in Management** tab on the left-hand side. Here, go to either the
    **Windows/Mac/Linux** or **Android** tab. Select the **+** button to add an **Interaction
    Profile** item to your project. Besides selecting the controllers of your VR headset
    in the newly opened menu, you should also add another **Interaction Profile**
    called **Hands Interaction Profile**. At its core, **Hand Interaction Profile**
    in **OpenXR** provides a standardized way to interpret hand gestures and movements
    across different VR headsets. Different VR headsets might have their own technology
    and methods for tracking hands. Without a standardized system, developers would
    need to write unique code for each headset’s hand-tracking system, which can be
    time-consuming and impractical.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Staying in the **OpenXR** subtab, select the **Hand Interaction Poses** and
    **Hand Tracking Subsystem** checkboxes, regardless of which VR headset you have.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you check **Hand Interaction Poses**, you are telling Unity to use a standard
    set of poses or gestures defined by the **OpenXR** standard. These include grabbing
    (grip), pointing (aim), pinching, and poking. So, instead of manually coding the
    detection of these gestures, Unity does it for you based on this standard. By
    selecting the **Hand Tracking Subsystem** checkbox, Unity uses the **OpenXR**
    standard to keep track of where the hands are and how they move. This subsystem
    is like the engine under the hood that keeps an eye on hand movements.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you have a Meta Quest device, you must also select the **Meta Hand Tracking
    Aim** checkbox. This feature enhances the existing hand-tracking by also understanding
    the direction or aim of the hands, giving your VR app a better sense of where
    users are pointing or what they might be trying to interact with. While our **Hand
    Interaction Profile** provides a base layer of understanding hand movements, these
    checkboxes dive deeper into gesture specifics, actual tracking, and device-specific
    features.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, let’s add a simple plane to the scene functioning as a ground floor and
    a cube to interact with testing out hand-tracking. You can achieve this by right-clicking
    in the hierarchy and selecting `Ground Floor` and position it at the origin (`0``0``0`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the previous step but instead of selecting `Hand Tracking Cube`. Position
    it at (`0`, `1.25`, `1`) and scale it to (`0.1`, `0.1`, `0.1`). In the `Hand Tracking
    Cube`, click the `XR Grab Interactable` in the search bar. Select the **XR Grab
    Interactable** script by double-clicking on it. You should see that a **Rigidbody**
    component has been automatically added to **InteractableCube**, alongside the
    **XR Grab Interactable** script. Inside the **Rigidbody** component, make sure
    that the **Use Gravity** and **Is Kinematic** checkboxes are selected so that
    our interaction with the cube is possible while obeying the laws of gravity.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t have access to a VR headset with hand-tracking capabilities, you
    can simply test this feature by using the XR Device Simulator, as detailed in
    the *Installing the XR Device Simulator* and *Using the XR Device Simulator* sections
    of [*Chapter 3*](B20869_03.xhtml#_idTextAnchor009). To switch to hand-tracking
    mode, simply press the *H* key.
  prefs: []
  type: TYPE_NORMAL
- en: It’s time to test out the scene using your VR headset. Start the scene as you
    typically would. You don’t even need to have the VR headset’s controllers on hand.
    Once the scene is running, adjust your head so that the external cameras on the
    VR headset are directed at your hands.
  prefs: []
  type: TYPE_NORMAL
- en: You should notice that the controller visuals have been replaced by hand visuals,
    mirroring the exact position and movements of your real hands, as shown in *Figure
    8**.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – The hand visuals you will see once you put away your controllers
    and direct your gaze toward your hands](img/B20869_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – The hand visuals you will see once you put away your controllers
    and direct your gaze toward your hands
  prefs: []
  type: TYPE_NORMAL
- en: Try moving each finger separately, shift your hands around, and even hide one
    hand behind your back. If everything was set up correctly and your VR headset
    fully supports hand-tracking, the virtual hands should mimic your real hand movements
    accurately. If you hide one hand behind your back, its corresponding virtual hand
    should vanish too.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let’s test the interaction with the cube in your scene using only hand-tracking.
    Aim your right hand’s laser pointer at the cube. Touch the tips of your right
    thumb and right index finger together, forming a near triangle shape, as shown
    in *Figure 8**.2*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – The overlayed hand visuals as you touch your right thumb and
    right index finger together in real life](img/B20869_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – The overlayed hand visuals as you touch your right thumb and right
    index finger together in real life
  prefs: []
  type: TYPE_NORMAL
- en: You are now grabbing the cube. While maintaining this position, point in various
    directions and observe the cube moving correspondingly.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You’ve now achieved a similar interactive experience in your
    VR scene with hand-tracking as you would have using standard VR controllers.
  prefs: []
  type: TYPE_NORMAL
- en: 'To explore how the hand-tracking features of the XR Interaction Toolkit work
    with other types of objects such as UI elements or buttons, delve into **Hands
    Interaction Demo**, which is available within the toolkit’s **Samples** area that
    we imported at the beginning. To access it, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Head to **Assets** in the **Project** window. From there, navigate to **Samples**
    | **XR Interaction Toolkit** | **Version Number** | **Hands Interaction Demo**
    | **Runtime**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, double-click on the **HandsDemoScene** Unity scene. If you prefer, you
    can quickly locate **HandsDemoScene** using the search bar in the **Project**
    window.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you’ve opened the scene in the Unity Editor, hit the **Play** button. This
    will let you experience firsthand how to engage with UI elements, press buttons,
    and even manipulate 3D objects using just your hands.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'By now, you should feel like a hand-tracking expert. The next section will
    introduce you to another advanced concept of XR development: eye-tracking.'
  prefs: []
  type: TYPE_NORMAL
- en: Interacting with objects in XR experiences via eye- or head-tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will not only learn about eye-tracking itself and how it
    can enrich your XR experience, but you will also implement eye-tracking functionalities
    to your XR experiences, regardless of whether your VR headset supports eye-tracking
    or not.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding eye-tracking and potential use cases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From reading a person’s intent to enhancing digital interactions, eye-tracking
    technology is revolutionizing how technologies of all kinds perceive and interpret
    human behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'The eyes are often dubbed the “windows to the soul” due to their ability to
    express and convey emotions, intent, and attention. Biologically speaking, several
    key aspects of the eyes play into this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pupil dilation**: Often a subconscious response, the pupils can dilate or
    contract based on emotional states, levels of attention, or reactions to stimuli.
    For instance, someone’s pupils might dilate upon seeing someone they’re attracted
    to or contract when exposed to bright light.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Saccades**: These are rapid, jerky movements of the eyes when they change
    focus from one point to another. Often unnoticed by us, saccades play a pivotal
    role in how we gather visual information from our surroundings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Blinking and micro-expressions**: The rate of blinking can indicate various
    states, from relaxation to stress. Furthermore, subtle movements around the eyes
    can give away fleeting emotions – these are known as micro-expressions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Eye-tracking technology revolves around monitoring and recording the eye’s
    movement and gaze points. Here’s how it typically works:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Light source**: Infrared light is directed toward the eyes. This light reflects
    off the cornea and retina.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sensors and cameras**: These detect the reflected light off the eyes. Advanced
    systems might use multiple cameras from different angles to capture a three-dimensional
    view of the eye’s movement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data processing**: The raw data captured by the sensors is processed using
    algorithms to deduce the direction of the gaze and the point of focus.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Representation**: The gaze data is usually represented as a heatmap or gaze
    plot on the observed medium, be it a computer screen or a physical environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eye-tracking bridges the gap between the digital and real worlds by making virtual
    interactions more human-like. When social avatars in XR mimic real eye movements
    by blinking, gazing, and showing emotions, it deepens the sense of presence and
    immersion for the user.
  prefs: []
  type: TYPE_NORMAL
- en: Likewise, eye-tracking can drastically improve the understanding of user intent
    in the XR space. This means that XR environments can adapt in real time to the
    user’s focus. For example, a horror game could trigger a scare only when the user
    is looking in the right direction, maximizing the emotional impact. By analyzing
    where users look, how often, and for how long, developers can glean valuable insights.
    This can guide design choices, ensure important elements capture attention, and
    refine user interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: Eye-tracking paves the way for more intuitive user interfaces. For instance,
    instead of navigating through menus using clunky hand controllers, users can simply
    gaze at a menu option to select it.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, you can enrich your XR scene with eye-tracking, regardless of whether
    your VR headset supports it or not. If this sparks your curiosity, follow along
    with the tutorial in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up an XR scene to support eye- and head-tracking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The XR Interaction Toolkit supports eye- and head-tracking, enhancing user engagement
    in XR apps. While eye-tracking specifically captures where the user’s eyes are
    focused, head-tracking determines the direction the user’s head is pointing or
    gazing. Let’s try out these tracking techniques ourselves by revisiting the basic
    VR scene we created in our last session.
  prefs: []
  type: TYPE_NORMAL
- en: Some key components of the XR Interaction Toolkit’s eye- and head-tracking functionalities
    are already inside of our scene. To observe them, click on the arrow button next
    to the **XR Interaction Hands Setup** prefab in the **Scene Hierarchy** window
    of your project to see its children. Navigate to **XR Origin (XR Rig)** | **Camera
    Offset** and enable the **XR Gaze Interactor** and **Gaze Stabilized** prefabs.
    These prefabs are not only part of the **XR Interaction Hands Setup** prefab,
    but also the **XR Interaction Setup** prefab, which you would use in your scene
    if you don’t include hand-tracking.
  prefs: []
  type: TYPE_NORMAL
- en: These prefabs work with all kinds of VR headsets, regardless of whether they
    support eye-tracking or not. If your headset doesn’t support eye-tracking, it
    will use the built-in head-tracking feature of the VR headset to estimate the
    head gaze.
  prefs: []
  type: TYPE_NORMAL
- en: 'While incorporating eye- and head-tracking into our scene is a significant
    step forward, it’s only part of the equation for crafting intuitive gaze-based
    interactions in XR. If it were that simple, there would be no need for this chapter,
    especially considering we’ve utilized this prefab in every VR scene throughout
    this book. To truly harness the capabilities of eye-tracking, we must also populate
    our scene with objects that can interact with the **XR Gaze Interactor** prefab.
    Let’s create these objects:'
  prefs: []
  type: TYPE_NORMAL
- en: To keep our scene organized, right-click on `Hand Tracking Cube` in the `Eye
    Tracking and Hand Tracking Interactables`. This GameObject will store two more
    cubes enabling different kinds of eye-tracking interactions alongside the cube
    we already created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s place all of the cubes on a very simple table. Create the table by clicking
    the `Table`, position it at (`0`, `0.5`, `0`), and scale it to (`3`, `1`, `1`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scale `Hand Tracking Cube` to (`0.5`, `0.5`, `0.5`) and position it on `Table`
    by changing the values to (`-1`, `0`, `0`). Also, make sure you uncheck the `Hand
    Tracking Cube` and select the `Hand Tracking Cube`. Rename the two cubes `Eye
    Tracking Cube 1` and `Eye Tracking Cube 2`. Change the position of `Eye Tracking
    Cube 1` to (`0`, `0`, `0`) and the position of `Eye Tracking Cube 2` to (`1`,
    `0`, `0`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, let’s add the function that when the user’s gaze is pointing at `Eye Tracking
    Cube 1`, a sphere will appear on top of it. We can add a sphere to our scene by
    selecting `Eye Tracking Cube 1` in the `0`, `1`, `0`) and scale it to (`0.5`,
    `0.5`, `0.5`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Similar to `Eye Tracking Cube 1`, some text should appear when the user of our
    VR scene looks at `Eye Tracking Cube 2`. To make the interaction more complex,
    the text should change, depending on whether the user looks at the cube and whether
    the cube is selected or deselected via a controller button press. To create the
    needed UI elements, right-click on `Eye Tracking Cube 2` in the `0`, `-1`, `-1.01`)
    and its `1`. Now, right-click on `No state detected!` in the `Interactable Text`,
    scale it to `0.005` in all directions, and position it at (`0.1`, `0.1`, `0`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s add some colorful materials to our scene to make it more visually appealing.
    Specifically, we want to add two materials for our cubes – one for their default
    appearance and one if they are hovered over. So, we will create a material for
    `Table` and one for `Materials` and double-click on it to open it. Create a new
    material inside the `Materials` folder by right-clicking, selecting `HighlightedCube`.
    In the `HighlightedCube`, click on the colored cell next to the `240`, G: `240`,
    B: `140`) with an `70`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Right-click inside the `Materials` folder and select `CubeMaterial`, `TableMaterial`,
    and `SphereMaterial` materials. For `CubeMaterial`, we chose a sophisticated red
    color (R: `186`, G: `6`, B: `6`); `TableMaterial` has been assigned a dark brown
    color (R: `58`, G: `40`, B: `3`); and for `SphereMaterial`, we selected a blue
    color (R: `29`, G: `120`, B: `241`) and made it appear metallic by setting the
    `1`. All of these materials have an `255`. Apply `CubeMaterial` to all three cubes
    in the scene by simply dragging and dropping the material from the `TableMaterial`
    and `SphereMaterial`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Figure 8**.3* shows what your VR scene should currently look like.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – The current status of the VR scene for eye-tracking](img/B20869_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – The current status of the VR scene for eye-tracking
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn how you can interact with these cubes via
    eye or head gaze.
  prefs: []
  type: TYPE_NORMAL
- en: Interacting with objects via eye and head gaze using the XR Interaction Toolkit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When a user looks at one of the two cubes we created for gaze-tracking, they
    should be able to interact with them as described in the previous section. To
    accomplish this, we need to adjust some components of our scene.
  prefs: []
  type: TYPE_NORMAL
- en: The **XR Gaze Interactor** prefab will only interact with objects in a scene
    that have either an **XR Simple Interactable** or **XR Grab Interactable** script
    attached to them and that have the **Allow Gaze Interaction** checkbox of the
    respective script enabled. This means we must add either one of these two scripts
    to our two new cubes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, we will add the **XR Simple Interactable** script to each of the
    two eye-tracking cubes. We need to perform the following three steps to achieve
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: Press *Ctrl*/*Cmd* and select both cubes in the **Scene** **Hierarchy** window.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the `XR Simple Interactable` into the search bar, and double-click on
    the script to add it to both cubes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the arrow next to the **Gaze Configuration** property of the **XR Simple
    Interactable** script component to open the gaze-related properties. Select the
    **Allow Gaze** **Interaction** checkbox.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we need to specify how we can interact with our two cubes via gaze. Let’s
    start with the first cube. Remember that our goal is to make the blue sphere appear
    on top of the first cube only when our eyes, head, or controllers are directed
    toward it. We can add this logic to the cube by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the `Eye Tracking Cube 1`, navigate to the end of the **XR Simple Interactable**
    script component. Click on the arrow next to **Interactable Events** to open it.
    Add two new events to the **First Hover Entered** function of **First/Last** **Hover**
    by clicking on the **+** button two times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s fill in the missing information of our newly created interactable events.
    As indicated by the `Eye Tracking Cube 1` from the **Scene Hierarchy** window
    into the first **None (Object)** cell. Repeat this for the second event by assigning
    it to **Sphere**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To change the material of `Eye Tracking Cube 1` when it is hovered over via
    controllers, the eyes, or the head, we must assign the necessary functions to
    each interactable event and provide them with the needed parameters. Select `HighlightedCube`
    material via the search bar and select it. For the `Eye Tracking Cube 1` with
    your controllers, eyes, or head, you will see the blue sphere appearing on top
    of it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once your controllers, eyes, or head are no longer directed toward `Eye Tracking
    Cube 1`, the cube’s material should change back to its default color and the sphere
    should disappear. To accomplish this, go to the `Eye Tracking Cube 1` and `CubeMaterial`,
    and select it. Repeat this process for **Sphere** by assigning it to the **GameObject**
    | **SetActive (bool)** function again and ensuring the newly appeared checkbox
    is not checked this time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To evaluate the logic we’ve put in place so far, it’s necessary to either disable
    or hide the blue **Sphere** object in our scene. This ensures that upon initial
    entry into the scene, the user doesn’t see it. You can achieve this by selecting
    **Sphere** within the **Scene Hierarchy** window and then deselecting the checkbox
    next to its name at the top of the **Inspector** window.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Voilà – we’ve completed all the required steps to add a powerful eye-tracking
    capability to our first cube. *Figure 8**.4* shows what the cube should look like
    once your eyes or head are directed toward it.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – The highlighted cube and blue sphere appear because of your
    eyes or head hovering over the cube](img/B20869_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – The highlighted cube and blue sphere appear because of your eyes
    or head hovering over the cube
  prefs: []
  type: TYPE_NORMAL
- en: If you are working with head gaze, you will only see the blue sphere when the
    cube is positioned at the center of your current field of view when you are wearing
    a VR headset or using the XR Device Simulator. You can observe the same effect
    when your controllers are pointing at the cube.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will add an even more powerful interaction to our second cube. Like the
    first cube, a text element will appear underneath the second cube when the eyes,
    head, or controllers are directed toward it. This time, however, the displayed
    text itself will change based on whether the cube is hovered over, selected, or
    deselected using a combination of eye, head, and controller interactions. Follow
    these steps to accomplish this goal:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Repeat all the steps you performed for `Eye Tracking Cube 1` with the following
    modification: drag and drop `Eye Tracking Cube 2` and **Canvas** into the two
    events of the **First Hover Entered** function and the **Last Hover** **Exited**
    function.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So far, the cube’s color changes and the text element becomes visible once the
    cube is hovered over. However, the text itself should also change to `Interactable
    Text` from the `Hovered` into the newly appeared empty text cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the cube is hovered over and selected by pressing the controller button,
    which is typically reserved for moving objects around, the text displayed underneath
    the cube should change. To implement this logic, scroll down to `Interactable
    Text` element from the `Selected` into the empty text field.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the cube is no longer selected via controllers, the text should change
    again. This can be accomplished by adding a new event to the `Interactable Text`
    to the `Deselected` into the newly created text cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before testing out the logic of the second cube, hide it in the scene by deselecting
    it in its **Inspector** window, as you did with the first cube.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is time to try out interacting with this cube. *Figure 8**.5* shows the cube
    in its three interactable states – being hovered over, being hovered over plus
    selected, and being hovered over but deselected.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – The cube in its three interactable states](img/B20869_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – The cube in its three interactable states
  prefs: []
  type: TYPE_NORMAL
- en: Hurray – you have successfully added different eye-tracking interactions to
    your scene! In the next section, you will learn how to set up a multiplayer XR
    game in Unity.
  prefs: []
  type: TYPE_NORMAL
- en: Building a VR multiplayer application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you’ll craft your first VR multiplayer application. Though
    it involves a fair amount of C# coding, you’ve gained enough knowledge to smoothly
    navigate this tutorial. But before we dive in, let’s pause and delve into the
    nuances of constructing multiplayer applications, the necessary components, and
    the appeal of multiplayer experiences within the XR context.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding multiplayer applications and multiplayer networking systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At its core, a **multiplayer** experience allows multiple users to interact
    within a shared digital environment simultaneously. This environment can range
    from simple text-based interfaces to complex virtual realities. The key components
    of a multiplayer experience typically include servers that host the game environment,
    networking systems that handle data synchronization and communication, player
    avatars, and game logic that governs interaction rules.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a multiplayer mode to a game can make it more unpredictable due to human
    behaviors and decision-making. By contrast, **single-player** modes are typically
    more controlled and can be designed around a predefined narrative.
  prefs: []
  type: TYPE_NORMAL
- en: By adding multiplayer capabilities to XR, users are encouraged to jointly engage
    in tasks, challenges, or experiences, making the environment feel more alive and
    dynamic. Examples include cooperative puzzle-solving, virtual team-building exercises,
    and joint exploration.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: If you are serious about becoming an XR developer, you should feel comfortable
    creating multiplayer XR experiences. Virtual meetups in forums such as VR Chat
    are among the most popular XR applications to date. These meetups underline the
    desire of people to hang out with other real humans in virtual worlds, instead
    of being only surrounded by virtual assets.
  prefs: []
  type: TYPE_NORMAL
- en: The most crucial part of any multiplayer game is the **multiplayer networking
    system**. At its essence, a multiplayer networking system is the digital backbone
    that enables various players to interact seamlessly within a shared environment.
    This system ensures that actions performed by one player are reflected accurately
    and consistently for all other players, creating a harmonized virtual experience.
  prefs: []
  type: TYPE_NORMAL
- en: A key component of every multiplayer networking system is **servers**. These
    are powerful computers that host the game’s digital environment. They are the
    central point that players connect to, and they maintain the authoritative state
    of the game. In some configurations, one of the players might act as a server,
    termed **peer-to-peer**, but dedicated servers are more common in larger games
    due to stability and scalability.
  prefs: []
  type: TYPE_NORMAL
- en: The individual devices or computers used by players are called **client systems**.
    They send data such as player movements or actions to the server and receive updates
    about the game world and other players.
  prefs: []
  type: TYPE_NORMAL
- en: In real-time games, even slight delays can affect gameplay. Multiplayer networking
    systems use techniques such as **lag compensation** to make sure players have
    smooth experiences. This involves predicting movements or actions and then reconciling
    differences once actual data arrives.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid multiplayer games becoming targets for cheating or hacking, networking
    systems employ measures such as data encryption, authoritative servers, and cheat
    detection tools.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond gameplay, players often wish to communicate, be it through text, voice,
    or other mediums. Networking systems provide the infrastructure for these interactions,
    ensuring real-time and clear communication.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an overview of the three main multiplayer network providers that are
    interesting for Unity developers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Photon Unity Networking** (**PUN**): PUN is a solution tailored for Unity’s
    multiplayer games. Its cloud-based approach means that developers don’t need to
    worry about server creation or maintenance. PUN offers a free tier. Although it
    comes with certain restrictions, the free version is heavily used by indie developers
    and hobbyists who are starting their journey into multiplayer game development.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mirror**: Mirror offers a community-powered, open source networking tool
    tailored for Unity. It’s an evolution of the deprecated Unity networking system.
    Being open source, it doesn’t impose licensing fees, making it an economical choice.
    Mirror is renowned for its flexibility and provides developers with a greater
    degree of control over their multiplayer logic. However, its customizable nature
    means that it presents a slightly more challenging learning curve for beginners.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Netcode**: Netcode is Unity’s proprietary solution for game networking, formerly
    known as *MLAPI*. This evolution and rebranding signifies Unity’s commitment to
    continuous improvement and its dedication to providing developers with top-tier
    tools for multiplayer game development. Being an intrinsic Unity solution, Netcode
    promises seamless integration with Unity’s ecosystem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following sections, we’ll create a multiplayer game of our own. For this,
    we’ll be leveraging Photon PUN 2 using the free tier. Its zero-cost barrier, combined
    with an intuitive setup process for beginners, makes it an ideal candidate for
    rapid prototyping and smaller-scale projects.
  prefs: []
  type: TYPE_NORMAL
- en: The next section will teach you how to set up PUN for the VR multiplayer game
    we are about to create.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up PUN for our VR multiplayer game
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our objective for this VR multiplayer game is both straightforward and robust,
    encompassing all essential elements needed for a multiplayer experience. We aim
    to design a VR scene where multiple users can join concurrently. Users should
    see themselves and each other through avatars that consist of a head and two controller
    components. They should witness the movements of others in this shared space in
    real time, observing how they maneuver or rotate their controllers. Moreover,
    through hand animations, they should be able to discern when others are pressing
    their hands.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start importing Photon PUN 2 into our project, let’s create a new
    scene for this part of this chapter. Follow these steps to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the `AdvancedXRTechniques` project, which we created in the previous sections
    of this chapter. Save the current scene using an expressive name such as `HandAndEyeTrackingScene`
    by navigating to **File** | **Save As** and typing in the scene name of your choice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to `MultiplayerScene`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the Unity Editor of your new scene, delete `XR Interaction Setup` via the
    search bar of the `0`,`0`,`0`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now that we’ve set up our scene, follow these steps to install and set up our
    networking system, PUN:'
  prefs: []
  type: TYPE_NORMAL
- en: We can install PUN like any other package via the Unity Asset Store. Visit [https://assetstore.unity.com/packages/tools/network/pun-2-free-119922](https://assetstore.unity.com/packages/tools/network/pun-2-free-119922)
    or search for `PUN 2 – FREE` via the Unity Asset Store (**Window** | **Asset Store**).
    Click the **Add to my Assets** button to add the package to your assets. Once
    the package has been added, a new button called **Open in Unity** will appear
    on the Asset Store’s website. Click it to open the package in the **Package Manager**
    window of your project. Press the **Download** and **Import** buttons to import
    the asset into your project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **PUN Wizard** window shown in *Figure 8**.6* will pop up in the Unity Editor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.6 – The PUN Wizard pop-up window](img/B20869_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – The PUN Wizard pop-up window
  prefs: []
  type: TYPE_NORMAL
- en: If you already have a Photon account, you can simply type in your **AppId**
    here. Otherwise, input your email and click on the **Setup Project** button once
    you are done. This will create an account and forward you to a web page where
    you can set your password.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: If you accidentally close this pop-up window, simply head to [https://www.photonengine.com/](https://www.photonengine.com/)
    and sign up there.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have signed in to your account, head to [https://dashboard.photonengine.com/](https://dashboard.photonengine.com/)
    and click on the **Create A New App** button. You will be forwarded to the page
    shown in *Figure 8**.7*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.7 – The web page you will be forwarded to once you create a new
    application on the Photon website](img/B20869_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – The web page you will be forwarded to once you create a new application
    on the Photon website
  prefs: []
  type: TYPE_NORMAL
- en: Choose the `My first Multiplayer Game`. Click the **CREATE** button – notice
    your newly created application in the dashboard. As shown in *Figure 8**.8*, you
    can find your Photon **App ID** in one of the layout elements of the dashboard.
    Copy it to connect to the server later.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.8 – The dashboard element containing your App ID](img/B20869_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – The dashboard element containing your App ID
  prefs: []
  type: TYPE_NORMAL
- en: Head back to your Unity project. If you still see the **PUN Wizard** window,
    you can directly paste your **App Id** inside of it and click the **Setup Project**
    button. Alternatively, you can type your **App Id** directly into **Photon Server
    Settings** by navigating to **Window** | **Photon Unity Networking** | **Highlight
    Server Settings**, as shown in *Figure 8**.9*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.9 – How to add the App Id to Photon Server Settings in Unity](img/B20869_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – How to add the App Id to Photon Server Settings in Unity
  prefs: []
  type: TYPE_NORMAL
- en: Now, it’s time to connect our Unity project to the server. The next section
    will show you how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to the server via Network Manager
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To connect our Unity project to the PUN server, we must add a new GameObject
    with an associated C# script to our scene. Let’s go through this process step
    by step:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new empty GameObject by right-clicking in the `Network Manager`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, we want to add a script to this GameObject that we can use to connect to
    the PUN server and check whether someone else joined the server. To create this
    script, navigate to the `Network Manager` and click the **Add** **Component**
    button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Search for `NetworkManager`, select the **New Script** option, and press the
    **Create and Add** button to create a C# script called **NetworkManager.cs** that
    is automatically added to **NetworkManager** as a component. Double-click on the
    script to open it in your preferred IDE.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **NetworkManager** script serves as a foundational element for initiating
    and managing network interactions in a VR multiplayer application using the PUN
    framework. Delete everything that is currently inside of the **NetworkManager**
    script so that you can start on a clean foundation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s start adding our code logic by importing the following libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `Photon.Pun` and `Photon.Realtime` libraries are vital for any PUN application
    as they grant access to core multiplayer networking functionalities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s define the `NetworkManager` class and some important constants:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `NetworkManager` class inherits from `MonoBehaviourPunCallbacks`. This inheritance
    means that it’s not just a standard Unity script (`MonoBehaviour`), but that it
    also has special callback functions provided by PUN that notify our script of
    various networking events.
  prefs: []
  type: TYPE_NORMAL
- en: While `ROOM_NAME` represents the name of the room players will join or create,
    `MAX_PLAYERS` defines the maximum number of players allowed in a room, which in
    this case is `5` players. We will need both constants later in the script.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s continue by connecting our application to Photon servers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Once the `Awake()` method is invoked, it initiates the connection to the PUN
    server by calling the `InitiateServerConnection()` method. If the client isn’t
    already connected to Photon, the `InitiateServerConnection()` method will attempt
    to connect to the server space using the **App Id** value that we inserted before.
    Refer to *Figure 8**.9* in case you missed this step. The second line of this
    method logs our attempt via a debug message.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the connection attempt is successful, the following method will be executed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This method is an override of the `OnConnectedToMaster` callback from Photon.
    It is triggered once the application successfully connects to the Photon Master
    Server. This method is an integral part of the PUN framework, allowing us to execute
    specific logic after a successful connection. Inside this method, we log the successful
    connection to the Master Server. We also call the `JoinOrCreateGameRoom()` method,
    which uses the constants we defined at the beginning of our script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the last line of this method, the client attempts to join or create a room,
    called **Multiplayer Room**, with five being the maximum allowed number of players
    in the room. If the room doesn’t exist, it creates one with the provided options.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next method in our code is an override:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This method is an override of the `OnJoinedRoom` callback from Photon. It’s
    triggered when the client successfully joins a room. In the last line, a debug
    message is printed to confirm successful room entry.
  prefs: []
  type: TYPE_NORMAL
- en: 'To manage new players, we can use the following method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This `OnPlayerEnteredRoom()` method is also an override of the `OnPlayerEnteredRoom`
    callback from Photon. It’s called whenever a new player enters the room. Once
    again, a debug message is printed in the last line to notify that another player
    has joined.
  prefs: []
  type: TYPE_NORMAL
- en: Hurray, you have implemented all the necessary components into the `NetworkManager`
    script! As you can see, it provides a foundational structure to connect to Photon’s
    servers, manage multiplayer rooms, and handle player interactions in a VR environment.
    Let’s see whether our code logic works. In the next section, you will find out
    how you can test a multiplayer application on a single device.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the multiplayer scene from one device
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To test a multiplayer scene from a single device, we need to run the scene
    twice. This can be accomplished by first building and running the scene on the
    computer and subsequently launching it from within the Editor. Here’s a step-by-step
    breakdown:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Unity Editor, head to **Files** | **Build Settings** and select the **Windows/
    Mac/ Linux** tab. Add the open scene you want to test, click the **Build** button,
    and choose a folder for the executable file you are about to build.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the file has been built, click on the **Play** button in the Unity Editor
    to start the scene.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, open the executable file that you just built.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Head to the **Console** window in the Unity Editor and check the **Debug** statements.
    If you did everything correctly, you will see the debug lines we defined in the
    scripts previously, as shown in *Figure 8**.10*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.10 – The debug lines you should see if everything works as expected](img/B20869_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – The debug lines you should see if everything works as expected
  prefs: []
  type: TYPE_NORMAL
- en: To ensure that our scene functions optimally, we must test its VR features thoroughly.
    From our assessments, navigating the scene is smooth, and both the headset and
    controllers are accurately tracked and positioned. Currently, our scene establishes
    a server connection as soon as the initial player activates it, and we have set
    up methods to detect the entry of new players.
  prefs: []
  type: TYPE_NORMAL
- en: However, our **XR Interaction Setup** isn’t entirely primed for multiplayer
    operations. It excels at managing locomotion and interactions, but it doesn’t
    feature animated models for different body parts. In simpler terms, if someone
    were to enter our multiplayer scene at this moment, they might only see the controller.
    Worse, they might not detect any part of our avatar because the controller models
    aren’t network-conscious. This means that the objects present aren’t synchronized
    across all players in the session. But don’t worry, we’ll tackle this issue in
    the upcoming section.
  prefs: []
  type: TYPE_NORMAL
- en: Using scripts to display our avatar’s hands and face
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our next objective is to display an avatar’s hands and face when it connects
    to a server. To achieve this, let’s append a new script to `Network Manager` by
    selecting it in the `NetworkPlayerPlacer` after clicking the `NetworkPlayerPlacer`.
    Open the script by double-clicking on it.
  prefs: []
  type: TYPE_NORMAL
- en: Managing player presence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `NetworkPlayerPlacer` script we are about to dive into is tailored for
    overseeing player presence, a core element in multiplayer VR applications. Specifically,
    this means to create and delete player avatars or their in-game representations.
    The `NetworkPlayerPlacer` script is tailored for this exact role. It begins with
    the following declarations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The import statements and class declarations bear a close resemblance to those
    in the `NetworkManager` script. Both classes are derived from `MonoBehaviourPunCallbacks`.
    `playerInstance` serves as a reference to the instantiated player object within
    the scene. Meanwhile, `PLAYER_PREFAB_NAME` is a constant string that holds the
    name of the player prefab set for instantiation. It’s anticipated that this prefab
    is registered and accessible in the Photon resources directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create the first method of the `NetworkPlayerPlacer` script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `OnJoinedRoom()` method overrides the `OnJoinedRoom` callback from Photon,
    which gets triggered when the local player successfully joins a room.
  prefs: []
  type: TYPE_NORMAL
- en: 'The base class implementation of `OnJoinedRoom` is called with `base.OnJoinedRoom()`.
    In the last line, the `SpawnPlayer()` method is called, which looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This method instantiates a new player object using Photon’s networked instantiation
    method. The new player will be spawned at the position and rotation of the `NetworkPlayerPlacer`
    object. This ensures that the player object is networked and synchronized across
    all clients in the room.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next method in our script overrides the `OnLeftRoom` callback from Photon,
    which is called when the local player leaves a room:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The `OnLeftRoom()` method consists of two calls: one to the base class implementation
    of `OnLeftRoom` and one to `DespawnPlayer()`, a method to despawn the player.
    Let’s have a look at the `DespawnPlayer()` method next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This method handles the despawning of the player object.
  prefs: []
  type: TYPE_NORMAL
- en: The `if` statement checks whether the `playerInstance` reference is not null.
    If this is true, a player object exists. The `PhotonNetwork.Destroy(playerInstance);`
    line destroys the networked player object. This will ensure that the object is
    removed not just locally but across all clients.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, the `NetworkPlayerSpawner` script, with its two key callback functions,
    `OnJoinedRoom()` and `OnLeftRoom()`, seamlessly handles the appearance and disappearance
    of player avatars in our VR multiplayer space, complementing the functionalities
    offered by the `NetworkManager` script.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a face and hands
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Earlier, we noted that the script anticipates a prefab called `Resources` folder.
    Unity uses this default naming convention when referencing objects by their name.
    To create the needed folder, click on the `Resources`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to create the **Network Player** prefab, which will represent
    our avatar via the network. Follow these steps to achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: Right-click in the `Network Player`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the **Photon View** component to it in the **Inspector** window by pressing
    the **Add Component** button and searching for it. Photon requires this component
    to instantiate the player on the server. Without it, the system wouldn’t recognize
    ownership or synchronize the player’s body parts accurately.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add three other empty GameObjects as children to `Network Player` by selecting
    `Network Player` in the `Head`, `Left Hand`, and `Right Hand`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, we need to create 3D objects to represent the three components of our avatar.
    For `Head`, we can use a simple `Head` in the `Head` and scale it to (`0.1`, `0.1`,
    `0.1`)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We could also use primitives for the hands of our avatar. However, since there
    are a lot of animated hand models available on the internet, we can simply use
    one of them. For this project, we used the **Oculus Hands** sample from the **Oculus
    Integration** package on Unity’s Asset Store ([https://assetstore.unity.com/packages/tools/integration/oculus-integration-82022](https://assetstore.unity.com/packages/tools/integration/oculus-integration-82022)).
    However, we suggest cloning the hands directly from this book’s GitHub repository.
    This avoids unnecessary additional content and missing assets due to frequent
    updates in the **Oculus** **Integration** package.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, select `Network Player` in the `Resources` folder in the `Network Player`
    in our **Scene Hierarchy** window anymore and can delete it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We must add another script to `Network Manager` to make sure the `Head`, `Left
    Hand`, and `Right Hand` components follow the position of the user’s headset and
    controllers. To do this, select `Network Manager` in the `NetworkPlayer` into
    the search bar, and create a new script with this name.
  prefs: []
  type: TYPE_NORMAL
- en: Now that our script has been created, let’s learn how we can use it to track
    the player’s position and movement.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking player position and movements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `NetworkPlayer` script controls how a player’s movements are tracked and
    represented within the multiplayer VR environment. The script encapsulates functionalities
    that ensure the position and rotation of the avatar’s head, left hand, and right
    hand are accurately tracked and updated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at the beginning of this script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Besides importing the regular Unity and PUN namespaces, a class named `NetworkPlayer`
    inheriting from `MonoBehaviour` is declared. The `Transform` variables, `head`,
    `leftHand`, and `rightHand`, represent the 3D position, rotation, and scale of
    a player’s VR avatar components in the game. The `PhotonView` component is fundamental
    for PUN, determining ownership and synchronization of objects in multiplayer.
    `InputActionAsset`, called `xriInputActions`, is a Unity-configured set of input
    definitions tailored for VR headsets. The `InputActionMap` variables, such as
    `headActionMap`, group related input actions, allowing for organized handling
    of inputs such as head or hand movements. Finally, the `InputAction` variables
    such as `headPositionAction` and `headRotationAction` detect individual input
    movements from the VR headset. To synchronize the player’s real-world VR movements
    with their in-game avatar, we need methods in our script. Let’s start with the
    first one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In the `Start()` method, `photonView` is initialized. Then, action maps of the
    head, left hand, and right hand are retrieved using their names. These names (`XRI
    Head`, `XRI LeftHand`, and `XRI RightHand`) are predefined in `Position` and `Rotation`
    actions for the head and each hand are retrieved. Lastly, all these actions are
    enabled so that they begin reading input values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next method in our script is the `Update()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In a Photon-powered multiplayer environment, numerous `NetworkPlayer` instances
    will emerge, representing each player in the game. However, on every player’s
    device, only a single instance genuinely represents that specific player, while
    the rest signify remote players. The `photonView.IsMine` property, when checked
    within the `Update()` method’s `if` statement, determines whether the `NetworkPlayer`
    instance on a given machine corresponds to the player using that machine. This
    differentiation is pivotal in multiplayer scenarios to identify who has control
    over certain activities or decisions. If it returns true, then the `NetworkPlayer`
    instance pertains to the local player of that device. Consequently, the visual
    representation of that player’s head and hands is deactivated, ensuring a seamless
    user experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within our multiplayer scene, two primary players exist: our local player (via
    `Network Player`). This remote player is visible to all other participants in
    the multiplayer environment. As the local player already has hand models, introducing
    an additional hand model from the remote player can lead to a disorienting experience,
    particularly given network update latencies, as you can see in *Figure 8**.11*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.11 – What happens when the SetActive(false) statements are not implemented](img/B20869_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 – What happens when the SetActive(false) statements are not implemented
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we ensure the elements of `Network Player` remain invisible to the user.
    Conversely, if the property returns `false`, it means the instance represents
    another participant as viewed from that device. Finally, the `if` statement also
    updates the position and rotation of the hands and head based on VR headset or
    VR controller movements using the `MapPosition()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s have a look at the `MapPosition()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The purpose of the `MapPosition()` method is to assign the correct position
    and rotation to the passed-in `if` statement and reads the corresponding input
    values. For example, if the node is `XRNode.Head`, it reads the position and rotation
    values from `headPositionAction` and `headRotationAction`, respectively. These
    `InputAction` variables fetch the latest position and rotation values from the
    VR headset at every frame. Finally, the `MapPosition()` method assigns the fetched
    position and rotation to the passed-in target transform.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, this method ensures that the in-game representation of the player
    moves in sync with their real-world movements in VR.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last method in this script is the `OnDestroy()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The `OnDestroy()` method simply disables all the input actions when the script
    is destroyed, ensuring no unwanted input readings are occurring in the background.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the `NetworkPlayer` script serves as a bridge, translating a player’s
    real-world VR movements into the virtual multiplayer space. By adding this script
    to our scene, players can experience a synchronous and immersive VR multiplayer
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: When we test the application now, we can see the head and both of the hands
    somewhere in the scene aligned with our controller position. Next, we’ll animate
    our hands when we press the trigger and grip button.
  prefs: []
  type: TYPE_NORMAL
- en: Animating the hand models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Your multiplayer application has come a long way. Now, let’s take the immersion
    one step further with hand animations. By mapping controller inputs to these animations,
    users will experience a heightened realism within the virtual world. If animations
    sound foreign to you, we recommend a quick detour to the *Understanding animations
    and animator systems* section of [*Chapter 5*](B20869_05.xhtml#_idTextAnchor016).
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the `Prefabs` folder and then follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Attach an `Animator`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In [*Chapter 5*](B20869_05.xhtml#_idTextAnchor016), we touched on **Animator**
    in Unity, which is essential for controlling character animations. **Animator**
    needs **Animator Controller** to manage how animations transition and interact.
    It also requires **Avatar**, which is a map of the character’s skeletal structure,
    ensuring animations fit and move realistically on the character.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2. For our avatars, we can select the **L_hand_skeletal_lowres** and **R_hand_skeletal_lowres**
    options from **Multiplayer Scene** | **Oculus** **Hands** | **Models**.
  prefs: []
  type: TYPE_NORMAL
- en: 3. However, we’re yet to acquire `Left Hand Animator` and `Right` `Hand Animator`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Double-click on `Left Hand Animator` to reveal an **Animator** window showcasing
    **Layers** and a **Parameters** tab. Start with the **Parameters** tab and introduce
    **Grip** and **Trigger** parameters, both of the *float* type. They indicate the
    pressure intensity of the trigger and grip actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Trigger** determines the degree of the hand’s pinch'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Grip** governs the degree of the fist’s bend'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, simply having these parameters won’t magically animate the fingers.
    To bring them to life, we need to bind these parameters to specific animation
    states or integrate them within blend trees. These concepts have different approaches
    to connecting animation states, so let’s compare them to see which approach fits
    our needs the best:'
  prefs: []
  type: TYPE_NORMAL
- en: '`0.5`, the hand might transition to **HandClosed**; otherwise, it will remain
    in the **HandOpen** state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Blend trees** enable seamless transitioning between multiple animations depending
    on one or more parameters. It’s analogous to using a music mixer to blend songs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For hand animations, it is useful to have three states: **HandOpen**, **HandHalfClosed**,
    and **HandFullyClosed**. Instead of basic transitions, which can feel abrupt,
    a blend tree can be used to fluidly move between these states, all based on the
    **Grip** value. This is why it is the best choice for our needs.'
  prefs: []
  type: TYPE_NORMAL
- en: To set up a blend tree, right-click in the **Base Layer** window’s center and
    opt for **Create State** | **From New Blend Tree**. Your **Baser Layer** window
    should be similar to what’s shown in *Figure 8**.12*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.12 – Base Layer in the Animator window](img/B20869_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 – Base Layer in the Animator window
  prefs: []
  type: TYPE_NORMAL
- en: 'By double-clicking on **Blend Tree** and selecting it afterward, you’ll be
    prompted to select a **Blend Type** property, which includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1D**: This pivots around a single parameter, such as pacing, to transition
    between animations such as walking and running'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2D Freeform Cartesian**: This employs two separate parameters – for instance,
    determining animation intensity based on a character’s mood and energy levels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2D Freeform Directional**: Here, two parameters act as directional vectors,
    such as aiming a weapon in horizontal and vertical directions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2D Simple Directional**: Typically, this uses two parameters, often *X* and
    *Y* inputs, resembling a joystick’s movements, to determine character direction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For nuanced hand animations using both **Grip** and **Trigger**, **2D Freeform
    Cartesian** is ideal. Each parameter operates autonomously: **Grip** dictates
    the intensity of the fist, while **Trigger** oversees the pinching gesture. Adjust
    **Blend Type** to **2D Freeform Cartesian**, then designate **Grip** for the *X*-axis
    and **Trigger** for the *Y*-axis in the **Inspector** window. In the same window,
    we need to define some key motions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For a solid setup, you should consider the following four key motions depicting
    the extremities of the hand movements:'
  prefs: []
  type: TYPE_NORMAL
- en: '`0`. Fortunately, we don’t need to create such an animation clip ourselves
    as the `Multiplayer Scene` | `Oculus Hands` | `Animations`folder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`0` and the `1`. This would be the **l_hand_pinch_anim** animation clip, which
    is in the same folder as **Take 001**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1` and a `0`. That would be the `Animations` folder as before.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1` for both cases. This would also be the `1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you did everything correctly, your **Blend Tree** should look as shown in
    *Figure 8**.13*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.13 – The Blend Tree settings in Unity](img/B20869_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 – The Blend Tree settings in Unity
  prefs: []
  type: TYPE_NORMAL
- en: Repeat this process for the **Right Hand** animator controller. Once we’ve done
    this, we can drag the just-created **Left Hand** and **Right Hand** animator controllers
    into the corresponding fields of the **Left Hand Model** and **Right Hand** **Model**
    prefabs.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we just need to create a script that will link the **Grip** and **Trigger**
    variables’ float values to the button inputs of our controller.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling the hand animations at runtime
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll be delving into the process of enabling hand animations
    in real time within our multiplayer VR application. This means that during the
    actual gameplay, as users interact with the virtual environment and other players,
    they can witness and experience hand gestures such as pinching or making a fist.
    This real-time interaction enhances the immersion and interactivity of our multiplayer
    VR game. By integrating these animations at runtime, users can seamlessly respond
    to game mechanics or communicate non-verbally with other players.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to do this by creating a script to hold our `HandControllerPresence`
    into the search bar, create a new script with the same name, and then open it.
    We’ll start by defining the necessary variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '`handVisualizationPrefab` is a public GameObject variable that refers to the
    hand model prefab that will be animated based on the user’s VR controller inputs.
    In our case, this will be the `triggerAction` and `gripAction` are serialized
    fields, meaning they can be assigned in the Unity Editor but remain private in
    the script. They are designed to fetch the current values of the trigger and grip
    inputs, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '`instantiatedHandVisual` and `handMotionController` are private variables that
    are used internally. The former stores the instantiated hand model in the scene,
    while the latter is a reference to the `Awake()` and `InitializeHandController()`
    methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Awake()` method is called when the script instance is loaded. Here, it
    calls the `InitializeHandController()` method to set up the hand visuals and animations.
    This method instantiates the hand model prefab in the scene and attaches it to
    the object to which this script is attached. Then, it fetches and stores the `handMotionController`
    variable. This `AdjustHandMotion()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This method fetches the current values of the trigger and grip inputs using
    the `triggerAction` and `gripAction` variables, respectively. These values are
    in the range of 0 (not pressed) to 1 (fully pressed).
  prefs: []
  type: TYPE_NORMAL
- en: 'The fetched values (`triggerIntensity` and `gripIntensity`) are then passed
    to the hand’s `SetFloat()` method. This effectively adjusts the hand’s animation
    based on the real-time inputs from the VR controller. Finally, we just need to
    call this method once per frame to ensure that the hand’s animations are updated
    in real time to match the user’s VR controller inputs. This is done in the `Update()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Now, we just need to assign the corresponding `handVisualiationPrefab` and the
    controller actions to the **Trigger Action** and **Grip Action** fields for both
    controllers, as shown in *Figure 8**.14* for the left controller.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 8.14 – \uFEFFThe HandControllerPresence script and its references\uFEFF\
    ](img/B20869_08_14.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 – The HandControllerPresence script and its references
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering why exactly are we using the **XRI LeftHand** references
    shown in *Figure 8**.14*. In our scene, we are using **XR Interaction Setup**,
    which comes with the XR Interaction Toolkit’s **Starter Assets**. This **XR Interaction
    Setup** uses **XRI Default Input Actions** as controller input handling.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s have a look at them by heading to **Samples** | **XR Interaction Toolkits**
    | **Your** **Version** | **Starter Assets** and double-clicking on **XRI Default
    Input Actions**. This will open the window shown in *Figure 8**.15*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.15 – The XRI Default Input Actions of the XR Interaction Toolkit](img/B20869_08_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 – The XRI Default Input Actions of the XR Interaction Toolkit
  prefs: []
  type: TYPE_NORMAL
- en: 'The XR Interaction Toolkit action-based input system in Unity allows for device-agnostic
    control setups. It uses **Action Maps** to group related actions, such as **XRI
    LeftHand Interaction**, to handle specific actions such as **Select Value** or
    **Activate Value**. These actions are then tied to specific inputs, called **bindings**,
    such as the **Grip** button or the **Trigger** button of an XR controller, allowing
    for flexible and intuitive control configurations across various XR devices. You
    can add additional action maps, actions, and bindings or change existing ones.
    You can even build a complete Input Actions setup of your own under **Assets**
    | **Create** | **Input Actions**. However, you’ll need to link this new Input
    Actions setup in the **Inspector** window of **Input Action Manager** under **Action
    Assets**. With that, we have implemented the animation of the hand models. To
    begin testing the application with a friend, follow these simple steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Open your Unity project and navigate to **File**, then **Build Settings**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the currently open scene to **Build Settings**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the **Build** option. When prompted, choose a directory that you can
    easily access and remember.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This action will generate an executable file along with other vital scene-related
    files in the chosen directory. For your friend to join in, share the entire folder
    with them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initiate your connection to the server by pressing the **Play** button. As you
    await your connection, instruct your friend to launch the executable file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: And voilà! You’ve successfully set up your first multiplayer environment. Now,
    both you and your friend can interact, as depicted in *Figure 8**.16*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.16 – Me and my friend in our multiplayer environment](img/B20869_08_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.16 – Me and my friend in our multiplayer environment
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You might have noticed a stunning deep-space background with a picturesque planet.
    This aesthetic touch is provided by the Skybox Volume 2 package, which we imported
    from the Unity Asset Store ([https://assetstore.unity.com/packages/2d/textures-materials/sky/skybox-volume-2-nebula-3392](https://assetstore.unity.com/packages/2d/textures-materials/sky/skybox-volume-2-nebula-3392)).
    It significantly enhances the ambiance of our environment.
  prefs: []
  type: TYPE_NORMAL
- en: As we approach the end of this chapter, you should now feel capable and confident
    in enhancing your XR experiences using advanced techniques. However, this doesn’t
    imply that every feature, such as eye-tracking or hand-tracking, should be mindlessly
    applied to all your projects. Nor does it imply that all your subsequent XR endeavors
    must be multiplayer applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'What it does mean is that you’ve broadened your horizons. You now possess a
    richer repertoire of interaction techniques to create your XR applications. Instead
    of indiscriminately applying eye-tracking to every scene element, for instance,
    you can selectively use it for components that would truly benefit from it. Imagine
    a virtual exhibition of firefighter vehicles: rather than overloading the user
    with information, you could seamlessly display technical specifications when the
    user’s gaze settles on a particular vehicle. Interactive components such as animated
    firefighters or explorable vehicle interiors can be activated based on the user’s
    visual focus, providing an engaging and dynamic user experience. Similarly, in
    a treasure-hunting game, a hint might reveal itself when a player’s gaze meets
    a mystical mirror. The possibilities are endless!'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to implement hand-tracking, gaze-tracking,
    and multiplayer capabilities into your XR scenes. Incorporating these high-level
    XR methods into your future XR projects will enable you to create much more intuitive,
    immersive, and fun experiences for users. Combined with the knowledge you gained
    in the previous chapters on how to create and deploy interactive XR experiences,
    you should feel comfortable developing a wide range of XR projects yourself, no
    matter if they involve coding, animations, particles, audio, or multiplayer support.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have a firm grip on the technical and programming aspects of XR,
    the next chapter will shift the spotlight to the business realm of XR. There,
    you’ll explore powerful XR plugins and toolkits beyond the XR Interaction Toolkit,
    ARKit, ARCore, and AR Foundation that could be valuable additions to your XR development
    skills moving forward. The subsequent chapter won’t just keep you up to date on
    the latest trends in XR but will also equip you with industry best practices to
    ensure the success of your projects.
  prefs: []
  type: TYPE_NORMAL
