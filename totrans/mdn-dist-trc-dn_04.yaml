- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Low-Level Performance Analysis with Diagnostic Tools
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用诊断工具进行低级性能分析
- en: While distributed tracing works great for microservices, it’s less useful for
    deep performance analysis within a process. In this chapter, we’ll explore .NET
    diagnostics tools that allow us to detect and debug performance issues and profile
    inefficient code. We’ll also learn how to perform ad hoc performance analysis
    and capture necessary information automatically in production.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然分布式跟踪对于微服务来说效果很好，但在进程内部进行深度性能分析时则不太有用。在本章中，我们将探讨 .NET 诊断工具，这些工具允许我们检测和调试性能问题，并分析低效代码。我们还将学习如何在生产中执行即席性能分析并自动捕获必要的信息。
- en: 'In this chapter, you will learn how to do the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习以下内容：
- en: Use .NET runtime counters to identify common performance problems
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 .NET 运行时计数器来识别常见的性能问题
- en: Use performance tracing to optimize inefficient code
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用性能跟踪优化低效代码
- en: Collect diagnostics in production
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生产中收集诊断信息
- en: By the end of this chapter, you will be able to debug memory leaks, identify
    thread pool starvation, and collect and analyze detailed performance traces with
    .NET diagnostics tools.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将能够使用 .NET 诊断工具调试内存泄漏、识别线程池饥饿，并收集和分析详细性能跟踪。
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The code for this chapter is available in this book’s repository on GitHub
    at [https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4).
    It consists of the following components:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可在本书的 GitHub 仓库中找到，网址为 [https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4)。它包括以下组件：
- en: The `issues` application, which contains examples of performance issues
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`issues` 应用程序，其中包含性能问题的示例'
- en: '`loadgenerator`, which is a tool that generates load to reproduce problems'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loadgenerator`，这是一个生成负载以重现问题的工具'
- en: 'To run samples and perform analysis, we’ll need the following tools:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行示例并执行分析，我们需要以下工具：
- en: .NET SDK 7.0 or later.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: .NET SDK 7.0 或更高版本。
- en: The .NET `dotnet-trace`, `dotnet-stack`, and `dotnet-dump` diagnostics tools.
    Please install each of them with `dotnet tool install –``global dotnet-<tool>`.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: .NET 的 `dotnet-trace`、`dotnet-stack` 和 `dotnet-dump` 诊断工具。请使用 `dotnet tool install
    –global dotnet-<tool>` 安装每个工具。
- en: Docker and `docker-compose`.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker 和 `docker-compose`.
- en: To run the samples in this chapter, start the observability stack, which consists
    of Jaeger, Prometheus, and the OpenTelemetry collector, with `docker-compose up`.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行本章中的示例，启动可观察性堆栈，该堆栈由 Jaeger、Prometheus 和 OpenTelemetry 收集器组成，使用 `docker-compose
    up`。
- en: Make sure that you start the `dotnet run -c Release` from the `issues` folder.
    We don’t run it in Docker so that it’s easier to use diagnostics tools.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您从 `issues` 文件夹中启动 `dotnet run -c Release`。我们不使用 Docker 运行它，这样更容易使用诊断工具。
- en: 'In the `OpenTelemetry.Instrumentation.Process` and `OpenTelemetry.Instrumentation.Runtime`
    NuGet packages and configured metrics for the HTTP client and ASP.NET Core. Here’s
    our metrics configuration:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `OpenTelemetry.Instrumentation.Process` 和 `OpenTelemetry.Instrumentation.Runtime`
    NuGet 包中配置了 HTTP 客户端和 ASP.NET Core 的指标。以下是我们的指标配置：
- en: Program.cs
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Program.cs
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4/issues/Program.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4/issues/Program.cs)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4/issues/Program.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4/issues/Program.cs)'
- en: Investigating common performance problems
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调查常见的性能问题
- en: Performance degradation is a symptom of some other issues such as race conditions,
    dependency slow-down, high load, or any other problem that causes your **service-level
    indicators** (**SLIs**) to go beyond healthy limits and miss **service-level objectives**
    (**SLOs**). Such issues may affect multiple, if not all, code paths and APIs,
    even if they’re initially limited to a specific scenario.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 性能下降是某些其他问题（如竞争条件、依赖项减速、高负载或任何导致您的 **服务级别指标**（**SLIs**）超出健康限制并错过 **服务级别目标**（**SLOs**）的问题）的症状。这些问题可能影响多个，如果不是所有代码路径和
    API，即使它们最初仅限于特定场景。
- en: For example, when a downstream service experiences issues, it can cause throughput
    to drop significantly for all APIs, including those that don’t depend on that
    downstream service. Retries, additional connections, or threads that handle downstream
    calls consume more resources than usual and take them away from other requests.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当下游服务出现问题时，它会导致所有 API 的吞吐量显著下降，包括那些不依赖于该下游服务的 API。重试、额外的连接或处理下游调用的线程消耗比平常更多的资源，并将它们从其他请求中夺走。
- en: Note
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Resource consumption alone, be it high or low, does not indicate a performance
    issue (or lack of it). High CPU or memory utilization can be valid if users are
    not affected. It could still be important to investigate when they are unusually
    high as it could be an early signal of a problem to come.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 仅凭资源消耗，无论是高是低，都不能表明存在性能问题（或缺乏性能）。如果用户不受影响，高 CPU 或内存利用率可能是有效的。当它们异常高时，调查它们仍然很重要，因为这可能是未来问题的早期信号。
- en: We can detect performance issues by monitoring SLIs and alerting them to violations.
    If you see that issues are widespread and not specific to certain scenarios, it
    makes sense to check the overall resource consumption for the process, such as
    CPU usage, memory, and thread counts, to find the bottleneck. Then, depending
    on the constrained resource, we may need to capture more information, such as
    dumps, thread stacks, detailed runtime, or library events. Let’s go through several
    examples of common issues and talk about their symptoms.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过监控 SLIs 并对违规情况进行警报来检测性能问题。如果你看到问题普遍存在，而不是特定于某些场景，那么检查整个进程的资源消耗，例如 CPU
    使用率、内存和线程计数，以找到瓶颈是有意义的。然后，根据受限的资源，我们可能需要捕获更多信息，例如转储、线程堆栈、详细的运行时或库事件。让我们通过几个常见问题的例子，并讨论它们的症状。
- en: Memory leaks
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存泄漏
- en: Memory leaks happen when an application consumes more and more memory over time.
    For example, if we cache objects in-memory without proper expiration and overflow
    logic, the application will consume more and more memory over time. Growing memory
    consumption triggers garbage collection, but the cache keeps references to all
    objects and GC cannot free them up.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，应用程序消耗越来越多的内存时会发生内存泄漏。例如，如果我们缓存对象而不进行适当的过期和溢出逻辑，应用程序随着时间的推移将消耗越来越多的内存。增长的内存消耗会触发垃圾回收，但缓存会保留对所有对象的引用，GC
    无法释放它们。
- en: 'Let’s reproduce a memory leak and go through the signals that would help us
    identify it and find the root cause. First, we need to run the `loadgenerator`
    tool:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重现一个内存泄漏，并探讨帮助我们识别它并找到根本原因的信号。首先，我们需要运行 `loadgenerator` 工具：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It makes 20 million requests and then stops, but if we let it run for a long
    time, we’ll see throughput dropping, as shown in *Figure 4**.1*:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 它发出了 2000 万次请求然后停止，但如果让它长时间运行，我们会看到吞吐量下降，如图 *图 4*.*1* 所示：
- en: '![Figure 4.1 – Service throughput (successful requests per second)](img/B19423_04_01.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1 – 服务吞吐量（每秒成功请求次数）](img/B19423_04_01.jpg)'
- en: Figure 4.1 – Service throughput (successful requests per second)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – 服务吞吐量（每秒成功请求次数）
- en: We can see periods when throughput drops and the service stops processing requests
    – let’s investigate why.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到吞吐量下降的时期，服务停止处理请求——让我们调查一下原因。
- en: .NET reports event counters that help us monitor the size of each GC generation.
    Newly allocated objects appear in **generation 0**; if they survive garbage collection,
    they get promoted to **generation 1**, and then to **generation 2**, where they
    stay until they’re collected or the process terminates. Large objects (that are
    85 KB or bigger) appear on a **large object** **heap** (**LOH**).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: .NET 报告事件计数器，帮助我们监控每个 GC 生成的大小。新分配的对象出现在 **0 代**；如果它们在垃圾回收中存活，它们会被提升到 **1 代**，然后到
    **2 代**，在那里它们会停留直到被收集或进程终止。大对象（85 KB 或更大）出现在 **大对象** **堆**（**LOH**）中。
- en: 'OpenTelemetry runtime instrumentations report generation sizes under the `process_runtime_dotnet_gc_heap_size_bytes`
    metric. It’s also useful to monitor the `process_memory_usage_bytes`. We can see
    generation 2 and physical memory consumption in *Figure 4**.2*:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: OpenTelemetry 运行时仪器在 `process_runtime_dotnet_gc_heap_size_bytes` 指标下报告生成大小。监控
    `process_memory_usage_bytes` 也很有用。我们可以在 *图 4*.*2* 中看到 2 代和物理内存消耗：
- en: '![Figure 4.2 – Memory consumption showing a memory leak in the application](img/B19423_04_02.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2 – 应用程序内存消耗显示内存泄漏](img/B19423_04_02.jpg)'
- en: Figure 4.2 – Memory consumption showing a memory leak in the application
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – 应用程序内存消耗显示内存泄漏
- en: We can see that generation 2 grows over time, along with the virtual memory.
    The physical memory used by the process goes up and down, which means that the
    OS started using disk in addition to RAM. This process is called **paging** or
    **swapping**, which is enabled (or disabled) at the OS level. When enabled, it
    may significantly affect performance since RAM is usually much faster than disk.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，随着时间推移，2代增长，虚拟内存也随之增长。进程使用的物理内存上下波动，这意味着操作系统开始使用磁盘，除了RAM。这个过程被称为**分页**或**交换**，这在操作系统级别是启用（或禁用）的。当启用时，它可能会显著影响性能，因为RAM通常比磁盘快得多。
- en: Eventually, the system will run out of physical memory and the pagefile will
    reach its size limit; then, the process will crash with an `OutOfMemoryException`
    error. This may happen earlier, depending on the environment and heap size configuration.
    For 32-bit processes, OOM happens when the virtual memory size reaches 4 GB as
    it runs out of address space. Memory limits can be configured or imposed by the
    application server (IIS), hosting providers, or container runtimes.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，系统将耗尽物理内存，页面文件将达到其大小限制；然后，进程将因`OutOfMemoryException`错误而崩溃。这可能会根据环境和堆大小配置而提前发生。对于32位进程，当虚拟内存大小达到4
    GB且地址空间耗尽时，OOM发生。内存限制可以由应用程序服务器（IIS）、托管提供商或容器运行时配置或强制实施。
- en: Kubernetes or Docker allows you to limit the virtual memory for a container.
    The behavior of different environments varies, but in general, the application
    is terminated with the `OutOfMemory` exit code after the limit is reached. It
    might take days, weeks, or even months for a memory leak to crash the process
    with `OutOfMemoryException`, so some memory leaks can stay dormant, potentially
    causing rare restarts and affecting only a long tail of latency distribution.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes或Docker允许您限制容器的虚拟内存。不同环境的行为各不相同，但通常情况下，当达到限制后，应用程序会以`OutOfMemory`退出代码终止。内存泄漏可能需要几天、几周甚至几个月才能导致进程因`OutOfMemoryException`而崩溃，因此一些内存泄漏可能处于休眠状态，可能引起罕见的重启，并仅影响长尾延迟分布。
- en: Memory leaks on the hot path can take the whole service down fast. When memory
    consumption grows quickly, garbage collection intensively tries to free up some
    memory, which uses the CPU and can pause managed threads.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 热路径上的内存泄漏可能导致整个服务快速崩溃。当内存消耗迅速增长时，垃圾回收会密集地尝试释放一些内存，这会使用CPU并可能导致托管线程暂停。
- en: 'We can monitor garbage collection for individual generations using .NET event
    counters and OpenTelemetry instrumentation, as shown in *Figure 4**.3*:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用.NET事件计数器和OpenTelemetry仪表板来监控单个代的垃圾回收，如图*图4**.3*所示：
- en: '![Figure 4.3 – Garbage collection rate per second for individual generations](img/B19423_04_03.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图4.3 – 单一代的垃圾回收速率每秒](img/B19423_04_03.jpg)'
- en: Figure 4.3 – Garbage collection rate per second for individual generations
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 – 单一代的垃圾回收速率每秒
- en: As you can see, the generation 0 and generation 1 collections happened frequently.
    Looking at the consistent memory growth and the frequency of garbage collection,
    we can now be pretty sure we’re dealing with a memory leak. We could also collect
    GC events from the `Microsoft-Windows-DotNETRuntime` event provider (we’ll learn
    how to do this in the next section) to come to the same conclusion.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，0代和1代的收集发生频率较高。观察一致的内存增长和垃圾回收的频率，我们现在可以相当肯定我们正在处理内存泄漏。我们还可以从`Microsoft-Windows-DotNETRuntime`事件提供程序收集GC事件（我们将在下一节中学习如何做），得出相同的结论。
- en: 'Let’s also check the CPU utilization (shown in *Figure 4**.4*) reported by
    the OpenTelemetry process instrumentation as the `process_cpu_time_seconds_total`
    metric, from which we can derive the utilization:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也检查OpenTelemetry进程仪表板报告的CPU利用率（如图*图4**.4*所示），作为`process_cpu_time_seconds_total`度量，从中我们可以推导出利用率：
- en: '![Figure 4.4 – CPU utilization during the memory leak](img/B19423_04_04.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图4.4 – 内存泄漏期间的CPU利用率](img/B19423_04_04.jpg)'
- en: Figure 4.4 – CPU utilization during the memory leak
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 – 内存泄漏期间的CPU利用率
- en: We can see that there are periods when both user CPU utilization and privileged
    (system) CPU utilization go up. These are the same periods when throughput dropped
    in *Figure 4**.1*. User CPU utilization is derived from the `System.Diagnostics.Process.UserProcessorTime`
    property, while system utilization (based on OpenTelemetry terminology) is derived
    from the `System.Diagnostics.Process.PriviledgedProcessorTime` property. These
    are the same periods when throughput dropped in *Figure 4**.1*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，当用户CPU利用率和特权（系统）CPU利用率同时上升时，这些时期与*图4.1*中吞吐量下降的时期相同。用户CPU利用率来源于`System.Diagnostics.Process.UserProcessorTime`属性，而系统利用率（基于OpenTelemetry术语）来源于`System.Diagnostics.Process.PriviledgedProcessorTime`属性。这些时期与*图4.1*中吞吐量下降的时期相同。
- en: Our investigation could have started with high latency, high error rate, a high
    number of process restarts, high CPU, or high memory utilization, and all of those
    are symptoms of the same problem – a memory leak. So, now, we need to investigate
    it further – let’s collect a memory dump to see what’s in there. Assuming you
    can reproduce the issue on a local machine, Visual Studio or JetBrains dotMemory
    can capture and analyze a memory dump. We will use `dotnet-dump`, which we can
    run on an instance experiencing problems. Check out the .NET documentation at
    [https://learn.microsoft.com/dotnet/core/diagnostics/dotnet-dump](https://learn.microsoft.com/dotnet/core/diagnostics/dotnet-dump)
    to learn more about the tool.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的调查可能从高延迟、高错误率、高进程重启次数、高CPU或高内存利用率开始，所有这些都是同一问题的症状——内存泄漏。因此，现在我们需要进一步调查它——让我们收集内存转储以查看其中有什么。假设你可以在本地机器上重现该问题，Visual
    Studio或JetBrains dotMemory可以捕获和分析内存转储。我们将使用`dotnet-dump`，我们可以在遇到问题的实例上运行它。查看[https://learn.microsoft.com/dotnet/core/diagnostics/dotnet-dump](https://learn.microsoft.com/dotnet/core/diagnostics/dotnet-dump)以了解更多关于该工具的信息。
- en: 'So, let’s capture the dump using the following command:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们使用以下命令来捕获转储：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once the dump has been collected, we can analyze it with Visual Studio, JetBrains
    dotMemory, or other tools that automate and simplify it. We’re going to do this
    the hard way with the `dotnet-dump` CLI tool:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦收集了转储，我们就可以使用Visual Studio、JetBrains dotMemory或其他自动化并简化分析的工具来分析它。我们将使用`dotnet-dump`
    CLI工具以困难的方式完成这项工作：
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This will open a prompt where we can run **SOS** commands. SOS is a debugger
    extension that allows us to examine running processes and dumps. It can help us
    find out what’s on the heap.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打开一个提示符，我们可以在其中运行**SOS**命令。SOS是一个调试器扩展，它允许我们检查正在运行的过程和转储。它可以帮助我们找出堆上的内容。
- en: 'We can do this with the `dumpheap -stat` command, which prints the count and
    total count and size of objects by their type, as shown in *Figure 4**.5*:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`dumpheap -stat`命令来完成这项工作，该命令按类型打印对象的计数、总计数和大小，如*图4.5*所示：
- en: '![Figure 4.5 – Managed heap stats showing ~20 million MemoryLeakController
    instances](img/B19423_04_05.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图4.5 – 显示约2000万个MemoryLeakController实例的托管堆统计信息](img/B19423_04_05.jpg)'
- en: Figure 4.5 – Managed heap stats showing ~20 million MemoryLeakController instances
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 – 显示约2000万个MemoryLeakController实例的托管堆统计信息
- en: Stats are printed in ascending order, so the objects with the biggest total
    size appear at the end. Here, we can see that we have almost 20 million `MemoryLeakController`
    instances, which consume about 1.5 GB of memory. The controller instance is scoped
    to the request, and it seems it is not collected after the request ends. Let’s
    find **the GC roots** – objects that keep controller instances alive.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 统计数据按升序打印，因此总大小最大的对象出现在末尾。在这里，我们可以看到我们几乎有2000万个`MemoryLeakController`实例，它们消耗了大约1.5GB的内存。控制器实例的范围是请求，看起来在请求结束后并没有被回收。让我们找到**GC根**——保持控制器实例存活的对象。
- en: We need to find the address of any controller instance. We can do this using
    its method table – the first hex number in each table row. The method table stores
    type information for each object and is an internal CLR implementation detail.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要找到任何控制器实例的地址。我们可以使用它的方法表——每个表格行中的第一个十六进制数字。方法表存储每个对象类型信息，是内部CLR实现细节。
- en: 'We can find the object address for it using another SOS command:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用另一个SOS命令来找到它的对象地址：
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This will print a table that contains the addresses of all `MemoryLeakController`
    instances. Let’s copy one of them so that we can find the GC root with it:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印一个包含所有`MemoryLeakController`实例地址的表格。让我们复制其中一个，以便我们可以用它与GC根一起找到：
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Figure 4**.6* shows the path from the GC root to the controller instance printed
    by the `gcroot` command:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4.6*显示了`gcroot`命令打印的从GC根到控制器实例的路径：'
- en: '![Figure 4.6 – ProcessingQueue is keeping the controller instances alive](img/B19423_04_06.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图4.6 – ProcessingQueue正在保持控制器实例存活](img/B19423_04_06.jpg)'
- en: Figure 4.6 – ProcessingQueue is keeping the controller instances alive
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 – ProcessingQueue正在保持控制器实例存活
- en: 'We can see that `issues.ProcessingQueue` is holding this and other controller
    instances. It uses `ConcurrentQueue<Action>` inside. If we were to check the controller
    code, we’d see that we added an action that uses `_logger` – a controller instance
    variable that implicitly keeps controller instances alive:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到`issues.ProcessingQueue`正在持有这个和其他控制器实例。它内部使用`ConcurrentQueue<Action>`。如果我们检查控制器代码，我们会看到我们添加了一个使用`_logger`的动作——这是一个控制器实例变量，它隐式地保持控制器实例存活：
- en: MemoryLeakController.cs
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: MemoryLeakController.cs
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4/issues/Controllers/MemoryLeakController.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4/issues/Controllers/MemoryLeakController.cs)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4/issues/Controllers/MemoryLeakController.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4/issues/Controllers/MemoryLeakController.cs)'
- en: To fix this, we’d need to stop capturing the controller’s logger in action and
    add size limits and backpressure to the queue.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要修复这个问题，我们需要停止在动作中捕获控制器的记录器，并给队列添加大小限制和背压。
- en: Thread pool starvation
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线程池饥饿
- en: Thread pool starvation happens when CLR does not have enough threads in the
    pool to process work, which can happen at startup or when the load increases significantly.
    Let’s reproduce it and see how it manifests.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当CLR没有足够的线程在池中处理工作负载时，就会发生线程池饥饿，这可能在启动时发生，或者在负载显著增加时发生。让我们重现它并看看它是如何表现的。
- en: 'With the **issues** app running, add some load using the following commands
    to send 300 concurrent requests to the app:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当**issues**应用程序运行时，使用以下命令添加一些负载，向应用程序发送300个并发请求：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, let’s check what happens with the throughput and latency. You might not
    see any metrics or traces coming from the application or see stale metrics that
    were reported before the load started. If you try to hit any API on the issue
    application, such as http://localhost:5051/ok, it will time out.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们检查吞吐量和延迟的情况。你可能看不到来自应用程序的任何指标或跟踪，或者看到在负载开始之前报告的过时指标。如果你尝试访问问题应用程序上的任何API，例如http://localhost:5051/ok，它将超时。
- en: If you check the CPU or memory for the **issues** process, you will see very
    low utilization – the process got stuck doing nothing. It lasts for a few minutes
    and then resolves – the service starts responding and reports metrics and traces
    as usual.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你检查**issues**进程的CPU或内存，你会看到非常低的利用率——进程卡在什么也不做。这会持续几分钟，然后解决——服务开始正常响应并报告指标和跟踪。
- en: 'One way to understand what’s going on when a process does not report metrics
    and traces is to use the `dotnet-counters` tool. Check out the .NET documentation
    at [https://learn.microsoft.com/dotnet/core/diagnostics/dotnet-counters](https://learn.microsoft.com/dotnet/core/diagnostics/dotnet-counters)
    to learn more about the tool. Now, let’s run it to see the runtime counters:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解当进程不报告指标和跟踪时发生了什么，可以使用`dotnet-counters`工具。查看.NET文档[https://learn.microsoft.com/dotnet/core/diagnostics/dotnet-counters](https://learn.microsoft.com/dotnet/core/diagnostics/dotnet-counters)以了解更多关于这个工具的信息。现在，让我们运行它来查看运行时计数器：
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'It should print a table consisting of runtime counters that change over time,
    as shown in *Figure 4**.7*:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 它应该打印一个表格，其中包含随时间变化的运行时计数器，如图*4.7*所示：
- en: '![Figure 4.7 – The dotnet-counters output dynamically showing runtime counters](img/B19423_04_07.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图4.7 – dotnet-counters输出动态显示运行时计数器](img/B19423_04_07.jpg)'
- en: Figure 4.7 – The dotnet-counters output dynamically showing runtime counters
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 – dotnet-counters输出动态显示运行时计数器
- en: Here, we’re interested in thread pool counters. We can see 1,212 work items
    waiting in the thread pool queue length and that it keeps growing along with the
    thread count. Only a few (if any) work items are completed per second.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们关注线程池计数器。我们可以看到有1,212个工作项在线程池队列长度中等待，并且随着线程数量的增加而持续增长。每秒只有少数（如果有的话）工作项被完成。
- en: 'The root cause of this behavior is the following code in the controller, which
    blocks the thread pool threads:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这种行为的根本原因是控制器中的以下代码，它阻止了线程池线程：
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: So, instead of switching to another work item, the threads sit and wait for
    the dummy call to complete. It affects all tasks, including those that export
    telemetry data to the collector – they are waiting in the same queue.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，线程不会切换到另一个工作项，而是坐着等待空调用完成。这影响所有任务，包括那些将遥测数据导出到收集器的任务 – 它们在同一个队列中等待。
- en: 'The runtime increases the thread pool size gradually and eventually, it becomes
    high enough to clean up the work item queue. Check out *Figure 4**.8* to see thread
    pool counter dynamics:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 运行时逐渐增加线程池大小，最终足够大以清理工作项队列。查看*图 4.8*以了解线程池计数器的动态变化：
- en: '![Figure 4.8 – The thread pool threads count and queue changes before and after
    starvation](img/B19423_04_08.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.8 – 饥饿前后线程池线程数和队列的变化](img/B19423_04_08.jpg)'
- en: Figure 4.8 – The thread pool threads count and queue changes before and after
    starvation
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 – 饥饿前后线程池线程数和队列的变化
- en: As you can see, we have no data for the time when starvation happened. But after
    the thread pool queue is cleared, we start getting the data and see that the runtime
    adjusts the number of threads to a lower value.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们没有关于饥饿发生时的数据。但是，在线程池队列清理后，我们开始获取数据，并看到运行时将线程数调整到更低的值。
- en: We just saw how problems on a certain code path can affect the performance of
    the whole process and how we can use runtime metrics and diagnostics tools to
    narrow them down. Now, let’s learn how to investigate performance issues specific
    to certain APIs or individual requests.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到某个代码路径上的问题如何影响整个过程的性能，以及我们如何使用运行时指标和诊断工具来缩小范围。现在，让我们学习如何调查特定于某些 API 或单个请求的性能问题。
- en: Profiling
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能分析
- en: If we analyze individual traces corresponding to thread pool starvation or memory
    leaks, we will not see anything special. They are fast under a small load and
    get slower or fail when the load increases.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们分析对应于线程池饥饿或内存泄漏的个别跟踪，我们不会看到任何特别之处。在轻负载下它们运行得很快，但在负载增加时速度会变慢或失败。
- en: However, some performance issues only affect certain scenarios, at least under
    typical load. Locks and inefficient code are examples of such operations.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一些性能问题仅影响某些场景，至少在典型负载下是这样。锁和效率低下的代码是这类操作的例子。
- en: We rarely instrument local operations with distributed tracing under the assumption
    that local calls are fast and exceptions have enough information for us to investigate
    failures.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很少在分布式跟踪下对本地操作进行仪器化，假设本地调用很快，异常有足够的信息供我们调查失败。
- en: But what happens when we have compute-heavy or just inefficient code in the
    service? If we look at distributed traces, we’ll see high latency and gaps between
    spans, but we wouldn’t know why it happens.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，当我们有计算密集型或效率低下的代码在服务中时会发生什么？如果我们查看分布式跟踪，我们会看到高延迟和跨度之间的间隙，但我们不知道为什么会发生。
- en: We know ahead of time that some operations, such as complex algorithms or I/O,
    can take a long time to complete or fail, so we can deliberately instrument them
    with tracing or just write a log record. But we rarely introduce inefficient code
    to the hot path intentionally; due to this, our ability to debug it with distributed
    tracing, metrics, or logs is limited.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提前知道某些操作，例如复杂算法或 I/O，可能需要很长时间才能完成或失败，因此我们可以故意用跟踪或只是写一个日志记录来跟踪它们。但我们很少故意将低效代码引入热点路径；因此，我们使用分布式跟踪、指标或日志进行调试的能力有限。
- en: In such cases, we need more precise signals, such as profiling. **Profiling**
    involves collecting call stacks, memory allocations, timings, and the frequency
    of calls. This can be done in-process using .NET profiling APIs that need the
    application to be configured in a certain way. Low-level performance profiling
    is usually done locally on a developer machine, but it used to be a popular mechanism
    among **Application Performance Monitoring** (**APM**) tools to collect performance
    data and traces.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们需要更精确的信号，例如性能分析。**性能分析**涉及收集调用栈、内存分配、计时和调用频率。这可以通过使用.NET性能分析API在进程内完成，这些API需要应用程序以某种方式配置。低级性能分析通常在开发者的本地机器上完成，但曾经是**应用程序性能监控**（**APM**）工具中收集性能数据和跟踪的一种流行机制。
- en: In this chapter, we’re going to use a different kind of profiling, also called
    performance tracing, which relies on `System.Diagnostics.Tracing.EventSource`,
    and can be done ad hoc. `EventSource` is essentially a platform logger – CLR,
    libraries, and frameworks write their diagnostics to event sources, which are
    disabled by default, but it’s possible to enable and control them dynamically.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用一种不同类型的分析，也称为性能追踪，它依赖于 `System.Diagnostics.Tracing.EventSource`，并且可以随时进行。`EventSource`
    实质上是一个平台日志记录器 – CLR、库和框架将它们的诊断信息写入事件源，这些事件源默认是禁用的，但可以动态地启用和控制。
- en: The .NET runtime and libraries events cover GC, tasks, the thread pool, the
    DNS, sockets, and HTTP, among other things. ASP.NET Core, Kestrel, Dependency
    Injection, Logging, and other libraries have their own event providers too.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: .NET 运行时和库事件涵盖了 GC、任务、线程池、DNS、套接字和 HTTP 等其他内容。ASP.NET Core、Kestrel、依赖注入、日志记录和其他库也有它们自己的事件提供者。
- en: You can listen to any provider inside the process using `EventListener` and
    access events and their payloads, but the true power of `EventSource` is that
    you can control providers from out-of-process over `dotnet-monitor` tool in [*Chapter
    2*](B19423_02.xhtml#_idTextAnchor038), *Native Monitoring* *in .NET*.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `EventListener` 监听进程内的任何提供者，并访问事件及其负载，但 `EventSource` 的真正威力在于您可以通过 `dotnet-monitor`
    工具从进程外部控制提供者，如 [*第 2 章*](B19423_02.xhtml#_idTextAnchor038) 中所述，*在 .NET* 中的 *原生监控*。
- en: Let’s see how performance tracing and profiling with `EventSource` can help
    us investigate specific issues.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看使用 `EventSource` 进行性能追踪和剖析如何帮助我们调查特定问题。
- en: Inefficient code
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 低效的代码
- en: 'Let’s run our demo application and see how inefficient code can manifest itself.
    Make sure the observability stack is running, then start the **issues** application,
    and then apply some load:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行我们的演示应用程序，看看低效的代码如何表现出来。确保可观察性堆栈正在运行，然后启动 **问题** 应用程序，然后应用一些负载：
- en: '[PRE10]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The load generator bombards the http://localhost:5051/spin?fib=<n> endpoint
    with 100 concurrent requests. The spin endpoint calculates an *n*th Fibonacci
    number; as you’ll see, our Fibonacci implementation is quite inefficient.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 负载生成器向 http://localhost:5051/spin?fib=<n> 端点发送 100 个并发请求。spin 端点计算第 *n* 个斐波那契数；正如您将看到的，我们的斐波那契实现相当低效。
- en: 'Assuming we don’t know how bad this Fibonacci implementation is, let’s try
    to investigate why this request takes so long. Let’s open Jaeger by going to http://localhost:16686,
    clicking on **Find traces**, and checking out the latency distribution, as shown
    in *Figure 4**.9*:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们不知道这个斐波那契实现有多糟糕，让我们尝试调查为什么这个请求花费了这么长时间。让我们通过访问 http://localhost:16686，点击
    **查找追踪**，并查看延迟分布来打开 Jaeger，如图 *图 4.9* 所示：
- en: '![Figure 4.9 – Latency distribution in Jaeger](img/B19423_04_09.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.9 – Jaeger 中的延迟分布](img/B19423_04_09.jpg)'
- en: Figure 4.9 – Latency distribution in Jaeger
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9 – Jaeger 中的延迟分布
- en: 'We can see that almost all requests take more than 2 seconds to complete. If
    you click on any of the dots, Jaeger will show the corresponding trace. It should
    look similar to the one shown in *Figure 4**.10*:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到几乎所有请求都需要超过 2 秒才能完成。如果您点击任何一个点，Jaeger 将显示相应的追踪。它应该看起来与 *图 4.10* 中显示的类似：
- en: '![Figure 4.10 – Long trace in Jaeger](img/B19423_04_10.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.10 – Jaeger 中的长追踪](img/B19423_04_10.jpg)'
- en: Figure 4.10 – Long trace in Jaeger
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10 – Jaeger 中的长追踪
- en: The load application is instrumented so that we can measure client latency too.
    We can see that the client request took 4.5 seconds, while the server-side request
    took about 1.5 seconds. In a spin request, we call the dummy controller of the
    same application and can see corresponding client and server spans. The only thing
    that stands out here is that there are plenty of gaps and we don’t know what happened
    there.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 负载应用程序被配置为我们可以测量客户端延迟。我们可以看到客户端请求花费了 4.5 秒，而服务器端请求大约花费了 1.5 秒。在 spin 请求中，我们调用相同应用程序的虚拟控制器，并可以看到相应的客户端和服务器跨度。这里唯一突出的是有很多空隙，我们不知道那里发生了什么。
- en: If we check out the metrics, we will see high CPU and high server latency, but
    nothing suspicious that can help us find the root cause. So, it’s time to capture
    some performance traces.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查指标，我们将看到高 CPU 和高服务器延迟，但没有可疑之处可以帮助我们找到根本原因。因此，是时候捕获一些性能追踪了。
- en: Multiple tools can capture performance traces for the process that experiences
    this issue, such as PerfView on Windows, or PerfCollect on Linux.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 多种工具可以捕获经历此问题的进程的性能追踪，例如 Windows 上的 PerfView 或 Linux 上的 PerfCollect。
- en: 'We’re going to use the cross-platform `dotnet-trace` CLI tool, which you can
    install and use anywhere. Go ahead and run it using the following command for
    10-20 seconds:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用跨平台的`dotnet-trace` CLI工具，您可以在任何地方安装和使用它。请运行以下命令10-20秒：
- en: '[PRE11]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: With this command, we’ve enabled the `Microsoft-DotNETCore-SampleProfiler` event
    source (among other default providers) to capture managed thread call stacks for
    the `dotnet-trace` tool by reading the .NET documentation at [https://learn.microsoft.com/dotnet/core/diagnostics/dotnet-trace](https://learn.microsoft.com/dotnet/core/diagnostics/dotnet-trace).
    We could also configure it to collect events from any other event source.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此命令，我们已启用`Microsoft-DotNETCore-SampleProfiler`事件源（以及其他默认提供者）来通过读取.NET文档[https://learn.microsoft.com/dotnet/core/diagnostics/dotnet-trace](https://learn.microsoft.com/dotnet/core/diagnostics/dotnet-trace)捕获`dotnet-trace`工具的托管线程调用堆栈。我们还可以将其配置为收集来自任何其他事件源的事件。
- en: 'The tool saves traces to the `issues.exe_*.nettrace` file, which we can analyze
    with it as well:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 工具将跟踪保存到`issues.exe_*.nettrace`文件中，我们也可以用它来分析：
- en: '[PRE12]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'It outputs the top (5 by default) methods that have been on the stack most
    of the time. *Figure 4**.11* shows some sample output:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 它输出在堆栈上花费时间最多的前（默认为5个）方法。*图4.11*显示了部分输出示例：
- en: '![Figure 4.11 – Top five methods on the stack](img/B19423_04_11.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图4.11 – 堆栈上的前五种方法](img/B19423_04_11.jpg)'
- en: Figure 4.11 – Top five methods on the stack
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11 – 堆栈上的前五种方法
- en: There are no details about the top line – this is due to unmanaged or dynamically
    generated code. But the second one is ours – the `MostInefficientFibonacci` method
    looks suspicious and is worth checking. It was on the call stack 29.3% of the
    time (exclusive percentage). Alongside nested calls, it was on the call stack
    31.74% of the time (inclusive percentage). This was easy, but in more complex
    cases, this analysis won’t be enough, and we might want to dig even further into
    popular call stacks.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 关于最上面的行没有详细信息——这是由于非托管或动态生成的代码造成的。但第二行是我们的——`MostInefficientFibonacci`方法看起来可疑，值得检查。它在调用堆栈上出现了29.3%的时间（排他性百分比）。在嵌套调用中，它在调用堆栈上出现了31.74%的时间（包含性百分比）。这很简单，但在更复杂的情况下，这种分析将不足以满足需求，我们可能需要进一步深入到流行的调用堆栈中。
- en: You can open the trace file with any of the performance analysis tools I mentioned
    previously. We’ll use SpeedScope ([https://www.speedscope.app/](https://www.speedscope.app/)),
    a web-based tool.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用我之前提到的任何性能分析工具打开跟踪文件。我们将使用SpeedScope([https://www.speedscope.app/](https://www.speedscope.app/))，这是一个基于Web的工具。
- en: 'First, let’s convert the trace file into `speedscope` format:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们将跟踪文件转换为`speedscope`格式：
- en: '[PRE13]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Then, we must drop the generated JSON file into SpeedScope via the browser.
    It will show the captured call stacks for each thread.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们必须通过浏览器将生成的JSON文件拖放到SpeedScope中。它将显示每个线程捕获的调用堆栈。
- en: 'You can click through different threads. You will see that many of them are
    sitting and waiting for work, as shown in *Figure 4**.12*:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以点击不同的线程。您会看到其中许多都在等待工作，如图*图4.12*所示：
- en: '![Figure 4.12 – The thread is waiting for work](img/B19423_04_12.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图4.12 – 线程正在等待工作](img/B19423_04_12.jpg)'
- en: Figure 4.12 – The thread is waiting for work
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.12 – 线程正在等待工作
- en: This explains the top line in the report – most of the time, threads are waiting
    in unmanaged code.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这解释了报告中的最上面一行——大多数时候，线程都在非托管代码中等待。
- en: 'There is another group of threads that is working hard to calculate Fibonacci
    numbers, as you can see in *Figure 4**.13*:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，还有另一组线程正在努力计算斐波那契数，如图*图4.13*所示：
- en: '![Figure 4.13 – Call stack showing controller invocation with Fibonacci number
    calculation](img/B19423_04_13.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图4.13 – 显示使用斐波那契数计算的控制器调用的调用堆栈](img/B19423_04_13.jpg)'
- en: Figure 4.13 – Call stack showing controller invocation with Fibonacci number
    calculation
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.13 – 显示使用斐波那契数计算的控制器调用的调用堆栈
- en: As you can see, we use a recursive Fibonacci algorithm without memorization,
    which explains the terrible performance.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们使用了一个没有记忆的递归斐波那契算法，这解释了糟糕的性能。
- en: We could have also used the `dotnet-stack` tool, which prints managed thread
    stack trace snapshots.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`dotnet-stack`工具，该工具可以打印托管线程堆栈跟踪快照。
- en: Debugging locks
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调试锁
- en: With performance tracing, we can detect code that actively consumes CPU, but
    what if nothing happens – for example, if we have a lock in our code? Let’s find
    out.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 使用性能跟踪，我们可以检测到积极消耗CPU的代码，但如果没有发生任何事情——例如，如果我们代码中有一个锁？让我们来看看。
- en: 'Let’s start the **issues** app and generate some load:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们启动**问题**应用并生成一些负载：
- en: '[PRE14]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: If we check the CPU and memory consumption, we can see that they are low and
    don’t grow much, the thread count doesn’t change much, the thread queue is empty,
    and the contention rate is low. At the same time, the throughput is low (around
    60 requests per second) and the latency is big (P95 is around 3 seconds). So,
    the application is doing nothing, but it can’t go faster. If we check the traces,
    we will see a big gap with no further data.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查CPU和内存消耗，我们可以看到它们很低，增长不多，线程计数变化不大，线程队列是空的，竞争率很低。同时，吞吐量很低（大约每秒60个请求）和延迟很大（P95大约是3秒）。所以，应用程序什么都没做，但它不能更快。如果我们检查跟踪信息，我们会看到一个很大的空白，没有进一步的数据。
- en: This issue is specific to the lock API; if we hit another API, such as `http://localhost:5051/ok`,
    it responds immediately. This narrows down our search for the lock API.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题特定于锁API；如果我们击中另一个API，例如`http://localhost:5051/ok`，它会立即响应。这缩小了我们搜索锁API的范围。
- en: Assuming we don’t know there is a lock there, let’s collect some performance
    traces again with `$ dotnet-trace collect --name issues`. If we get the `topN`
    stacks, as in the previous example, we won’t see anything interesting – just threads
    waiting for work – locking is fast; waiting for the locked resource to become
    available takes much longer.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们不知道那里有一个锁，让我们再次使用`$ dotnet-trace collect --name issues`收集一些性能跟踪信息。如果我们得到`topN`堆栈，就像之前的例子一样，我们不会看到任何有趣的东西
    – 只是有线程在等待工作 – 锁定很快；等待被锁定资源变得可用需要更长的时间。
- en: We can dig deeper into the generated trace file to find actual stack traces
    on what happens in the lock controller. We’re going to use PerfView on Windows,
    but you can use PerfCollect on Linux, or other tools such as JetBrains dotTrace
    to open trace files and find stack traces.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以更深入地挖掘生成的跟踪文件，以找到锁控制器中实际发生的事情的堆栈跟踪。我们将在Windows上使用PerfView，但你也可以在Linux上使用PerfCollect，或其他工具，如JetBrains
    dotTrace，来打开跟踪文件并找到堆栈跟踪。
- en: 'Let’s open the trace file with PerfView and then click on the `LockController.Lock`,
    as shown in *Figure 4**.14*:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用PerfView打开跟踪文件，然后单击`LockController.Lock`，如图*图4*.14所示：
- en: '![Figure 4.14 – Finding LockController stacks across all threads](img/B19423_04_14.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图4.14 – 在所有线程中查找LockController堆栈](img/B19423_04_14.jpg)'
- en: Figure 4.14 – Finding LockController stacks across all threads
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.14 – 在所有线程中查找LockController堆栈
- en: We can see that `LockController` rarely appears on the call stack, as well as
    its nested calls – we can tell since both the inclusive and exclusive percentages
    are close to 0\. From this, we can conclude that whatever we’re waiting for is
    asynchronous; otherwise, we would see it on the call stack.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到`LockController`很少出现在调用堆栈上，以及它的嵌套调用 – 我们可以从包含和排除的百分比都接近0来判断。从这个角度来看，我们可以得出结论，我们正在等待的东西是异步的；否则，我们会在调用堆栈上看到它。
- en: 'Now, let’s right-click on the `LockController` line and click on `LockController`
    stacks. Switch to the **CallTree** tab, as shown in *Figure 4**.15*:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们右键单击`LockController`行，然后单击`LockController`堆栈。切换到**调用树**选项卡，如图*图4*.15所示：
- en: '![Figure 4.15 – Call stack with LockController.Lock](img/B19423_04_15.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图4.15 – 带有LockController.Lock的调用堆栈](img/B19423_04_15.jpg)'
- en: Figure 4.15 – Call stack with LockController.Lock
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.15 – 带有LockController.Lock的调用堆栈
- en: We can see that the controller calls `SemaphoreSlim.WaitAsync` – this should
    be our first suspect. It would explain the low CPU, low memory usage, and no anomalies
    in the thread counts. It still makes clients wait and keeps client connections
    open.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到控制器调用了`SemaphoreSlim.WaitAsync` – 这应该是我们的首要嫌疑人。它解释了低CPU、低内存使用和线程计数没有异常。它仍然让客户端等待并保持客户端连接打开。
- en: Note
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We can only see the synchronous part of the call stack in *Figure 4**.15* –
    it does not include `WaitAsync` or anything that happens after that.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图4*.15中，我们只能看到调用堆栈的同步部分 – 它不包括`WaitAsync`或之后发生的事情。
- en: The analysis we’ve done here relies on luck. In real-world scenarios, this issue
    would be hidden among other calls. We would have multiple suspects and would need
    to collect more data to investigate further. Since we’re looking for asynchronous
    suspects, collecting task-related events with `dotnet-trace` from the `System.Threading.Tasks.TplEventSource`
    provider would be useful.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里所做的分析依赖于运气。在现实世界的场景中，这个问题会隐藏在其他调用中。我们会有多名嫌疑人，需要收集更多数据来进一步调查。由于我们正在寻找异步嫌疑人，使用`dotnet-trace`从`System.Threading.Tasks.TplEventSource`提供者收集与任务相关的事件将很有用。
- en: 'The issue is obvious if we look into the code, but it can be hidden well in
    real-world code, behind feature flags or third-party libraries:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看代码，问题很明显，但在现实世界的代码中，它可能被很好地隐藏在功能标志或第三方库后面：
- en: LockController.cs
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: LockController.cs
- en: '[PRE15]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4/issues/Controllers/LockController.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4/issues/Controllers/LockController.cs)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4/issues/Controllers/LockController.cs](https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter4/issues/Controllers/LockController.cs)'
- en: The problem here is that we put a lock around the HTTP call to the downstream
    service. If we wrap only `ThreadUnsafeOperation` into a synchronous lock, we’ll
    see a much higher throughput of around 20K requests per second and low latency
    with P95 of around 20 milliseconds.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的问题在于我们在对下游服务的HTTP调用周围设置了一个锁。如果我们只将`ThreadUnsafeOperation`包装在一个同步锁中，我们将看到大约每秒20K的请求吞吐量和大约20毫秒的P95低延迟。
- en: Performance tracing is a powerful tool that allows us to capture low-level data
    reported by the .NET runtime, standard, and third-party libraries. In the examples
    we have covered in this chapter, we run diagnostics tools ad hoc and on the same
    host as the service. This is reasonable when you’re reproducing issues locally
    or optimizing your service on the dev box. Let’s see what we can do in a more
    realistic case with multiple instances of services running and restricted SSH
    access.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 性能跟踪是一个强大的工具，它允许我们捕获.NET运行时、标准和第三方库报告的低级别数据。在本章中我们讨论的示例中，我们在与服务的同一主机上运行诊断工具。当你本地重现问题或优化开发环境中的服务时，这是合理的。让我们看看在具有多个服务实例运行和受限SSH访问的更真实情况下我们能做什么。
- en: Using diagnostics tools in production
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在生产中使用诊断工具
- en: In production, we need to be able to collect some data proactively with reasonable
    performance and a telemetry budget so that we can analyze data afterward.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产中，我们需要能够以合理的性能和遥测预算主动收集一些数据，以便我们可以在之后分析数据。
- en: It’s difficult to reproduce an issue on a specific instance of a running process
    and collect performance traces or dumps from it in a secure and distributed application.
    If an issue such as a slow memory leak or a rare deadlock affects just a few instances,
    it might be difficult to even detect it and, when detected, the instance has already
    been recycled and the issue is no longer visible.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个运行进程的特定实例上重现问题并从它那里收集性能跟踪或转储，在安全且分布式的应用程序中是困难的。如果像缓慢的内存泄漏或罕见的死锁这样的问题只影响少数实例，甚至可能很难检测到它，并且在检测到时，实例已经被回收，问题不再可见。
- en: Continuous profiling
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连续剖析
- en: What we’re looking for is a continuous profiler – a tool that collects sampled
    performance traces. It can run for short periods to minimize the performance impact
    of collection on each instance and send profiles to central storage, where they
    can be stored, correlated with distributed traces, queried, and viewed. Distributed
    tracing supports sampling and a profiler can use it to capture traces and profiles
    consistently.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在寻找的是一个连续剖析器——一个收集样本性能跟踪的工具。它可以运行很短的时间，以最小化收集对每个实例的性能影响，并将配置文件发送到中央存储，在那里它们可以被存储、与分布式跟踪相关联、查询和查看。分布式跟踪支持采样，剖析器可以使用它来一致地捕获跟踪和配置文件。
- en: 'Many observability vendors, such as Azure Monitor, New Relic, Dynatrace, and
    others, provide continuous profilers for .NET. For example, Azure Monitor allows
    us to navigate to profiles from traces, as you can see in *Figure 4**.16*:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 许多可观察性供应商，如Azure Monitor、New Relic、Dynatrace等，为.NET提供连续的剖析器。例如，Azure Monitor允许我们从跟踪中导航到配置文件，正如你在*图4.16*中看到的那样：
- en: '![Figure 4.16 – Navigating to a profile from a trace in Azure Monitor](img/B19423_04_16.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图4.16 – 在Azure Monitor中从跟踪导航到配置文件](img/B19423_04_16.jpg)'
- en: Figure 4.16 – Navigating to a profile from a trace in Azure Monitor
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.16 – 在Azure Monitor中从跟踪导航到配置文件
- en: 'We will see a long trace for the inefficient code examples we went through
    earlier in this chapter, but the continuous profiler was enabled and captured
    some of these calls. If we click on the profiler icon, we will see the call stack,
    similar to the one we captured with `dotnet-collect`, as shown in *Figure 4**.17*:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到本章前面讨论的不高效代码示例的长时间跟踪，但连续剖析器已启用并捕获了一些这些调用。如果我们点击剖析器图标，我们将看到调用堆栈，类似于我们使用`dotnet-collect`捕获的，如图*图4.17*所示：
- en: '![Figure 4.17 – Profile showing a recursive call stack with the MostInefficientFibonacci
    method](img/B19423_04_17.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图4.17 – 显示包含MostInefficientFibonacci方法的递归调用堆栈的配置文件](img/B19423_04_17.jpg)'
- en: Figure 4.17 – Profile showing a recursive call stack with the MostInefficientFibonacci
    method
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.17 – 显示包含MostInefficientFibonacci方法的递归调用堆栈的配置文件
- en: With a continuous profiler, we can debug inefficient code in a matter of seconds,
    assuming that the problem is reproduced frequently enough so that we can capture
    both distributed trace and profile for it.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 使用持续分析器，我们可以在几秒钟内调试低效的代码，前提是问题足够频繁地重现，以便我们可以捕获分布式跟踪和对其进行分析。
- en: The dotnet-monitor tool
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: dotnet-monitor 工具
- en: Beyond profiling individual calls, we also need to be able to capture dumps
    proactively and on demand. It’s possible to configure .NET to capture dumps when
    a process crashes, but it doesn’t always work in containers and it’s not trivial
    to access and transfer dumps.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 除了分析单个调用之外，我们还需要能够主动和按需捕获转储。可以配置 .NET 在进程崩溃时捕获转储，但在容器中这并不总是有效，而且访问和传输转储并不简单。
- en: 'With `dotnet-monitor`, we can capture logs, memory, and GC dumps, and collect
    performance traces in the same way we did with `dotnet` diagnostic tools:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `dotnet-monitor`，我们可以以与 `dotnet` 诊断工具相同的方式捕获日志、内存和 GC 转储，并收集性能跟踪：
- en: Performance traces from event sources can be collected with the `dotnet-monitor`
    `/trace` API or the `dotnet-trace` CLI tool
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用 `dotnet-monitor` `/trace` API 或 `dotnet-trace` CLI 工具收集来自事件源的性能跟踪。
- en: Dumps can be collected with the `/dump` API or the `dotnet-dump` tool
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用 `/dump` API 或 `dotnet-dump` 工具收集转储
- en: Event counters can be collected with the `/metrics` API or the `dotnet-counters`
    tool
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用 `/metrics` API 或 `dotnet-counters` 工具收集事件计数器
- en: 'Check out the `dotnet-monitor` documentation to learn more about these and
    other HTTP APIs it provides: [https://github.com/dotnet/dotnet-monitor/tree/main/documentation](https://github.com/dotnet/dotnet-monitor/tree/main/documentation).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 查阅 `dotnet-monitor` 文档以了解这些以及其他它提供的 HTTP API：[https://github.com/dotnet/dotnet-monitor/tree/main/documentation](https://github.com/dotnet/dotnet-monitor/tree/main/documentation)。
- en: We can also configure triggers and rules that proactively collect traces or
    dumps based on CPU or memory utilization, GC frequency, and other runtime counter
    values. Results are uploaded to configurable external storage.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以配置触发器和规则，根据 CPU 或内存利用率、GC 频率和其他运行时计数器值主动收集跟踪或转储。结果会被上传到可配置的外部存储。
- en: We looked at some features of `dotnet-monitor` in [*Chapter 2*](B19423_02.xhtml#_idTextAnchor038),
    *Native Monitoring in .NET*, where we run it as a sidecar container in Docker.
    Similarly, you can run it as a sidecar in Kubernetes.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 [*第 2 章*](B19423_02.xhtml#_idTextAnchor038)，*Native Monitoring in .NET*
    中查看了一些 `dotnet-monitor` 的功能，其中我们在 Docker 中将其作为边车容器运行。同样，你可以在 Kubernetes 中将其作为边车运行。
- en: Summary
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Performance issues affect the user experience by decreasing service availability.
    Distributed tracing and common metrics allow you to narrow down the problem to
    a specific service, instance, API, or another combination of factors. When it’s
    not enough, you could increase resolution by adding more spans, but at some point,
    the performance impact and cost of the solution would become unreasonable.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 性能问题通过降低服务可用性来影响用户体验。分布式跟踪和常用指标允许你将问题缩小到特定的服务、实例、API 或其他因素的组合。当这还不够时，你可以通过添加更多的跨度来提高分辨率，但最终，解决方案的性能影响和成本可能会变得不合理。
- en: .NET runtime metrics provide insights into CLR, ASP.NET Core, Kestrel, and other
    components. Such metrics can be collected with OpenTelemetry, `dotnet-counters`,
    or `dotnet-monitor`. It could be enough to root cause an issue, or just provide
    input on how to continue the investigation. The next step could be capturing process
    dumps and analyzing memory or threads’ call stacks, which can be achieved with
    `dotnet-dump`.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: .NET 运行时指标提供了对 CLR、ASP.NET Core、Kestrel 以及其他组件的洞察。这些指标可以通过 OpenTelemetry、`dotnet-counters`
    或 `dotnet-monitor` 收集。这些指标可能足以找到问题的根本原因，或者只是提供如何继续调查的输入。下一步可能是捕获进程转储和分析内存或线程的调用栈，这可以通过
    `dotnet-dump` 实现。
- en: For problems specific to certain scenarios, performance traces provide details
    so that we can see what happens in the application or under the hood in third-party
    library code. Performance traces are collected with `dotnet-trace` or `dotnet-monitor`.
    By capturing performance traces, we can see detailed call stacks, get statistics
    on what consumes CPU, and monitor contentions and garbage collection more precisely.
    This is not only a great tool to investigate low-level issues but also to optimize
    your code.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特定场景的问题，性能跟踪提供了详细信息，使我们能够看到应用程序中发生的情况或在第三方库代码底层的操作。性能跟踪可以通过 `dotnet-trace`
    或 `dotnet-monitor` 收集。通过捕获性能跟踪，我们可以看到详细的调用栈，获取关于消耗 CPU 的统计信息，并更精确地监控竞争和垃圾回收。这不仅是一个调查底层问题的优秀工具，也是优化代码的好工具。
- en: Collecting low-level data in a secure, multi-instance environment is challenging.
    Continuous profilers can collect performance traces and other diagnostics on-demand,
    on some schedule, or by reacting to certain triggers. They also can take care
    of storing data in a central location and then visualizing and correlating it
    with other telemetry signals.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在安全的多实例环境中收集低级数据具有挑战性。连续分析器可以按需、按某些计划或通过响应某些触发器收集性能跟踪和其他诊断信息。它们还可以负责将数据存储在中央位置，并将其与其他遥测信号可视化并关联起来。
- en: The `dotnet-monitor` tool can run as a sidecar and then provide essential features
    to diagnostics data proactively or on-demand and send it to external storage.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`dotnet-monitor`工具可以作为边车运行，然后主动或按需提供诊断数据的基本功能，并将其发送到外部存储。'
- en: In this chapter, you learned how to collect diagnostics data using .NET diagnostics
    tools and how to use it to solve several classes of common performance issues.
    Applying this knowledge, along with what we learned about metrics, distributed
    tracing, and logs previously, should allow you to debug most distributed and local
    issues in your application.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学习了如何使用.NET诊断工具收集诊断数据，以及如何使用它来解决几类常见的性能问题。结合我们之前学到的关于指标、分布式跟踪和日志的知识，应该能够让您调试应用程序中的大多数分布式和本地问题。
- en: So, now, you know everything you need to leverage auto-instrumentation and make
    use of telemetry created by someone else. In the next chapter, we’ll learn how
    to enrich auto-generated telemetry and tailor it to our needs.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在您已经知道如何利用自动仪表化和使用他人创建的遥测数据。在下一章中，我们将学习如何丰富自动生成的遥测数据，并使其符合我们的需求。
- en: Questions
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What would you check first to understand whether an application was healthy?
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您首先会检查什么来了解应用程序是否健康？
- en: If you were to see a major performance issue affecting multiple different scenarios,
    how would you investigate it?
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您看到影响多个不同场景的主要性能问题，您会如何调查它？
- en: What’s performance tracing and how can you leverage it?
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 性能跟踪是什么？您如何利用它？
- en: 'Part 2: Instrumenting .NET Applications'
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2部分：仪表化.NET应用程序
- en: This part provides an in-depth overview and practical guide for .NET tracing,
    metrics, logs, and beyond. We’ll start by learning about OpenTelemetry configuration
    and then dive deep into manual instrumentation, using different signals.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分提供了对.NET跟踪、指标、日志以及更多内容的深入概述和实践指南。我们将从了解OpenTelemetry配置开始，然后深入探讨手动仪表化，使用不同的信号。
- en: 'This part has the following chapters:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 5*](B19423_05.xhtml#_idTextAnchor083), *Configuration and Control
    Plane*'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第5章*](B19423_05.xhtml#_idTextAnchor083)，*配置和控制平面*'
- en: '[*Chapter 6*](B19423_06.xhtml#_idTextAnchor098), *Tracing Your Code*'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第6章*](B19423_06.xhtml#_idTextAnchor098)，*跟踪您的代码*'
- en: '[*Chapter 7*](B19423_07.xhtml#_idTextAnchor115), *Adding Custom Metrics*'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第7章*](B19423_07.xhtml#_idTextAnchor115)，*添加自定义指标*'
- en: '[*Chapter 8*](B19423_08.xhtml#_idTextAnchor131), *Writing Structured and Correlated
    Logs*'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第8章*](B19423_08.xhtml#_idTextAnchor131)，*编写结构化和关联日志*'
