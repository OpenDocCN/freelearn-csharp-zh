- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performance Optimization and Profiling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we delve into the crucial aspect of ensuring that our code
    not only functions correctly but also runs efficiently. While writing code that
    works is essential, optimizing its performance is equally vital, especially in
    today’s fast-paced digital landscape where users expect swift and responsive applications.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding chapters, we laid the groundwork by mastering unit testing,
    **test-driven development** ( **TDD** ), advanced debugging strategies, and code
    analysis. Now, we shift our focus to the optimization and profiling tools available
    within Visual Studio 2022, empowering ourselves to fine-tune our applications
    for optimal performance.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, we will explore various techniques and methodologies
    aimed at enhancing the speed, responsiveness, and resource efficiency of our software.
    We’ll begin by introducing the fundamentals of performance optimization and the
    importance of utilizing profiling tools to identify bottlenecks and areas for
    improvement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Key topics covered in this chapter include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to performance optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing Visual Studio profiling tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing CPU usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory profiling and optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing database interaction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By mastering these concepts and techniques, we’ll learn how to pinpoint performance
    issues, optimize critical sections of our code base, and ensure that our applications
    deliver a seamless user experience under various workloads and conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin our journey toward building faster, more efficient software together.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While writing this chapter, I used the following version of Visual Studio:'
  prefs: []
  type: TYPE_NORMAL
- en: Visual Studio Enterprise 2022 Version 17.12.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preview 1.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code files for this chapter can found at [https://github.com/PacktPublishing/Mastering-Visual-Studio-2022/tree/main/ch04](https://github.com/PacktPublishing/Mastering-Visual-Studio-2022/tree/main/ch04)
    .
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to performance optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performance optimization in software development involves refining an application
    to operate with maximum efficiency, minimizing resource consumption such as memory,
    CPU, and bandwidth. This process includes analyzing performance at various development
    stages, often focusing on enhancing efficiency once a stable version of the product
    has been established.
  prefs: []
  type: TYPE_NORMAL
- en: The main step in our performance optimization process is *identifying bottlenecks*
    , as it allows us to pinpoint the specific areas of our code that are causing
    performance issues. **Bottlenecks** are points in our code where the execution
    slows down significantly, often due to resource constraints or inefficient algorithms.
    By identifying these bottlenecks, we can focus our optimization efforts on where
    they will have the most impact, leading to more efficient and faster applications.
    This targeted approach not only improves the performance of our application but
    also enhances the overall user experience by reducing load times and improving
    responsiveness. Furthermore, identifying bottlenecks early in the development
    process can prevent costly rework and delays, as it becomes more economical to
    address performance issues before they become entrenched in our application’s
    architecture. In essence, the ability to identify and address bottlenecks is a
    key skill for us as developers aiming to create high-performance applications,
    ensuring that our software runs smoothly and efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance optimization could be performed at different levels and offers
    different paths of exploration:'
  prefs: []
  type: TYPE_NORMAL
- en: At the design level, the architecture of the system plays a crucial role in
    its performance. Designing with performance in mind involves making strategic
    decisions about how the system interacts with hardware and network resources.
    For instance, reducing network latency can be achieved by minimizing network requests,
    ideally making a single request instead of multiple. This approach not only reduces
    the load on the network but also simplifies the application’s architecture, making
    it easier to maintain and scale.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation choices in the source code also have a significant impact on
    system optimization. Employing efficient coding practices is crucial for achieving
    system optimization. This includes avoiding unnecessary computations, which can
    significantly reduce the computational overhead of the application. For example,
    using **Language-Integrated Query** ( **LINQ** ) for data manipulation can lead
    to code that is more readable yet potentially more efficient than traditional
    loops. Additionally, utilizing C#’s asynchronous programming features, such as
    async and await, can help to improve the responsiveness of applications by allowing
    them to perform other tasks while waiting for long-running operations to complete.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of algorithms and data structures is a critical factor in system
    performance. Efficient algorithms and data structures can significantly reduce
    the time complexity of operations, allowing the system to handle larger datasets
    and more complex tasks with ease. Ideally, algorithms should operate at constant
    (O(1)), logarithmic (O(log n)), linear (O(n)), or log-linear (O(n log n)) time
    complexities. Algorithms with quadratic complexity (O(n^2)) can struggle to scale
    efficiently, especially as the size of the dataset grows. Similarly, abstract
    data types, which encapsulate data and operations in a single entity, can be more
    efficient for system optimization compared to more complex data structures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing at the build level involves making decisions during the build process
    that can tailor the application for specific processor models or environments.
    This can include disabling unnecessary software features, which can reduce the
    size of the executable and improve its performance. Additionally, build-level
    optimizations can involve the use of compiler flags that enable specific optimizations,
    such as loop unrolling or function inlining, which can improve the efficiency
    of the generated code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may have noticed that I categorized the algorithm by complexity using Big
    O notation; let’s take a refresher about this notation.
  prefs: []
  type: TYPE_NORMAL
- en: Big O notation serves as a mathematical representation utilized to characterize
    the performance or complexity of algorithms, particularly concerning their runtime
    behavior with increasing input size.
  prefs: []
  type: TYPE_NORMAL
- en: Proficiency in understanding Big O notation holds significant importance for
    software engineers. It equips us with the ability to evaluate and contrast the
    efficiency of diverse algorithms. Consequently, we can make well-informed decisions
    regarding the selection of algorithms suitable for specific scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following points outline the well-known Big O notations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Constant time O(1)** : Execution time remains unchanged irrespective of input
    data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logarithmic time O(log n)** : Complexity increases by one unit for each doubling
    of input data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linear time O(n)** : Execution time increases linearly with the size of the
    input data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Log-linear time O(n log n)** : Complexity grows as a combination of linear
    and logarithmic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quadratic time O(n^2)** : Time taken is proportional to the square of the
    number of elements'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cubic time O(n^3)** : Execution time is proportional to the cube of the number
    of elements'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exponential time O(2^n)** : Time doubles for every new element added'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Factorial time O(n!)** : Complexity grows factorially based on the size of
    the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve explored the fundamental principles of performance optimization
    in software development, let’s delve into practical methods for identifying and
    addressing performance issues. Some invaluable tools for our endeavor are Visual
    Studio profiling tools. By leveraging the capabilities of these tools, we can
    gain deeper insights into our application’s performance and streamline the optimization
    process. Let’s examine how Visual Studio profiling tools can be effectively utilized
    to enhance the performance of our software applications.
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing Visual Studio profiling tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Visual Studio profiling tools are a suite of performance measurement and diagnostic
    tools integrated into Visual Studio. In this section, we will explore how to use
    it and explore what tools are offered to explore and monitor our applications.
  prefs: []
  type: TYPE_NORMAL
- en: To open the Performance Profiler, we go to **Debug** | **Performance Profiler**
    or press *Alt* + *F2* .
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Performance Profiler](img/B22218_04_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Performance Profiler
  prefs: []
  type: TYPE_NORMAL
- en: Before clicking on the **Start** button, let’s review the various options the
    Performance Analyzer feature offers us for profiling our applications.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing .NET asynchronous events
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: .NET’s async and await features allow us to analyze the asynchronous events
    that are organized chronologically, displaying start time, end time, and duration,
    in a table of activities that occurred during our profiling session. Tasks are
    labeled in the **Name** column.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – .NET Async](img/B22218_04_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – .NET Async
  prefs: []
  type: TYPE_NORMAL
- en: If a task isn’t complete within the collection session, an *Incomplete* label
    appears in the **End** **Time** column.
  prefs: []
  type: TYPE_NORMAL
- en: To investigate a specific task or activity, we can right-click the row and select
    **Go To Source File** to see where in our code that activity happened.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the characteristics of async performance is crucial. While async
    methods are meant to enhance application responsiveness and scalability, they
    do introduce some overhead due to the state machine created by the compiler. However,
    this overhead is generally minimal and efficient for high-throughput scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: When comparing the performance of async versus sync code, we need to consider
    the nature of the operations being performed. For operations inherently asynchronous
    (such as **input/output** ( **I/O** )-bound operations), async methods can provide
    significant performance benefits by freeing up threads to handle other requests
    while waiting. However, for CPU-bound operations, the performance difference between
    async and sync methods may be negligible.
  prefs: []
  type: TYPE_NORMAL
- en: For effective performance monitoring of our application using .NET’s async and
    await features, we can utilize the **.NET Async** tool in Visual Studio for detailed
    analysis of asynchronous code execution. Additionally, external tools such as
    Stackify Retrace offer comprehensive monitoring capabilities for .NET applications,
    including support for async/await. Understanding the performance characteristics
    of async methods and the nature of the operations being performed is crucial for
    making informed decisions about when to employ async programming patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring with .NET Counters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Visual Studio 2022 integrates the . **NET Counters** tool, which is an advanced
    feature that allows developers like us to visualize performance counters over
    time directly within the Visual Studio profiler. This tool proves to be particularly
    useful for monitoring and analyzing various metrics of .NET applications, such
    as CPU usage, garbage collector heap size, and any custom counters we might have
    implemented in our applications. This integration enables us to leverage the power
    of .NET Counters directly from within the Visual Studio environment, providing
    a more seamless and integrated experience for performance monitoring and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Visual Studio 2022 enhanced the .NET Counters tool to support two innovative
    metrics: **UpDownCounter** and **ObservableCounter** . **UpDownCounter** enables
    real-time tracking of values with both incremental and decremental changes, which
    is ideal for monitoring dynamic values such as user interactions in web applications.
    On the other hand, **ObservableCounter** autonomously manages aggregated totals,
    offering customizable callback delegates for precise control. This feature can
    be particularly useful for optimizing server resources by efficiently managing
    active session totals.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, a filter flyout feature in the tool allows us to conveniently
    filter data points based on tags. This dynamic adjustment feature significantly
    enhances the flexibility and streamlining of monitoring dynamic values in our
    projects.
  prefs: []
  type: TYPE_NORMAL
- en: While collecting data, we can see live values of .NET Counters and view graphs
    of multiple counters simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – .NET Counters](img/B22218_04_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – .NET Counters
  prefs: []
  type: TYPE_NORMAL
- en: Once we stop collection, we get a detailed report showing minimum, maximum,
    and average values for each counter in the selected time range. This report provides
    us with a comprehensive overview of the performance metrics of our application,
    helping us identify and address performance bottlenecks more effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking .NET Object Allocation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **.NET Object Allocation Tracking** tool proves particularly valuable for
    understanding allocation patterns in .NET code and optimizing an application’s
    memory usage by identifying the most memory-intensive methods. However, it’s important
    to note that while this tool can reveal where objects are allocated, it does not
    elucidate why an object remains in memory.
  prefs: []
  type: TYPE_NORMAL
- en: We initiate the tool by clicking on the **Start** button, and the tool offers
    the **Start with collection paused** option before starting the profiler if we
    prefer to commence with data collection paused.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Start analysis](img/B22218_04_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Start analysis
  prefs: []
  type: TYPE_NORMAL
- en: This allows us to manually commence data collection by clicking the **Record**
    button in the diagnostic session view.
  prefs: []
  type: TYPE_NORMAL
- en: After executing the tool, we can halt the collection or close our application
    to review the data. The tool furnishes comprehensive memory allocation data, including
    the location of objects that are allocating memory and the amount of memory those
    objects are allocating. It also displays the number of objects that occupy memory
    within a specific allocation type or function, as well as the amount of memory
    consumed instead of the number of objects.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the tool presents a **Collections** view, which illustrates how
    many objects were collected or retained during garbage collection, offering insights
    into garbage collection events such as the type of garbage collection, the reason
    for the event, and the size of the **large object heap** ( **LOH** ) and **pinned
    object heap** ( **POH** ) after the garbage collector was executed.
  prefs: []
  type: TYPE_NORMAL
- en: Viewing the event
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Events** **Viewer** tool allows us to examine the collected information
    after our application has stopped, like a post-mortem analysis. It displays a
    list of events such as module load, thread start, and system configuration, aiding
    in diagnosing our application’s performance within the Visual Studio profiler.
  prefs: []
  type: TYPE_NORMAL
- en: To enable custom **Event Tracing for Windows** ( **ETW** ) events, we can instrument
    our code with custom events and configure them to appear in the Events Viewer.
    This involves setting up the provider’s name and GUID for our custom event code.
    For C# custom event code, we set the same provider’s name value that we used when
    declaring our event code, and for the native custom event code, we set the provider
    GUID based on the GUID for the custom event code. Once configured, these custom
    events will appear in the Events Viewer when we collect a diagnostics trace.
  prefs: []
  type: TYPE_NORMAL
- en: The Events Viewer can display up to 20,000 events at a time. To focus on specific
    events, we can filter the display by selecting the **Event** filter. Additionally,
    we can see the percentage of the total number of events that occurred for each
    provider, providing a breakdown of where our time is being spent. This filtering
    and breakdown help in identifying the most relevant events for our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to enable custom ETW events:'
  prefs: []
  type: TYPE_NORMAL
- en: First, build our custom event code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, in the **Performance Profiler** window (accessed via *Alt* + *F2* ), enable
    **Events Viewer** and select the **Settings** icon next to it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the dialog box, enable the first row under **Additional Providers** and configure
    it according to our custom event code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For native custom event code, we set the **Provider GUID** value and leave the
    **Provider Name** value empty or use its default value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For C# custom event code, we set the same **Provider Name** value that we used
    when declaring our event code, leaving the **Provider GUID** empty. After configuration,
    our custom events will appear in the Events Viewer when we collect a diagnostics
    trace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This tool is particularly useful for us as developers looking to diagnose performance
    issues or understand the behavior of our applications in detail. It provides a
    comprehensive view of our application’s activity and performance metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing File I/O
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **File IO** tool in Visual Studio is a powerful profiling tool designed
    to help us optimize our file I/O operations, thereby improving the performance
    of our applications. This tool is particularly useful for diagnosing slow loading
    times and inefficient data read or write patterns. It provides detailed information
    about file read and write operations during a profiling session, including the
    files accessed, the target process for each file, and aggregate information for
    each file. The tool also offers features such as the **Duplication Factor** ,
    which helps us identify whether more data is being read or written than necessary,
    indicating potential areas for optimization, such as caching results of file reads.
  prefs: []
  type: TYPE_NORMAL
- en: The File IO tool provides file read and write information with files read during
    the profiling session. The files are autogenerated in a report after collection
    and arranged by their target process with aggregate information displayed. If
    we right-click on one of the rows, we can go to the source in our code. If an
    aggregate row was read multiple times, we can expand it to see the individual
    read operations for that file with its frequency, if they were read multiple times.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing database performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Database Profiler** tool in Visual Studio is a feature designed to help
    us developers diagnose and optimize the performance of database operations within
    our applications. It proves particularly useful for applications that use .NET
    Core with either ADO.NET or Entity Framework Core, offering insights into database
    activities such as query execution times, the connection strings used, and where
    in the code these queries are being made. This tool is part of the Performance
    Profiler in Visual Studio and has been available since Visual Studio 2019 version
    16.3 onwards.
  prefs: []
  type: TYPE_NORMAL
- en: Once we start the profiling session, we interact with our application as we
    would normally, performing actions that we suspect might be causing database performance
    issues. After completing our actions, we stop the collection in Visual Studio.
    The tool then processes the collected data and displays a table of the queries
    that occurred during our profiling session, along with a graph showing the timing
    and frequency of these queries. This information can help us identify long-running
    queries, inefficient connection strings, or other performance bottlenecks in our
    database operations.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the Database Profiler tool supports analyzing traces collected
    using dotnet trace, allowing us to collect data from anywhere .NET Core runs,
    including Linux, and analyze it in Visual Studio. This feature is particularly
    useful for diagnosing performance issues in environments where Visual Studio is
    not installed or for scripting the collection of performance traces.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the Database Profiler tool in Visual Studio is a powerful diagnostic
    tool for us developers working with .NET Core applications that interact with
    databases. It provides detailed insights into database operations, helping us
    identify and resolve performance issues more effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Instrumenting our .NET applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Visual Studio, we utilize instrumentation tools for collecting precise call
    counts and call times, which are crucial for performance profiling and optimization.
    There are two main types of instrumentation methods available:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Static instrumentation** : This method involves modifying the program’s files
    before they run. We use a tool called VSInstr to insert instrumentation code into
    the application’s binaries. Static instrumentation is effective for collecting
    detailed timing data but can break strong name signing due to file modification.
    It also requires files to be deployed in a specific order, which can be cumbersome
    for complex programs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic instrumentation** : Introduced in Visual Studio 2022 version 17.5,
    this method does not alter the program’s files. Instead, it loads the files into
    memory and modifies them at runtime to collect instrumentation information. Dynamic
    instrumentation provides more accurate information, especially for smaller parts
    of the program, and allows for the investigation of specific code sections. It
    avoids the issue of breaking strong name signing since the instrumentation happens
    at runtime. This approach simplifies the process of finding and instrumenting
    files, especially in complex programs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This tool is like the CPU Usage tool but focuses on wall clock time instead
    of CPU utilization, making it suitable for scenarios where understanding the execution
    time of functions is critical.
  prefs: []
  type: TYPE_NORMAL
- en: By using the aforementioned tools, we can gather valuable insights into our
    application’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: However, to ensure optimal accuracy in our performance measurements, it’s advisable
    to profile our applications in the **Release** mode rather than the **Debug**
    mode. Debug builds can introduce additional overhead, potentially skewing our
    results.
  prefs: []
  type: TYPE_NORMAL
- en: When we find ourselves needing to inspect variable values or use breakpoints
    during analysis, we should consider leveraging the debugger-integrated tools found
    in the **Diagnostic Tools** window. These tools are tailored for such tasks and
    may offer a more suitable environment for our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: To gain a holistic understanding of our application’s performance, we can take
    advantage of Visual Studio’s capability to utilize multiple profiling tools simultaneously.
    This approach allows us to examine our application’s performance from different
    perspectives, providing a more comprehensive analysis.
  prefs: []
  type: TYPE_NORMAL
- en: You may have noticed that I have missed three tools offered by the Profiler
    Performance tools. I will highlight them in the next sections, beginning with
    the CPU Usage analyzer.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing CPU Usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **CPU Usage** tool in Visual Studio is designed to help us identify high
    CPU utilization and other related performance issues in our applications. It can
    be used for both local trace sessions and production environments, providing insights
    into where optimizations might be needed. To use the CPU Usage tool without the
    debugger, we should set the solution configuration to **Release** and select **Local
    Windows Debugger (or Local Machine)** as the deployment target. Under available
    tools, we select **CPU Usage** , and then we select **Start** .
  prefs: []
  type: TYPE_NORMAL
- en: If we enable the start with collection paused option, data collection will not
    begin until we select the **Record** button in the diagnostic session view. After
    the app starts, the diagnostic session begins, displaying CPU usage data. Once
    we’re finished collecting data, we select **Stop Collection** . The tool then
    analyzes the data and displays a report, which can be filtered and searched for
    specific threads or nodes.
  prefs: []
  type: TYPE_NORMAL
- en: The CPU Usage tool is particularly useful for diagnosing performance issues
    in our code base, identifying bottlenecks, and understanding CPU usage patterns.
    It provides automatic insights and various views of our data, enabling us to analyze
    and diagnose performance issues effectively. This tool is beneficial in production
    and difficult to debug at the moment but can be captured and analyzed using this
    tool to understand potential causes and suggest fixes.
  prefs: []
  type: TYPE_NORMAL
- en: The CPU Usage tool is particularly useful for diagnosing slow-downs, process
    hangs, and identifying bottlenecks in your code base, making it an essential tool
    for optimizing application performance.
  prefs: []
  type: TYPE_NORMAL
- en: When running this profiler tools spot the usage of CPU/second and collect traces
    generating a report with a graph to visualize the peak and valley of CPU usage.
  prefs: []
  type: TYPE_NORMAL
- en: When we first start the CPU Usage toll, it will collect a large amount of data
    per second to analyze what is going on in our application, and by default, it’s
    set at **1000** samples/second.
  prefs: []
  type: TYPE_NORMAL
- en: We can personalize the rate of the number of samples collected by second, by
    clicking on the gear at the right of the **CPU Usage** label in the profile console
    ( *Figure 4* *.1* ) before hitting the **Start** button. Depending on our needs,
    we can adjust the accuracy of their results and the data collection time.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – The CPU Usage settings](img/B22218_04_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – The CPU Usage settings
  prefs: []
  type: TYPE_NORMAL
- en: When we stop the collection or shut down our application, CPU usage tools generate
    reports. Initially, we land on the summary page, which displays a swim lane graph,
    the **Top Functions** section, and the **Hot** **Path** section.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – The Summary page](img/B22218_04_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – The Summary page
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can narrow down the potential bottleneck by right-clicking and dragging
    on the graph to surround the peak we want to focus on. By doing that, we effectively
    filter the graph by time.
  prefs: []
  type: TYPE_NORMAL
- en: 'By clicking on the **Open details…** link, we can digg deeper into the five
    other views:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Caller/Callee**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Call Tree**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Modules**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Functions**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flame Graph**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s understand each of these in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: In the **Caller/Callee** view, we can observe the relationship between a selected
    function and the functions that called it ( **Calling Functions** ) as well as
    the functions it called ( **Called Functions** ). It offers insights into the
    total time taken by the selected function and its percentage of the overall app
    running time. Additionally, it provides information on the time spent exclusively
    in the function body ( **Function Body** ). This view helps us understand the
    impact of a function on the application’s performance and identify potential bottlenecks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Caller/Callee](img/B22218_04_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – Caller/Callee
  prefs: []
  type: TYPE_NORMAL
- en: The **Call Tree** view presents a hierarchical representation of the function
    calls in our application, starting from the top-level pseudo-node. It includes
    system and framework code (under an **[External Code]** node) as well as user-code
    methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.8 – The Call Tree view](img/B22218_04_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – The Call Tree view
  prefs: []
  type: TYPE_NORMAL
- en: This view is useful for understanding the sequence and nesting of function calls,
    aiding in the identification of the most CPU-intensive paths in our application.
  prefs: []
  type: TYPE_NORMAL
- en: In the **Modules** view, we can see a list of modules containing functions,
    which can be particularly useful when analyzing external code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.9 – The Modules view](img/B22218_04_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – The Modules view
  prefs: []
  type: TYPE_NORMAL
- en: It helps us understand which modules are contributing the most to CPU usage,
    assisting in the identification of third-party libraries or system components
    that might be impacting performance.
  prefs: []
  type: TYPE_NORMAL
- en: The **Functions** view lists all the functions in our application, sorted by
    their CPU usage. It provides detailed information such as **Total CPU** (the time
    spent by the function and any functions it called) and **Self CPU** (the time
    spent exclusively in the function body).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.10 – The Functions view](img/B22218_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – The Functions view
  prefs: []
  type: TYPE_NORMAL
- en: This view is essential for identifying the most resource-intensive functions
    in our application and focusing on optimization efforts.
  prefs: []
  type: TYPE_NORMAL
- en: A **flame graph** is a visualization that represents the call stack of our application
    over time. It helps in identifying hot paths, that is sequences of function calls
    that consume a significant amount of CPU time. The width of each function in the
    graph corresponds to the amount of CPU time it consumes, making it easier to spot
    performance bottlenecks. The **Flame Graph** view is particularly useful for understanding
    the overall CPU usage patterns of our application and pinpointing specific areas
    for optimization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Flame Graph](img/B22218_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – Flame Graph
  prefs: []
  type: TYPE_NORMAL
- en: We can flip the view according to our preferences by using the **Flip Flame
    Graph** option and zooming in on our point of interest.
  prefs: []
  type: TYPE_NORMAL
- en: For debugging sessions, the CPU Usage tool can be accessed through the **Diagnostic
    Tools** window, which appears automatically unless turned off. We can select whether
    to see **CPU Usage** , **Memory Usage** , or both, with the **Select Tools** setting
    on the toolbar. The tool is enabled by default for CPU utilization analysis.
  prefs: []
  type: TYPE_NORMAL
- en: When the debugger pauses, the CPU Usage tool in the **Diagnostic Tools** window
    collects information about the functions executing in our application, listing
    the functions performing work and providing a timeline graph for focusing on specific
    segments of the sampling session.
  prefs: []
  type: TYPE_NORMAL
- en: While CPU usage and memory usage are related, they do not have a direct correlation.
    The impact of one on the other depends on the specific tasks being performed by
    the applications running on the system. Monitoring both metrics is crucial for
    optimizing system performance and effectively managing energy consumption.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve explored how to analyze CPU usage, let’s continue our journey
    by monitoring memory allocation.
  prefs: []
  type: TYPE_NORMAL
- en: Memory profiling and optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just as a reminder, a memory leak occurs when a computer program mishandles
    memory allocations, leading to unreleased memory that is no longer needed. .NET
    applications are generally less vulnerable to memory leaks due to automatic garbage
    collection and the fact that .NET applications are written in managed code. This
    means that the runtime has control over memory allocation and deallocation. However,
    if we produce code with smells or misuse the disposable pattern, memory leaks
    can still occur.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will explore how we can leverage Visual Studio to resolve
    memory leaks using the **Memory Usage** profiling tools and then the diagnostic
    tools available while debugging.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Memory Usage tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To find and resolve a memory leak, we can use the **Memory Usage** tool in Visual
    Studio. It is a robust profiling feature designed to monitor and analyze our application’s
    memory usage effectively. It supports various application types, including .NET,
    ASP.NET, C++, and mixed-mode applications. This versatile tool can be utilized
    both with and without the debugger, catering to different development scenarios.
    One of its key strengths lies in its ability to identify memory leaks and inefficient
    memory usage patterns.
  prefs: []
  type: TYPE_NORMAL
- en: During our diagnostic sessions, the Memory Usage tool provides a timeline graph
    illustrating memory fluctuations as our application runs. This graphical representation
    aids in pinpointing areas of our code that may be collecting or generating data
    inefficiently, potentially leading to memory leaks or excessive memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – Memory usage graph](img/B22218_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – Memory usage graph
  prefs: []
  type: TYPE_NORMAL
- en: We can take detailed snapshots of our application’s memory state at different
    intervals using this tool. These snapshots can then be compared to pinpoint the
    root causes of memory issues. They showcase critical metrics such as the total
    number of objects and bytes in memory, along with the differences between consecutive
    snapshots.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – The Memory Usage report types](img/B22218_04_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 – The Memory Usage report types
  prefs: []
  type: TYPE_NORMAL
- en: We can delve deeper into these snapshots through detailed Memory Usage report
    views, gaining insights into the types and instances present in each snapshot
    or the variances between two snapshots.
  prefs: []
  type: TYPE_NORMAL
- en: Once data collection is stopped, the Memory Usage tool presents an overview
    page containing memory usage data. This overview helps us grasp the memory impact
    of our application and spot areas that could benefit from optimization.
  prefs: []
  type: TYPE_NORMAL
- en: For more advanced analysis, the Memory Usage tool provides insights into various
    memory issues such as duplicate strings, sparse arrays, and event handler leaks,
    particularly beneficial for managed memory analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.14 – The Memory Usage report insights](img/B22218_04_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 – The Memory Usage report insights
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging these insights allows us to identify and resolve common memory problems
    more efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Some scenarios could need to focus and dig deeper into some specific part of
    our code base, and for that, we can use the diagnostic tools available in debugging
    mode.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Memory Usage while debugging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this part, I will create a small console app that you can retrieve on GitHub.
    The following code includes a **while** loop that fills a List of string with
    a large random string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we begin by setting breakpoints in our application
    where we suspect the memory leak might be occurring. This could be at the start
    of a function or a region of code that we suspect is causing the memory leak.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, since the code is straightforward, we will set breakpoints
    at the closing bracket of the loop.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, in more complex scenarios, we can utilize other profiler tools
    to identify suspect locations in our code base.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will use the **diagnostic tools** , and by default, it opens at the
    launch of the debugger. If not, you can reach it by navigating to the top bar
    menu and clicking **Debug** | **Windows** | **Show** **Diagnostic Tools** .
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15 – Diagnostic Tools](img/B22218_04_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 – Diagnostic Tools
  prefs: []
  type: TYPE_NORMAL
- en: In the **Diagnostic Tools** window, we retrieve the **Events** , **Process Memory**
    , and **CPU** usage graph. For our example, we will observe the **Process Memory**
    section, where memory usage increases as the loop iterates. Additionally, we can
    monitor the work of the garbage collector to assess its impact on memory allocation.
  prefs: []
  type: TYPE_NORMAL
- en: To analyze the heap stack, we can take a snapshot with the **Take Snapshot**
    option. This action will generate a report of the memory state at different intervals,
    as we observed in the profiler tools description previously.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.16 – A Memory Usage snapshot](img/B22218_04_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.16 – A Memory Usage snapshot
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 4* *.16* , we can observe that the **Heap Size** and number of **Objects**
    dangerously increase between our snapshots. This indicates poor deallocation,
    or in other words, a memory leak.
  prefs: []
  type: TYPE_NORMAL
- en: We can dig deeper by clicking on the **View Heap** option to explore **Object
    Type** by size and number.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.17 – Memory Usage](img/B22218_04_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.17 – Memory Usage
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the cause of our memory leak is **List<String>** , which
    will never deallocate.
  prefs: []
  type: TYPE_NORMAL
- en: While CPU and memory can provide insight into the performance of our application,
    most of the latency observed in software comes from the database interactions
    and in the next section, we will see how to optimize them.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing database interactions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Visual Studio 2022 introduces a new analyzer tool named **Database Profiler**
    . It allows us to explore the database interactions in our application.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will explore how to use the Database Profiler tools and
    how they can help us identify query optimization opportunities in our code base.
  prefs: []
  type: TYPE_NORMAL
- en: To open it, we go through **Performance Profiler** and select **Database** ,
    where we can combine it with **CPU Usage** for more insight.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.18 – Selecting the Database and CPU Usage tools](img/B22218_04_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.18 – Selecting the Database and CPU Usage tools
  prefs: []
  type: TYPE_NORMAL
- en: When we click on the **Start** button, the profiler will launch our application
    and start collecting data. During this time, we can perform long-running actions
    on our application to identify the root cause of latency.
  prefs: []
  type: TYPE_NORMAL
- en: After clicking on the **Stop Collection** button, we launch the generation of
    the report.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.19 – Database report](img/B22218_04_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.19 – Database report
  prefs: []
  type: TYPE_NORMAL
- en: 'The database report will show us a table with information about queries executed;
    by default, it shows columns with the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: The start time of the query
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The SQL code of the query
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The duration of execution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of records affected
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of records reads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In larger applications, we can incorporate multiple databases, such as when
    utilizing the **Command-Query Responsibility Segregation** ( **CQRS** ) pattern
    alongside database replication, for example. To help us in our investigation,
    we can display three more columns by right-clicking on the display one and checking
    which one we need.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.20 – Database report managing column](img/B22218_04_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.20 – Database report managing column
  prefs: []
  type: TYPE_NORMAL
- en: 'The additional columns are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Database**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Connection String**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Query Source** displaying the data provider used (EFCore, Dapper, ADO.NET,
    or Others)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coupling with the CPU Usage tools, we can easily zoom on the consuming period
    to examine query launch in this time.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.21 – Database report time filtered](img/B22218_04_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.21 – Database report time filtered
  prefs: []
  type: TYPE_NORMAL
- en: When we identify a query that might catch our attention due to its long transaction
    duration or large number of associated queries, we could investigate.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in the list of queries, we can easily jump into the code source for
    further investigations or even refactoring, by right-clicking on the row we are
    interested in and selecting **Go To** **Source File**
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.22 – Go To Source File](img/B22218_04_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.22 – Go To Source File
  prefs: []
  type: TYPE_NORMAL
- en: 'The power of Visual Studio profiling tools lies in the ability to combine some
    of its tools for a comprehensive investigation. My advice is to utilize the three
    tools highlighted in this chapter: the Memory Usage, CPU Usage, and Database tools.
    This will help in quickly identifying issues such as queries that generate excessive
    memory allocation and CPU utilization, especially when working with **Object Relational
    Mapping** ( **ORMs** ), such as EF Core or Dapper, that need to instantiate objects
    to run queries.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter provided key insights to leverage Visual Studio profiling tools
    to aid in our investigation and optimization of performance bottlenecks, resulting
    in improved application performance.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, we’ve covered a range of topics, from understanding
    the fundamentals of performance optimization to utilizing Visual Studio’s profiling
    tools effectively. We’ve learned how to analyze CPU usage and identify memory
    and database bottlenecks to identify and optimize critical sections and our code
    base for improved performance.
  prefs: []
  type: TYPE_NORMAL
- en: As we conclude this chapter, we mark the end of the first part of our journey
    in mastering core development skills. From unit testing and TDD to advanced debugging
    strategies, code analysis, and now performance optimization and profiling, you
    have laid a solid foundation for your development journey.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming chapters, we’ll continue to expand our horizons, delving into
    advanced topics such as multi-platform app UI development, advanced web development
    tools, machine learning integration, and advanced cloud integration and services.
  prefs: []
  type: TYPE_NORMAL
- en: To start the second part, we will dive into the world of cross-platform development
    by exploring tools offered by Visual Studio for MAUI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 2: Advancing Development Horizons'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this second part, we focus on expanding your development expertise with advanced
    techniques for building versatile applications leveraging Visual Studio. From
    multi-platform app UI development and advanced web tools to machine learning integration
    and cloud services, these chapters equip you with the skills to build modern,
    scalable, and intelligent applications, pushing the boundaries of your development
    capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B22218_05.xhtml#_idTextAnchor092) , *Multi-Platform App UI Development*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B22218_06.xhtml#_idTextAnchor112) , *Advanced Web Development
    Tools*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B22218_07.xhtml#_idTextAnchor124) , *Machine Learning Integration*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B22218_08.xhtml#_idTextAnchor132) , *Advanced Cloud Integration
    and Services*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
