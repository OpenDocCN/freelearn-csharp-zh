<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<meta charset="utf-8"/>
<meta content="packt" name="generator"/>
<title>19 Introduction to Microservices Architecture</title>


<!-- kobo-style -->
<style id="koboSpanStyle" type="text/css" xmlns="http://www.w3.org/1999/xhtml">.koboSpan { -webkit-text-combine: inherit; }</style>
</head>
<body>

<h1 data-number="20">19 Introduction to Microservices Architecture</h1>

<h2 data-number="20.1">Before you begin: Join our book community on Discord</h2>
<p>Give your feedback straight to the author himself and chat to other early readers on our Discord server (find the "architecting-aspnet-core-apps-3e" channel under EARLY ACCESS SUBSCRIPTION).</p>
<p><a href="https://packt.link/EarlyAccess">https://packt.link/EarlyAccess</a></p>
<p><img alt="Qr code Description automatically generated" src="img/file127.png" style="width:10em"/></p>
<p>The chapter covers some essential microservices architecture concepts. It is designed to get you started with those principles and an overview of the concepts surrounding microservices, which should help you make informed decisions about whether to go for a microservices architecture or not.Since microservices architecture is larger in scale than the previous application-scale patterns we visited and often involves complex components or setup, there is very limited C# code in the chapter. Instead, I explain the concepts and list open-source or commercial offerings that you can leverage to apply these patterns to your applications. Moreover, you should not aim to implement many of the pieces discussed in the chapter because it can be a lot of work to get them right, and they don’t add business value, so you are better off just using an existing implementation instead. There is more context about this throughout the chapter.Monolithic architecture patterns, such as Vertical Slice and Clean Architecture, are still good to know, as you can apply those to individual microservices. Don’t worry—all of the knowledge you have acquired since the beginning of this book is not forfeit and is still worthwhile.In this chapter, we cover the following topics:</p>
<ul>
<li>What are microservices?</li>
<li>An introduction to event-driven architecture</li>
<li>Getting started with message queues</li>
<li>Implementing the Publish-Subscribe pattern</li>
<li>Introducing Gateway patterns</li>
<li>Project – REPR.BFF—that transforms the REPR project into microservices</li>
<li>Revisiting the CQRS pattern</li>
<li>The Microservices Adapter pattern</li>
</ul>
<p>Let’s get started!</p>


<h2 data-number="20.2">What are microservices?</h2>
<p>Microservices represent an application that is divided into multiple smaller applications. Each application, or microservice, interacts with the others to create a scalable system. Usually, microservices are deployed to the cloud as containerized or serverless applications.Before getting into too many details, here are the principles to keep in mind when building microservices:</p>
<ul>
<li>Each microservice should be a cohesive unit of business.</li>
<li>Each microservice should own its data.</li>
<li>Each microservice should be independent of the others.</li>
</ul>
<p>Furthermore, everything we have studied so far—the other principles of designing software—applies to microservices but on another scale. For example, you don’t want tight coupling between microservices (solved by microservices independence), but the coupling is inevitable (as with any code). There are numerous ways to solve this problem, such as the Publish-Subscribe pattern.There are no hard rules about how to design microservices, how to divide them, how big they should be, and what to put where. Nevertheless, I’ll lay down a few foundations to help you get started and orient your journey into microservices.</p>

<h3 data-number="20.2.1">Cohesive unit of business</h3>
<p>A microservice should have a single business responsibility. Always design the system with the domain in mind, which should help you divide the application into multiple pieces. If you know <strong>Domain-Driven Design</strong> (<strong>DDD</strong>), a microservice will most likely represent a <strong>Bounded Context</strong>, which in turn is what I call a <em>cohesive unit of business</em>. Basically, a cohesive unit of business (or bounded context) is a self-contained part of the domain with limited interactions with other parts.Even if a <strong>microservice</strong> has <em>micro</em> in its name, it is more important to group logical operations under it than to aim at a micro-size. Don’t get me wrong here; if your unit is tiny, that’s even better. However, suppose you split a unit of business into multiple smaller parts instead of keeping it together (breaking cohesion); you are likely to introduce useless chattiness within your system (coupling between microservices). This could lead to performance degradation and to a system that is harder to debug, test, maintain, monitor, and deploy. Moreover, it is easier to split a big microservice into smaller pieces than to assemble multiple microservices back together. Try to apply the SRP to your microservices: a microservice should have only one reason to change unless you have a good reason to do otherwise.</p>


<h3 data-number="20.2.2">Ownership of data</h3>
<p>Each microservice should be the source of truth of its cohesive unit of business. A microservice should share its data through an API (a web API/HTTP, for example) or another mechanism (integration events, for example). It should own that data and not share it with other microservices directly at the database level.For instance, two different microservices should never access the same relational database table. If a second microservice needs some of the same data, it can create its own cache, duplicate the data, or query the owner of that data but not access the database directly; <strong>never</strong>.This data-ownership concept is probably the most critical part of the microservices architecture and leads to microservices independence. Failing at this will most likely lead to a tremendous number of problems. For example, if multiple microservices can read or write data in the same database table, each time something changes in that table, all of them must be updated to reflect the changes. If different teams manage the microservices, that means cross-team coordination. If that happens, each microservice is not independent anymore, which opens the floor to our next topic.</p>


<h3 data-number="20.2.3">Microservice independence</h3>
<p>At this point, we have microservices that are cohesive units of business and own their data. That defines <strong>independence</strong>.This independence allows the systems to scale while having minimal to no impact on the other microservices. Each microservice can also scale independently without needing the whole system to be scaled. Additionally, when the business requirements grow, each part of that domain can evolve independently.Furthermore, you could update one microservice without impacting the others or even have a microservice go offline without the whole system stopping.Of course, microservices have to interact with one another, but the way they do should define how well your system runs. A little like Vertical Slice architecture, you are not limited to using one set of architectural patterns; you can independently make specific decisions for each microservice. For example, you could choose a different way for how two microservices communicate with each other versus two others. You could even use different programming languages for each microservice.</p>
<blockquote>
<p>I recommend sticking to one or a few programming languages for smaller businesses and organizations, as you most likely have fewer developers, and each has more to do. Based on my experience, you want to ensure business continuity when people leave and make sure you can replace them and not sink the ship due to some obscure technologies used here and there (or too many technologies).</p>
</blockquote>
<p>Now that we’ve covered the basics, let’s jump into the different ways microservices can communicate using event-driven architecture.</p>



<h2 data-number="20.3">An introduction to event-driven architecture</h2>
<p><strong>Event-driven architecture</strong> (<strong>EDA</strong>) is a paradigm that revolves around consuming streams of events, or data in motion, instead of consuming static states.What I define by a static state is the data stored in a relational database table or other types of data stores, like a NoSQL documents store. That data is dormant in a central location and waiting for actors to consume and mutate it. It is stale between every mutation; the data (a record, for example) represents a finite state.On the other hand, data in motion is the opposite: you consume the ordered events and determine the change in state that each event brings or what process the program should trigger in reaction to an event.What is an event? People often interchange the words event, message, and command. Let’s try to clarify this:</p>
<ul>
<li>A message is a piece of data that represents something.</li>
<li>A message can be an object, a JSON string, bytes, or anything else your system can interpret.</li>
<li>An event is a message that represents something that happened in the past.</li>
<li>A command is a message sent to tell one or more recipients to do something.</li>
<li>A command is sent (past tense), so we can also consider it an event.</li>
</ul>
<p>A message usually has a payload (or body), headers (metadata), and a way to identify it (this can be through the body or headers).We can use events to divide a complex system into smaller pieces or have multiple systems talk to each other without creating tight coupling. Those systems could be subsystems or external applications, such as microservices.Like a REST API's <strong>Data Transfer Objects (DTOs)</strong>, events become the data contracts that tie the multiple systems together (coupling). It is essential to think about that carefully when designing events. Of course, we cannot foresee the future, so we can only do so much to get it perfect the first time. We can version the events to improve maintainability.EDA is a fantastic way of breaking tight coupling between microservices but requires rewiring your brain to learn this newer paradigm. Tooling is becoming more mature, and expertise is less scarce than more linear ways of thinking (like using point-to-point communication and relational databases). However, this is slowly changing and well worth learning.Before moving further, we can categorize events into the following overlapping buckets:</p>
<ul>
<li>Domain events</li>
<li>Integration events</li>
<li>Application events</li>
<li>Enterprise events</li>
</ul>
<p>As we explore next, all types of events play a similar role with different intents and scopes.</p>

<h3 data-number="20.3.1">Domain events</h3>
<p>A domain event is a term based on DDD representing an event in the domain. This event could then trigger other pieces of logic to be executed subsequently. It allows us to divide a complex process into multiple smaller processes. Domain events work well with domain-centric designs, like Clean Architecture, as we can use them to split complex domain objects into multiple smaller pieces. Domain events are usually application events. For example, we can use MediatR to publish domain events inside an application.To summarize, <strong>domain events integrate pieces of domain logic together while keeping the domain logic segregated</strong>, leading to loosely coupled components that hold one domain responsibility each (single responsibility principle).</p>


<h3 data-number="20.3.2">Integration events</h3>
<p>Integration events are like domain events but propagate messages to external systems, integrating multiple systems together while keeping them independent. For example, a microservice could send the <code>new user registered</code> event message that other microservices react to, like saving the <code>user id</code> to enable additional capabilities or sending a greeting email to that new user.We use a message broker or message queue to publish such events. We explore those after covering application and enterprise events.To summarize, <strong>integration events integrate multiple systems together while keeping them independent</strong>.</p>


<h3 data-number="20.3.3">Application events</h3>
<p>An application event is an event that is internal to an application; it is just a matter of scope. If the event is internal to a single process, that event is also a domain event (most likely). If the event crosses microservices boundaries that your team owns (the same application), it is also an integration event. The event itself won’t be different; it is the reason why it exists and its scope that describes it as an application event or not.To summarize, <strong>application events are related to a single application</strong>.</p>


<h3 data-number="20.3.4">Enterprise events</h3>
<p>An enterprise event describes an event that crosses internal enterprise boundaries. These are tightly coupled with your organizational structure. For example, a microservice sends an event that other teams, part of other divisions or departments, consume.The governance model around those events should differ from application events only your team consumes and be more strict with strong oversight.Someone must consider who can consume that data, under what circumstances, the impact of changing the event schema (data contract), schema ownership, naming conventions, data-structure conventions, and more, or risk building an unstable data highway.</p>
<blockquote>
<p>I like to see EDA as a central <strong>data highway</strong> in the middle of applications, systems, integrations, and organizational boundaries, where the events (data) flow between systems in a loosely coupled manner.</p>
<blockquote>
<p>It’s like a highway where cars flow between cities (without traffic jams). The cities are not controlling what car goes where but are open to visitors.</p>
</blockquote>
</blockquote>
<p>To summarize, <strong>enterprise events are integration events that cross organizational boundaries</strong>.</p>


<h3 data-number="20.3.5">Conclusion</h3>
<p>In this overview of event-driven architecture, we defined events, messages, and commands. An event is a snapshot of the past, a message is data, and a command is an event that suggests other systems take action. Since all messages are from the past, calling them events is accurate. We then organized events into a few overlapping buckets to help identify the intents. We can send events for different objectives, but whether it is about designing independent components or reaching out to different parts of the business, an event remains a payload that respects a certain format (schema). That schema is the data contract (coupling) between the consumers of those events. That data contract is probably the most important piece of it all: break the contract, break the system.Now, let’s see how event-driven architecture can help us follow the <strong>SOLID</strong> principles at cloud-scale:</p>
<ul>
<li><strong>S</strong>: Systems are independent of each other by raising and responding to events. The events themselves are the glue that ties those systems together. Each piece has a single responsibility.</li>
<li><strong>O</strong>: We can modify the system’s behaviors by adding new consumers to a particular event without impacting the other applications. We can also raise new events to build a new process without affecting existing applications.</li>
<li><strong>L</strong>: N/A</li>
<li><strong>I</strong>: Instead of building a single system, EDA allows us to create multiple smaller systems that integrate through data contracts (events), and those contracts are the messaging interfaces of the system.</li>
<li><strong>D</strong>: EDA enables systems to break tight coupling by depending on the events (interfaces/abstractions) instead of communicating directly with one another, inverting the dependency flow.</li>
</ul>
<p>EDA does not only come with advantages; it also has a few drawbacks that we explore in subsequent sections of the chapter.Next, we explore message queues followed by the Publish-Subscribe pattern, two ways of interacting with events.</p>



<h2 data-number="20.4">Getting started with message queues</h2>
<p>A <strong>message queue</strong> is nothing more than a queue we leverage to send ordered messages. A queue works on a <strong>First In, First Out</strong> (<strong>FIFO</strong>) basis. If our application runs in a single process, we could use one or more <code>Queue&lt;T&gt;</code> instances to send messages between our components or a <code>ConcurrentQueue&lt;T&gt;</code> instance to send messages between threads. Moreover, queues can be managed by an independent program to send messages in a distributed fashion (between applications or microservices).A distributed message queue can add more or less features to the mix, especially for cloud programs that handle failures at more levels than a single server. One of those features is the <strong>dead letter queue</strong>, which stores messages that failed some criteria in another queue. For example, if the target queue is full, a message could be sent to the <strong>dead letter queue</strong> instead. One could requeue such messages by putting the message back at the end of the queue.</p>
<blockquote>
<p>Beware that requeing messages change the order of the messages. If the order is important in your app, consider this.</p>
</blockquote>
<p>Many messaging queue protocols exist; some are proprietary, while others are open source. Some messaging queues are cloud-based and used <em>as a service</em>, such as Azure Service Bus and Amazon Simple Queue Service. Others are open source and can be deployed to the cloud or on-premises, such as Apache ActiveMQ.If you need to process messages in order and want each message to be delivered to a single recipient at a time, a <strong>message queue</strong> seems like the right choice. Otherwise, the <strong>Publish-Subscribe</strong> pattern could be a better fit for you.Here is a basic example that illustrates what we just discussed:</p>
<figure>
<img alt="Figure 19.1: A publisher that enqueues a message with a subscriber that dequeues it" src="img/file128.png"/><figcaption aria-hidden="true">Figure 19.1: A publisher that enqueues a message with a subscriber that dequeues it</figcaption>
</figure>
<p>For a more concrete example, in a distributed user registration process, when a user registers, we could want to do the following:</p>
<ul>
<li>Send a confirmation email.</li>
<li>Process their picture and save one or more thumbnails.</li>
<li>Send an onboarding message to their in-app mailbox.</li>
</ul>
<p>To sequentially achieve this, one operation after the other, we could do the following:</p>
<figure>
<img alt="Figure 19.2: A process flow that sequentially executes three operations that happen after a user creates an account" src="img/file129.png"/><figcaption aria-hidden="true">Figure 19.2: A process flow that sequentially executes three operations that happen after a user creates an account</figcaption>
</figure>
<p>In this case, the user would not receive the <em>Onboarding Message</em> if the process crashes during the <em>Process Thumbnail</em> operation. Another drawback would be that to insert a new operation between the <em>Process Thumbnail</em> and <em>Send an onboarding message</em> steps, we’d have to modify the <em>Send an onboarding message</em> operation (tight coupling).If the order does not matter, we could queue all the messages from the <em>Auth Server</em> instead, right after the user’s creation, like this:</p>
<figure>
<img alt="Figure 19.3: The Auth Server is queuing the operations sequentially while different processes execute them in parallel" src="img/file130.png"/><figcaption aria-hidden="true">Figure 19.3: The Auth Server is queuing the operations sequentially while different processes execute them in parallel</figcaption>
</figure>
<p>This process is better, but the <em>Auth Server</em> now controls what should happen once a new user has been created. The <em>Auth Server</em> was queuing an event in the previous workflow that told the system that a new user registered. However, now, it has to be aware of the post-processing workflow to queue each operation sequentially to enqueue the correct commands. Doing this is not wrong in itself and is easier to follow when you dig into the code, but it creates tighter coupling between the services where the <em>Auth Server</em> is aware of the external processes. Moreover, it packs too many responsibilities into the <em>Auth Server</em>.</p>
<blockquote>
<p>SRP-wise, why would an authentication/authorization server be responsible for anything other than authentication, authorization, and managing that data?</p>
</blockquote>
<p>If we continue from there and want to add a new operation between two existing steps, we would only have to modify the <em>Auth Server</em>, which is less error-prone than the preceding workflow.If we want the best of both worlds, we could use the <strong>Publish-Subscribe</strong> pattern instead, which we cover next, and continue building on top of this example.</p>

<h3 data-number="20.4.1">Conclusion</h3>
<p>If you need messages to be delivered sequentially, a queue might be the right tool. The example we explored was destined to fail from the beginning, but it allowed us to explore the thinking process behind designing the system. Sometimes, the first idea is not the best and can be improved by exploring new ways of doing things or learning new skills. Being open-minded to the ideas of others can also lead to better solutions.</p>
<blockquote>
<p>Sometimes, just speaking out loud makes our own brain solve the issue by itself. So explain the problem to someone and see what happens.</p>
</blockquote>
<p>Message queues are amazing at buffering messages for high-demand scenarios where an application may not be able to handle spikes of traffic. In that case, the messages are enqueued so the application can catch up at its own speed, reading them sequentially.Implementing distributed message queues requires a lot of knowledge and effort and is not worth it for almost all scenarios. The big cloud providers like AWS and Azure offer fully managed message queue systems as a service. You can also look at <strong>ActiveMQ</strong>, <strong>RabbitMQ</strong>, or any <strong>Advanced Message Queuing Protocol</strong> (<strong>AMQP</strong>) broker.One essential aspect of choosing the right queue system is whether you are ready and have the skills to manage your own distributed message queue. Suppose you want to speed up development, cut infrastructure management costs, and have enough money. In that case, you could use a fully managed offering for at least your production environment, especially if you expect a large volume of messages. On the other hand, using a local or on-premise instance for development or smaller-scale usage may save you a considerable sum of money. Choosing an open source system with fully managed cloud offerings is a good way to achieve both: low local development cost with an always available high-performance cloud production offering that the service provider maintains for you.Another aspect is to base your choice on needs. Have clear requirements and ensure the system you choose does what you need. Some offerings cover multiple use cases like queues and pub-sub, leading to a simplified tech stack requiring fewer skills.Before moving to the pub-sub pattern, let’s see how message queues can help us follow the <strong>SOLID</strong> principles at the app scale:</p>
<ul>
<li><strong>S</strong>: Helps centralize and divide responsibilities between applications or components without them directly knowing each other, breaking tight coupling.</li>
<li><strong>O</strong>: Allows us to change the message producer’s or subscriber’s behaviors without the other knowing about it.</li>
<li><strong>L</strong>: N/A</li>
<li><strong>I</strong>: Each message and handler can be as small as needed, while each microservice indirectly interacts with the others to solve the bigger problem.</li>
<li><strong>D</strong>: By not knowing the other dependencies (breaking tight coupling between microservices), each microservice depends only on the messages (abstractions) instead of concretions (the other microservices API).</li>
</ul>
<p>One drawback is the delay between enqueuing a message and processing a message. We talk about delay and latency in more detail in subsequent sections.</p>



<h2 data-number="20.5">Implementing the Publish-Subscribe pattern</h2>
<p>The <strong>Publish-Subscribe</strong> pattern (Pub-Sub) is similar to what we did using <strong>MediatR</strong> and what we explored in the <em>Getting started with message queues</em> section. However, instead of sending one message to one handler (or enqueuing a message), we publish (send) a message (event) to zero or more subscribers (handlers). Moreover, the publisher is unaware of the subscribers; it only sends messages out, hoping for the best (also known as <strong>fire and forget</strong>).</p>
<blockquote>
<p>Using a message queue does not mean you are limited to only one recipient.</p>
</blockquote>
<p>We can use <strong>Publish-Subscribe</strong> in-process or in a distributed system through a <strong>message broker</strong>. The message broker is responsible for delivering the messages to the subscribers. Using a message broker is the way to go for microservices and other distributed systems since they are not running in a single process.This pattern has many advantages over other ways of communication. For example, we could recreate the state of a database by replaying the events that happened in the system, leading to the <strong>event sourcing</strong> pattern. More on that later.The design depends on the technology used to deliver the messages and the system's configuration. For example, you could use <strong>MQTT</strong> to deliver messages to <strong>Internet of Things</strong> (<strong>IoT</strong>) devices and configure them to retain the last message sent on each topic. That way, when a device connects to a topic, it receives the latest message. You could also configure a <strong>Kafka</strong> broker that keeps a long history of messages and asks for all of them when a new system connects to it. All of that depends on your needs and requirements.</p>
<blockquote>
<p><strong>MQTT and Apache Kafka</strong></p>
<blockquote>
<p>If you were wondering what MQTT is, here is a quote from their website <a href="https://adpg.link/mqtt">https://adpg.link/mqtt</a>:</p>
</blockquote>
<blockquote>
<p><em>“MQTT is an OASIS standard messaging protocol for the Internet of Things (IoT). It is designed as an extremely lightweight publish/subscribe messaging transport […]”</em></p>
</blockquote>
<blockquote>
<p>Here is a quote from Apache Kafka’s website <a href="https://adpg.link/kafka">https://adpg.link/kafka</a>:</p>
</blockquote>
<blockquote>
<p><em>“Apache Kafka is an open-source distributed event streaming platform […]”</em></p>
</blockquote>
</blockquote>
<p>We cannot cover every single scenario of every system that follows every protocol. Therefore, I’ll highlight some shared concepts behind the Pub-Sub design pattern so you know how to get started. Then, you can dig into the specific technology you want (or need) to use.A topic is a way to organize events, a channel, a place to read or write specific events so consumers know where to find them. As you can probably imagine, sending all events to the same place is like creating a relational database with a single table: it would be suboptimal and hard to manage, use, and evolve.To receive messages, subscribers must subscribe to topics (or the equivalent of a topic):</p>
<figure>
<img alt="Figure 19.4: A subscriber subscribes to a pub-sub topic" src="img/file131.png"/><figcaption aria-hidden="true">Figure 19.4: A subscriber subscribes to a pub-sub topic</figcaption>
</figure>
<p>The second part of the Pub-Sub pattern is to publish messages, like this:</p>
<figure>
<img alt="Figure 19.5: A publisher is sending a message to the message broker. The broker then forwards that message to N subscribers, where N can be zero or more" src="img/file132.png"/><figcaption aria-hidden="true">Figure 19.5: A publisher is sending a message to the message broker. The broker then forwards that message to <em>N</em> subscribers, where <em>N</em> can be zero or more</figcaption>
</figure>
<p>Many abstracted details here depend on the broker and the protocol. However, the following are the two primary concepts behind the Publish-Subscribe pattern:</p>
<ul>
<li>Publishers publish messages to topics.</li>
<li>Subscribers subscribe to topics to receive messages when they are published.</li>
</ul>
<blockquote>
<p>Security is a crucial implementation detail not illustrated here. Security is mandatory in most systems; not every subsystem or device should have access to all topics.</p>
</blockquote>
<p>Publishers and subscribers could be any part of any system. For example, many Microsoft Azure services are publishers (for example, Blob storage). You can then have other Azure services (for example, Azure Functions) subscribe to those events and react to them.You can also use the <strong>Publish-Subscribe</strong> pattern inside your applications—there’s no need to use cloud resources for that; this can even be done inside the same process (we explore this in the next chapter).The most significant advantage of the Publish-Subscribe pattern is breaking tight coupling between systems. One system publishes events while others consume them without the systems knowing each other.That loose coupling leads to scalability, where each system can scale independently, and messages can be processed in parallel using the required resources. It is also easier to add new processes to a workflow since the systems are unaware of the others. To add a new process that reacts to an event, you only have to create a new microservice, deploy it, start to listen to one or more events, and process them.On the downside, the message broker can become the application’s single point of failure and must be configured appropriately. It is also essential to consider the best delivery policies for each message type. An example of a policy could be to ensure the delivery of crucial messages while delaying less time-sensitive messages and dropping unimportant messages during load surges.If we revisit our previous example using Publish-Subscribe, we end up with the following simplified workflow:</p>
<figure>
<img alt="Figure 19.6: The Auth Server is publishing an event representing the creation of a new user. The broker then forwards that message to the three subscribers, who then execute their tasks in parallel" src="img/file133.png"/><figcaption aria-hidden="true">Figure 19.6: The Auth Server is publishing an event representing the creation of a new user. The broker then forwards that message to the three subscribers, who then execute their tasks in parallel</figcaption>
</figure>
<p>Based on this workflow, we broke the tight coupling between the <em>Auth Server</em> and the post-registration process. The <em>Auth Server</em> is unaware of the workflow, and the individual services are unaware of each other. Moreover, if we want to add a new task, we only have to create or update a microservice that subscribes to the right topic (in this case, the “new user registered” topic).The current system does not support synchronization and does not handle process failures or retries, but it is a good start since we combine the pros of the message queue examples and leave the cons behind.Using an event broker inverts the dependency flow. The diagrams we explored show the message flow, but here’s what happens on the dependency sides of things:</p>
<figure>
<img alt="Figure 19.7: A diagram representing the inverted dependency flow of using the pub-sub pattern" src="img/file134.png"/><figcaption aria-hidden="true">Figure 19.7: A diagram representing the inverted dependency flow of using the pub-sub pattern</figcaption>
</figure>
<p>Now that we have explored the Publish-Subscribe pattern, we look at message brokers, then dig deeper into EDA and leverage the Publish-Subscribe pattern to create a persistent database of events that can be replayed: the Event Sourcing pattern.</p>

<h3 data-number="20.5.1">Message brokers</h3>
<p>A message broker is a program that allows us to send (<strong>publish</strong>) and receive (<strong>subscribe</strong>) messages. It plays the mediator role at scale, allowing multiple applications to talk to each other without knowing each other (<strong>loose coupling</strong>). The message broker is usually the central piece of any event-based distributed system that implements the publish-subscribe pattern.An application (<strong>publisher</strong>) publishes messages to topics, while other applications (<strong>subscribers</strong>) receive messages from those topics. The notion of <strong>topics</strong> may differ from one protocol or system to another, but all systems I know have a topic-like concept to route messages to the right place. For example, you can publish to the <code>Devices</code> topic using Kafka, but to <code>devices/abc-123/do-something</code> using MQTT.How you name your topics depends significantly on the system you are using and the scale of your installation. For example, MQTT is a lightweight event broker recommending a path-like naming convention. On the other hand, Apache Kafka is a full-featured event broker and event streaming platform that is not opinionated about topic names, leaving you in charge of that. Depending on the scale of your implementation, you can use the entity name as the topic name or may need prefixes to identify who in the enterprise can interact with what part of the system. Due to the small scale of the examples in the chapter, we stick with simple topic names, making the examples easier to understand.The message broker is responsible for forwarding the messages to the registered recipients. The lifetime of those messages can vary by broker or even per individual message or topic.There are multiple message brokers out there using different protocols. Some brokers are cloud-based, such as Azure Event Grid. Other brokers are lightweight and more suited for IoT, such as Eclipse Mosquitto/MQTT. In contrast to MQTT, others are more robust and allow for high-velocity data streaming, such as Apache Kafka.What message broker to use should be based on the requirements of the software you are building. Moreover, you are not limited to one broker. Nothing stops you from picking a message broker that handles the dialogs between your microservices and using another to handle the dialogs with external IoT devices. If you are building a system in Azure, want to go serverless, or prefer paying for SaaS components that scale without investing maintenance time, you can leverage Azure services such as Event Grid, Service Bus, and Queue Storage. If you prefer open-source software, you can choose Apache Kafka and even run a fully managed cloud instance as a service using Confluent Cloud if you don’t want to manage your own cluster.</p>


<h3 data-number="20.5.2">The event sourcing pattern</h3>
<p>Now that we have explored the Publish-Subscribe pattern, learned what an event is, and talked about event brokers, it is time to explore <strong>how to replay the state of an application</strong>. To achieve this, we can follow the <strong>event sourcing pattern</strong>.The idea behind event sourcing is to <strong>store a chronological list of events</strong> instead of a single entity, where that collection of events becomes the source of truth. That way, every single operation is saved in the right order, helping with concurrency. Moreover, we could replay all of these events to generate an object’s current state in a new application, allowing us to deploy new microservices more easily.Instead of just storing the data, if the system propagates it using an event broker, other systems can cache some of it as one or more <strong>materialized views</strong>.</p>
<blockquote>
<p>A <strong>materialized view</strong> is a model created and stored for a specific purpose. The data can come from one or more sources, improving performance when querying that data. For example, the application returns the materialized view instead of querying multiple other systems to acquire the data. You can view the materialized view as a cached entity that a microservice stores in its own database.</p>
</blockquote>
<p>One of the drawbacks of event sourcing is data consistency. There is an unavoidable delay between when a service adds an event to the store and when all the other services update their materialized views. We call this phenomenon <strong>eventual consistency</strong>.</p>
<blockquote>
<p><strong>Eventual consistency</strong> means that the data will be consistent at some point in the future, but not outright. The delay can be from a few milliseconds to much longer, but the goal is to keep that delay as small as possible.</p>
</blockquote>
<p>Another drawback is the complexity of creating such a system compared to a single application that queries a single database. Like the microservices architecture, event sourcing is not just rainbows and unicorns. It comes at a price: <strong>operational complexity</strong>.</p>
<blockquote>
<p>In a microservices architecture, each piece is smaller, but gluing them together has a cost. For example, the infrastructure to support microservices is more complex than a monolith (one app and one database). The same goes for event sourcing; all applications must subscribe to one or more events, cache data (materialized view), publish events, and more. This <strong>operational complexity</strong> represents the shift of complexity from the application code to the operational infrastructure. In other words, it requires more work to deploy and maintain multiple microservices and databases and to fight the possible instability of network communication between those external systems than it does for a single application containing all of the code. Monoliths are simpler: they work or don’t; they rarely partially work.</p>
</blockquote>
<p>A crucial aspect of event sourcing is appending new events to the store and never changing existing events (append-only). In a nutshell, microservices communicating using the Pub-Sub pattern publish events, subscribe to topics, and generate materialized views to serve their clients.</p>


<h3 data-number="20.5.3">Example</h3>
<p>Let’s explore an example of what could happen if we combine what we just studied. <strong>Context</strong>: We need to build a program that manages IoT devices. We begin by creating two microservices:</p>
<ul>
<li>The <code>DeviceTwin</code> microservice handles an IoT device’s twin’s data (digital representation of the device).</li>
<li>The <code>Networking</code> microservice manages the networking-related information of IoT devices (how to reach a device).</li>
</ul>
<p>As a visual reference, the final system could look as follows (we cover the <code>DeviceLocation</code> microservice later):</p>
<figure>
<img alt="Figure 19.8: Three microservices communicating using the Publish-Subscribe pattern" src="img/file135.png"/><figcaption aria-hidden="true">Figure 19.8: Three microservices communicating using the Publish-Subscribe pattern</figcaption>
</figure>
<p>Here are the user interactions and the published events:</p>
<ol>
<li>A user creates a twin in the system named Device 1. The <code>DeviceTwin</code> microservice saves the data and publishes the <code>DeviceTwinCreated</code> event with the following payload:</li>
</ol>
<div><pre><code>{
    "id": "some id",
    "name": "Device 1",
    "other": "properties go here..."
}</code></pre>
</div>
<p>In parallel, the <code>Networking</code> microservice must know when a device is created, so it subscribed to the <code>DeviceTwinCreated</code> event. When a new device is created, the <code>Networking</code> microservice creates default networking information for that device in its database; the default is <code>unknown</code>. This way, the <code>Networking</code> microservice knows what devices exist or not:</p>
<figure>
<img alt="Figure 19.9: A workflow representing the creation of a device twin and its default networking information" src="img/file136.png"/><figcaption aria-hidden="true">Figure 19.9: A workflow representing the creation of a device twin and its default networking information</figcaption>
</figure>
<ol>
<li>A user then updates the networking information of that device and sets it to <code>MQTT</code>. The <code>Networking</code> microservice saves the data and publishes the <code>NetworkingInfoUpdated</code> event with the following payload:</li>
</ol>
<div><pre><code>{
    "deviceId": "some id",
    "type": "MQTT",
    "other": "networking properties..."
}</code></pre>
</div>
<p>This is demonstrated by the following diagram:</p>
<figure>
<img alt="Figure 19.10: A workflow representing updating the networking type of a device" src="img/file137.png"/><figcaption aria-hidden="true">Figure 19.10: A workflow representing updating the networking type of a device</figcaption>
</figure>
<ol>
<li>A user changes the device’s display name to <code>Kitchen Thermostat</code>, which is more relevant. The <code>DeviceTwin</code> microservice saves the data and publishes the <code>DeviceTwinUpdated</code> event with the following payload. The payload uses <strong>JSON patch</strong> to publish only the differences instead of the whole object (see the <em>Further reading</em> section for more information):</li>
</ol>
<div><pre><code>{
    "id": "some id",
    "patches": [
        { "op": "replace", "path": "/name", "value": "Kitchen Thermostat" },
    ]
}</code></pre>
</div>
<p>The following diagram demonstrates this:</p>
<figure>
<img alt="Figure 19.11: A workflow representing a user updating the name of the device to Kitchen Thermostat" src="img/file138.png"/><figcaption aria-hidden="true">Figure 19.11: A workflow representing a user updating the name of the device to Kitchen Thermostat</figcaption>
</figure>
<p>From there, let’s say another team designed and built a new microservice that organizes the devices at physical locations. This new <code>DeviceLocation</code> microservice allows users to visualize their devices’ location on a map, such as a map of their house.The <code>DeviceLocation</code> microservice subscribes to all three events to manage its materialized view, like this:</p>
<ul>
<li>When receiving a <code>DeviceTwinCreated</code> event, it saves its unique identifier and display name.</li>
<li>When receiving a <code>NetworkingInfoUpdated</code> event, it saves the communication type.</li>
<li>When receiving a <code>DeviceTwinUpdated</code> event, it updates the device’s display name.</li>
</ul>
<p>When the service is deployed for the first time, it replays all events from the beginning (<strong>event sourcing</strong>); here is what happens:</p>
<ol>
<li><code>DeviceLocation</code> receives the <code>DeviceTwinCreated</code> event and creates the following model for that object:</li>
</ol>
<div><pre><code>{
    "device": {
        "id": "some id",
        "name": "Device 1"
    },
    "networking": {},
    "location": {...}
}</code></pre>
</div>
<p>The following diagram demonstrates this:</p>
<figure>
<img alt="Figure 19.12: The DeviceLocation microservice replaying the DeviceTwinCreated event to create its materialized view of the device twin" src="img/file139.png"/><figcaption aria-hidden="true">Figure 19.12: The DeviceLocation microservice replaying the DeviceTwinCreated event to create its materialized view of the device twin</figcaption>
</figure>
<ol>
<li>The <code>DeviceLocation</code> microservice receives the <code>NetworkingInfoUpdated</code> event, which updates the networking type to <code>MQTT</code>, leading to the following:</li>
</ol>
<div><pre><code>{
    "device": {
        "id": "some id",
        "name": "Device 1"
    },
    "networking": {
        "type": "MQTT"
    },
    "location": {...}
}</code></pre>
</div>
<p>The following diagram demonstrates this:</p>
<figure>
<img alt="Figure 19.13: The DeviceLocation microservice replaying the NetworkingInfoUpdated event to update its materialized view of the device twin" src="img/file140.png"/><figcaption aria-hidden="true">Figure 19.13: The DeviceLocation microservice replaying the NetworkingInfoUpdated event to update its materialized view of the device twin</figcaption>
</figure>
<ol>
<li>The <code>DeviceLocation</code> microservice receives the <code>DeviceTwinUpdated</code> event, updating the device’s name. The final model looks like this:</li>
</ol>
<div><pre><code>{
    "device": {
        "id": "some id",
        "name": "Kitchen Thermostat"
    },
    "networking": {
        "type": "MQTT"
    },
    "location": {...}
}</code></pre>
</div>
<p>The following diagram demonstrates this:</p>
<figure>
<img alt="Figure 19.14: The DeviceLocation microservice replaying the DeviceTwinUpdated event to update its materialized view of the device twin" src="img/file141.png"/><figcaption aria-hidden="true">Figure 19.14: The DeviceLocation microservice replaying the DeviceTwinUpdated event to update its materialized view of the device twin</figcaption>
</figure>
<p>From there, the <code>DeviceLocation</code> microservice is initialized and ready. Users could set the kitchen thermostat’s location on the map or continue using the other microservices. When a user queries the <code>DeviceLocation</code> microservice for information about <code>Kitchen Thermostat</code>, it displays the <strong>materialized view</strong>, which contains all the required information without sending external requests.With that in mind, we could spawn new instances of the <code>DeviceLocation</code> microservice or other microservices, and they could generate their materialized views from past events—all of that with very limited to no knowledge of other microservices. In this type of architecture, a microservice can only know about events, not the other microservices. How a microservice handles events should be relevant only to that microservice, never to the others. The same applies to both publishers and subscribers.This example illustrates the event sourcing pattern, integration events, the materialized view, the use of a message broker, and the Publish-Subscribe pattern.In contrast, using direct communication (HTTP, gRPC, and so on) would look like this:</p>
<figure>
<img alt="Figure 19.15: Three microservices communicating directly with one another" src="img/file142.png"/><figcaption aria-hidden="true">Figure 19.15: Three microservices communicating directly with one another</figcaption>
</figure>
<p>If we compare both approaches by looking at the first diagram (<em>Figure 16.7</em>), we can see that the message broker plays the role of a <strong>mediator</strong> and breaks the direct coupling between the microservices. By looking at the preceding diagram (<em>Figure 16.14</em>), we can see the tight coupling between the microservices, where the <code>DeviceLocation</code> microservice would need to interact with the <code>DeviceTwin</code> and <code>Networking</code> microservices directly to build the equivalent of its materialized view. Furthermore, the <code>DeviceLocation</code> microservice translates one interaction into three since the <code>Networking</code> microservice also talks to the <code>DeviceTwin</code> microservice, leading to indirect tight coupling between microservices, which can negatively impact performance.Suppose eventual consistency is not an option, or the Publish-Subscribe pattern cannot be applied or could be too hard to apply to your scenario. In this case, microservices can directly call each other. They can achieve this using HTTP, gRPC, or any other means that best suits that particular system’s needs.I won’t be covering this topic in this book, but one thing to be careful of when calling microservices directly is the indirect call chain that could bubble up fast. You don’t want your microservices to create a super deep call chain, or your system will likely become very slow. Here is an abstract example of what could happen to illustrate what I mean:</p>
<figure>
<img alt="Figure 19.16: A user calling microservice A, which then triggers a chain reaction of subsequent calls, leading to disastrous performance" src="img/file143.png"/><figcaption aria-hidden="true">Figure 19.16: A user calling microservice A, which then triggers a chain reaction of subsequent calls, leading to disastrous performance</figcaption>
</figure>
<p>In terms of the preceding diagram, let’s think about failures (for one). If microservice C goes offline, the whole request ends with an error. No matter the measures we put in place to mitigate the risks, if microservice C cannot recover, the system will remain down; goodbye to microservices’ promise of independence. Another issue is latency: ten calls are made for a single operation; that takes time.Such chatty systems have most likely emerged from an incorrect domain modeling phase, leading to multiple microservices working together to handle trivial tasks. Now think of <em>Figure 16.15</em> but with 500 microservices instead of 6. That could be catastrophic!This type of interdependent microservices system is known as the <strong>Death Star anti-pattern</strong>. We can see the Death Star anti-pattern as a <em>distributed big ball of mud</em>. One way to avoid such pitfalls is to ensure that the bounded contexts are well segregated and that responsibilities are well distributed. A good domain model should allow you to avoid building a Death Star and create the “most correct” system possible instead. No matter the type of architecture you choose, if you are not building the right thing, you may end up with a big ball of mud or a Death Star. Of course, the Pub-Sub pattern and EDA can help us break the tight coupling between microservices to avoid such issues.</p>


<h3 data-number="20.5.4">Conclusion</h3>
<p>The Publish-Subscribe pattern uses events to break tight coupling between parts of an application. In a microservices architecture, we can use a message broker and integration events to allow microservices to talk to each other indirectly. The different pieces are now coupled with the data contract representing the event (its schema) instead of each other, leading to a potential gain in flexibility. One risk of this type of architecture is breaking events’ consumers by publishing breaking changes in the event’s format without letting them know or without having events versioning in place so they can adapt to the changes. Therefore, it is critical to think about event schema evolutions thoroughly. Most systems evolve, as will events, but since schemas are the glue between systems in a Publish-Subscribe model, it is essential to treat them as such. Some brokers, like Apache Kafka, offer a schema store and other mechanisms to help with these; some don’t.Then, we can leverage the event sourcing pattern to persist those events, allowing new microservices to populate their databases by replaying past events. The event store then becomes the source of truth of those systems. Event sourcing can also become very handy for tracing and auditing purposes since the whole history is persisted. We can also replay messages to recreate the system’s state at any given point in time, making it very powerful for debugging purposes. The storage size requirement for the event store is something to consider before going down the event sourcing path. The event store could grow quite large because we have been keeping all messages since the beginning of time and could grow fast based on the number of events sent. You could compact the history to reduce the data size but lose part of the history. Once again, you must decide based on the requirements and ask yourself the appropriate questions. For example, is it acceptable to lose part of the history? How long should we keep the data? Do we want to keep the original data in cheaper storage if we need it later? Do we even need replaying capabilities? Can we afford to keep all the data forever? What are the data retention policies or regulations the system must follow? Craft your list of questions based on the specific business problem you want to solve. This advice applies to all aspects of software engineering: clearly define the business problem first, then find how to fix it. Such patterns can be compelling but take time to learn and implement. Like message queues, cloud providers offer fully managed brokers as a service. Those can be faster to get started with than building and maintaining your own infrastructure. If building servers is your thing, you can use open-source software to “economically” build your stack or pay for managed instances of such software to save yourself the trouble. The same tips as with message queues apply here; for example, you can leverage a managed service for your production environment and a local version on the developer’s machine.Apache Kafka is one of the most popular event brokers that enables advanced functionalities like event streaming. Kafka has partially and fully managed cloud offerings like Confluent Cloud. Redis Pub/Sub is another open-source project with fully managed cloud offerings. Redis is also a key-value store trendy for distributed caching scenarios. Other offerings are (but are not limited to) Solace PubSub+, RabbitMQ, and ActiveMQ. Once again, I suggest comparing the offerings with your requirements to make the best choice for your scenarios.Now, let’s see how the Publish-Subscribe pattern can help us follow the <strong>SOLID</strong> principles at cloud-scale:</p>
<ul>
<li><strong>S</strong>: Helps centralize and divide responsibilities between applications or components without them directly knowing each other, breaking tight coupling.</li>
<li><strong>O</strong>: Allows us to change how publishers and subscribers behave without directly impacting the other microservices (breaking tight coupling between them).</li>
<li><strong>L</strong>: N/A</li>
<li><strong>I</strong>: Each event can be as small as needed, leading to multiple smaller communication interfaces (data contracts).</li>
<li><strong>D</strong>: The microservices depend on events (abstractions) instead of concretions (the other microservices), breaking tight coupling between them and inverting the dependency flow.</li>
</ul>
<p>As you may have noticed, pub-sub is very similar to message queues. The main difference is the way messages are read and dispatched:</p>
<ul>
<li>Queues: Messages are pulled one at a time, consumed by one service, and then disappear.</li>
<li>Pub-Sub: Messages are read in order and sent to all consumers instead of to only one, like with queues.</li>
</ul>
<blockquote>
<p>I intentionally kept the <strong>Observer design pattern</strong> out of this book since we rarely need it in .NET. C# offers multicast events, which are well-versed in replacing the Observer pattern (in most cases). If you don’t know the Observer pattern, don’t worry–chances are, you will never need it. Nevertheless, if you already know the Observer pattern, here are the differences between it and the Pub-Sub pattern.</p>
<blockquote>
<p>In the Observer pattern, the subject keeps a list of its observers, creating direct knowledge of their existence. Concrete observers also often know about the subject, leading to even more knowledge of other entities and more coupling.</p>
</blockquote>
<blockquote>
<p>In the Pub-Sub pattern, the publisher is not aware of the subscribers; it is only aware of the message broker. The subscribers are not aware of the publishers either, only of the message broker. The publishers and subscribers are linked only through the data contract of the messages they publish or receive.</p>
</blockquote>
<blockquote>
<p>We could view the Pub-Sub pattern as the distributed evolution of the Observer pattern or, more precisely, like adding a mediator to the Observer pattern.</p>
</blockquote>
</blockquote>
<p>Next, we explore some patterns that directly call other microservices by visiting a new kind of <strong>Façade</strong>: the <strong>Gateway</strong>.</p>



<h2 data-number="20.6">Introducing Gateway patterns</h2>
<p>When building a microservices-oriented system, the number of services grows with the number of features; the bigger the system, the more microservices you have.When you think about a user interface that has to interact with such a system, this can become tedious, complex, and inefficient (dev-wise and speed-wise). Gateways can help us achieve the following:</p>
<ul>
<li>Hide complexity by routing requests to the appropriate services.</li>
<li>Hide complexity by aggregating responses and translating one external request into many internal ones.</li>
<li>Hide complexity by exposing only the subset of features that a client needs.</li>
<li>Translate a request into another protocol.</li>
</ul>
<p>A gateway can also centralize different processes, such as logging and caching requests, authenticating and authorizing users and clients, enforcing request rate limits, and other similar policies.You can see gateways as façades, but instead of being a class in a program, it is a program of its own, shielding other programs. There are multiple variants of the Gateway pattern, and we explore many of them here.Regardless of the type of gateway you need, you can code it yourself or leverage existing tools to speed up the development process.</p>
<blockquote>
<p>Beware that there is a strong chance that your homemade gateway version 1.0 has more flaws than a proven solution. This tip is not only applicable to gateways but to most complex systems. That being said, sometimes, no proven solution does exactly what we want, and we have to code it ourselves, which is where the real fun begins!</p>
</blockquote>
<p>An open-source project that could help you out is Ocelot (<a href="https://adpg.link/UwiY">https://adpg.link/UwiY</a>). It is an API gateway written in C# that supports many things that we expect from a gateway. You can route requests using configuration or write custom code to create advanced routing rules. Since it is open source, you can contribute to it, fork it, and explore the source code if necessary.If you want a managed offering with a long list of features, you can explore Azure API Management (<a href="https://adpg.link/8CEX">https://adpg.link/8CEX</a>). It supports security, load-balancing, routing, and more. It also offers a service catalog where teams can consult and manage the APIs with internal teams, partners, and customers.We can see a gateway as a <strong>reverse proxy</strong> that offers advanced functionalities. A Gateway fetches the information clients request, which can come from one or more resources, possibly from one or more servers. A reverse proxy usually routes a request to only one server. A reverse proxy often serves as a load balancer. Microsoft released a reverse proxy named YARP, written in C# and open-source (<a href="https://adpg.link/YARP">https://adpg.link/YARP</a>). Microsoft built it for their internal teams. YARP is now part of Azure App Service (<a href="https://adpg.link/7eu4">https://adpg.link/7eu4</a>). If YARP does what you need, it seems like a stable enough product to invest in that will evolve and be maintained over time. A significant advantage of such a service is the ability to deploy it with your application, optionally as a container, allowing us to use it locally during development.Now, let’s explore a few types of gateways.</p>

<h3 data-number="20.6.1">Gateway Routing pattern</h3>
<p>We can use this pattern to hide the complexity of our system by having the gateway route requests to the appropriate services.For example, let’s say we have two microservices: one that holds our device data and another that manages device locations. We want to show the latest known location of a specific device (<code>id=102</code>) and display its name and model.To achieve that, a user requests the web page, and then the web page calls two services (see the following diagram). The <code>DeviceTwin</code> microservice is accessible from <code>service1.domain.com</code>, and the <code>Location</code> microservice is accessible from <code>service2.domain.com</code>. From there, the web application must track the two services, their domain name, and their operations. The UI has to handle more complexity as we add more microservices. Moreover, if we decide to change <code>service1</code> to <code>device-twins</code> and <code>service2</code> to <code>location</code>, we’d also need to update the web application. If there is only a UI, it is still not so bad, but if we have multiple user interfaces, each has to handle that complexity.Furthermore, if we want to hide the microservices inside a private network, it would be impossible unless all the user interfaces are also part of that private network (which exposes it). Here’s the diagram representing the interactions mentioned previously:</p>
<figure>
<img alt="Figure 19.17: A web application and a mobile app that are calling two microservices directly" src="img/file144.png"/><figcaption aria-hidden="true">Figure 19.17: A web application and a mobile app that are calling two microservices directly</figcaption>
</figure>
<p>We can implement a gateway that does the routing for us to fix some of these issues. That way, instead of knowing what services are accessible through what sub-domain, the UI only has to know the gateway:</p>
<figure>
<img alt="Figure 19.18: A web application and a mobile app that are calling two microservices through a gateway application" src="img/file145.png"/><figcaption aria-hidden="true">Figure 19.18: A web application and a mobile app that are calling two microservices through a gateway application</figcaption>
</figure>
<p>Of course, this brings some possible issues to the table as the gateway becomes a single point of failure. We could consider using a load balancer to ensure we have strong enough availability and fast enough performance. Since all requests pass through the gateway, we may also need to scale it up at some point.We should also ensure the gateway supports failure by implementing different resiliency patterns, such as <strong>Retry</strong> and <strong>Circuit Breaker</strong>. The chances that an error will occur on the other side of the gateway increase with the number of microservices you deploy and the number of requests sent to those microservices.You can also use a routing gateway to reroute the URI to create easier-to-use URI patterns. You can also reroute ports; add, update, or remove HTTP headers; and more. Let’s explore the same example but using different URIs. Let’s assume the following:</p>
<table>
<tbody>
<tr class="odd">
<td><strong>Microservice</strong></td>
<td><strong>URI</strong></td>
</tr>
<tr class="even">
<td>API 1 (get a device)</td>
<td><code>internal.domain.com:8001/{id}</code></td>
</tr>
<tr class="odd">
<td>API 2 (get a device location)</td>
<td><code>internal.domain.com:8002/{id}</code></td>
</tr>
</tbody>
</table>
Table 19.1: Internal microservice URI patterns.
<p>UI developers would have a harder time remembering what port is leading to what microservice and what is doing what (and who could blame them?). Moreover, we could not transfer the requests as we did earlier (only routing the domain). We could use the gateway as a way to create memorable URI patterns for developers to consume, like these:</p>
<table>
<tbody>
<tr class="odd">
<td><strong>Gateway URI</strong></td>
<td><strong>Microservice URI</strong></td>
</tr>
<tr class="even">
<td><code>gateway.domain.com/devices/{id}</code></td>
<td><code>internal.domain.com:8001/{id}</code></td>
</tr>
<tr class="odd">
<td><code>gateway.domain.com/devices/{id}/location</code></td>
<td><code>internal.domain.com:8002/{id}</code></td>
</tr>
</tbody>
</table>
Table 19.1: Memorable URI patterns that are easier to use and semantically meaningful.
<p>As we can see, we took the ports out of the equation to create usable, meaningful, and easy-to-remember URIs.However, we are still making two requests to the gateway to display one piece of information (the location of a device and its name/model), which leads us to our next Gateway pattern.</p>


<h3 data-number="20.6.2">Gateway Aggregation pattern</h3>
<p>Another role we can give to a gateway is aggregating requests to hide complexity from its consumers. Aggregating multiple requests into one makes it easier for consumers of a microservices system to interact with it; clients need to know about one endpoint instead of multiple. Moreover, it moves the chattiness from the client to the gateway, which is closer to the microservices, lowering the many calls’ latency, and thus making the request-response cycle faster.Continuing with our previous example, we have two UI applications that contain a feature to show a device’s location on a map before identifying it using its name/model. To achieve this, they must call the device twin endpoint to obtain the device’s name and model and the location endpoint to get its last known location. So, two requests to display a small box times two UIs means four requests to maintain a simple feature. If we extrapolate, we could end up managing a huge number of HTTP requests for a handful of features.Here is a diagram showing our feature in its current state:</p>
<figure>
<img alt="Figure 19.19: A web application and a mobile app that are calling two microservices through a gateway application" src="img/file146.png"/><figcaption aria-hidden="true">Figure 19.19: A web application and a mobile app that are calling two microservices through a gateway application</figcaption>
</figure>
<p>To remedy this problem, we can apply the Gateway Aggregation pattern to simplify our UIs and offload the responsibility of managing those details to the gateway.By applying the Gateway Aggregation pattern, we end up with the following simplified flow:</p>
<figure>
<img alt="Figure 19.20: A gateway that aggregates the response of two requests to serve a single request from both a web application and a mobile app" src="img/file147.png"/><figcaption aria-hidden="true">Figure 19.20: A gateway that aggregates the response of two requests to serve a single request from both a web application and a mobile app</figcaption>
</figure>
<p>In the previous flow, the Web App calls the Gateway that calls the two APIs, then crafts a response combining the two responses it got from the APIs. The Gateway then returns that response to the Web App. With that in place, the Web App is loosely coupled with the two APIs while the Gateway plays the intermediary. With only one HTTP request, the Web App has all the information it needs, aggregated by the Gateway.Next, let’s explore the steps that occurred. The following diagram shows that the Web App makes a single request (1) while the gateway makes two calls (2 and 4). In the diagram, the requests are sent in series, but we could have sent them in parallel to speed things up:</p>
<figure>
<img alt="Figure 19.21: The order in which the requests take place" src="img/file148.png"/><figcaption aria-hidden="true">Figure 19.21: The order in which the requests take place</figcaption>
</figure>
<p>Like the routing gateway, an aggregation gateway can become the bottleneck of your application and a single point of failure, so beware of that.Another important point is the latency between the gateway and the internal APIs. The clients will wait for every response if the latency is too high. So, deploying the gateway close to the microservices it interacts with could become crucial for system performance. The gateway can also implement caching to improve performance further and make subsequent requests faster.Next, we explore another type of gateway that creates specialized gateways instead of generic ones.</p>


<h3 data-number="20.6.3">Backend for Frontend pattern</h3>
<p>The Backend for Frontend (BFF) pattern is yet another variation of the Gateway pattern. With Backend for Frontend, instead of building a general-purpose gateway, we build a gateway per user interface (for each application that interacts with the system), lowering the complexity. Moreover, it allows for fine-grained control of what endpoints are exposed. It removes the chances of app B breaking when changes are made to app A. Many optimizations can come out of this pattern, such as sending only the data that’s required for each call instead of sending data that only a few applications are using, saving some bandwidth along the way.Let’s say that our Web App needs to display more data about a device. To achieve that, we would need to change the endpoint and send that extra information to the mobile app as well. However, the mobile app doesn’t need that information since it doesn’t have room on its screen to display it. Next is an updated diagram that replaces the single gateway with two gateways, one per frontend:</p>
<figure>
<img alt="Figure 19.22: Two Backend for Frontend gateways; one for the Web App and one for the Mobile App" src="img/file149.png"/><figcaption aria-hidden="true">Figure 19.22: Two Backend for Frontend gateways; one for the Web App and one for the Mobile App</figcaption>
</figure>
<p>Doing this allows us to develop specific features for each frontend without impacting the other. Each gateway now shields its particular frontend from the rest of the system and the other frontend. This is the most important benefit this pattern brings: client independence.Once again, the Backend for Frontend pattern is a gateway. Like other variations of the Gateway pattern, it can become the bottleneck of its frontend and its single point of failure. The good news is that the outage of one BFF gateway limits the impact to a single frontend, shielding the other frontends from that downtime.</p>


<h3 data-number="20.6.4">Mixing and matching gateways</h3>
<p>Now that we’ve explored three variations of the Gateway pattern, it is important to note that we can mix and match them, either at the codebase level or as multiple microservices.For example, a gateway can be built for a single client (backend for frontend), perform simple routing, and aggregate results.We can also mix them as different applications, for example, by putting multiple backend for frontend gateways in front of a more generic gateway to simplify the development and maintenance of those backend for frontend gateways.Beware that each hop has a cost. The more pieces you add between your clients and your microservices, the more time it will take for those clients to receive the response (latency). Of course, you can put mechanisms in place to lower that overhead, such as caching or non-HTTP protocols such as gRPC, but you still must consider it. That goes for everything, not just gateways.Here is an example illustrating this:</p>
<figure>
<img alt="Figure 19.23: A mix of the Gateway patterns" src="img/file150.png"/><figcaption aria-hidden="true">Figure 19.23: A mix of the Gateway patterns</figcaption>
</figure>
<p>As you’ve possibly guessed, the Generic Gateway is the single point of failure of all applications, while at the same time, each backend for frontend gateway is a point of failure for its specific client.</p>
<blockquote>
<p>A <strong>service mesh</strong> is an alternative to help microservices communicate with one another. It is a layer, outside of the application, that proxies communications between services. Those proxies are injected on top of each service and are referred to as <strong>sidecars</strong>. The service mesh can also help with distributed tracing, instrumentation, and system resiliency. If your system needs service-to-service communication, a service mesh would be an excellent place to look.</p>
</blockquote>


<h3 data-number="20.6.5">Conclusion</h3>
<p>A gateway is a façade that shields or simplifies access to one or more other services. In this section, we explored the following:</p>
<ul>
<li><strong>Routing</strong>: This forwards a request from point A to point B (a reverse proxy).</li>
<li><strong>Aggregation</strong>: This combines the result of multiple sub-requests into a single response.</li>
<li><strong>Backend for Frontend</strong>: This is used in a one-to-one relationship with a frontend.</li>
</ul>
<p>We can use any microservices pattern, including gateways, and like any other pattern, we can mix and match them. Just consider the advantages, but also the drawbacks, that they bring to the table. If you can live with them, you’ve got your solution.Gateways often become the single point of failure of the system, so that is a point to consider. On the other hand, a gateway can have multiple instances running simultaneously behind a load balancer. Moreover, we must also consider the delay added by calling a service that calls another service since that slows down the response time.All in all, a gateway is a great tool to simplify consuming microservices. They also allow hiding the microservices topology behind them, possibly even isolated in a private network. They can also handle cross-cutting concerns such as security.</p>
<blockquote>
<p>It is imperative to use gateways as a requests passthrough and avoid coding business logic into them; gateways are just reverse proxies. Think single responsibility principle: a gateway is a façade in front of your microservices cluster. Of course, you can unload specific tasks into your gateways like authorization, resiliency (retry policies, for example), and similar cross-cutting concerns, but the business logic must remain in the backend microservices.</p>
<blockquote>
<p>The BFF’s role is to simplify the UI, so moving logic from the UI to the BFF is encouraged.</p>
</blockquote>
</blockquote>
<p>In most cases, I recommend against rolling out your hand-crafted gateway and suggest leveraging existing offerings instead. There are many open-source and cloud gateways that you can use in your application. Using existing components leaves you more time to implement the business rules that solve the issues your program is trying to tackle.Of course, cloud-based offerings exist, like Azure Application Gateway and Amazon API Gateway. Both are extendable with cloud offerings like load balancers and <strong>web application firewalls</strong> (<strong>WAF</strong>). For example, Azure Application Gateway also supports autoscaling, zone redundancy, and can serve as <strong>Azure Kubernetes Service</strong> (<strong>AKS</strong>) Ingress Controller (in a nutshell, it controls the traffic to your microservices cluster).If you want more control over your gateways or to deploy them with your application, you can leverage one existing options, like Ocelot, YARP, or Envoy.Ocelot is an open source production-ready API Gateway programmed in .NET. Ocelot supports routing, request aggregation, load-balancing, authentication, authorization, rate limiting, and more. It also integrates well with Identity Server. In my eyes, the biggest advantage of Ocelot is that you create the .NET project yourself, install a NuGet package, configure your gateway, and then deploy it like any other ASP.NET Core application. Since Ocelot is written in .NET, extending it if needed or contributing to the project or its ecosystem is easier.To quote their GitHub <code>README.md</code> file: « <em>YARP is a reverse proxy toolkit for building fast proxy servers in .NET using the infrastructure from ASP.NET and .NET. The key differentiator for YARP is that it's been designed to be easily customized and tweaked to match the specific needs of each deployment scenario.</em> »Envoy is an « <em>open source edge and service proxy, designed for cloud-native applications</em> », to quote their website. Envoy is a <strong>Cloud Native Computing Foundation</strong> (<strong>CNCF</strong>) graduated project originally created by Lyft. Envoy was designed to run as a separate process from your application, allowing it to work with any programming language. Envoy can serve as a gateway and has an extendable design through TCP/UDP and HTTP filters, supports HTTP/2 and HTTP/3, gRPC, and more.Which offering to choose? If you are looking for a fully managed service, look at the cloud provider’s offering of your choice. Consider YARP or Ocelot if you are looking for a configurable reverse proxy or gateway that supports the patterns covered in this chapter. If you have complex use cases that Ocelot does not support, you can look into Envoy, a proven offering with many advanced capabilities. Please remember that these are just a few possibilities that can play the role of a gateway in a microservices architecture system and are not intended to be a complete list.Now, let’s see how gateways can help us follow the <strong>SOLID</strong> principles at cloud-scale:</p>
<ul>
<li><strong>S</strong>: A gateway can handle routing, aggregation, and other similar logic that would otherwise be implemented in different components or applications.</li>
<li><strong>O</strong>: I see many ways to tackle this one, but here are two takes on this:</li>
</ul>
<ol>
<li>Externally, a gateway could reroute its sub-requests to new URIs without its consumers knowing about it, as long as its contract does not change.</li>
<li>Internally, a gateway could load its rules from configurations, allowing it to change without updating its code.</li>
</ol>
<ul>
<li><strong>L</strong>: N/A</li>
<li><strong>I</strong>: Since a backend for frontend gateway serves a single frontend system, one contract (interface) per frontend system leads to multiple smaller interfaces instead of one big general-purpose gateway.</li>
<li><strong>D</strong>: We could see a gateway as an abstraction, hiding the real microservices (implementations) and inverting the dependency flow.</li>
</ul>
<p>Next, we build a BFF and evolve e-commerce application from <em>Chapter 18</em>.</p>



<h2 data-number="20.7">Project – REPR.BFF</h2>
<p>This project leverages the Backend for Frontend (BFF) design pattern to reduce the complexity of using the low-level API of the <em>REPR project</em> we created in <em>Chapter 18</em>. The BFF endpoints act as several types of gateway we explore.This design makes two layers of API, so let’s start here.</p>

<h3 data-number="20.7.1">Layering APIs</h3>
<p>From a high-level architecture perspective, we can leverage multiple layers of APIs to group different levels of operation granularity. For example, in this case, we have two layers:</p>
<ul>
<li>Low-level APIs that offer atomic foundational operations.</li>
<li>High-level APIs that offer domain-specific functionalities.</li>
</ul>
<p>Here’s a diagram that represents this concept (high-level APIs are BFFs in this case, but the design could be nuanced):</p>
<figure>
<img alt="Figure 19.24: diagram showcasing a two-layer architecture." src="img/file151.png"/><figcaption aria-hidden="true">Figure 19.24: diagram showcasing a two-layer architecture.</figcaption>
</figure>
<p>The low-level layer showcases atomic foundational operations, like adding an item to the shopping basket and removing an item from the shopping basket. Those operations are simple, so they are more complicated to use. For example, loading the products in the user’s shopping cart requires multiple API calls, one to get the items and quantity and one per item to get the product details like its name and price. The high-level layer offers domain-specific functionalities, which are easier to use but can become more complex. For example, a single endpoint could handle adding, updating, and deleting items from the shopping basket, making its usage trivial for its consumer but its logic more complex to implement. Moreover, the product team could prefer a shopping cart to a shopping basket, so the endpoint’s URL could reflect this.Let’s have a look at the advantages and disadvantages.</p>

<h4 data-number="20.7.1.1">Advantages of a two-layer design</h4>
<ul>
<li><strong>Separation of Concerns:</strong> This architecture separates the generic functionalities from the domain-specific ones, promoting cleaner code and modularization.</li>
<li><strong>Scalability:</strong> Each layer can be scaled independently based on the demand.</li>
<li><strong>Flexibility and Reusability:</strong> The low-level APIs can be reused across multiple high-level functionalities or applications, promoting code reusability.</li>
<li><strong>Optimized Data Fetching:</strong> BFFs can call multiple low-level APIs, aggregate responses, and send only the necessary data to the frontend, reducing payload sizes and making frontend development more straightforward.</li>
<li><strong>Easier Maintenance:</strong> We can address issues in a specific domain without touching the low-level generic APIs. On the other hand, we can fix an issue in a lower-level API, which will propagate to all the domains.</li>
<li><strong>Tailored User Experience:</strong> High-level APIs can be crafted specifically for individual client types (web, mobile, etc.), ensuring an optimal user experience.</li>
<li><strong>Security:</strong> Domain-specific functionalities can implement additional security measures relevant to their context without burdening the low-level APIs with unnecessary complexity.</li>
</ul>


<h4 data-number="20.7.1.2">Disadvantages of a two-layer design</h4>
<ul>
<li><strong>Increased Complexity:</strong> Maintaining two layers introduces additional deployment, monitoring, and management complexity.</li>
<li><strong>Potential Performance Overhead:</strong> An additional layer introduces latency, especially if not properly optimized.</li>
<li><strong>Duplication:</strong> There's potential for code duplication when similar logic gets implemented in multiple high-level functionalities.</li>
<li><strong>Tight Coupling Concerns:</strong> Changes in the low-level APIs can impact multiple domain-specific functionalities. A poor design could lead to a tightly coupled distributed system.</li>
<li><strong>Coordination Required:</strong> As the system evolves, ensuring that the low-level APIs meet the needs of all high-level functionalities requires more coordination among development teams.</li>
<li><strong>Overhead in Development:</strong> Developers need to consider two layers, which can slow down the development process, especially if there's a need to modify both layers to achieve a specific feature or fix.</li>
<li><strong>Potential for Stale Data:</strong> If high-level functionalities cache data from low-level APIs, there's potential for serving stale data to users.</li>
<li><strong>Increased Risk of Failures:</strong> Introducing additional APIs increases the odds of one of them experiencing issues or outages.</li>
</ul>
<p>While a two-layer design can offer flexibility and optimization, it also introduces additional complexities. The decision to use such an architecture should be based on the specific needs of the project, the anticipated scale, and the capabilities of the development and operations teams.We look at booting up these APIs next.</p>



<h3 data-number="20.7.2">Running the microservices</h3>
<p>Let’s start by exploring the deployment topology. First, we split the <em>Chapter 18</em> REPR project into two services: <em>Baskets</em> and <em>Products</em>. Then, we add a <em>BFF</em> API that fronts the two services to simplify using the system. We do not have a UI per se, but one <code>http</code> file per project exists to simulate HTTP requests. Here’s a diagram that represents the relationship between the different services:</p>
<figure>
<img alt="Figure 19.25: a diagram that represents the deployment topology and relationship between the different services" src="img/file152.png"/><figcaption aria-hidden="true">Figure 19.25: a diagram that represents the deployment topology and relationship between the different services</figcaption>
</figure>
<p>The easiest and most extendable way to start the projects is to use Docker, but it is optional; we can also start the three projects manually. Using Docker opens many possibilities, like using a real SQL Server to persist the data between runs and add more pieces to our puzzle, like a Redis cache or an event broker, to name a few.Let’s start by manually starting the apps.</p>

<h4 data-number="20.7.2.1">Manually starting the projects</h4>
<p>We have three projects and need three terminals to start them all. From the chapter directory, you can execute the following commands, one set per terminal window, and all projects should start:# In one terminal</p>
<div><pre><code>cd REPR.Baskets
dotnet run
# In a second terminal
cd REPR.Products
dotnet run
# In a third terminal
cd REPR.BFF
dotnet run</code></pre>
</div>
<p>Doing this should work. You can use the <code>PROJECT_NAME.http</code> files to test the APIs.Next, let’s explore the second option about using Docker.</p>


<h4 data-number="20.7.2.2">Using Docker Compose to run the projects</h4>
<p>At the same level as the solution file, the <code>docker-compose.yml</code>, <code>docker-compose.override.yml</code>, and various <code>Dockerfile</code> files are preconfigured to make the projects start in the correct order.</p>
<blockquote>
<p>Here’s a link to get started with Docker: <a href="https://adpg.link/1zfM">https://adpg.link/1zfM</a></p>
</blockquote>
<p>Since ASP.NET Core uses HTTPS by default, we must register a development certificate with the container, so let’s start here.</p>

<h5 data-number="20.7.2.2.1">Configuring HTTPS</h5>
<p>This section quickly explores using PowerShell to set up HTTPS on Windows. If you are using a different operating system or if the instructions are not working, please consult the official documentation: <a href="https://adpg.link/o1tu">https://adpg.link/o1tu</a>First, we must generate a development certificate. In a PowerShell terminal, run the following commands:</p>
<div><pre><code>dotnet dev-certs https -ep "$env:APPDATA\ASP.NET\Https\adpg-net8-chapter-19.pfx" -p devpassword
dotnet dev-certs https --trust</code></pre>
</div>
<p>The preceding commands create a <code>pfx</code> file with the password <code>devpassword</code> (you must provide a password, or it won’t work), then tell .NET to trust the dev certificates.From there, the <code>ASPNETCORE_Kestrel__Certificates__Default__Path</code> and <code>ASPNETCORE_Kestrel__Certificates__Default__Password</code> environment variables are configured in the <code>docker-compose.override.yml</code> file and should be taken into account.</p>
<blockquote>
<p>If you change the certificate location or the password, you must update the <code>docker-compose.override.yml</code> file.</p>
</blockquote>


<h5 data-number="20.7.2.2.2">Composing the application</h5>
<p>Now that we set up HTTPS, we can build the container using the following commands:</p>
<div><pre><code>docker compose build</code></pre>
</div>
<p>We can execute the following command to start the containers:</p>
<div><pre><code>docker compose up</code></pre>
</div>
<p>This should start the containers and feed you an aggregated log with a color per service. The beginning of the log trail should look like this:</p>
<div><pre><code>[+] Running 3/0
 ✔ Container c19-repr.products-1  Created    0.0s
 ✔ Container c19-repr.baskets-1   Created    0.0s
 ✔ Container c19-repr.bff-1       Created    0.0s
Attaching to c19-repr.baskets-1, c19-repr.bff-1, c19-repr.products-1
c19-repr.baskets-1   | info: Microsoft.Hosting.Lifetime[14]
c19-repr.baskets-1   |       Now listening on: http://[::]:80
c19-repr.baskets-1   | info: Microsoft.Hosting.Lifetime[14]
c19-repr.baskets-1   |       Now listening on: https://[::]:443
...</code></pre>
</div>
<p>To stop the services, press <code>Ctrl+C</code>. When you want to destroy the running application, enter the following command:</p>
<div><pre><code>docker compose down</code></pre>
</div>
<p>Now, with <code>docker compose up</code>, our services should be running. To make sure, let’s try them out.</p>



<h4 data-number="20.7.2.3">Briefly testing the services</h4>
<p>The project contains the following services, each containing an <code>http</code> file you can leverage to query the services using Visual Studio or in VS Code using an extension:</p>
<table>
<tbody>
<tr class="odd">
<td>Service</td>
<td>HTTP file</td>
<td>Host</td>
</tr>
<tr class="even">
<td><code>REPR.Baskets</code></td>
<td><code>REPR.Baskets.http</code></td>
<td><a href="https://localhost:60280">https://localhost:60280</a></td>
</tr>
<tr class="odd">
<td><code>REPR.BFF</code></td>
<td><code>REPR.BFF.http</code></td>
<td><a href="https://localhost:7254">https://localhost:7254</a></td>
</tr>
<tr class="even">
<td><code>REPR.Products</code></td>
<td><code>REPR.Products.http</code></td>
<td><a href="https://localhost:57362">https://localhost:57362</a></td>
</tr>
</tbody>
</table>
Table 19.3: each service, HTTP file, and HTTPS hostname and port.
<p>We can leverage the HTTP requests from each directory to test the API. I suggest starting by trying the low-level APIs, then the BFF, so you know if something is wrong with them directly instead of wondering what is wrong with the BFF (which calls the low-level APIs).</p>
<blockquote>
<p>I use the <em>REST Client</em> extension in VS Code (<a href="https://adpg.link/UCGv">https://adpg.link/UCGv</a>) and the built-in support in Visual Studio 2022 version 17.6 or later.</p>
</blockquote>
<p>Here’s a part of the <code>REPR.Baskets.http</code> file:</p>
<div><pre><code>@Web_HostAddress = https://localhost:60280
@ProductId = 3
@CustomerId = 1
GET {{Web_HostAddress}}/baskets/{{CustomerId}}
###
POST {{Web_HostAddress}}/baskets
Content-Type: application/json
{
    "customerId": {{CustomerId}}, 
    "productId": {{ProductId}}, 
    "quantity": 10
}
...</code></pre>
</div>
<p>The highlighted lines are variables that the requests reuse. The <code>###</code> characters act as a separator between requests. In VS or VS Code, you should see a <code>Send request</code> button on top of each request. Executing the <code>POST</code> request, then the <code>GET</code> should output the following:</p>
<div><pre><code>HTTP/1.1 200 OK
Content-Type: application/json; charset=utf-8
[
  {
    "productId": 3,
    "quantity": 10
  }
]</code></pre>
</div>
<p>If you can reach one endpoint, this means the service is running. Nonetheless, feel free to play with the requests, modify them, and add more.</p>
<blockquote>
<p>I did not move the tests over from <em>Chapter 18</em>. Automating the validation of our deployment could be a good exercise for you to test your testing skills.</p>
</blockquote>
<p>After you validate that the three services are running, we can continue and look at how the BFF communicates with the Baskets and Products services.</p>



<h3 data-number="20.7.3">Creating typed HTTP clients using Refit</h3>
<p>The BFF service must communicate to the Baskets and Products services. The services are REST APIs, so we must leverage HTTP. We could leverage the out-of-the-box ASP.NET Core <code>HttpClient</code> class and <code>IHttpClientFactory</code> interface, then send raw HTTP requests to the downstream APIs. On the other hand, we could also create a typed client, which translates the HTTP calls to simple method calls with evocative names. We are exploring the second option, encapsulating the HTTP calls inside the typed clients.The concept is simple: we create one interface per service and translate its operation into methods. Each interface revolves around a service. Optionally, we can aggregate the services under a master interface to inject the aggregate service and have access to all child services. Moreover, this central access point allows us to reduce the number of injected services to one and improve discoverability with IntelliSense. Here’s a diagram representing this concept:</p>
<figure>
<img alt="Figure 19.25: UML class diagram representing a generic typed client class hierarchy." src="img/file153.png"/><figcaption aria-hidden="true">Figure 19.25: UML class diagram representing a generic typed client class hierarchy.</figcaption>
</figure>
<p>In the preceding diagram, the <code>IClient</code> interface is composed and exposes the other typed clients, each of which queries a specific downstream API.In our case, we have two downstream services, so our interface hierarchy looks like the following:</p>
<figure>
<img alt="Figure 19.26: UML class diagram representing the BFF downstream typed client class hierarchy." src="img/file154.png"/><figcaption aria-hidden="true">Figure 19.26: UML class diagram representing the BFF downstream typed client class hierarchy.</figcaption>
</figure>
<p>After implementing this, we can query the downstream APIs from our code without worrying about their data contract because our client is strongly typed.We leverage <em>Refit</em>, an open-source library, to implement the interfaces automatically.</p>
<blockquote>
<p>We could use any other library or barebone ASP.NET Core <code>HttpClient</code>; it does not matter. I picked <em>Refit</em> to leverage its code generator, save myself the trouble of writing the boilerplate code, and save you the time of reading through such code. Refit on GitHub: <a href="https://adpg.link/hneJ">https://adpg.link/hneJ</a>.</p>
<blockquote>
<p>I used the out-of-the-box <code>IHttpClientFactory</code> functionalities in the past, so if you want to reduce the number of dependencies in your project, you can also use that instead. Here’s a link to help you get started: <a href="https://adpg.link/HCj7">https://adpg.link/HCj7</a>.</p>
</blockquote>
</blockquote>
<p>Refit acts like Mapperly and generates code based on attributes, so all we have to do is define our methods, and Refit writes the code.</p>
<blockquote>
<p>The <em>BFF</em> project references the <em>Products</em> and <em>Baskets</em> projects to reuse their DTOs. I could have architected this in many different ways, including hosting the typed client in a library of its own so we could share it between many projects. We could also extract the DTOs from the web applications to one or more shared projects so we don’t depend on the web applications themselves. For this demo, there is no need to overengineer the solution.</p>
</blockquote>
<p>Let’s look at the typed client interfaces, starting with the <code>IBasketsClient</code> interface:</p>
<div><pre><code>using Refit;
using Web.Features;
namespace REPR.BFF;
public interface IBasketsClient
{
    [Get("/baskets/{query.CustomerId}")]
    Task&lt;IEnumerable&lt;Baskets.FetchItems.Item&gt;&gt; FetchCustomerBasketAsync(
        Baskets.FetchItems.Query query,
        CancellationToken cancellationToken);
    [Post("/baskets")]
    Task&lt;Baskets.AddItem.Response&gt; AddProductToCart(
        Baskets.AddItem.Command command,
        CancellationToken cancellationToken);
    [Delete("/baskets/{command.CustomerId}/{command.ProductId}")]
    Task&lt;Baskets.RemoveItem.Response&gt; RemoveProductFromCart(
        Baskets.RemoveItem.Command command,
        CancellationToken cancellationToken);
    [Put("/baskets")]
    Task&lt;Baskets.UpdateQuantity.Response&gt; UpdateProductQuantity(
        Baskets.UpdateQuantity.Command command,
        CancellationToken cancellationToken);
}</code></pre>
</div>
<p>The preceding interface leverages Refit’s attributes (highlighted) to explain to its code generator what to write. The operations themselves are self-explanatory and carry the features’ DTOs over HTTP.Next, we look at the <code>IProductsClient</code> interface:</p>
<div><pre><code>using Refit;
using Web.Features;
namespace REPR.BFF;
public interface IProductsClient
{
    [Get("/products/{query.ProductId}")]
    Task&lt;Products.FetchOne.Response&gt; FetchProductAsync(
        Products.FetchOne.Query query,
        CancellationToken cancellationToken);
    [Get("/products")]
    Task&lt;Products.FetchAll.Response&gt; FetchProductsAsync(
        CancellationToken cancellationToken);
}</code></pre>
</div>
<p>The preceding interface is similar to <code>IBasketsClient</code> but creates a typed bridge on the <em>Products</em> API.</p>
<blockquote>
<p>The generated code contains much gibberish code and would be very hard to clean enough to make it relevant to study, so let’s assume those interfaces have working implementations instead.</p>
</blockquote>
<p>Next, let’s look at our aggregate:</p>
<div><pre><code>public interface IWebClient
{
    IBasketsClient Baskets { get; }
    IProductsClient Catalog { get; }
}</code></pre>
</div>
<p>The preceding interface exposes the two clients we had Refit generate for us. Its implementation is fairly straightforward as well:</p>
<div><pre><code>public class DefaultWebClient : IWebClient
{
    public DefaultWebClient(IBasketsClient baskets, IProductsClient catalog)
    {
        Baskets = baskets ?? throw new ArgumentNullException(nameof(baskets));
        Catalog = catalog ?? throw new ArgumentNullException(nameof(catalog));
    }
    public IBasketsClient Baskets { get; }
    public IProductsClient Catalog { get; }
}</code></pre>
</div>
<p>The preceding default implementation composes itself through constructor injection, exposing the two typed clients.Of course, dependency injection means we must register services with the container. Let’s start with some configuration. To make the setup code parametrizable and allow the Docker container to override those values, we extract the services base addresses to the settings file like this (<code>appsettings.Development.json</code>):</p>
<div><pre><code>{
  "Downstream": {
    "Baskets": {
      "BaseAddress": "https://localhost:60280"
    },
    "Products": {
      "BaseAddress": "https://localhost:57362"
    }
  }
}</code></pre>
</div>
<p>The preceding code defines two keys, one per service, which we then load individually in the <code>Program.cs</code> file, like this:</p>
<div><pre><code>using Refit;
using REPR.BFF;
using System.Collections.Concurrent;
using System.Net;
var builder = WebApplication.CreateBuilder(args);
var basketsBaseAddress = builder.Configuration
    .GetValue&lt;string&gt;("Downstream:Baskets:BaseAddress") ?? throw new NotSupportedException("Cannot start the program without a Baskets base address.");
var productsBaseAddress = builder.Configuration
    .GetValue&lt;string&gt;("Downstream:Products:BaseAddress") ?? throw new NotSupportedException("Cannot start the program without a Products base address.");</code></pre>
</div>
<p>The preceding code loads the two configurations into variables.</p>
<blockquote>
<p>We can leverage all the techniques we learned in <em>Chapter 9</em>, <em>Options, Settings, and Configuration</em>, to create a more elaborate system.</p>
</blockquote>
<p>Next, we register our Refit clients like this:</p>
<div><pre><code>builder.Services
    .AddRefitClient&lt;IBasketsClient&gt;()
    .ConfigureHttpClient(c =&gt; c.BaseAddress = new Uri(basketsBaseAddress))
;
builder.Services
    .AddRefitClient&lt;IProductsClient&gt;()
    .ConfigureHttpClient(c =&gt; c.BaseAddress = new Uri(productsBaseAddress))
;</code></pre>
</div>
<p>In the preceding code, calling the <code>AddRefitClient</code> method replaces the .NET <code>AddHttpClient</code> method and registers our auto-generated client with the container. Because Refit registration returns an <code>IHttpClientBuilder</code> interface, we can use the <code>ConfigureHttpClient</code> method to configure the <code>HttpClient</code> as we would any other typed HTTP client. In this case, we set the <code>BaseAddress</code> property to the values of the previously loaded settings.Next, we must also register our aggregate:</p>
<div><pre><code>builder.Services.AddTransient&lt;IWebClient, DefaultWebClient&gt;();</code></pre>
</div>
<p>I picked a transient state because the service only fronts other services, so it will serve the other services as they are registered, regardless of whether it is the same instance every time. Moreover, it needs a transient or scoped lifetime because the BFF must manage who is the current customer, not the client. It would be quite a security vulnerability to allow users to decide who they want to impersonate for every request.</p>
<blockquote>
<p>The project does not authenticate the users, but the service we explore next is designed to make this evolve, abstracting and managing this responsibility so we could add authentication without impacting the code we are writing.</p>
</blockquote>
<p>Let’s explore how we manage the current user.</p>


<h3 data-number="20.7.4">Creating a service that serves the current customer</h3>
<p>To keep the project simple, we are not using any authentication or authorization middleware, yet we want our BFF to be realistic and to handle who’s querying the downstream APIs. To achieve this, let’s create the <code>ICurrentCustomerService</code> interface that abstracts this away from the consuming code:</p>
<div><pre><code>public interface ICurrentCustomerService
{
    int Id { get; }
} </code></pre>
</div>
<p>The only thing that interface does is provide us with the identifier representing the current customer. Since we do not have authentication in the project, let’s implement a development version that always returns the same value:</p>
<div><pre><code>public class FakeCurrentCustomerService : ICurrentCustomerService
{
    public int Id =&gt; 1;
}</code></pre>
</div>
<p>Finally, we must register it in the <code>Program.cs</code> class like this:</p>
<div><pre><code>builder.Services.AddScoped&lt;ICurrentCustomerService, FakeCurrentCustomerService&gt;();</code></pre>
</div>
<p>With this last piece, we are ready to write some features in our BFF service.</p>
<blockquote>
<p>In a project that uses authentication, you can inject the <code>IHttpContextAccessor</code> interface into a class to access the current <code>HttpContext</code> object that contains a <code>User</code> property that enables access to the current user’s <code>ClaimsPrincipal</code> object, which should include the current user’s <code>CustomerId</code>. Of course, you must ensure the authentication server returns such a claim. You must register the accessor using the following method before using it: <code>builder.Services.AddHttpContextAccessor()</code>.</p>
</blockquote>


<h3 data-number="20.7.5">Features</h3>
<p>The BFF service serves an unexisting user interface, yet we can imagine what it needs to do; it must:</p>
<ul>
<li>Serve the product catalog so customers can browse the shop.</li>
<li>Serve a specific product to render a product details page.</li>
<li>Serve the list of items in a user’s shopping cart.</li>
<li>Enable users to manage their shopping cart by adding, updating, and removing items.</li>
</ul>
<p>Of course, the list of features could go on, like allowing the users to purchase the items, which is the ultimate goal of an e-commerce website. However, we are not going that far. Let’s start with the catalog.</p>

<h4 data-number="20.7.5.1">Fetching the catalog</h4>
<p>The catalog acts as a routing gateway and forwards the requests to the <code>Products</code> downstream service.The first endpoint serves the whole catalog by using our typed client (highlighted):</p>
<div><pre><code>app.MapGet(
    "api/catalog",
    (IWebClient client, CancellationToken cancellationToken)
        =&gt; client.Catalog.FetchProductsAsync(cancellationToken)
);</code></pre>
</div>
<p>Sending the following requests should hit the endpoint:</p>
<div><pre><code>GET https://localhost:7254/api/catalog</code></pre>
</div>
<p>The endpoint should respond with something like the following:</p>
<div><pre><code>HTTP/1.1 200 OK
Content-Type: application/json; charset=utf-8
{
  "products": [
    {
      "id": 2,
      "name": "Apple",
      "unitPrice": 0.79
    },
    {
      "id": 1,
      "name": "Banana",
      "unitPrice": 0.30
    },
    {
      "id": 3,
      "name": "Habanero Pepper",
      "unitPrice": 0.99
    }
  ]
}</code></pre>
</div>
<p>Here’s a visual representation of what happens:</p>
<figure>
<img alt="Figure 19.27: a sequence diagram representing the BFF routing the request to the Products service" src="img/file155.png"/><figcaption aria-hidden="true">Figure 19.27: a sequence diagram representing the BFF routing the request to the Products service</figcaption>
</figure>
<p>The other catalog endpoint is very similar and also simply routes the request to the correct downstream service:</p>
<div><pre><code>app.MapGet(
    "api/catalog/{productId}",
    (int productId, IWebClient client, CancellationToken cancellationToken)
        =&gt; client.Catalog.FetchProductAsync(new(productId), cancellationToken)
);</code></pre>
</div>
<p>Sending an HTTP call will result in the same as calling it directly because the BFF only acts as a router.We explore more exciting features next.</p>


<h4 data-number="20.7.5.2">Fetching the shopping cart</h4>
<p>The <em>Baskets</em> service only stores the <code>customerId</code>, <code>productId</code>, and <code>quantity</code> properties. However, a shopping cart page displays the product name and price, but the <em>Products</em> service manages those two properties.To overcome this problem, the endpoint acts as an aggregation gateway. It queries the shopping cart and loads all the products from the <em>Products</em> service before returning an aggregated result, removing the burden of managing this complexity from the client/UI.Here’s the code main feature code:</p>
<div><pre><code>app.MapGet(
    "api/cart",
    async (IWebClient client, ICurrentCustomerService currentCustomer, CancellationToken cancellationToken) =&gt;
    {
        var basket = await client.Baskets.FetchCustomerBasketAsync(
            new(currentCustomer.Id),
            cancellationToken
        );
        var result = new ConcurrentBag&lt;BasketProduct&gt;();
        await Parallel.ForEachAsync(basket, cancellationToken, async (item, cancellationToken) =&gt;
        {
            var product = await client.Catalog.FetchProductAsync(
                new(item.ProductId),
                cancellationToken
            );
            result.Add(new BasketProduct(
                product.Id,
                product.Name,
                product.UnitPrice,
                item.Quantity
            ));
        });
        return result;
    }
);</code></pre>
</div>
<p>The preceding code starts by fetching the items from the Baskets service and then loads the products using the <code>Parallel.ForEachAsync</code> method before returning the aggregated result.The <code>Parallel</code> class allows us to execute multiple operations in parallel, in this case, multiple HTTP calls. There are many ways of achieving a similar result using .NET, and this is one of those. When an HTTP call succeeds, it adds a <code>BasketProduct</code> item to the <code>result</code> collection. Once all operations are completed, the endpoint returns the collection of <code>BasketProduct</code> objects, which contains all the combined information required by the user interface to display the shopping cart. Here’s the <code>BasketProduct</code> class:</p>
<div><pre><code>public record class BasketProduct(int Id, string Name, decimal UnitPrice, int Quantity)
{
    public decimal TotalPrice =&gt; UnitPrice * Quantity;
}</code></pre>
</div>
<p>The sequence of this endpoint is like this (the <code>loop</code> represents the <code>Parallel.ForEachAsync</code> method):</p>
<figure>
<img alt="Figure 19.28: A sequence diagram representing the shopping cart endpoint interacting with the Products and the Baskets downstream services." src="img/file156.png"/><figcaption aria-hidden="true">Figure 19.28: A sequence diagram representing the shopping cart endpoint interacting with the Products and the Baskets downstream services.</figcaption>
</figure>
<p>Since the requests to the <em>Products</em> service are sent in parallel, we cannot predict the order they will complete. Here is an excerpt from the application log depicting what can happen (I omitted the logging code in the book, but it is available on GitHub):</p>
<div><pre><code>trce: GetCart[0]
      Fetching product '3'.
trce: GetCart[0]
      Fetching product '2'.
trce: GetCart[0]
      Found product '2'(Apple).
trce: GetCart[0]
      Found product '3'(Habanero Pepper).</code></pre>
</div>
<p>The preceding trace shows that we requested products <code>3</code> and <code>2</code> but received inverted responses (<code>2</code> and <code>3</code>). This is a possibility when running code in parallel.When we send the following request to the BFF:</p>
<div><pre><code>GET https://localhost:7254/api/cart</code></pre>
</div>
<p>The BFF returns a response similar to the following:</p>
<div><pre><code>HTTP/1.1 200 OK
Content-Type: application/json; charset=utf-8
[
  {
    "id": 3,
    "name": "Habanero Pepper",
    "unitPrice": 0.99,
    "quantity": 10,
    "totalPrice": 9.90
  },
  {
    "id": 2,
    "name": "Apple",
    "unitPrice": 0.79,
    "quantity": 5,
    "totalPrice": 3.95
  }
]</code></pre>
</div>
<p>The preceding example showcases the aggregated result, simplifying the logic the client (UI) must implement to display the shopping cart.</p>
<blockquote>
<p>Since we are not ordering the results, the items will not always be in the same order. As an exercise, you could sort the results using one of the existing properties or add a property that saves when a customer adds the item to the cart and sort the items using this new property; the first item added is displayed first, and so on.</p>
</blockquote>
<p>Let’s move to the last endpoint and explore how the BFF manages the shopping cart items.</p>


<h4 data-number="20.7.5.3">Managing the shopping cart</h4>
<p>One of the primary goals of our BFF is to reduce the frontend’s complexity. When examining the <em>Baskets</em> service, we realized it would add a bit of avoidable complexity if we were only to serve the raw operation, so instead, we decided to encapsulate all of the shopping cart logic behind a single endpoint. When a client POST to the <code>api/cart</code> endpoint, it:</p>
<ul>
<li>Adds a non-existent item.</li>
<li>Update an existing item’s quantity.</li>
<li>Remove an item that has a quantity equal to 0 or less.</li>
</ul>
<p>With this endpoint, the clients don’t have to worry about adding or updating. Here’s a simplified sequence diagram that represents this logic:</p>
<figure>
<img alt="Figure 19.29: A sequence diagram that displays the high-level algorithm of the cart endpoint." src="img/file157.png"/><figcaption aria-hidden="true">Figure 19.29: A sequence diagram that displays the high-level algorithm of the cart endpoint.</figcaption>
</figure>
<p>As the diagram depicts, we call the remove endpoint if the quantity is inferior or equal to zero. Otherwise, we try to add the item to the basket. If the endpoint returns a <code>409 Conflict</code>, we try to update the quantity. Here’s the code:</p>
<div><pre><code>app.MapPost(
    "api/cart",
    async (UpdateCartItem item, IWebClient client, ICurrentCustomerService currentCustomer, CancellationToken cancellationToken) =&gt;
    {
        if (item.Quantity &lt;= 0)
        {
            await RemoveItemFromCart(
                item,
                client,
                currentCustomer,
                cancellationToken
            );
        }
        else
        {
            await AddOrUpdateItem(
                item,
                client,
                currentCustomer,
                cancellationToken
            );
        }
        return Results.Ok();
    }
);</code></pre>
</div>
<p>The preceding code follows the same pattern but contains the previously explained logic. We explore the two highlighted methods next, starting with the <code>RemoveItemFromCart</code> method:</p>
<div><pre><code>static async Task RemoveItemFromCart(UpdateCartItem item, IWebClient client, ICurrentCustomerService currentCustomer, CancellationToken cancellationToken)
{
    try
    {
        var result = await client.Baskets.RemoveProductFromCart(
            new Web.Features.Baskets.RemoveItem.Command(
                currentCustomer.Id,
                item.ProductId
            ),
            cancellationToken
        );
    }
    catch (ValidationApiException ex)
    {
        if (ex.StatusCode != HttpStatusCode.NotFound)
        {
            throw;
        }
    }
}</code></pre>
</div>
<p>The highlighted code of the preceding block leverages the typed HTTP client and sends a remove item command to the <em>Baskets</em> service. If the item is not in the cart, the code ignores the error and continues. Why? Because it does not affect the business logic or the end-user experience. Maybe the customer clicked the remove or update button twice. However, the code propagates to the client any other error.Let’s explore the <code>AddOrUpdateItem</code> method’s code:</p>
<div><pre><code>static async Task AddOrUpdateItem(UpdateCartItem item, IWebClient client, ICurrentCustomerService currentCustomer, CancellationToken cancellationToken)
{
    try
    {
        // Add the product to the cart
        var result = await client.Baskets.AddProductToCart(
            new Web.Features.Baskets.AddItem.Command(
                currentCustomer.Id,
                item.ProductId,
                item.Quantity
            ),
            cancellationToken
        );
    }
    catch (ValidationApiException ex)
    {
        if (ex.StatusCode != HttpStatusCode.Conflict)
        {
            throw;
        }
        // Update the cart
        var result = await client.Baskets.UpdateProductQuantity(
            new Web.Features.Baskets.UpdateQuantity.Command(
                currentCustomer.Id,
                item.ProductId,
                item.Quantity
            ),
            cancellationToken
        );
    }
}</code></pre>
</div>
<p>The preceding logic is very similar to the other method. It starts by adding the item to the cart. If it receives a <code>409 Conflict</code>, it tries to update its quantity. Otherwise, it lets the exception bubble up the stack to let an exception middleware catch it later to uniformize the error messages.With that code in place, we can send <code>POST</code> requests to the <code>api/cart</code> endpoint for adding, updating, and removing an item from the cart. The three operations return an empty <code>200 OK</code> response.Assuming we have an empty shopping cart, the following request adds <em>10</em> <em>Habanero Peppers</em> (<code>id=3</code>) to the shopping cart:</p>
<div><pre><code>POST https://localhost:7254/api/cart
Content-Type: application/json
{
    "productId": 3, 
    "quantity": 10
}</code></pre>
</div>
<p>The following request adds <em>5 Apples</em> (<code>id=2</code>) to the cart:</p>
<div><pre><code>POST https://localhost:7254/api/cart
Content-Type: application/json
{
    "productId": 2, 
    "quantity": 5
}</code></pre>
</div>
<p>The following request updates the quantity to <em>20</em> <em>Habanero Peppers</em> (<code>id=3</code>) :</p>
<div><pre><code>POST https://localhost:7254/api/cart
Content-Type: application/json
{
    "productId": 3, 
    "quantity": 20
}</code></pre>
</div>
<p>The following request removes the <em>Apples</em> (<code>id=2</code>) from the cart:</p>
<div><pre><code>POST https://localhost:7254/api/cart
Content-Type: application/json
{
    "productId": 2, 
    "quantity": 0
}</code></pre>
</div>
<p>Leaving us with <em>20</em> <em>Habanero Peppers</em> in our shopping cart (<code>GET https://localhost:7254/api/cart</code>):</p>
<div><pre><code>[
  {
    "id": 3,
    "name": "Habanero Pepper",
    "unitPrice": 0.99,
    "quantity": 20,
    "totalPrice": 19.80
  }
]</code></pre>
</div>
<p>The requests of the previous sequence are all in the same format, reaching the same endpoint but doing different things, which makes it very easy for the frontend client to manage.</p>
<blockquote>
<p>If you prefer having the UI to manage the operations individually or want to implement a batch update feature, you can; this is only an example of what you can leverage a BFF for.</p>
</blockquote>
<p>We are now done with the BFF service.</p>



<h3 data-number="20.7.6">Conclusion</h3>
<p>In this section, we learned about using the Backend for Frontend (BFF) design pattern to front a micro e-commerce web application. We discussed layering APIs and the advantages and disadvantages of a two-layer design. We autogenerated strongly typed HTTP clients using Refit, managed a shopping cart, and fetched the catalog from the BFF. We learned how to use a BFF to reduce complexity by moving domain logic from the frontend to the backend by implementing multiple Gateway patterns.Here are a few benefits that we explored:</p>
<ul>
<li>The BFF pattern can significantly simplify the interaction between frontend and backend systems. It provides a layer of abstraction that can reduce the complexity of using low-level atomic APIs. It separates generic and domain-specific functionalities and promotes cleaner, more modular code.</li>
<li>A BFF can act as a gateway that routes specific requests to relevant services, reducing the work the frontend has to perform. It can also serve as an aggregation gateway, gathering data from various services into a unified response. This process can simplify frontend development by reducing the complexity of the frontend and the number of separate calls the frontend must make. It can also reduce the payload size transported between the frontend and backend.</li>
<li>Each BFF is tailored to a specific client, optimizing the frontend interaction.</li>
<li>A BFF can handle issues in one domain without affecting the low-level APIs or the other applications, thus providing easier maintenance.</li>
<li>A BFF can implement security logic, such as specific domain-oriented authentication and authorization rules.</li>
</ul>
<p>Despite these benefits, using a BFF may also increase complexity and introduce potential performance overhead. Using a BFF is no different than any other pattern and must be counter-balanced and adapted to the specific needs of a project.Next, we revisit CQRS on a distributed scale.</p>



<h2 data-number="20.8">Revisiting the CQRS pattern</h2>
<p><strong>Command Query Responsibility Segregation</strong> (<strong>CQRS</strong>) applies the <strong>Command Query Separation</strong> (<strong>CQS</strong>) principle. Compared to what we saw in <em>Chapter 14</em>, <em>Mediator and CQRS Design Patterns</em>, we can push CQRS further using microservices or serverless computing. Instead of simply creating a clear separation between commands and queries, we can divide them even more using multiple microservices and data sources.<strong>CQS</strong> is a principle stating that a method should either return data or mutate data, but not both. On the other hand, <strong>CQRS</strong> suggests using one model to read the data and one model to mutate the data.<strong>Serverless computing</strong> is a cloud execution model where the cloud provider manages the servers and allocates the resources on-demand, based on usage and configuration. Serverless resources fall into the platform as a service (PaaS) offering.Let’s come back to our IoT example again. We queried the last known location of a device in the previous examples, but what about the device updating that location? This can mean pushing many updates every minute. To solve this issue, we are going to use CQRS and focus on two operations:</p>
<ul>
<li>Updating the device location.</li>
<li>Reading the last known location of a device.</li>
</ul>
<p>Simply put, we have a <code>Read Location</code> microservice, a <code>Write Location</code> microservice, and two databases. Remember that each microservice should own its data. This way, a user can access the last known device location through the read microservice (query model), while a device can punctually send its current position to the write microservice (command model). By doing this, we split the load from reading and writing the data as both occur at different frequencies:</p>
<figure>
<img alt="Figure 19.30: Microservices that apply CQRS to divide the reads and writes of a device’s location" src="img/file158.png"/><figcaption aria-hidden="true">Figure 19.30: Microservices that apply CQRS to divide the reads and writes of a device’s location</figcaption>
</figure>
<p>In the preceding schema that illustrates the concept, the reads are queries, and the writes are commands. How to update the Read DB once a new value is added to the Write DB depends on the technology at play. One essential thing in this type of architecture is that, per the CQRS pattern, a command should not return a value, enabling a “fire and forget” scenario. With that rule in place, consumers don’t have to wait for the command to complete before doing something else.</p>
<blockquote>
<p>Fire and forget does not apply to every scenario; sometimes, we need synchronization. Implementing the Saga pattern is one way to solve coordination issues.</p>
</blockquote>
<p>Conceptually, we can implement this example by leveraging serverless cloud infrastructures, such as Azure Functions. Let’s revisit this example using a high-level conceptual serverless design:</p>
<figure>
<img alt="Figure 19.31: Using Azure services to manage a CQRS implementation" src="img/file159.png"/><figcaption aria-hidden="true">Figure 19.31: Using Azure services to manage a CQRS implementation</figcaption>
</figure>
<p>The previous diagram illustrates the following:</p>
<ol>
<li>The device sends its location regularly by posting it to <em>Azure Function 1</em>.</li>
<li><em>Azure Function 1</em> then publishes the <code>LocationAdded</code> event to the event broker, which is also an event store (the Write DB).</li>
<li>All subscribers to the <code>LocationAdded</code> event can now handle the event appropriately, in this case, <em>Azure Function 2</em>.</li>
<li><em>Azure Function 2</em> updates the device's last known location in the <em>Read DB</em>.</li>
<li>Any subsequent queries should result in reading the new location.</li>
</ol>
<p>The message broker is also the event store in the preceding diagram, but we could store events elsewhere, such as in an Azure Storage Table, in a time-series database, or in an Apache Kafka cluster. Azure-wise, the datastore could also be CosmosDB. Moreover, I abstracted this component for multiple reasons, including the fact that there are multiple “as-a-service” offerings to publish events in Azure and multiple ways of using third-party components (both open-source and proprietary).Furthermore, the example demonstrates <strong>eventual consistency</strong> well. All the last known location reads between <em>steps 1</em> and <em>4</em> get the old value while the system processes the new location updates (commands). If the command processing slows down for some reason, a longer delay could occur before the next read database updates. The commands could also be processed in batches, leading to another kind of delay. No matter what happens with the command processing, the read database is available all that time, whether it serves the latest data or not and whether the write system is overloaded or not. This is the beauty of this type of design, but it is more complex to implement and maintain.</p>
<blockquote>
<p><strong>Time-series databases</strong> are optimized for temporally querying and storing data, where you always append new records without updating old ones. This kind of NoSQL database can be useful for temporal-intensive usage, like metrics.</p>
</blockquote>
<p>Once again, we used the Publish-Subscribe pattern to get another scenario going. Assuming that events are persisted forever, the previous example could also support event sourcing. Furthermore, new services could subscribe to the <code>LocationAdded</code> event without impacting the code that has already been deployed. For example, we could create a SignalR microservice that pushes the updates to its clients. It is not CQRS-related, but it flows well with everything that we’ve explored so far, so here is an updated conceptual diagram:</p>
<figure>
<img alt="Figure 19.32: Adding a SignalR service as a new subscriber without impacting the other part of the system" src="img/file160.png"/><figcaption aria-hidden="true">Figure 19.32: Adding a SignalR service as a new subscriber without impacting the other part of the system</figcaption>
</figure>
<p>The SignalR microservice could be custom code or an Azure SignalR Service (backed by another Azure Function); it doesn’t matter. With this design, the Web App could know that a change occurred before the Read DB gets updated.</p>
<blockquote>
<p>With this design, I wanted to illustrate that dropping new services into the mix is easier when using a Pub-Sub model than with point-to-point communication.</p>
</blockquote>
<p>As you can see, a microservices system adds more and more small pieces that indirectly interconnect with each other over one or more message brokers. Maintaining, diagnosing, and debugging such systems is harder than with a single application; that’s the <strong>operational complexity</strong> we discussed earlier. However, containers can help deploy and maintain such systems.Starting in ASP.NET Core 3.0, the ASP.NET Core team invested much effort into <strong>distributed tracing</strong>. Distributed tracing is necessary to find failures and bottlenecks related to an event that flows from one program to another (such as microservices). If something bugs out, it is important to trace what the user did to isolate the error, reproduce it, and then fix it. The more independent pieces there are, the harder it can become to make that trace possible. This is outside the scope of this book, but it is something to consider if you plan to leverage microservices.</p>

<h3 data-number="20.8.1">Advantages and potential risks</h3>
<p>This section explores some advantages and risks of separating a data store's read and write operations using the CQRS pattern.</p>

<h4 data-number="20.8.1.1">Benefits of the CQRS pattern</h4>
<ul>
<li><strong>Scalability:</strong> Given that read and write workloads can be scaled independently, CQRS can lead to much higher scalability in a distributed cloud- or microservices-based applications.</li>
<li><strong>Simplified and Optimized Models:</strong> It separates the read model (query responsibility) and write model (command responsibility), which simplifies application development and can optimize performance.</li>
<li><strong>Flexibility:</strong> Different models increase the number of choices one can make, increasing flexibility.</li>
<li><strong>Enhanced Performance:</strong> CQRS can prevent unnecessary data fetching and allows choosing an optimized database for each job, improving the performance of both read and write operations.</li>
<li><strong>Increased Efficiency:</strong> It enables parallel development on complex applications, as teams can work independently on the separate read and write sides of the application.</li>
</ul>


<h4 data-number="20.8.1.2">Potential Risks of using the CQRS pattern</h4>
<ul>
<li><strong>Complexity:</strong> CQRS adds complexity to the system. It may not be necessary for simple CRUD apps and could over-complicate the application unnecessarily. Therefore, using CQRS only in complex systems and when the advantages outweigh the cons is advisable.</li>
<li><strong>Data Consistency:</strong> It can introduce eventual consistency issues between the read and write sides because the read model's updates are asynchronous, which might not fit every business requirement.</li>
<li><strong>Increased Development Effort:</strong> CQRS could mean increased development, testing, and maintenance efforts due to handling two separate models and more pieces.</li>
<li><strong>Learning Curve:</strong> The pattern has its own learning curve. Team members unfamiliar with the CQRS pattern will require training and to gain some experience.</li>
<li><strong>Synchronization Challenges:</strong> Maintaining synchronization between the read and write models can be challenging, especially in high data volume cases.</li>
</ul>



<h3 data-number="20.8.2">Conclusion</h3>
<p>CQRS helps divide queries and commands and helps encapsulate and isolate each block of logic independently. Mixing that concept with serverless computing or microservices architecture allows us to scale reads and writes independently. We can also use different databases, empowering us with the tools we need for the transfer rate required by each part of that system (for example, frequent writes and occasional reads or vice versa).Major cloud providers like Azure and AWS provide serverless offerings to help support such scenarios. Each cloud provider’s documentation should help you get started. Meanwhile, for Azure, we have Azure Functions, Event Grid, Event Hubs, Service Bus, Cosmos DB, and more. Azure also offers bindings between the different services that are triggered or react to events for you, removing a part of the complexity yet locking you down with that vendor.Now, let’s see how CQRS can help us follow the <strong>SOLID</strong> principles at the cloud scale:</p>
<ul>
<li><strong>S</strong>: Dividing an application into smaller reads and writes applications (or functions) leans toward encapsulating single responsibilities into different programs.</li>
<li><strong>O</strong>: CQRS, mixed with serverless computing or microservices, helps extend the software without needing us to modify the existing code by adding, removing, or replacing applications.</li>
<li><strong>L</strong>: N/A</li>
<li><strong>I</strong>: CQRS set us up to create multiple small interfaces (or programs) with a clear distinction between commands and queries.</li>
<li><strong>D</strong>: N/A</li>
</ul>



<h2 data-number="20.9">Exploring the Microservice Adapter pattern</h2>
<p>The Microservice Adapter pattern allows adding missing features, adapting one system to another, or migrating an existing application to an event-driven architecture model, to name a few possibilities. The Microservice Adapter pattern is similar to the Adapter pattern we cover in <em>Chapter 9</em>, <em>Structural Patterns</em>, but applied to a microservices system that uses event-driven architecture instead of creating a class to adapt an object to another signature.In the scenarios we cover in this section, the microservices system represented by the following diagram can be replaced by a standalone application as well; this pattern applies to all sorts of programs, not just microservices, which is why I abstracted away the details:</p>
<figure>
<img alt="Figure 19.33: Microservice system representation used in the subsequent examples" src="img/file161.png"/><figcaption aria-hidden="true">Figure 19.33: Microservice system representation used in the subsequent examples</figcaption>
</figure>
<p>Here are the examples we are covering next and possible usages of this pattern:</p>
<ul>
<li>Adapting an existing system to another.</li>
<li>Decommissioning a legacy application.</li>
<li>Adapting an event broker to another.</li>
</ul>
<p>Let’s start by connecting a standalone system to an event-driven one.</p>

<h3 data-number="20.9.1">Adapting an existing system to another</h3>
<p>In this scenario, we have an existing system of which we don’t control the source code or don’t want to change, and we have a microservices system built around an event-driven architecture model. We don’t have to control the source code of the microservices system either as long as we have access to the event broker.Here is a diagram that represents this scenario:</p>
<figure>
<img alt="Figure 19.34: A microservices system that interacts with an event broker and an existing system that is disconnected from the microservices" src="img/file162.png"/><figcaption aria-hidden="true">Figure 19.34: A microservices system that interacts with an event broker and an existing system that is disconnected from the microservices</figcaption>
</figure>
<p>As we can see from the preceding diagram, the existing system is disconnected from the microservices and the broker. To adapt the existing system to the microservices system, we must subscribe or publish certain events. Let’s see how to read data from the microservices (subscribe to the broker) and then update that data into the existing system.When we control the existing system’s code, we can open the source code, subscribe to one or more topics, and change the behaviors from there. In our case, we don’t want to do that or can’t, so we can’t directly subscribe to topics, as demonstrated by the following diagram:</p>
<figure>
<img alt="Figure 19.35: Missing capabilities to connect an existing system to an event-driven one" src="img/file163.png"/><figcaption aria-hidden="true">Figure 19.35: Missing capabilities to connect an existing system to an event-driven one</figcaption>
</figure>
<p>This is where the microservice adapter comes into play and allows us to fill the capability gap of our existing system. To add the missing link, we create a microservice that subscribes to the appropriate events, then apply the changes in the existing system, like this:</p>
<figure>
<img alt="Figure 19.36: An adapter microservice adding missing capabilities to an existing system" src="img/file164.png"/><figcaption aria-hidden="true">Figure 19.36: An adapter microservice adding missing capabilities to an existing system</figcaption>
</figure>
<p>As we can see in the preceding diagram, the <code>Adapter</code> microservice gets the events (subscribes to one or more topics) and then uses that data from the microservices system to execute some business logic on the existing system.In this design, the new <code>Adapter</code> microservice allowed us to add missing capabilities to a system we had no control over with little to no disruption to users’ day-to-day activities.The example assumes the existing system had some form of extensibility mechanism like an API. If the system does not, we would have to be more creative to interface with it.For example, the microservices system could be an e-commerce website, and the existing system could be a legacy inventory management system. The adapter could update the legacy system with new order data.The existing system could also be an old <strong>customer relationship management</strong> (<strong>CRM</strong>) system that you want to update when users of the microservices application execute some actions, like changing their phone number or address.The possibilities are almost endless; you create a link between an event-driven system and an existing system you don’t control or don’t want to change. In this case, the microservice adapter allows us to follow the <strong>Open-Closed principle</strong> by extending the system without changing the existing pieces. The primary drawback is that we are deploying another microservice that has direct coupling with the existing system, which may be best for temporary solutions. On that same line of thought, next, we replace a legacy application with a new one with limited to no downtime.</p>


<h3 data-number="20.9.2">Decommissioning a legacy application</h3>
<p>In this scenario, we have a legacy application to decommission and a microservices system to which we want to connect some existing capabilities. To achieve this, we can create one or more adapters to migrate all features and dependencies to the new model.Here is a representation of the current state of our system:</p>
<figure>
<img alt="Figure 19.37: The original legacy application and its dependencies" src="img/file165.png"/><figcaption aria-hidden="true">Figure 19.37: The original legacy application and its dependencies</figcaption>
</figure>
<p>The preceding diagram shows the two distinct systems, including the legacy application we want to decommission. Two other applications, dependency A and B, directly depend on the legacy application. The exact migration flow is strongly dependent on your use case. If you want to keep the dependencies, we want to migrate them first. To do that, we can create an event-driven <code>Adapter</code> microservice that breaks the tight coupling between the dependencies and the legacy application like this:</p>
<figure>
<img alt="Figure 19.38: Adding a microservice adapter that implements the event-driven flow required to break tight coupling between the dependencies and the legacy application" src="img/file166.png"/><figcaption aria-hidden="true">Figure 19.38: Adding a microservice adapter that implements the event-driven flow required to break tight coupling between the dependencies and the legacy application</figcaption>
</figure>
<p>The preceding diagram shows an <code>Adapter</code> microservice and the rest of a microservices system that communicates using an event broker. As we explored in the previous example, the adapter was placed there to connect the legacy application to the microservices. Our scenario focuses on removing the legacy application and migrating its two dependencies. Here, we carved out the required capabilities using the adapter, allowing us to migrate the dependencies to an event-driven model and break tight coupling with the legacy application. Such migration could be done in multiple steps, migrating each dependency one by one, and we could even create one adapter per dependency. For the sake of simplicity, I chose to draw only one adapter. You may want to revisit this choice if your dependencies are large or complex.Once we are done migrating the dependencies, our systems look like the following:</p>
<figure>
<img alt="Figure 19.39: The dependencies are now using an event-driven architecture, and the adapter microservice is bridging the gap between the events and the legacy system" src="img/file167.png"/><figcaption aria-hidden="true">Figure 19.39: The dependencies are now using an event-driven architecture, and the adapter microservice is bridging the gap between the events and the legacy system</figcaption>
</figure>
<p>In the preceding diagram, the Adapter microservice executes the operations against the legacy application API that the two dependencies were doing before. The dependencies are now publishing events instead of using the API. For example, when an operation happens in <code>DependencyB</code>, it publishes an event to the broker. The Adapter microservice picks up that event and executes the original operation against the API. Doing this creates more complexity and is a temporary state.With this new architecture in place, we can start migrating existing features away from the legacy application into the new application without impacting the dependencies; we broke tight coupling.</p>
<blockquote>
<p>From this point forward, we are applying the <strong>Strangler Fig</strong> pattern to migrate the legacy system piece by piece to our new architecture. For the sake of simplicity, think of the Strangler Fig pattern as migrating features from one application to another, one by one. In this case, we replaced one application with another, but we could also use the same patterns to split an application into multiple smaller applications (like microservices).</p>
<blockquote>
<p>I left a few links in the further reading section in case migrating legacy systems is something you do or simply if you want to know more about that pattern.</p>
</blockquote>
</blockquote>
<p>The following diagram is a visual representation that adds the modern application we are building to replace the legacy application. That new modern application could also be a purchased product you are putting in place instead; the concepts we are exploring apply to both use cases, but the exact steps are directly related to the technology at play.</p>
<figure>
<img alt="Figure 19.40: The modern application to replace the legacy application is starting to emerge by migrating capabilities to that new application" src="img/file168.png"/><figcaption aria-hidden="true">Figure 19.40: The modern application to replace the legacy application is starting to emerge by migrating capabilities to that new application</figcaption>
</figure>
<p>In the preceding diagram, we see the new modern application has appeared. Each time we deploy a new feature to the new application, we can remove it from the adapter, leading to a graceful transition between the two models. At the same time, we are keeping the legacy application in place to continue to provide the capabilities that are not yet migrated.Once all the features we want to keep are migrated, we can remove the adapter and decommission the legacy application, leading to the following system:</p>
<figure>
<img alt="Figure 19.41: The new system topology after the retirement of the legacy application, showing the new modern application and its two loosely coupled dependencies" src="img/file169.png"/><figcaption aria-hidden="true">Figure 19.41: The new system topology after the retirement of the legacy application, showing the new modern application and its two loosely coupled dependencies</figcaption>
</figure>
<p>The preceding diagram shows the new system topology encompassing a new modern application and the two original dependencies that are now loosely coupled through event-driven architecture. Of course, the bigger the migration, the more complex it will be and the longer it will take, but the Adapter Microservice pattern is one way to help do a partial or complete migration from one system to another.Like the preceding example, the main advantage is adding or removing capabilities without impacting the other systems, which allows us to migrate and break the tight coupling between the different dependencies. The downside is the added complexity of this temporary solution. Moreover, during the migration step, you will most likely need to deploy both the modern application and the adapter in the correct sequence to ensure both systems are not handling the same events twice, leading to duplicate changes. For example, updating the phone number to the same value twice should be all right because it leads to the same final data set. However, creating two records instead of one should be more important to mitigate as it may lead to integrity errors in the data set. For example, creating an online order twice instead of once could create customer dissatisfaction or internal issues.And voilà, we decommissioned a system using the Microservice Adapter pattern without breaking its dependencies. Next, we look at an <strong>Internet of Things</strong> (<strong>IoT</strong>) example.</p>


<h3 data-number="20.9.3">Adapting an event broker to another</h3>
<p>In this scenario, we are adapting an event broker to another. In the following diagram, we look at two use cases: one that translates events from broker B to broker A (left) and the other that translates events from broker A to broker B (right). Afterwards, we explore a more concrete example:</p>
<figure>
<img alt="Figure 19.42: An adapter microservice that translates events from broker B to broker A (left) and from broker A to broker B (right)" src="img/file170.png"/><figcaption aria-hidden="true">Figure 19.42: An adapter microservice that translates events from broker B to broker A (left) and from broker A to broker B (right)</figcaption>
</figure>
<p>We can see the two possible flows in the preceding diagram. The first flow, on the left, allows the adapter to read events from broker B and publish them to broker A. The second flow, on the right, enables the adapter to read events from broker A and publish them to broker B. Those flows allow us to translate or copy events from one broker to another by leveraging the Microservice Adapter pattern.</p>
<blockquote>
<p>In <em>Figure 16.35</em>, there is one adapter per flow. I did that to make the two flows as independent as possible, but the adapters could be a single microservice.</p>
</blockquote>
<p>This pattern can be very useful for an IoT system where your microservices leverage Apache Kafka internally for its full-featured suite of event-streaming capabilities but use MQTT to communicate with the low-powered IoT devices that connect to the system. An adapter can solve this problem by translating the messages from one protocol to the other. Here is a diagram that represents the complete flows, including a device and the microservices:</p>
<figure>
<img alt="Figure 19.43: Complete protocol adapter flows, including a device and microservices" src="img/file171.png"/><figcaption aria-hidden="true">Figure 19.43: Complete protocol adapter flows, including a device and microservices</figcaption>
</figure>
<p>Before we explore what the events could be, let’s explore both flows step by step. The left flow allows getting events inside the system from the devices through the following sequence:</p>
<ol>
<li>A device publishes an event to the MQTT broker.</li>
<li>The adapter reads that event.</li>
<li>The adapter publishes a similar or different event to the Kafka broker.</li>
<li>Zero or more microservices subscribed to the event act on it.</li>
</ol>
<p>On the other hand, the right flow allows getting events out of the system to the devices through the following sequence:</p>
<ol>
<li>A microservice publishes an event to the Kafka broker.</li>
<li>The adapter reads the event.</li>
<li>The adapter publishes a similar or different event to the MQTT broker.</li>
<li>Zero or more devices subscribed to the event act on it.</li>
</ol>
<p>You don’t have to implement both flows; the adapter could be bidirectional (supporting both flows), we could have two unidirectional adapters that support one of the flows, or we could allow the communication to flow only one way (in or out but not both). The choice relates to your specific use cases.Concrete examples of sending a message from a device to a microservice (left flow) could be sending its GPS position, a status update (the light is now on), or a message indicating a sensor failure.Concrete examples of sending a message to a device (right flow) could be to remotely control a speaker’s volume, flip a light on, or send a confirmation that a message has been acknowledged.In this case, the adapter is not a temporary solution but a permanent capability. We could leverage such adapters to create additional capabilities with minimal impact on the rest of the system. The primary downside is deploying one or more other microservices, but your system and processes are probably robust enough to handle that added complexity when leveraging such capabilities.This third scenario that leverages the Microservice Adapter is our last. Hopefully, I sparked your imagination enough to leverage this simple yet powerful design pattern.</p>


<h3 data-number="20.9.4">Conclusion</h3>
<p>We explored the Microservice Adapter pattern that allows us to connect two elements of a system by adapting one to the other. We explored how to push information from an event broker into an existing system that does not support such capabilities. We also explored how to leverage an adapter to break tight coupling, migrate features into a newer system, and decommission a legacy application seamlessly. We finally connected two event brokers through an adapter microservice, allowing a low-powered IoT device to communicate with a microservices system without draining their battery and without the complexity it would incur to use a more complex communication protocol.This pattern is very powerful and can be implemented in many ways, but it all depends on the exact use cases. You can write an adapter using a serverless offering like an Azure function, no-code/low-code offerings like Power Automate, or C#. Of course, these are just a few examples. The key to designing the correct system is to nail down the problem statement because once you know what you are trying to fix, the solution becomes clearer.Now, let’s see how the Microservice Adapter pattern can help us follow the <strong>SOLID</strong> principles at cloud-scale:</p>
<ul>
<li><strong>S</strong>: The microservice adapter helps manage long- or short-term responsibilities. For example, adding an adapter that translates between two protocols or creating a temporary adapter to decommission a legacy system.</li>
<li><strong>O</strong>: You can leverage microservice adapters to dynamically add or remove features without impacting or with limited impact on the rest of the system. For example, in the IoT scenario, we could add support for a new protocol like AMQP without changing the rest of the system.</li>
<li><strong>L</strong>: N/A</li>
<li><strong>I</strong>: Adding smaller adapters can make changes easier and less risky than updating large legacy applications. As we saw in the legacy system decommissioning scenario, we could also leverage temporary adapters to split large applications into smaller pieces.</li>
<li><strong>D</strong>: A microservice adapter inverts the dependency flow between the system it adapts. For example, in the legacy system decommissioning scenario, the adapter reversed the flow from the two dependencies to the legacy system by leveraging an event broker.</li>
</ul>



<h2 data-number="20.10">Summary</h2>
<p>The microservices architecture is different from everything we’ve covered in this book and how we build monoliths. Instead of one big application, we split it into multiple smaller ones called microservices. Microservices must be independent of one another; otherwise, we will face the same problems associated with tightly coupled classes, but at the cloud scale.We can leverage the Publish-Subscribe design pattern to loosely couple microservices while keeping them connected through events. Message brokers are programs that dispatch those messages. We can use event sourcing to recreate the application’s state at any point in time, including when spawning new containers. We can use application gateways to shield clients from the microservices cluster’s complexity and publicly expose only a subset of services.We also looked at how we can build upon the CQRS design pattern to decouple reads and writes of the same entities, allowing us to scale queries and commands independently. We also looked at using serverless resources to create that kind of system.Finally, we explored the Microservice Adapter pattern that allowed us to adapt two systems together, decommission a legacy application, and connect two event brokers. This pattern is simple but powerful at inverting the dependency flow between two dependencies in a loosely coupled manner. The use of the pattern can be temporary, as we saw in the legacy application decommissioning scenario, or permanent, as we saw in the IoT scenario.On the other hand, microservices come at a cost and are not intended to replace all that exists. Building a monolith is still a good idea for many projects. Starting with a monolith and migrating it to microservices when scaling is another solution. This allows us to develop the application faster (monolith). It is also easier to add new features to a monolith than it can be to add them to a microservice application. Most of the time, mistakes cost less in a monolith than in a microservices application. You can also plan your future migration toward microservices, which leads to the best of both worlds while keeping operational complexity low. For example, we could leverage the Publish-Subscribe pattern through MediatR notifications in your monolith and migrate the events dispatching responsibility to a message broker later when migrating your system to microservices architecture (if the need ever arises). We are exploring ways to organize our monolith in <em>Chapter 20</em>, <em>Modular Monolith</em>.I don’t want you to discard the microservices architecture, but I want to ensure you weigh up the pros and cons of such a system before blindly jumping in. Your team’s skill level and ability to learn new technologies may also impact the cost of jumping into the microservices boat.<strong>DevOps</strong> (development [Dev] and IT operations [Ops]) or <strong>DevSecOps</strong> (adding security [Sec] to the DevOps mix), which we do not cover in the book, is essential when building microservices. It brings deployment automation, automated quality checks, auto-composition, and more. Your microservices cluster will be very hard to deploy and maintain without that.Microservices are great when you need scaling, want to go serverless, or split responsibilities between multiple teams, but keep the operational costs in mind.In the next chapter, we combine the microservices and monolith worlds.</p>


<h2 data-number="20.11">Questions</h2>
<p>Let’s take a look at a few practice questions:</p>
<ol>
<li>What is the most significant difference between a <strong>message queue</strong> and a <strong>pub-sub</strong> model?</li>
<li>What is <strong>event sourcing</strong>?</li>
<li>Can an <strong>application gateway</strong> be both a <strong>routing gateway</strong> and an <strong>aggregation gateway</strong>?</li>
<li>Is it true that real CQRS requires a serverless cloud infrastructure?</li>
<li>What is a significant advantage of using the BFF design pattern?</li>
</ol>


<h2 data-number="20.12">Further reading</h2>
<p>Here are a few links that will help you build on what you learned in this chapter:</p>
<ul>
<li>Event Sourcing pattern by Martin Fowler: <a href="https://adpg.link/oY5H">https://adpg.link/oY5H</a></li>
<li>Event Sourcing pattern by Microsoft: <a href="https://adpg.link/ofG2">https://adpg.link/ofG2</a></li>
<li>Publisher-Subscriber pattern by Microsoft: <a href="https://adpg.link/amcZ">https://adpg.link/amcZ</a></li>
<li>Event-driven architecture by Microsoft: <a href="https://adpg.link/rnck">https://adpg.link/rnck</a></li>
<li>Microservices architecture and patterns on microservices.io: <a href="https://adpg.link/41vP">https://adpg.link/41vP</a></li>
<li>Microservices architecture and patterns by Martin Fowler: <a href="https://adpg.link/Mw97">https://adpg.link/Mw97</a></li>
<li>Microservices architecture and patterns by Microsoft: <a href="https://adpg.link/s2Uq">https://adpg.link/s2Uq</a></li>
<li>RFC 6902 (JSON Patch): <a href="https://adpg.link/bGGn">https://adpg.link/bGGn</a></li>
<li>JSON Patch in ASP.NET Core web API: <a href="https://adpg.link/u6dw">https://adpg.link/u6dw</a></li>
</ul>
<p>Strangler Fig Application pattern:</p>
<ul>
<li>Martin Fowler: <a href="https://adpg.link/Zi9G">https://adpg.link/Zi9G</a></li>
<li>Microsoft: <a href="https://adpg.link/erg2">https://adpg.link/erg2</a></li>
</ul>


<h2 data-number="20.13">Answers</h2>
<ol>
<li>The message queue gets a message and has a single subscriber dequeue it. If nothing dequeues a message, it stays in the queue indefinitely (FIFO model). The Pub-Sub model gets a message and sends it to zero or more subscribers.</li>
<li>Event sourcing is the process of chronologically accumulating events that happened in a system instead of persisting in the current state of an entity. It allows you to recreate the entity's state by replaying those events.</li>
<li>Yes, you can mix Gateway patterns (or sub-patterns).</li>
<li>No, you can deploy micro-applications (microservices) on-premises if you want to.</li>
<li>It separates generic functionalities from app-specific ones, promoting cleaner code and modularization. It also helps simplify the frontend.</li>
</ol>


</body>
</html>
