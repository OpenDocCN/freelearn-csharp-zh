- en: '13'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Driving Change
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout the book, we have talked about the technical side of observability
    and discussed how to trace calls, record metrics, report events, or use auto-collected
    telemetry provided by platforms and libraries. Here, we’re going to talk about
    the organizational aspects of implementing observability solutions.
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll look into the benefits of and reasons for changing your existing
    solution and discuss associated costs. Then, we’ll go through the implementation
    stages and come up with a brief. Finally, we’ll see how to leverage observability
    to drive and improve the development process.
  prefs: []
  type: TYPE_NORMAL
- en: 'in this chapter, you’ll learn how to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Decide whether you need a better observability solution and which level is right
    for you
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop an onboarding plan and start implementing it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use observability to help with daily development tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you should be ready to propose an observability
    solution and onboarding plan for your organization.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the importance of observability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you’re reading this book, you’re probably at least entertaining the idea
    of improving the observability story of your application. Maybe it’s hard to understand
    how customers use your system or it takes a lot of time to understand what exactly
    went wrong when someone reports an issue. In the worst case, it takes a lot of
    time to just notice that the system is unhealthy and users are affected. Or, maybe
    you want to minimize such risks in your future projects.
  prefs: []
  type: TYPE_NORMAL
- en: In any case, these pain points brought you here and they should guide you further
    to find the right observability level and approach for your system.
  prefs: []
  type: TYPE_NORMAL
- en: Even if we clearly see the problem and how it can be solved with better observability,
    we usually still need to get other people working on the system onboard with this
    vision. Astoundingly, they might have quite different feelings about the same
    problems and might not consider them worthy of solving.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let me share a few common points I have heard arguing that a problem is not
    important:'
  prefs: []
  type: TYPE_NORMAL
- en: When a customer reports an issue, we can ask for a timestamp and find operations
    at that time by customer identifier. Then we can find any suspicious logs, get
    the request ID, and then find correlated logs on other services.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we see an issue in production, we open related dashboards and start visually
    correlating metrics until we can guess what’s broken and then mitigate it. We
    have experts and an excellent set of runbooks for typical issues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can do a user study or customer research to get extensive information on
    how people use the system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We ask customers to enable verbose logs and reproduce the problem, then send
    us logs that we’ll parse based on our expert knowledge of the system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Each of these approaches is totally valid. They *already* solve the problem,
    your team *already* knows how to use them, and some of them are still necessary
    and quite useful even with perfect observability solutions in place.
  prefs: []
  type: TYPE_NORMAL
- en: So, essentially, when we consider the approach to observability, we need to
    break the status quo and convince ourselves and our organization that it’s worth
    it. To achieve this, we first need to clearly outline the pain points and understand
    the cost of keeping things as they are.
  prefs: []
  type: TYPE_NORMAL
- en: The cost of insufficient observability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Your organization might already have some common incident metrics in place that
    we can rely on, such as **MTTM** (**mean time to mitigate**), **MTTR** (**mean
    time to recover**), **MTBF** (**mean time between failures**), or others. They
    are somewhat subjective and depend on what qualifies as an incident, or what recovery
    means, but roughly show how fast we can investigate incidents and how frequently
    they happen.
  prefs: []
  type: TYPE_NORMAL
- en: If incidents take a lot of time to resolve and happen frequently, it’s likely
    that our organization cares deeply about them and would be interested in improving
    the situation.
  prefs: []
  type: TYPE_NORMAL
- en: Ironically, we need at least some level of observability to notice there is
    an incident and to measure how long it takes to resolve. If we don’t have even
    this in place, we can start to manually track when things get broken and how long
    it takes us to discover and resolve issues. However subjective it is, it’s better
    than nothing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some things rarely appear in such metrics directly: how bad is your on-call
    experience? How much time does onboarding take before someone can be on-call independently?
    How many issues end up closed with something such as “cannot reproduce,” “not
    enough information,” “probably a network or hardware error”; get lost in ping-pong
    between teams; or get moved to backlogs and never resolved?'
  prefs: []
  type: TYPE_NORMAL
- en: It should be feasible to measure some of such things. For example, we can label
    issues that can’t be investigated further due to a lack of telemetry. If they
    represent a significant portion of your bugs, it’s something worth improving.
  prefs: []
  type: TYPE_NORMAL
- en: As a team, you can also do an experiment for a week or two to roughly measure
    the time spent investigating issues. How much time does it take to investigate
    when there is enough data? Or, how much time is wasted investigating issues and
    meeting a dead end due to a lack of telemetry or finding a trivial transient network
    issue?
  prefs: []
  type: TYPE_NORMAL
- en: By minimizing the time necessary to find the root cause of an issue, we improve
    the user experience. We notice incidents earlier and resolve them faster. We also
    improve our work-life balance and focus on creative work instead of grepping megabytes
    of logs.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: There could be other data, such as business analytics, support stats, public
    reviews, or anything else, showing that a noticeable number of users are leaving
    us because of unresolved technical issues. If you need to convince your organization
    to invest in observability, finding such data and showing how a better observability
    story can improve things could be a good way to approach it.
  prefs: []
  type: TYPE_NORMAL
- en: So, the first step is to understand whether current tools and processes are
    effective and have a rough understanding of how better observability could improve
    things. The next step is to understand the cost of the solution.
  prefs: []
  type: TYPE_NORMAL
- en: The cost of an observability solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can roughly break down the costs into two groups: implementation and telemetry
    backend costs.'
  prefs: []
  type: TYPE_NORMAL
- en: We need to add instrumentation, tune and customize telemetry collection, learn
    how to use new tools, create alerts and dashboards, and build new processes around
    them. When onboarding a mature and stable system, we should also consider risks
    – we might break something and temporarily make it less reliable.
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed in [*Chapter 9*](B19423_09.xhtml#_idTextAnchor148), *Best Practices*,
    we can always choose the level of detail and amount of customization to help us
    keep the costs within the given budget.
  prefs: []
  type: TYPE_NORMAL
- en: The minimalistic approach would be to start with network-level auto-instrumentation
    for actively developed services and then add context, customizations, and manual
    instrumentation as we go.
  prefs: []
  type: TYPE_NORMAL
- en: By using OpenTelemetry and shared instrumentation libraries, we can also rely
    on vendors to provide common visualizations, alerts, dashboards, queries, and
    analysis for typical technologies. As a result, it’s almost free to get started.
  prefs: []
  type: TYPE_NORMAL
- en: Telemetry backend
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can host the observability stack ourselves or use one of the available platforms.
    Either way, there will be recurring costs associated with using the solution.
  prefs: []
  type: TYPE_NORMAL
- en: These costs depend on telemetry volume, retention period, the number of services
    and instances, and many other factors, including the support plan.
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout the book, we have discussed how to optimize telemetry collection
    while keeping the system observable enough for our needs: traces can be sampled,
    metrics should have low cardinality, and events and logs can be sampled too or
    kept in cold but indexed storage.'
  prefs: []
  type: TYPE_NORMAL
- en: It’s a good idea to try a few different backends – luckily, many platforms have
    a free tier or trial period and, most importantly, you can instrument the system
    once with OpenTelemetry and pump data into multiple backends to compare the experience
    and get an idea of what the cost of using them would look like. Once you start
    relying on a specific backend for alerts or daily tasks, it will be more difficult
    to switch between vendors.
  prefs: []
  type: TYPE_NORMAL
- en: During this experiment, you will also get a better understanding of the necessary
    data retention period, sampling rate, and other parameters, and will be able to
    pick them along with the vendor.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When running a modern cloud application under scale, it’s not possible to operate
    it without an observability solution, so it’s not a question of whether you need
    one but rather how many details you need to collect and which observability vendor
    out there works best for your system and budget.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, we can start small and incrementally tune collection to add or
    remove details, while keeping it within a reasonable budget.
  prefs: []
  type: TYPE_NORMAL
- en: We should also define what success means – it could be an MTTR improvement,
    subjective user experience, on-call engineer happiness, anything else that matters
    for your organization, or any combination of these.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now talk more about the implementation details and try to make this journey
    less painful.
  prefs: []
  type: TYPE_NORMAL
- en: The onboarding process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The need for distributed tracing and visibility of all parts of the system comes
    from the complexity of modern applications. For example, we need to know how a
    serverless environment interacts with cloud storage to debug configuration issues
    or optimize performance. Or, maybe we want to know why certain requests fail in
    the downstream service without asking someone to help.
  prefs: []
  type: TYPE_NORMAL
- en: To make the most of distributed tracing, we have to onboard the whole system
    (or at least a significant part of it), making sure all services create correlated
    and coherent telemetry, write it to a place where different teams can access it,
    and reuse the same tooling to do analysis.
  prefs: []
  type: TYPE_NORMAL
- en: So, implementing an observability solution is an organization-wide effort, and
    it makes sense to start with a pilot project instrumenting a small part of the
    system. Let’s outline its scope and goals.
  prefs: []
  type: TYPE_NORMAL
- en: The pilot phase
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of this project is to get hands-on experience with observability, discover
    any significant technical or process issues early, and better understand the scope
    and effort needed for the rest of the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll need a few (at least two) services to start instrumenting:'
  prefs: []
  type: TYPE_NORMAL
- en: That are in active development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That interact with each other
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That have no (or few) internal dependencies except on each other
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That have teams working on them in close contact
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From the technical side, we’ll use this phase to make the following decisions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Instrumentation SDKs**: I hope I convinced you to use .NET platform capabilities
    and OpenTelemetry, but you probably need to decide what to do with existing instrumentation
    code and tooling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context propagation standards**: Using W3C Trace Context would be a good
    start. We may also need to decide whether and how to propagate baggage, or how
    to pass context over non-HTTP/proprietary protocols.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sampling strategy**: Rate-based, percentage-based, parent-based, tail-based
    – these are good things to decide early on and identify whether you need an OpenTelemetry
    collector or can rely on an observability vendor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Which vendor to use and a migration plan from your current one**: We’ll discuss
    technical aspects and trade-offs when instrumenting existing systems in [*Chapter
    15*](B19423_15.xhtml#_idTextAnchor233), *Instrumenting* *Brownfield Applications*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of the pilot phase, we should have a clear understanding of what
    onboarding takes, what the challenges are, and how we will solve them.
  prefs: []
  type: TYPE_NORMAL
- en: We will also have a small part of the system instrumented – it’s a good time
    to check whether we see any improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking progress
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the perfect world, after instrumentation is deployed, we’d be able to resolve
    all incidents in no time and investigate all the tricky bugs we’ve hunted down
    for months. I wish that was the case.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are at least several challenges along the way:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Change is hard**: People prefer to use tools they know well, especially when
    they’re dealing with incidents in production. It would be a good exercise to do
    the same investigation with the new observability solution after the incident
    is resolved and compare the experiences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s best to start playing with new tools at development time or when investigating
    low-priority failures. In any case, it takes time and practice to learn about
    and trust new tools.
  prefs: []
  type: TYPE_NORMAL
- en: '**You’ll discover new issues**: Looking at traces or a service map for the
    first time, I always learn something new about my code. It’s common to discover
    calls to external systems you didn’t expect (for example, auth calls made under
    the hood), unnecessary network calls, wrong retry logic, calls that should run
    in parallel but run sequentially, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Basic auto-instrumentation is not sufficient**: Without application context
    or manual instrumentation for certain libraries and scenarios, our ability to
    find, understand, and aggregate related telemetry is limited.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It will take a few iterations to see an improvement – make sure to collect feedback
    and understand what’s working and what’s not.
  prefs: []
  type: TYPE_NORMAL
- en: It also takes time and dedication. Demos, success stories, shared case studies,
    and documentation on how to get started should create awareness and help people
    get curious and learn faster.
  prefs: []
  type: TYPE_NORMAL
- en: Iterating
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'So, after the initial instrumentation, we’re not quite ready to roll it out
    to the rest of the system. Here’re a few things to do first:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tune instrumentation libraries**: Remove verbose and noisy signals or enable
    useful attributes that are off by default. If some parts of your stack don’t have
    auto-instrumentation available, start writing your own.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Add essential application context**: Finding common properties and standardizing
    attribute names or baggage keys across your organization will have a huge impact
    down the road. We’ll talk more about it in [*Chapter 14*](B19423_14.xhtml#_idTextAnchor220),
    *Creating Your* *Own Conventions*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Start building backend-specific tooling**: Alerts, dashboards, and workbooks
    will help us validate whether we have enough context and telemetry to run our
    system and migrate to the new solution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this stage, you should be able to see positive outcomes. It probably
    will not yet have moved the needle for the whole system, and there might not be
    enough data for the services involved in the experiment, but you should at least
    see some success stories and be able to show examples of where the new solution
    shone.
  prefs: []
  type: TYPE_NORMAL
- en: If you see cases where the new observability story should have helped but has
    not, it is a good idea to investigate why and tune instrumentation further. While
    iterating, it’s also worth paying attention to backend costs and optimizing telemetry
    collection if you see the potential for a significant reduction without noticeable
    impact.
  prefs: []
  type: TYPE_NORMAL
- en: The goal here is to create a good enough instrumentation approach and any necessary
    tooling around it. We iterate fast and keep the number of participating services
    small so we can still change direction and make breaking changes.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have finalized all the decisions and implemented and validated them
    on a small part of the system, we should be able to rely on new observability
    solutions for most of the monitoring and debugging needs. Before we roll them
    out, we still need to document them and create reusable artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: Documenting and standardizing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The main outcome of the pilot phase is clarity on how to make the rest of the
    system observable and the specific benefits it will bring.
  prefs: []
  type: TYPE_NORMAL
- en: 'To maximize the impact of this phase, we need to make it easier for other services
    to be onboarded. We can help them by doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Documenting new solutions and processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing demos and starters showing how to use backends and configure them,
    and add alerts and dashboards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Producing common artifacts that include any customizations:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Context propagators, samplers, or instrumentations
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Attribute names or helpers that efficiently populate them
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Starter packs that bring all OpenTelemetry dependencies and enable telemetry
    collection in a uniform manner
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Common configuration options
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we’re ready to instrument and onboard the rest of the system. It will
    probably take some time to align attribute names, configuration, or backend plans.
    We also need to keep tracking progress toward original goals and apply necessary
    changes when things don’t work. Let’s talk about a few things that can slow us
    down.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding pitfalls
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The challenge with distributed tracing and observability is that they’re most
    effective when a distributed application produces coherent signals: trace context
    is propagated, sampling algorithms are aligned to produce full traces, all signals
    use the same attribute names, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: While OpenTelemetry solves most of these concerns, it still relies on the application
    to bring all the pieces together and use coherent signals. It becomes a problem
    for huge organizations where one service deviating from the standard breaks the
    correlation for the rest of the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few things to avoid when onboarding your system:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Starting too big**: If multiple teams work on instrumentation independently,
    they will inevitably develop different solutions optimized for their services.
    Aligning these solutions after onboarding is finalized would be a difficult project
    on its own. Each team would have an impression that things work for them, but
    end-to-end customer issues would still take months to resolve.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Not sharing telemetry across the system**: When investigating issues or analyzing
    performance and usage, it’s beneficial to be able to see how other services process
    requests. Without doing so, we will end up in the same place where each cross-service
    problem involves some amount of ping-pong and problems are not resolved fast enough.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Not enforcing standards**: Inconsistent telemetry would make us come back
    to grepping logs and make customers and ourselves unhappy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Not using new tools and capabilities**: We talked about how migrating from
    familiar tooling is hard. We need to put enough effort into advocating, promoting,
    explaining, documenting, and improving things to make sure people use them. Sunsetting
    old tools, once new ones are more capable and fast, is one (sometimes unpopular)
    way to make sure everyone switches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Not using the observability stack at development or test time**: Investigating
    flaky tests is one of the cheapest ways to debug tricky issues. Traces can help
    a lot, so make sure tests send traces and logs by default and it’s super easy
    to enable tracing on dev machines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Building things that are hard to use or not reliable**: While some friction
    is expected, we should make sure most of it happens during the pilot phase. If
    you decide to build your own observability stack based on OSS solutions, you should
    expect that certain things such as navigating across tools will be difficult and
    you’ll need to put a decent amount of effort into making them usable. Another
    big investment is building and maintaining reliable telemetry pipelines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hopefully, we can avoid most of these issues and onboard a significant part
    of the system onto our new observability stack.
  prefs: []
  type: TYPE_NORMAL
- en: As we continue onboarding, we should see improvement toward our initial goals
    and may need to adjust them. During the process, we probably learned a lot about
    our system and are now dealing with new challenges we could not see before due
    to a lack of observability.
  prefs: []
  type: TYPE_NORMAL
- en: The journey does not end here. In the same way that we never stop writing tests,
    we should incorporate and leverage observability in day-to-day tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous observability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Observability should not be added as an afterthought when service or feature
    development is over. When implementing a complex feature across several services
    or just adding a new external dependency, we can’t rely on users telling us when
    it’s broken. Tests usually don’t cover every aspect and don’t represent user behavior.
  prefs: []
  type: TYPE_NORMAL
- en: If we don’t have a reliable telemetry signal, we can’t say whether the feature
    works or whether customers use it.
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating observability into the design process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Making sure we have telemetry in place is part of feature design work. The
    main questions the telemetry should answer are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Who uses this feature and how much?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does it work? Does it break something else?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does it work as expected? Does it improve things as expected?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we can rely on the existing telemetry to answer these questions, awesome!
  prefs: []
  type: TYPE_NORMAL
- en: We should design instrumentation in a way that covers multiple things at once.
    For example, when we switch from one external HTTP dependency to a new one, we
    can leverage existing auto-collected traces and metrics. A common processor that
    stamps application context on all spans will take care of traces from the new
    dependency as well.
  prefs: []
  type: TYPE_NORMAL
- en: If we use feature flags, we should make sure we record them on telemetry for
    operations that participate in an experiment. We can record them on events or
    spans, for example, following OpenTelemetry semantic conventions for feature flags
    available at [https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/trace/semantic_conventions/feature-flags.md](https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/trace/semantic_conventions/feature-flags.md).
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, default telemetry is not sufficient and we need to add custom
    events, traces, metrics, or at least extra attributes. It’s rarely a good idea
    to limit instrumentation to a new log record unless we write it in a structured
    and aggregable way.
  prefs: []
  type: TYPE_NORMAL
- en: Once a feature is proven useful and fully rolled out, we might want to remove
    this additional telemetry along with the feature flag. It’s a great approach if
    we’re sure it’s not necessary anymore. Cleaning up and iterating on instrumentation
    is another important aspect.
  prefs: []
  type: TYPE_NORMAL
- en: Housekeeping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As with any other code, instrumentation code degrades and becomes less useful
    when neglected.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the test code, any observability code we write is less reliable than
    application code – it’s also hard to notice when it reports something incorrect
    as there is no functional issue. So, validating it with testing or manual checks
    and fixing it in a timely manner is important.
  prefs: []
  type: TYPE_NORMAL
- en: This is one of the reasons to use popular instrumentation libraries – they have
    been through excessive testing by other people. Keeping your instrumentation libraries
    up to date and sharing custom ones across the company (or open sourcing them)
    will result in better instrumentation quality.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important part is to make small improvements as you notice issues:
    add missing events, spans, and attributes (don’t forget to check if there is a
    common one), structure and optimize logs, and adjust their verbosity.'
  prefs: []
  type: TYPE_NORMAL
- en: These changes might be risky. We might remove something that people rely on
    for alerting or analysis. There could be some additional guards in place that
    prevent it – code reviews, documentation, and tests, but it is rarely possible
    to account for everything, so be cautious when removing or renaming things.
  prefs: []
  type: TYPE_NORMAL
- en: Another risk is adding something expensive or verbose that would either impact
    application availability, overwhelm telemetry pipelines, or significantly increase
    your observability bill. Paying attention to the dev and test telemetry and knowing
    what’s on the hot path should prevent obvious mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: Building reliable telemetry pipelines with rate-limiting should decrease the
    severity of such incidents when they make it to production.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, observability code is not very different from any other piece
    of infrastructure. Implementing it starts with some research and experiments and
    works best when we tune and improve it along with our application.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed how to implement and roll out observability solutions
    in your organization. These efforts can be motivated and justified by the current
    monitoring infrastructure not being efficient in investigating and resolving customer
    issues.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed how we can rely on existing metrics or data to understand whether
    there is room for improvement and estimate the cost of inaction. Then we looked
    into common costs associated with implementing and running a modern observability
    solution – the easiest way to find out is to run a small experiment and compare
    different vendors.
  prefs: []
  type: TYPE_NORMAL
- en: We explored how we can approach onboarding by starting with a pilot project
    on a small part of the system and iterating and validating results before we roll
    it out to the rest of the system. Finally, we discussed the importance of incorporating
    observability into daily tasks and evolving it along with the code.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter should help you justify initial observability investments and gradually
    implement the solution across the system. In the next chapter, we’ll talk more
    about unifying telemetry collection and introducing your own standards.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Should we look for a single backend for all telemetry signals or a combination
    of them optimized for individual telemetry signals?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How would you approach standardizing baggage propagation and usage in your system?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You’re adding a cache to the service. When would you add instrumentation? How
    would you approach it?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Becoming a Rockstar SRE* by Jeremy Proffitt and Rod Anami'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
