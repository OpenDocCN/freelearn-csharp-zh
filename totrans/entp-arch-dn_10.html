<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-199"><a id="_idTextAnchor344"/>10</h1>
<h1 id="_idParaDest-200"><a id="_idTextAnchor345"/>Master Data Management</h1>
<p>In the previous chapter, we showed you a method to design information entities in such a way that they do not have any technical coupling, in an effort for the information system containing them to be free to evolve when the business changes. If the data model is a pure reflection of the business represented, it makes it much easier to follow business changes (and change is the only constant) because there won’t be some technical constraint in our way forcing us to compromise on the quality of the design, and thus on the performance of the system as a whole.</p>
<p>In this chapter, we will start talking about the implementation of the data model into something concrete (if this can be said about software, which is mostly virtual). It is only in <em class="italic">Chapters 16 to 19</em> that we will code what we will call for the rest of the book the “<em class="italic">data referential(s)</em>”. For now, we will start some actual software architecture to welcome the data model, persist the associated entities, and so on. There are many responsibilities in the data referential, and the discipline of handling these essential resources in an information system is <a id="_idIndexMarker498"/>called <strong class="bold">Master Data Management</strong> (<strong class="bold">MDM</strong>). At first sight, these responsibilities might look like those you would trust a database with, or even find in a resource-based API. But this chapter should convince you that there are many more things to the model that justify this use of a neologism like “data referential”.</p>
<p>In addition to defining the functions of the data referential, MDM is about choosing the right architecture, defining the streams of data in the overall information system, and even putting in place governance of the data, which involves finding who is responsible for what action on the data in order to keep the system in shape. Having clean and available master data may be the single most important factor in the quality of the system. Reporting cannot be done without clean data and most business processes depend on the availability of the data referential. Also, some regulatory reasons, such as accounting or compliance questions, demand high-quality data.</p>
<p>After showing the different types of data referential that you may encounter – or create – in an information system, we will finish this chapter with an overview of possible issues with data, patterns of use, possible evolution of data in time, and some other general topics that will hopefully provide you with up-to-date knowledge about the MDM architecture.<a id="_idTextAnchor346"/></p>
<h1 id="_idParaDest-201"><a id="_idTextAnchor347"/>Responsibilities around the data</h1>
<p>The <a id="_idIndexMarker499"/>concept of the data referential as a unique point of truth for the data entities of a given domain has already been explained globally, but we have not formally described what functional responsibilities are contained in it. This section will explain each of the main responsibilities and features of the referential. Looking at the responsibilities explained in the following subsections, you might ask why we’re talking about the data referential instead of simply using the better-known expression of a database, but we will see in the second part of this chapter that a referential is much more than this<a id="_idTextAnchor348"/>.</p>
<h2 id="_idParaDest-202"><a id="_idTextAnchor349"/>Persistence</h2>
<p><strong class="bold">Persistence</strong> is<a id="_idIndexMarker500"/> the responsibility that immediately comes to mind when we talk about managing data. After all, when we trust an information system’s data, the very first demand we have is that the computer does not forget it once it has learned about it. This demand is crucial, as even an electricity failure should not have an impact on it. This is why databases were invented and why engineers went such a long way to ensure the safe travel of data between memory and hard disks, both ways.</p>
<p>Persistence may be often<a id="_idIndexMarker501"/> reduced to <strong class="bold">CRUD</strong> (which stands for <strong class="bold">Create, Read, Update, Delete</strong> – the four main operations on data), but this concept is way too limited compared to the features encompassed by the data referential, though it is enough for most of the standard uses of low-importance data in the information system. Since we talk here about primary data used in many places in the information system, some other aspects of persistence have to be taken into account. The first one was talked about at length in <a href="B21293_04.xhtml#_idTextAnchor121"><em class="italic">Chapter 4</em></a> – namely, time. When one includes time in the MDM equation, storing the so-called “current” state of the data (which, most of the time, is only the last-known or best-known state of the corresponding business reality) suddenly becomes much more complicated and means, at least, storing the different states of data over time, with an indication of time to follow the history of these successive states.</p>
<p>As we explained in <a href="B21293_05.xhtml#_idTextAnchor164"><em class="italic">Chapter 5</em></a>, a good MDM is a “know it all” system that, instead of states, should store the actual commands modifying the data to enable us to retrace why the state of such an entity has evolved. This means that what will be written in the database is not a state with a date but, ideally, a “delta” command causing a change from one state to another – for example, modifying the zip code in the first address of an author in our sample information system. This way, not only can we reconstitute the state of a business entity at any time in its life cycle but we also avoid the complexity of optimistic/pessimistic locks, transactions, data reconciliation, compensation, and so on.</p>
<p>Metadata is<a id="_idIndexMarker502"/> also an important addition to the simple CRUD approach. Indeed, it is of great importance in the manipulation of master data to be able to retrieve and manipulate information linked to the data changes – for example, its author, the IP of the machine where the command came from, the identifier of the interaction that has caused this change, the actual date of the interaction, maybe also a value date if it has been stipulated by the author, and so on. This allows for traceability, which becomes more and more important for the main business entities in an information system. It also provides powerful insights into the data itself. Being able to analyze the history of the data will help you fight fraud (for example, by checking which entity changes its bank coordinates often, or limiting how many representatives of a given company can change in a given period of time). It can also help with some regulatory questions that are becoming more and more common, as we will see a bit later when talking about data deletion.</p>
<p>When talking about persistence, we often think of a given entity (and I, for one, have only been given examples of such atomic manipulations previously mentioned), but the ability to manipulate masses of data is also an important responsibility of the data referential. In most cases, this translates into being able to perform actions in batches, but the consequences are also in terms of performance management and the capacity to handle referential-wide transactions (which are very different from business entity-centered translations, which the data referential should help elimina<a id="_idTextAnchor350"/>te).</p>
<h2 id="_idParaDest-203"><a id="_idTextAnchor351"/>The question of identifiers</h2>
<p>As <a id="_idIndexMarker503"/>soon as a business entity unit is created, the question of how to identify it arises, since persistence is the capability of retrieving data that the information system has been given, and this naturally means that a deterministic way must exist to point at this given entity. At the very least, a system-wide identifier should exist to do so. It can take a lot of forms but, for the sake of applicability, we will consider the following as a URI, for example, <code>https://demoeditor.com/authors/202312-007</code> or <code>urn:com:demoeditor:library:books:978-2409002205</code>. This kind of identifier is supposed to be understood globally, by any module participating in the information<a id="_idIndexMarker504"/> system. It acts a bit as the ubiquitous language in Domain-Driven Design but allows pointing at a given entity instead of defining a business concept.</p>
<p>Of course, local identifiers may exist. For example, the book pointed at by <code>urn:com:demo</code><strong class="source-inline">
editor:library:books:978-2409002205</strong> could be stored in a MongoDB database where its technical <code>ObjectID</code> would be <code>22b2e840-27ed-4315-bb33-dff8e95f1709</code>. This kind of identifier is local to the module it belongs to. Thus, it is generally a bad idea to make it known by other modules, as a change in the implementation could alter the link and make it impossible for them to retrieve the entity they were pointing at.</p>
<p>An entity can also have business identifiers that are not local per se but bear no guarantee of being understood anywhere in the information system. The book generally identified by <code>urn:com:demoeditor:library:books:978-2409002205</code> could be retrieved only by its 13-digit ISBN <code>978-2409002205</code>; in fact, it is the variable part of the unique system identifier. However, other identifiers exist. For example, the same book can also be retrieved by its 10-digit ISBN, which is <code>240900220X</code>. Business identifiers can also be created inside the information system for particular uses. In our sample edition company, one could imagine that a serial number is applied to a book to keep track at the printing station, where batches are used and a single integer might be easier to handle than a full-blown ISBN, without risking any confusion as the workshop only prints books of the sample editor.</p>
<p>Additional technical identifiers are more often encountered, particularly in information systems with legacy software applications. Indeed, those generally insist on having their own identifiers. This way, the accounting system of <em class="italic">DemoEditor</em> might know the <code>urn:com:demoeditor:</code><strong class="source-inline">
library:books:978-2409002205</strong> book by its local identifier, <code>BK4648</code>. The ERP system might have a technical identifier of <code>00000786</code> if the book is the 786th product that has been entered into it. And so on. Of course, the dream would be that all software applications are modern and can handle an externally-provided, HTTP-standards-aligned URN. But this is rarely the case and even modern web applications seem to forget that interoperating with other applications means using the URL that they provide indiscriminately.</p>
<p>To provide a good service and account for this reality of information systems, the data referential should provide the capacity to store the business identifiers for the other software<a id="_idIndexMarker505"/> modules participating in the system. At the very least, this should be a dictionary of identifiers associated with an entity, with each value pointed at by a key that globally identifies the module in the system. For example, <code>urn:com:demoeditor:accounting</code> could be the key that points to <code>BK4648</code> and <code>urn:com:demoeditor:erp</code> could point to <code>00000786</code>. When defining the keys, there is a natural tendency to use the name of the specific software used to implement the function, and it would not matter much because the identifier is indeed specific to this software. But it still remains a good idea to stay generic in order to prepare for any cases. To give just an example, in the fusion of two administrative regions in France, it proved very useful to have such a separation. The two existing software applications for finance management were competing to have a unique market after the merger. It happened that one of the software applications was more customizable than the other and could handle external identifiers, which was part of the decision to keep it as the new unique finance management application. However, since the identifiers used by the abandoned software were prefixed by a vendor mark and the key for the software that stayed was not generic but used its name, there were some strange associations of identifiers such as <code>urn:fr:region:VENDOR1=VENDOR2-KEY</code>. Since the two brands were well-known competing companies in France and the merger of the two administrative regions caused lots of team modification and change management, this additional confusion quickly became an irritant, with people not even able to tell which software they should use to manipulate financial data. In the end, switching to a generic key such as <code>urn:fr:region:FINANCE</code> really helped, even if this sounded like a little technical move.</p>
<p>I will finish this review of identifiers with a very special case, which is the change of business identifier. Identifiers are, by essence, stable since they should be a deterministic way to point at an entity in the information system. A documented case of a change of global identifier is when a social security number is designated to a person who has not been born yet, typically because surgery is necessary on a fetus. As the first digit in French social security numbers uses the ISO gender equality standard to specify the gender of the owner, it may happen that instead of using 1 (for male) or 2 (for female), a social security number starts with 0 (for unknown). The identifier is then changed to a new one after the birth of the individual since the first number is then known (or maybe unknown in some other conditions – in this case, the norm specifies the number should be 9 for an undetermined gender). This is admittedly a very special case that provokes the change of the global, system-wide identifier. However, the architecture of the system has to be able to handle <em class="italic">any</em> existing business case (which does not mean there cannot be some manual adjustment for these cases) to be considered “al<a id="_idTextAnchor352"/>igned.”</p>
<h2 id="_idParaDest-204"><a id="_idTextAnchor353"/>The single entity reading responsibility</h2>
<p>Persistence<a id="_idIndexMarker506"/> really means nothing if data stored somewhere cannot be retrieved afterward for subsequent use. This is why reading data is the second responsibility of the data referential that we will study. This section details the different kinds of read operations and, contrarily to persisting the data, they are actually very diverse in their forms.</p>
<p>The first reading act we naturally think of is the retrieval of a unique entity, directly using its identifier. In API terms, this sums up as calling a <code>GET</code> operation on the URL that has been sent back in the response under the <code>Location</code> header when creating the entity. Or at least, this sends the latest known state of the data because parameters can be added to specify which time version of the data should be retrieved. This normally raises the question of how to get the state of data since we said we would store changes, not states. The response there can be simple or complex, depending on the level of detail we go into. If we radically apply the “<em class="italic">premature optimization is the root of all evil</em>” principle made popular by Donald Knuth, then it is enough to specify that states can be deduced from changes by applying them to the previous state and consider this recursion uses the initial state of the date, which is an empty set of attributes designated by a unique identifier.</p>
<p>I know very well that most technically minded people (and thus at least 99% of you reading this book) will always think a step further and ponder the huge performance problem the data referential would have to deal with if each <code>GET</code> operation caused the iterative application of hundreds of patches to an entity in order to find its state at some point of its life cycle. The very least we would do would be to cache the calculated states to improve on this. But when you think about it, the vast majority of read operations ask for the best-so-far state of the entity, which is the latest known state. So, to improve storage while still keeping good performance, caching the last known state of the entities is the right choice.</p>
<p>But there are, of course, some exceptions and, as has been many times explained in this book, business-justified exceptions have to be taken into account – not only because it is the goal of the alignment but mostly because these exceptions are generally great challenges on the data model and if it can accommodate them while staying simple, it means this design is mature and has a much greater chance of correctness and, hence, stability. One such exception can be when the data is often read using a <code>date</code> parameter value. In this case, improving the performance might mean storing<a id="_idIndexMarker507"/> all calculated states, but this uses lots of storage and wastes most of it, as not all states will be called in time. A good compromise might be to store only a state calculated every 20, 50, or 100 changes. This way, we can start from an existing state all the time and quickly calculate the specified state because we need only apply a few limited patches to the data. Depending on business constraints, some states that are more often used than others can be the milestones that are kept in the cache. For example, in financial applications, it is generally interesting to keep the value just before and just after the change of fiscal year.</p>
<p>Another detail that has got to be taken into account is the optional possibility of inserting modifications in the life cycle of the entity. I understand how this may sound weird to “rewrite the history” and insert changes with potential impacts on the following ones, but there are some cases where this makes sense. For example, I have seen this happen in accounting systems when errors have been made and calculation rules were reapplied to find the correct result, inserting correcting operations at the time the initial error arose. Again, this is a rare case and it should be conditioned by strict authorization rules, but the situation has to be cited for the sake of exha<a id="_idTextAnchor354"/>ustivity.</p>
<h2 id="_idParaDest-205"><a id="_idTextAnchor355"/>Other kinds of reading responsibilities</h2>
<p>There are<a id="_idIndexMarker508"/> cases when the unique system-wide identifier of a business entity is not known or has been forgotten (which means not stored outside its original referential) and, in this case, the responsibility of searching the entities corresponding to given criteria has to be used. This responsibility is often<a id="_idIndexMarker509"/> called <strong class="bold">querying data</strong>. Based on the criteria specified in the request, the operation will return a set of results, which can be an empty set or one that contains corresponding data. There can be cases when the query attributes are such that the results will always contain zero or one entity – for example, because the constraint used is a unique business identifier. But there can also be cases where the results are particularly numerous, and an additional responsibility <a id="_idIndexMarker510"/>called <strong class="bold">pagination</strong> will be quite useful to reduce bandwidth consumption.</p>
<p>Pagination can be active (the client specifies which page of data they want) but also passive (the server restricts the amount of data and provides a means for the client to request the next page of data). A standard way to implement the first approach is to use the <code>$skip</code> and <code>$top</code> attributes, as specified<a id="_idIndexMarker511"/> in the <code>$filter</code> attribute, which is used to specify the <a id="_idIndexMarker513"/>constraints reducing the query results that have been cited previously, when talking about performance in retrieving data. This book is not the place to explain the richness of this standard, which is sadly not used as often as it should be. Most API implementers indeed chose to use their attribute names, without realizing that they recreate functions (such as pagination offset, for example) that have been done so many times that they are completely normalized. Lack of interest in standards, and a “not invented here” syndrome that many developers suffer, are dragging our whole industry back. But enough ranting about this: a complete chapter has been dedicated to the importance of norms and standards, so we will just close the subject by taking you to the study of the OData standard, or in this case, the GraphQL syntax as well, since these two approaches can be seen as competing (though they are complementary one to the other, and a great API exposes both protocols).</p>
<p>Another type of reading responsibility is reporting: this can sometimes be implemented directly by the data referential but this is quite rare, as reporting is often done by crossing data coming from several business domains. Even if there are only a few of the reporting needs that demand such an external, shared-responsibility implementation, then it is better to handle all data for reporting to this entity. Depending on the technology you use, this may be a data warehouse, an OLAP cube, a data lake, or any other application. Again, the implementation does not really matter: as long as you are clean on the interfaces, you may change them any time you like with limited impact on the system.</p>
<p>In the case of reporting, these interfaces can be solicited with different time-based approaches:</p>
<ul>
<li>A synchronous, on-demand, call is always possible but generally not used for performance reasons, at least in complex reports (this is the “pull” mode). Indeed, if the reporting system needs to wait for all sources to answer and then only calculates the aggregations on its side, the results are, of course, as fresh as possible, but they may take minutes to come and this is often not acceptable by users.</li>
<li>The <a id="_idIndexMarker514"/>asynchronous, regular read of data, is the most commonly used pattern. Here, data is collected at a given frequency (once a day or more often, sometimes down to once an hour), generally by an ETL, and sent to the data warehousing system where it is aggregated and prepared for reporting. This way, reports are sent to users quicker (sometimes, they are even produced and made available directly upon data retrieval). The counterpart is that the data is not as fresh as possible, and moving the cursor to a quicker sending of data increases the consumption of resources. Optimizations are possible – for example, by reducing the transfer to only new or updated data – but this only goes some way into improving the minimal time needed to update the whole data warehouse. The greatest technical drawback of this approach is that most of the calculations are reproduced even if the source data has not changed, which is a waste of resources.</li>
<li>To go on further in the “push” approach, it is possible to use webhooks to register data refresh to an event of source data change. This way, the calculations are reproduced only when the data has changed, and the moment is as close as possible to the interaction that has changed the data, which means the reports are very fresh most of the time. Dealing with large amounts of events is a challenge, but grouping the changes into minimum packages (or with a maximum freshness time constraint) can help.</li>
<li>A very modern but technically demanding approach is to mix these “push” and “calculate on demand” strategies by using a system with queues of messages containing data changes and a dedicated architecture to apply fine-grained computations on each of these messages as needed. Such implementations of a big data approach include Kafka architectures or Apache Spark clusters. The goal here is not to detail these approaches but just to explain that they will collect all events at the source and then smartly calculate the consequences in aggregated data (the smartness being in the fact that they know the consequences and calculate only what is needed and they can balance these computations on many machines of a cluster and grouping the result in the end). They can even go as far as producing the final reports on aggregated data and making them available to end users, achieving a complete “push” paradigm.</li>
</ul>
<p>These four<a id="_idIndexMarker515"/> approaches are symbolically represented in the following schema:</p>
<div><div><img alt="Figure 10.1 – Modes of reporting" src="img/Figure_10.1_B21293.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – Modes of reporting</p>
<p>To be exhaustive on these additional reading responsibilities, indexing is another function that is used to accelerate data (and some simple aggregates) reading. It does not go as far in data transformation as big data and the preceding reporting approaches, but can already prepare a few aggregates (such as sums, local joins, and so on) and make them available through simple protocols as raw data. Indexing engines such as SOLR or Elasticsearch are generally used to accompany the data referential on the speed of data retrieval. In this case, the data referential itself concentrates on data consistency and validation rules and then handles reference data to the indexing system to make it available in quick-<a id="_idTextAnchor356"/>read operations.</p>
<h2 id="_idParaDest-206"><a id="_idTextAnchor357"/>The complex art of deleting data</h2>
<p>If deltas <a id="_idIndexMarker516"/>are stored instead of states, there is not much difference between <code>POST</code>, <code>PUT</code>, and <code>PATCH</code> operations on a resource, as they all translate into a change in the state of the entity, the particular case of a resource creation being a change from something completely empty. But as far as the <code>DELETE</code> operation is concerned, we are in a different situation. Indeed, we could blindly apply the same principle and consider that <code>DELETE</code> removes all attributes of an entity and brings it back to its initial state, but that would not be exactly true as the entity still keeps an identifier (otherwise, one would not be able to delete it). This means it is not in the same state as when it did not exist and there is no way to go back to this situation.</p>
<p>The best way to handle the situation is generally to use a particular attribute of the date stating that it is not active anymore. When using a <code>status</code> attribute to keep the value of the calculated position in the life cycle of an entity, this attribute may be used with a value such as <code>archived</code> to realize a similar operation. This is the way the data referential can store the fact that the data has been deleted without actually suppressing data (which is incompatible with what has been said previously about the data referential and its responsibility for history persistence). Of course, this creates a bit of complexity in the referential because it has to take this into account in every operation it allows. For example, reading some piece of data that is inactive should react as if the data did not exist (with a <code>404</code> result, in the case of API access), unless in the exceptional case that the user accessing the referential has an <code>archive</code> role and can read erased data. Other questions naturally arise then, such as the possibility of reactivating data and continuing its life cycle (hint: it is generally a bad idea, as many business rules are not thought to handle this very peculiar case).</p>
<p>But let’s stop this digression here and come back to the initial idea of data conservation even after a deletion command has been issued. The functional rationale behind this is mainly regulatory, such as traceability, but also prohibiting data erasing for other purposes, such as forensics after a cyber-attack. An interesting fact is that some regulations also exist that specify when data should indeed be erased for real (and not simply rendered inactive). For example, the European GDPR states that personal data should not be kept longer than some legally defined periods, depending on the processes they are associated with. In the case of personal data collected for marketing reasons (with the consent of the user, of course), the delay is generally a year. After this time, without renewal of storage consent, the <a id="_idIndexMarker517"/>data shall be erased from the information system that has collected it. That means the actual removal of the data everywhere it may be (which <a id="_idTextAnchor358"/>includes backups).</p>
<h2 id="_idParaDest-207"><a id="_idTextAnchor359"/>Relation to specialized links</h2>
<p>As always, the <a id="_idIndexMarker518"/>devil lies in details, and links can become a problem when dealing with data. Imagine we use a link between a book and an <code>author</code> entity. The simplest expression of such an RFC link is the following:</p>
<pre class="source-code">
{
    „isbn13": „978-2409002205",
    „title": [
        {
            „lang": „fr-FR",
            «value»: «Open Data - Consommation, traitement, analyse et visualisation de la donnée publique»
        }
    ],
    "additionalIdentifiers": [
        {
            "key": "urn:com:demoeditor:accounting",
            "value": "BK4648"
        }
    ],
    "links": [
        {
            "rel": "self",
            "href": "https://demoeditor.com/library/books/978-2409002205"
        },
        {
            "rel": "author",
            "href": "https://demoeditor.com/authors/202312-007",
            "title": "JP Gouigoux"
        }
    ]
}</pre> <p>The links are often inherited from specialized links – in our case, a specialized author link that could contain additional important information in its schema, for example, extract restricted to the portion of JSON that has changed, for readability purposes:</p>
<pre class="source-code">
{
    "rel": "author",
    "href": "https://demoeditor.com/authors/202312-007",
    "title": "JP Gouigoux",
    "authorMainContactPhone": "+33 787 787 787"
}</pre> <p>Having <a id="_idIndexMarker519"/>additional information in links is useful when you know this is a piece of information that will frequently be used when manipulating the link, as it avoids an additional roundtrip to the other API to find this information. Of course, there should be a right balance, and including the phone number here is questionable because it can be considered volatile data, not changing frequently but on some specific occasions in the mass of authors of an editor’s database. The consequence is that all links should – in this case – be updated, which accounts for quite a large amount of work. When you know it is a piece of data that does not change (the author’s name, for example, does not change very often) or even data that should never be changed for regulatory reasons (an approved version, for example, should not be modified even if further versions appear), there is no such problem.</p>
<p>This is the first <a id="_idIndexMarker520"/>issue that should be taken care of with links. The second one is more subtle: since the <code>title</code> attribute (which is not an extended one added through inheritance but exists in the standard RFC link definition) has been used to store the common designation of the author, as expected from the definition of this attribute in the RFC, deleting an author will end up with their name still existing in the book’s data referential through these links. This may be interesting for archiving reasons (even if we do not deal with this author anymore, for example, even though they have died, the books are still in their name). However, in some other regulatory contexts, this can be a tough problem: if we go back to the example of the European GDPR “right to be forgotten” for personal data, that means that when the author is deleted from the database, we should also go over all the books they authored and replace the <code>title</code> contents with something like <code>N/A (GDPR)</code>. This is how <code>DELETE</code> operations can work under specific func<a id="_idTextAnchor360"/>tional circumstances!</p>
<h2 id="_idParaDest-208"><a id="_idTextAnchor361"/>So-called secondary features</h2>
<p>Though <a id="_idIndexMarker521"/>we might think we have covered all the responsibilities of the data referential since we passed on the four letters of the CRUD acronym, the spectrum of a good application is much larger. To be thorough, we should talk about all the functions that are generally called “secondary,” even though they are critical and – in some cases – equally important as the persistence of the data itself.</p>
<p>The first one of these<a id="_idIndexMarker522"/> additional features is <strong class="bold">security</strong>. There should be no doubt about the importance of this one anymore, but if it is necessary to convince anyone, let’s just stress the fact that the four criteria commonly used in security categorization are all about data:</p>
<ul>
<li><strong class="bold">Availability</strong>: The <a id="_idIndexMarker523"/>data should be available to authorized persons, which means denial of service (among others) has to be treated. Though unavailable data is a good way to prevent leakage or unauthorized access, it remains the primary criterion, as the whole idea is to provide a service in a solid way. Availability also means that a simple mishap should not get the whole system offline.</li>
<li><strong class="bold">Integrity</strong>: The <a id="_idIndexMarker524"/>data should not be tampered with by anyone and its correctness should be guaranteed – the consequence being that all functions underlying the service have to be secured as well (database, network, source code, etc.).</li>
<li><strong class="bold">Confidentiality</strong>: This<a id="_idIndexMarker525"/> is the counterpart of the first criterion, as access should be forbidden to non-authorized requesters. It is the basis for authorization management systems (more on this in the next chapter).</li>
<li><strong class="bold">Traceability</strong>: This <a id="_idIndexMarker526"/>criterion is a more recent one but becomes more and more important with regulations on IT systems; it states that the modification and use of data should be stored in a log that cannot be tampered with, allowing it to retrieve what happened back in time. Traceability is most important after an attack has happened to understand where the vulnerability was and what the attackers have done.</li>
</ul>
<p><strong class="bold">Performance</strong> and <strong class="bold">robustness</strong> are<a id="_idIndexMarker527"/> also so-called secondary features that have a high importance in MDM. They are<a id="_idIndexMarker528"/> very much linked to the first criterion (availability). Indeed, the robustness of the software underpins its capacity to answer requests in time with great confidence, and performance is a quality associated with the availability of the data. After all, if someone gets a response to their request for<a id="_idIndexMarker529"/> data after 5 minutes, they would not think of the service as being available, though it could be qualified as such since the data indeed arrived… at some point. Rapid availability of data has often been a drive to move existing “manual” information systems to a software-oriented approach.</p>
<p>Dealing with these features is the subject of many books, so we will just leave it there for now, since those are indeed responsibilities expec<a id="_idTextAnchor362"/>ted from the data referential.</p>
<h2 id="_idParaDest-209"><a id="_idTextAnchor363"/>Metadata and special kinds of data</h2>
<p>Finally, the<a id="_idIndexMarker530"/> data referential should handle data and also metadata. Metadata <a id="_idIndexMarker531"/>is all the information that sits around the data entities themselves and allows for a good comprehension of them. This provides some additional richness to the data, but please be aware that metadata should have a different life cycle from the data itself. For example, storing information about the history of data is not metadata, though it can abide by the definition of metadata just given. As has been exposed many times now, the data referential keeps track of every change in the entities it hosts. So, information about who changed what at what times is data and not metadata for a complete and correct data referential. In the same way, dates of changes, indicators of modification, or reading frequency can be directly deduced from the series of operations in the data referential, so they are not metadata either.</p>
<p>A good example of metadata is the units associated with numeric data. Having a number in a named attribute of the entity is often not enough. Sure, the attribute can have a name that describes its content and also the units (examples would be <code>populationInMillions</code>, <code>lengthInMillimeters</code>, or <code>nbDaysBackupRotation</code>), but that does not make it any easier to manipulate the values and, in addition, that makes for longer names, which can be a bit cumbersome when the unit sounds obvious. Having metadata somewhere in the schema of the referential that states that <em class="italic">this</em> attribute of <em class="italic">this</em> entity uses <em class="italic">this</em> unit is a better way to communicate the handling of the data, and can also help in some modern database engines to directly calculate formulas between attributes that are not on the same scale of units, and even provide some warning when the formulas are not safe in terms of units definition, such as adding meters and seconds. These new servers generally use a standard definition of units that includes the powers in <code>kN</code> unit is associated with M1K1S-2 as seen previously, but with a multiplier of 10^3 and the name <code>kiloNewton</code>.</p>
<p>Geographical attributes are another good example of metadata addition to the usual data in a database. Generally, longitude and latitude were expressed as double-precision numbers in <code>lon</code> and <code>lat</code> attributes, but this did not account for the kind of world-map projection (which can create some discrepancies in the number) and would not prevent silly computations such as adding the two numbers. With database or geographical servers able to understand the metadata added to the coordinates data, it is now possible to calculate distance, transpose coordinates from one projection system to another, and so on.</p>
<p>Metadata is <a id="_idIndexMarker532"/>the long-forgotten <a id="_idIndexMarker533"/>cousin of data. Apart from CMIS, the standard for electronic document management systems, where they enjoy first-order citizenship (supporting groups of metadata implemented in schemas that can be applied to the documents, used in the queries when searching, and sometimes even versioned independently of the documents supporting them), there are not that many standards that formalize them. The evolution of this depends entirely on engineers who are interested in doing their jobs in a professional and clean manner. As long as “quick and dirty” tricks are used in the software programming and the structuring of information systems, metadata will continue to be set aside. When people – hopefully after reading this book and some others advising in the same quality and long-term approach – decide that the burden of the coupling is too high and they have to address the problem by modernizing their information system, metadata use should naturally rise, making it in time as standard and usual as any other practice.</p>
<p>Now that we know how the data referential should be defined, we will dive into how this can<a id="_idTextAnchor364"/> be provided by a software system.</p>
<h1 id="_idParaDest-210"><a id="_idTextAnchor365"/>The different kinds of data referential applications</h1>
<p>We will not talk <a id="_idIndexMarker534"/>about technical aspects in this section (this is the role of the following one, called <em class="italic">Some architectural choices</em>) but about architectural strategies to structure the data persistence.</p>
<p>In the previous chapter, the metaphor of the flower was introduced to show how data can be organized inside an entity. We will follow this idea to represent how persistence can be implemented in the data referential that manages instances of such an entity. Before we dive into the main architectures, please remember that the main criteria of choice should always remain functional, which, in the case of data, means that the life cycle in your system is what will drive you principally into this or that architectural choice. Also keep in mind that the <em class="italic">people</em> aspect of data management is as important as the <em class="italic">technical</em> aspect; governance, designation of people responsible, and good communication about which team owns which pieces of data are essential to the correc<a id="_idTextAnchor366"/>t use of data in your organization.</p>
<h2 id="_idParaDest-211"><a id="_idTextAnchor367"/>Centralized architecture</h2>
<p>The centralized (or “unique”) referential is the simplest one (as shown in <em class="italic">Figure 10</em><em class="italic">.2</em>) that <a id="_idIndexMarker535"/>everybody first thinks of and that solves so many problems in the information system when it can be applied: it consists of having a single storage mechanism for every bit of data concerning a <a id="_idIndexMarker536"/>given type of entity (including, of course, history, metadata, and so on). This way, all services working in the system know that, when needing to read or write something, they have to address their request to a single repository service, as the whole “flower” is in one well-known place.</p>
<div><div><img alt="Figure 10.2 – Centralized data referential architecture" src="img/Figure_10.2_B21293.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – Centralized data referential architecture</p>
<p>The great thing about this approach is that it simplifies the work for everyone in the information system. Of course, this <a id="_idIndexMarker537"/>constitutes a <strong class="bold">single point of failure</strong> (<strong class="bold">SPOF</strong>) and, if the implementing application is down, all applications needing this referential information will be impacted. But this is just a technical problem, with many battle-proven solutions such as active/active synchronization of the database, scaling of the application server, redundancy of hardware, and so on. By now, you should also be convinced that the functional aspects are <a id="_idIndexMarker538"/>always more important to take into account than the technical ones. As technicians, we tend to focus on low-occurrence problems such as hardware failure or a locked transaction, whereas the immensely <a id="_idIndexMarker539"/>greater problems in information systems nowadays are duplicates of data, poor cleanliness of the inputs, and other commonly observed issues that urgently need to be addressed. The SPOF might be more important in the people organization: a centralized data referential might mean that a single team or even a single person is in charge of the management of this set of data, and some drawbacks are always possible with too much centralization (feedback not taken into account, the re<a id="_idTextAnchor368"/>lative obscurity of the changes, etc.).</p>
<h2 id="_idParaDest-212"><a id="_idTextAnchor369"/>Clone architecture</h2>
<p>One way to address<a id="_idIndexMarker540"/> this SPOF limitation is to locally copy some of the data that is needed by important applications. In this case, some applications will keep part of the “flower” in their own persistence system, and it is their choice to manage how fresh the data should be compared to the central referential, which remains the global single version of the truth.</p>
<p>When<a id="_idIndexMarker541"/> the data is initially scattered around an information system, it can be a first step toward cleaning it, by obeying centralized business rules while still keeping the data as it was stored. The advantage is that, for legacy applications, nothing changes: they still consume the data locally, so all reading functions work as before. With some effort, writing commands can even be kept in the software – for example, by using database triggers that will implement a return of data to the unique referential. Most of the time, though, and particularly if the application is composable and has a unique graphical interface to create entities, it is easier to plug the referential GUI into this application instead of the legacy form.</p>
<p>The main difficulty with this approach is consistency: as there are several copies of data in the system, discrepancies can happen and it is thus important to keep them as reduced in time and impact as possible. If applications are well separated in function silos, it can end up being very easy, but if the way the application has been decomposed is bad, then you may have to implement distributed transactions, which can be quite complicated. Eventual consistency will be your friend in this situation, but it may not be applicable everywhere.</p>
<p>The most <a id="_idIndexMarker542"/>efficient form of the clone architecture is the following one, where the cloning of the data (only part of the flower, as only a partial set of the petals are normally useful) is synchronously based on events in the data referential and the data modification GUI has been replaced by the one coming from the centralized data-managing application:</p>
<div><div><img alt="Figure 10.3 – Cloned data referential, the most efficient form" src="img/Figure_10.3_B21293.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – Cloned data referential, the most efficient form</p>
<p>An option <a id="_idIndexMarker543"/>in this form is to add a synchronization mechanism for all the data, which compensates at night for messages of data change that could be skipped during the day due to network micro-failures or such low-frequency but still existing incidents if one does not want to put a<a id="_idIndexMarker544"/> full-blown <strong class="bold">message-oriented middleware</strong> (<strong class="bold">MOM</strong>) to work for this simple stream.</p>
<p>An alternative to the first form is when the synchronization connector uses an asynchronous, typically time-based mechanism to keep the clone database similar to the referential information. The best approach in this case is to call the data referential APIs, as they give the best quality of information:</p>
<div><div><img alt="Figure 10.4 – Cloned data referential, with asynchronous alternative" src="img/Figure_10.4_B21293.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4 – Cloned data referential, with asynchronous alternative</p>
<p>An <a id="_idIndexMarker545"/>often-seen alternative (but I really do not recommend it) is to have an ETL perform the synchronization, as shown in <em class="italic">Figure 10</em><em class="italic">.5</em>. This<a id="_idIndexMarker546"/> is often seen in companies that have invested lots of money in ETL to keep data in sync with their system and use this tool for everything. When there is an API (and every good data referential should have one), it is better to not couple ourselves directly on the data. Sadly, lots of companies still have this kind of stream in place, starting their own “spaghetti dish” of an information system, with all responsibilities and streams of data entangled and not clearly defined (see <a href="B21293_01.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a> for more explanation on this).</p>
<div><div><img alt="Figure 10.5 – Cloned data referential, using an ETL (not recommended)" src="img/Figure_10.5_B21293.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5 – Cloned data referential, using an ETL (not recommended)</p>
<p>As explained <a id="_idIndexMarker547"/>previously, some implementations cannot be changed and have to rely on their legacy GUI. In <a id="_idIndexMarker548"/>this case, the only possible approach is to rely on specific triggers on the database to get the creation and modification commands and send them as requests to the MDM application:</p>
<div><div><img alt="Figure 10.6 – Cloned data referential, with legacy GUI still in place" src="img/Figure_10.6_B21293.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.6 – Cloned data referential, with legacy GUI still in place</p>
<p>The difficulty in this approach is when the data changes in the data referential due to some business rules, as the change cannot be sent back to the GUI. Indeed, most applications will keep the state of the data when they have submitted the change to their server. Even for the rare applications that listen to the returned data by their back office, the difficulty is that the complete roundtrip will not be finished before this reading, and the “updated” data will only be the latest in the local database, but not the latest that <a id="_idIndexMarker549"/>will come back moments later from the webhook callback. When stuck in this situation, it is best to explain to the users that this is a temporary situation before reaching the centralized referential architecture and that they can refresh their GUI a bit later to see the effects of their change. Even better, learn how to use the new<a id="_idIndexMarker550"/> centralized referential, which will always give them the freshest information, at the price of using two graphical interfaces instead of one (which is not such a high price when those are web applications that can be opened in two browser tabs).</p>
<p class="callout-heading">Important note</p>
<p class="callout">In <a href="B21293_08.xhtml#_idTextAnchor271"><em class="italic">Chapter 8</em></a>, we briefly talked about enterprise integration patterns. They are the ideal bricks to construct the synchronization connectors that we talked about previously, particularly <a id="_idIndexMarker551"/>if a <strong class="bold">message-oriented middleware</strong> (<strong class="bold">MOM</strong>) solution is put in place during the project of information syst<a id="_idTextAnchor370"/>em reorganization/data referential structuring.</p>
<h2 id="_idParaDest-213"><a id="_idTextAnchor371"/>Consolidated and distributed architectures</h2>
<p>This type of<a id="_idIndexMarker552"/> referential <a id="_idIndexMarker553"/>consists of exposing, from a central point of view (an API, generally), data that is actually placed into different parts of the information system. Generally, the core of the flower and some petals are in the data referential dedicated persistence. But for other petals, persistence can stay in the business applications they are associated with because it is considered they know the content of these petals better. In the most collaborative form of this approach, the referential exposes the full data for every actor of the information system and shares ownership of the petals:</p>
<div><div><img alt="Figure 10.7 – Consolidated referential architecture" src="img/Figure_10.7_B21293.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.7 – Consolidated referential architecture</p>
<p>The<a id="_idIndexMarker554"/> data referential can produce an entire flower of data through, and expose it in, its API but that means it <a id="_idIndexMarker555"/>has to consume the different petals it does not own from the business applications (keeping a local cache of these petals is a choice of implementation based on freshness, rate of change, and performance but does not change the ownership of the data). To expose the whole flower with fresh content, the data referential needs to have access to its own database, and also to the business applications data (or, again, the cache it may keep locally).</p>
<p>Also, some applications, such as <code>App2</code> in <em class="italic">Figure 10</em><em class="italic">.7</em>, may not need anything other than the petal they own (notice that, of course, everyone has the core of the flower, by definition). Some applications, such as <code>App1</code>, may need some additional petals, and in this case, they have to call the data referential API to obtain this data.</p>
<p>Another difference has been made in <em class="italic">Figure 10</em><em class="italic">.7</em> to show that the data referential may use a business application API to obtain the data (best case) or may resort to direct access to the database of the business application, which causes more coupling but is sometimes the only way to go. The alternative shown on the right is dangerous and should not be applied: in this case, <code>App3</code> is not talked to but this is not the main problem. The actual issue is that using an ETL to feed the referential database should never be done, as this shortcuts the business and validation rules inside the data referential. No application should ever touch the referential database but the referential application itself. In fact, this rule is so important that, when deploying on-premises, it is a good practice to hide, obfuscate, deny access, or use any other possible way to prevent anyone from directly accessing a referential database. The results are already bad enough when this is a “normal” database, with its trail of coupling and other bad consequences; doing so on such an important database is the recipe for problems.</p>
<p>When the <a id="_idIndexMarker556"/>data referential <a id="_idIndexMarker557"/>exposes all the data possible on an entity (the complete “flower”), the architecture is also called “consolidated.” It is possible, in some cases, that some bits of data are only useful by the owner application and will not be of any use to anyone else. In this case, the term “consolidated” is not appropriate as some data is – willingly – not available, and the referential should be considered “distributed” only. Such a situation would be schematized as follows:</p>
<div><div><img alt="Figure 10.8 – Distributed referential architecture" src="img/Figure_10.8_B21293.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.8 – Distributed referential architecture</p>
<p>The main<a id="_idIndexMarker558"/> difficulty of a distributed referential architecture is to maintain performance. Optimizations are of course possible, such as the cache mechanism we talked about or the parallelism of calls to the different business applications when no cache is used, but all of these technical additions come with a price that should not be underestimated, particularly when we know that the situation is temporary and that the goal is a centralized architecture. It often happens that a “temporary” situation, supposedly cheaper and made as a stepping stone to the next architecture, actually costs as much as directly putting in place the target architecture. Most of the time, the decision comes from the fact that the difficulties of the target vision are well known, but the ones associated with the intermediate step are less envisioned, mostly because these unstable situations are numerous and thus not as well documented as the final architecture.</p>
<p>Let me give<a id="_idIndexMarker559"/> you an example of how hard it can be to set up an intermediate distributed system, by talking about the pagination of data. When calling the data referential API with a <code>$top=10</code> query attribute, if the referential is distributed and consolidates data from two business applications, it will have to make two requests to the application, but the limiting thing is that, depending on the order of the data requested by the <code>$order</code> attribute, there may be zero data coming from one source and 10 pieces from the other one, or the other way around, or any situation between these two extremes. This means that the gateway in charge of merging the data will have to take 10 lines from one application and 10 lines from the other, then re-apply an ordering algorithm on the 20 lines, finally sending the first 10 to the requesting client and discarding the following 10 lines.</p>
<p>Do not<a id="_idIndexMarker560"/> think it would be easier to use a local cache, as you would have to implement the query mechanism on it in addition to the ordering algorithm just talked about. Imagine if this has to be done with more applications! With 5 business applications, you already cache 50 lines in order to actually use only 10, which is an 80% waste of resources. You may think of pre-querying the applications in order to know which will provide data out of the filtered values, but that means you should already query one application and then adjust the counting querying to the other ones, maybe to realize that the optimization will not reduce the number of queries but only the number of lines retrieved. The choice of a pivot application may be difficult in itself for a resulting improvement that may be weak since we deal with reduced sets of data anyway (this is the goal of paginating the requests). Wait! We have not talked yet about the worst part of it: when paginating for the 10th page of data (between 90 and 100, if we stay on 10-line pages), you will not be able to simply call 10 lines from each of the 5 applications, because there may be one application that will account for almost all the lines in the order applied since the beginning of the pagination, and some others will provide nothing in the same range. This means that you may very well have the first result coming from an application only when calling the 10th page! You now see it coming, don’t you? Yes, we will have to query the 5 applications for 100 lines to extract the 10 lines corresponding to the 90 to 100 range of the aggregated data, which means a huge waste of 98%… and, the cherry on this sad cake is that, if an application does not support dynamic range, you will have to query it several times in order to compose the complete range of data needed. Sure, it may be possible with some implementations to keep cursors on the database queries in the state, but that means that your application is now stateful, and this will account for some other <a id="_idIndexMarker561"/>technical limitations in terms of scalability. Well, the only thing that will save us there is that, generally, the users will stop at the second or third page of data, refining their <code>$filter</code> attribute to reach quicker results.</p>
<p>Consistency<a id="_idIndexMarker562"/> problems also exist, but they are a bit easier to deal with as long as the cutting of data follows a functionally logical order. This is generally the case because the distribution of data is done in business applications, so the risk that they have duplicate data (apart from the core of the flower, <a id="_idTextAnchor372"/>of course, which is always shared) is normally very low.</p>
<h2 id="_idParaDest-214"><a id="_idTextAnchor373"/>Other types of referential architectures</h2>
<p>A “virtual” data referential <a id="_idIndexMarker563"/>is a particular case<a id="_idIndexMarker564"/> of a “distributed” referential where the central part simply holds no data by itself, and thus has no persistance, relying on the surrounding business applications databases. Schematically, this is the following state:</p>
<div><div><img alt="Figure 10.9 – Virtual referential architecture" src="img/Figure_10.9_B21293.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.9 – Virtual referential architecture</p>
<p>Other, more exotic, referential architectures exist but it does not sound really useful to expose them here. For those of you who are curious, the French government-issued document called <em class="italic">Cadre Commun d’Architecture des Référentiels</em> (common framework for <a id="_idIndexMarker565"/>referential architectures, freely available on the internet and in the French language) should not be a limitation, as the different possibilities are shown using mainly diagrams.</p>
<p>Now the architecture patterns have been shown, we can talk about the implementation itself, including what technical choices <a id="_idTextAnchor374"/>should be made and how when creating the data referential.</p>
<h1 id="_idParaDest-215"><a id="_idTextAnchor375"/>Some architectural choices</h1>
<p>One of the first is, of course, the database. By the way, I should even say the persistence mechanism because a database is a very well-known persistence mechanism, but there are others, as we will see at the end of this section. Some other technical considerations will have to be dealt with – in particular, on the streams of data.</p>
<p>This section<a id="_idIndexMarker566"/> will also be an opportunity for a little rant about the dogmas in IT, and how they delay the long-awaited industrialization of information systems. Lots of technical choices remain based on the knowledge of the teams available rather than on the adequateness of the functional problem at hand. This is not to say that competencies should not be taken into account, but training should sometimes be forced on technical people who have not changed their way of thinking for decades and may hinder your information system development because they simply apply the wrong tool to the problem. You have likely heard the proverb “When all you have is a hammer, all problems look like nails.” If you have this kind of person in your team, a manager’s job is to open their eyes through <a id="_idTextAnchor376"/>training, whether it be internal, external, formal, or not.</p>
<h2 id="_idParaDest-216"><a id="_idTextAnchor377"/>Tabular versus NoSQL</h2>
<p>One of the <a id="_idIndexMarker567"/>very first decisions one has to make when implementing the data referential is the kind of database paradigm to use. Should it be tabular or document-oriented? SQL or NoSQL? Knowing that the natural shape of 99% of business entities is a document structure with many levels, like a tree of attributes and arrays with varying depth, the obvious choice if you want to reach business / IT alignment should be a NoSQL database, adapted to the shape of your data: document-based NoSQL if you manage business entities, or graph-based NoSQL if you manipulate data entities linked to other entities by many typed relationships, causing a network of entities that can be traversed by many paths, and so on.</p>
<p>If one really<a id="_idIndexMarker568"/> applies business/IT alignment and looks for a persistence mechanism that closely mimics the shape of their data, SQL tabular databases should be used for business entities that are naturally tabular… which is almost never! Sure, there are cases, just like there are some for key-value pair lists in the NoSQL domain, but they are very scarce. In fact, it looks like the main reason SQL is still largely used for the data referential is simply history. And this is a justified reason when dealing with legacy software… After all, if it has worked for years this way, you are better off not touching it. But the real problem is when a new data referential, designed during a project of information system modernization, also uses a non-efficient approach.</p>
<p>Why do I say <em class="italic">non-efficient</em>? The history of computer science and databases should be invoked in order to explain why… In the old times of data storage, when spin disks were used with random-access controllers, data was not randomized in the magnetic disks but placed in sequences of blocks (preferably on the outermost lines of the hard disk, as the linear speed was higher, providing for quicker reads). In order to quickly access the right block, database engines would force the size of a line of data in order to quickly jump to the next, knowing the total length of each line of data in advance. This is why old types of strings in the database required a fixed length, by the way. This is also why the data has to be stored in tabular blocks, and structured data decomposed into many tables where lines are related to each other by keys, as this was the only way to calculate the next block index.</p>
<p>These assumptions came with a high price, though: since data was tabular, the only way to store multiple values for an attribute of an entity was to create another table in the database and join the two lines of data. The consequence of this was that complicated mechanisms were necessary to handle global consistency, such as transactions. In turn, transactions made it necessary to create the concepts of pessimistic and optimistic locks, then manage isolation levels for transactions (as the only fully <strong class="bold">ACID</strong> ones, which are the serializable transactions, have a dramatic impact on performance), then deadlock management and so many other complicated things.</p>
<p>When you think about it and realize that hard disk controllers have been providing randomized access for decades (and the very concept of a spinning disk does not exist in SSD), it is hard to understand why the consequences of this remain so pervasive today. One of the reasons is the change management, as nobody likes changing. But if there is a job where one should adapt and embrace change, that should definitely be a developer. I can also understand that SQL is still used in workshops where people only know this as a persistence technique. It is much better to start an important work with maybe not the best tool but one that is well known by <a id="_idIndexMarker569"/>the whole team, and I would not advise starting with a complex technology that nobody knows. But in this particular case of not using NoSQL for a business entity data referential, there would be two problems:</p>
<ul>
<li>First, this would be a training problem, as these technologies have been here for more than a decade now, and returns on experience are perfectly established, with trustworthy operators.</li>
<li>Second, there are actually few technologies as easy as document-based NoSQL. Take MongoDB, for example – writing a full-fledged JSON entity into a MongoDB-compatible database is as simple as follows (example in C#):<pre class="source-code">
MongoDBConnection conn = new MongoDBConnection(ConnectionString);
conn.Insert("Actors", "{ 'lastName': 'Gouigoux', 'firstName': 'Jean-Philippe', 'addresses': [ { 'city': 'Vannes', 'zipCode': '56000' } ] }");</pre><p class="list-inset">The equivalent with <a id="_idIndexMarker570"/>an SQL-based tabular <strong class="bold">RDBMS</strong> (short for, <strong class="bold">Relational Database Management System</strong>) is the following:</p><pre class="source-code"><strong class="bold">SQLConnection conn = new SQLConnection(ConnectionString);</strong>
<strong class="bold">SQLTransaction transac = new SQLTransaction(conn);</strong>
<strong class="bold">try {</strong>
<strong class="bold">    transac.Begin();</strong>
<strong class="bold">    SQLCommand comm = new SQLCommand(conn, "INSERT INTO ACTORS (lastName, firstName) VALUES (@lastName, @firstName)");</strong>
<strong class="bold">    Comm.Parameters.Add(new SQLParameter("@lastName", "Gouigoux"));</strong>
<strong class="bold">    Comm.Parameters.Add(new SQLParameter("@firstName", "Jean-Philippe"));</strong>
<strong class="bold">    string idActor = Comm.ExecuteGetId();</strong>
<strong class="bold">    comm = new SQLCommand(conn, "INSERT INTO ADRESSES (id, city, zipcode) VALUES (@id, @city, @zipcode)");</strong>
<strong class="bold">    Comm.Parameters.Add(new SQLParameter("@id", idActor);</strong>
<strong class="bold">    Comm.Parameters.Add(new SQLParameter("@city", "Vannes"));</strong>
<strong class="bold">    Comm.Parameters.Add(new SQLParameter("@zipcode", "56000"));</strong>
<strong class="bold">    Comm.Execute();</strong>
<strong class="bold">    transac.Commit();</strong>
<strong class="bold">} catch (Exception ex) {</strong>
<strong class="bold">    transac.Rollback();</strong>
<strong class="bold">    throw new ApplicationException("Transaction was cancelled", ex);</strong>
<strong class="bold">}</strong></pre><p class="list-inset">And I am not even talking about <a id="_idIndexMarker571"/>the <strong class="bold">Data Definition Language</strong> (<strong class="bold">DDL</strong>) commands to create the tables and columns, which would add many lines. MongoDB does not need any, as it is schemaless and collections are created as objects are added.</p></li> </ul>
<p>Again, there<a id="_idIndexMarker572"/> are cases where SQL is needed. Reporting tools are very numerous using this grammar and it is good practice to expose SQL endpoint to access data, as it eases its consumption. Big data tools and even NoSQL databases have SQL endpoints. This is valuable as there are lots of people who are competent in using this way of interrogating data and computing complex aggregations. However, choosing a tabular database to store structured data just in order to be able to use a well-known query language is a problem, as it will cause lots of unwanted complexity. In your next data referential, please consider using NoSQL, as you will gain a lot of time with it. And if you know this kind of project will arrive next on your project portfolio, start getting training for your team. Only a few days are required to understand everything that is needed to be proficient with document-based NoSQL servers such as MongoDB, <a id="_idTextAnchor378"/>and they are extremely well adapted to storing business entities.</p>
<h2 id="_idParaDest-217"><a id="_idTextAnchor379"/>CQRS and event sourcing</h2>
<p>While we <a id="_idIndexMarker573"/>are at it, you may also want to ditch your old data stream architectures where reading and writing are handled by the same process. After all, these two sets of operations are so different in their frequency (most <strong class="bold">Line Of Business</strong> (<strong class="bold">LOB</strong>) applications have 80% of reads and 20% of writes), functions (no locks necessary for reading, consistency needed for writing), and performance (low importance for unique writing, very important for massive queries) that it sounds logical to separate them.</p>
<p>This is what <strong class="bold">Command and Query Responsibility Segregation</strong> (<strong class="bold">CQRS</strong>) is about: it separates<a id="_idIndexMarker574"/> the storage system receiving the commands for altering or creating the data from the system ready to answer queries on the same data. Event sourcing is closely associated with this architectural approach as it stores a series of business events generated by writing commands and lets the queries use this storage to get the aggregated results they need in a highly scalable way, thus allowing performance on large data.</p>
<p>In some way, CQRS could be thought of as a type of referential architecture between the distributed and the clone approaches. It does not separate data between applications with a criterion that is on the data itself, but rather on the kind of operation that is going to be performed on it (mainly, write or different kinds of reads). At the same time, the prepared reading servers can be considered as clones of the <em class="italic">single version of the truth</em> data. As their number can rise without any limit since the single version of the truth is in the main persistence, the performance can always be adjusted, however complex the queries and with high volumes as well.</p>
<p>Again, this is not the place to discuss these subjects in detail but they had to be cited in a chapter about data referential and MDM, as they are th<a id="_idTextAnchor380"/>e indisputable best approach to implementing high-volume solutions.</p>
<h2 id="_idParaDest-218"><a id="_idTextAnchor381"/>One more step beyond databases – storing in memory</h2>
<p>Let’s come back to our discussion about the origin of the tabular database system and even a bit before. Why<a id="_idIndexMarker575"/> do we actually need database and storage systems? Mostly because hard disks can store more data than only RAM, and because databases would not fit in small amounts of RAM. Thus, it is necessary to have a system that is good at quickly putting data on disk (in order to keep it safe in case of hardware failure, the database first writes in the log files) and good at retrieving some parts of data from the disk and putting them back into memory for application use (this is the SQL part and, in particular, the role of the <code>SELECT</code> and <code>WHERE</code> keywords).</p>
<p>Of course, this was a major problem when computers had 640 kilobytes of RAM and databases would need a few megabytes. But how about today? Sure, there are huge databases, but we commonly have databases with a few gigabytes only. And how about server RAM? Well, it is very common to have servers with tens of gigabytes, and it is easy to acquire online servers with 192 GB RAM. In this case, why is there a need for manipulating data in and out of the disks? Sure, SSD disks are some kind of memory, but they are still slower than RAM. Also, there is indeed this persistence under hardware failure that has to be taken care of. But how about the manipulation of data itself? Would the manipulation of queries into RAM not go much quicker?</p>
<p>In fact, it does and there is a rarely-used and scarcely-known technique called “object prevalence” that acts as an in-memory database. We are not talking about files stored in a RAM disk or a high-speed SSD, but having the data used directly in the object-oriented model of your application. How can we be sure not to lose any data if there is a hardware failure, you might ask? Well, exactly as a database does: by keeping a disk-based log of all the commands sent to the system. The difference then is that the reference model for manipulating data and extracting, filtering, and aggregating results is not on some tabular writing on the disk that has to be accompanied with indexes in order to improve performance, but directly in the RAM, and in a binary format that is the one directly used by your application, which means nothing can go faster. By doing so, requests in SQL are replaced by code in your language of choice – for example, C# with LINQ queries.</p>
<p>It is quite astonishing that object prevalence has never reached a wider audience but all the people I know who used it were convinced of its high value. Personally, when I need to implement a data referential that is limited in volume but has one of the following requirements, I always go for this technology:</p>
<ul>
<li>High performance required</li>
<li>Very complex queries that would be hard to write in SQL</li>
<li>A data model that evolves often</li>
</ul>
<p>One of the best data referential implementations that I have participated in was on a project calculating advanced financial simulations and optimizing them with genetic algorithms; the performance boost was huge and the ability to write extremely complex cases of data manipulations made the whole project a clear win for the customer, who was surprised in the first test drives by the sheer velocity of the simulations – the old platform this one replaced provided results in minutes, whereas the new one responded in no more than a few seconds.</p>
<p>Another <a id="_idIndexMarker576"/>example of a successful implementation was in the handling of low-moving data such as country codes. In this particular example, people were not feeling great with the in-memory approach, even though data is safe in logs on the disk (and we even had a backup, as a third set of data for improved reassurance). So, testing this quite innovative approach with some data that they could easily feed back into the data referential made it more comfortable for a first try of the technology. The test went well, but the customer did not expand it to other data. Sadly, I do not know more examples of uses of this technology, which is a bit sad as the potential was huge.</p>
<p>Though this example may not be the best as this technology did not hit it off, the message still remains: in order to respect business/IT alignment, which is the best way to ensure long-term evolution, always favor a technology that closely fits your business needs and data shape.</p>
<p>In the last section of the book, we are going to talk again about time<a id="_idTextAnchor382"/> and how it influences what we do with data referential, in our case.</p>
<h1 id="_idParaDest-219"><a id="_idTextAnchor383"/>Patterns of data evolution in time</h1>
<p>In <a href="B21293_04.xhtml#_idTextAnchor121"><em class="italic">Chapter 4</em></a>, we studied<a id="_idIndexMarker577"/> the importance of time management in information systems, and one of the major impacts of time handling is on data. Data handled in MDM systems must be taken into account with the time factor, and we talked abundantly about data history mana<a id="_idTextAnchor384"/>gement. But the very act of MDM should also be done according to time.</p>
<h2 id="_idParaDest-220"><a id="_idTextAnchor385"/>Data governance</h2>
<p><strong class="bold">Data governance</strong> is <a id="_idIndexMarker578"/>the act of establishing functional responsibilities around the management of data referential. Who is responsible for which reference data? Who can manipulate and clean the data? Who decides the evolution of the model? How are impacted teams and applications informed about changes? What business rules should be followed when manipulating the data? When should data be erased or archived? Those all are governance questions and they are always related to time. In particular, the responses have to be reviewed at regular periods, just like business processes, in order for the data to remain under control.</p>
<p>Data governance is mostly handled in the second layer of the Cigref map, which is the business capability map and usually contains a zone dedicated to reference data management. This is where you should draw the different data referentials, and store the detailed definition of the entities that are stored, along with the versions to prove compatibility between them or document incompatible changes. Here, you should also find at least the name and contact of two of the main data governance roles:</p>
<ul>
<li><strong class="bold">The data owner</strong>: This <a id="_idIndexMarker579"/>person is ultimately responsible for the quality and usability of the data inside the information system. They define all the business rules around the data: how it must be manipulated, who can access it, in which conditions, and so on.</li>
<li><strong class="bold">The data steward</strong>: Under <a id="_idIndexMarker580"/>the delegation of the data owner, this person is responsible for the daily maintenance of the data. Following data manipulation rules issued by the data owner, they clean data and ensure its availability and integrity, as well as the respect for authorization rules.</li>
</ul>
<p>One of the obvious consequences of having data governance is that there is a clear responsibility for a given data referential. Having shared responsibility for a referential is a problem because there can be competing needs that evolve in an uncontrolled evolution of the entity’s format or the services provided. In the worst case scenario, the IT team does not know who to consider as the decider and implements both demands, making the data referential progressively harder to use and not fit for its purpose. Having no responsibility is even worse because, as the implementation belongs to the IT team, the technical people become, by default, the owners of the data, which may be the worst move ever as they do not have the best knowledge of the business stakes associated with the data. Sure, they basically know what the data is about (after all, we all know in a company what a customer is or a product) but again, the devil lies in the details, and when the IT team is in charge of defining data, no one should act surprised that organizations only support one address, or that there is no distinction between a product and an article. Such mistakes would never be made by a specialist in the subject, and we all know how destructive a bad entity definition can be. So leaving such business-driven decisions to the IT team because nobody wants to<a id="_idTextAnchor386"/> take ownership is a risky move and everyone should be warned about this.</p>
<h2 id="_idParaDest-221"><a id="_idTextAnchor387"/>Progressive implementation of a unique referential</h2>
<p>When <a id="_idIndexMarker581"/>presenting<a id="_idIndexMarker582"/> the distributed and consolidated data referential architectures, it has been stated that, sometimes, these intermediary steps toward a centralized referential (which, most of the time, is the ultimate goal) can cost as much as directly going to the target state because of hidden efforts or lesser-known shortcomings. On the contrary, there are times when directly addressing the final vision is impossible, and convergence toward this should be done in several progressive steps. This might be because the information system is so coupled that a violent move may destroy it; most of the time, the problem is with the human capacity to embrace change, and a progressive approach has to be followed for the organization itself to be able to adjust.</p>
<p>This has been the case for me in many situations where I consulted for companies who, in order to successfully manage their merger or acquisition of another company, needed to apply a merging program to the two information systems, incorporating them into a single system. This kind of thing generally takes years in big organizations (the quickest that I have ever witnessed was done in less than 18 months, but all flags were green, which rarely happens). As you will see in the following sections, these plans need numerous steps to be realized.</p>
<p>For privacy reasons, I will show a mix of two progressive transformations that I designed for a public customer (a fusion of two regional councils in France) and an agriculture cooperative that was born out of the merger of two giant entities in the West of France. Both of them needed to address the MDM of the individuals and legal entities that their information systems deal with (customers, agents, prospects, farmers, students, etc.). In order to simplify the diagrams, I will consider the starting point to be where the two entities each had a consolidated data referential, with some applications showing a clone referential pattern. This often happens when there are many applications needing referential data: the most important are directly plugged into the highest-level data referential application, and the secondary<a id="_idIndexMarker583"/> applications are simply cloning what happens in their leading business application. In the following schema, I have also highly reduced the number of applications, again, for simplification reasons. I have not drawn the relationships between them and with other software in the information system, as they were mostly ERPs with much interoperability.</p>
<h3>Step 1 – same infrastructure but no link</h3>
<p>All this being <a id="_idIndexMarker584"/>said, the first step can be schematized as this:</p>
<div><div><img alt="Figure 10.10 – Fusion of two MDM systems – step 1" src="img/Figure_10.10_B21293.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.10 – Fusion of two MDM systems – step 1</p>
<p>The two companies have completely separate MDM systems, hence data referential for their “actors,” if this is the name we should use to describe the entities at play. Notice that most applications are different in each case, except for <code>App1</code>, which is a common ERP between the two companies (this does not mean it will be compatible, as versions may differ and customization definitely will, but this can make a good candidate to put things in common at some point). The very first step is, of course, to connect the two internal networks, even if everything that will be shown next could very well be applied by only communicating through the Internet.</p>
<h3>Step 2 – providing a common interface</h3>
<p>The second<a id="_idIndexMarker585"/> step was to provide all users in the new fusion entity with an API to read actors:</p>
<div><div><img alt="Figure 10.11 – Fusion of two MDM systems – step 2" src="img/Figure_10.11_B21293.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.11 – Fusion of two MDM systems – step 2</p>
<p>Notice how symmetrical the diagram is: choosing a neutral pivotal format was of utmost importance, as using the proprietary format of one of the companies would have been a clear disadvantage to the other one (which would have to change all its connectors and mapping code) and this would have caused human problems, as tensions are always exacerbated during company fusions, particularly when they were previously competitors. We thus spent a lot of time crafting a nice pivotal format for the users, using the best data representations coming from both sides. At this step, not only is reading the sole operation available but no company can read the other’s data! You might wonder how useful this step is since the goal is to reach a unique MDM system for both companies and, for now, it does not change anything. In fact, it is indeed harder for no functional effect, but preparing a common pivotal format is the basis for adequately sharing data. Also, it provides a way for all new software functions that would be created during the fusion process to read actors in a standardized, fusion-ready way. This means we will not have to come back to these new applications, and this is much-appreciated news when you know hundreds of applications have to be dealt with in the whole project. Finally, it started the work on the mediation connectors (there again, this is the kind of thing that is best implemented in Apache Camel, or another flavor of enterprise integration patterns), which was an important piece of work, better started early in the project.</p>
<h3>Step 3 – merging data from the secondary source with the primary</h3>
<p>From<a id="_idIndexMarker586"/> now on, we will only represent the difference in streams from the point of view of company A, but the opposite is always true. The next step was to start obtaining some data from one information system and transferring it to the other. Again, this was very progressive: it was only done for the reading operations of the data for now and, as shown in the following diagram, the data was first read on the system of the person initiating the request, and then only completed with data “from the other side of the barrier.” At any time, the data from the originating side would win, except if the date of modification clearly showed that the data coming from the other information system was fresher.</p>
<div><div><img alt="Figure 10.12 – Fusion of two MDM systems – step 3" src="img/Figure_10.12_B21293.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.12 – Fusion of two MDM systems – step 3</p>
<p>For this previous step to work, it was necessary to find a way to look for similar actors, for example, with their VAT number or other business identifiers.</p>
<h3>Step 4 – storing identifiers from the other source</h3>
<p>Since this is a<a id="_idIndexMarker587"/> complex operation to realize, once the correspondence was found, the technical identifier from one side was stored in the other, and vice versa, which will allow for quicker access next time. This was the first time the system would write in the MDM system, but this was limited to storing the identifier from the other side:</p>
<div><div><img alt="Figure 10.13 – Fusion of two MDM systems – step 4" src="img/Figure_10.13_B21293.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.13 – Fusion of two MDM systems – step 4</p>
<p>However, this opened up<a id="_idIndexMarker588"/> an entirely new approach to sharing data because, once the <em class="italic">write</em> authorizations were provided and the “external” identifier known, each side was able to share information with the other side.</p>
<h3>Step 5 – sending information to the other side</h3>
<p>Every time <a id="_idIndexMarker589"/>there was a change in the actor on one side, the other was informed. The receiving information system was free to deal with this information at its own pace, maybe doing nothing with it the first time, but then choosing which pieces of data were interesting and storing them, and so on. At this point, keeping the origin of the data change was necessary in order to not start a loop of information, sending back to the initial information system the event that the data changed because of its initial event. The diagram – once again represented only from A to B for simplicity reasons – was as follows:</p>
<div><div><img alt="Figure 10.14 – Fusion of two MDM systems – step 5" src="img/Figure_10.14_B21293.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.14 – Fusion of two MDM systems – step 5</p>
<p>Now, since the initial write was started and information systems (and people) were starting to trust<a id="_idIndexMarker590"/> each other better, the next step was to generalize the modification of data.</p>
<h3>Step 6 – centralizing the writing of data and extending the perimeter</h3>
<p>This means<a id="_idIndexMarker591"/> that both sides started to use the centralized API in writing and the implementation of this API was to push the data on both sides, in order for each information system to know about the latest data. Again, using the data depended on whether the <a id="_idIndexMarker592"/>receiving end knew the actor (or should record it) but, in some cases, data was simply ignored, for example, when this was about a change in a supplier that was only used in the other company. As for prospects, though, the data was shared because the commercial approach started to get unified between the two parts of the slowly emerging fusion company.</p>
<div><div><img alt="Figure 10.15 – Fusion of two MDM systems – step 6" src="img/Figure_10.15_B21293.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.15 – Fusion of two MDM systems – step 6</p>
<p>The enterprise integration pattern used in the MOM implementation was a “duplicate message” one, sending the data pushed by the initial request in two similar messages to the mediation routes and waiting for both acknowledgment messages to come back in order to emit its own acknowledgment along the route it was called by, effectively creating a robust delivery of the change both sides.</p>
<h3>Step 7 – access unified</h3>
<p>This was<a id="_idIndexMarker593"/> the time when the old data referential started to act only as gatekeepers for the messages, checking that they were related to their side of the information system. But, since actors were now largely shared, this was not such an important feature, so some applications started to register their actors’ messages directly to the top data referential:</p>
<div><div><img alt="Figure 10.16 – Fusion of two MDM systems – step 7" src="img/Figure_10.16_B21293.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.16 – Fusion of two MDM systems – step 7</p>
<p><code>App1</code> (an ERP used on both sides) was a great candidate to start this new approach, as the mediation connector to it could directly be shared between the two information systems, making for the first common deployment, thus lowering the height of the “barrier.” Since this approach worked quite well, it was a kickstart for the rest of the applications and some dedicated connectors quickly appeared on the other application, which was easy since the common pivotal format had then evolved and was easier than the previous ones, also covering more business cases.</p>
<h3>Step 8 – eliminating unnecessary calls</h3>
<p>The<a id="_idIndexMarker594"/> situation rapidly evolved to something schematized as this, where the old MDM system basically had nothing more to do since all data was coming from the new centralized one:</p>
<div><div><img alt="Figure 10.17 – Fusion of two MDM systems – step 8" src="img/Figure_10.17_B21293.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.17 – Fusion of two MDM systems – step 8</p>
<p>In addition, some<a id="_idIndexMarker595"/> applications, such as <code>App7</code>, had time to evolve and were able to directly take some JSON-representing actors without resorting to a mediation connector. Also, some applications started to be used in common between the two organizations (which clearly appeared more and more like becoming a single organization at this point), and <code>App4</code> disappeared in favor of the common use of <code>App6</code>.</p>
<h3>Step 9 – removing unnecessary intermediate applications</h3>
<p>Some “low strategy” applications <a id="_idIndexMarker596"/>remained under the control of business applications such as <code>App3</code>, but this was not a problem as their parent application was now under the main data referential and would deal with the change of format for them. These applications did not see any change in the system, which was great for their users, who were not impacted at all by the otherwise major change. The resulting information system started to look like the following:</p>
<div><div><img alt="Figure 10.18 – Fusion of two MDM systems – step 9" src="img/Figure_10.18_B21293.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.18 – Fusion of two MDM systems – step 9</p>
<p>Since <code>App6</code> was used by all teams, the barrier between the two formerly separated companies went <a id="_idIndexMarker597"/>one more step down, reaching a point where it was not a problem, as it only divided some secondary business applications used in some corner cases by a dedicated team on activities that were not part of the fusion process. There was now a unique centralized MDM system, with a few important applications acting as local referential for actors on which secondary applications would clone some parts of data. This took many years in total but the objectives were reached: merging the actors used by both sides and doing this in <a id="_idTextAnchor388"/>such a progressive way that the business was never affected by technical choices.</p>
<h2 id="_idParaDest-222"><a id="_idTextAnchor389"/>Keeping an eye on dogma and useless complexity</h2>
<p>I hope I have convinced you in this chapter (and in this book in general) to keep a critical eye on technologies and practices for which their use sounds obvious. As with SQL databases for data referential storage, or hard disk-based data manipulation, there are lots of preconceived approaches in the development itself that do not fit the problem very well when you think purely in business/IT alignment terms.</p>
<p>Just one example is data validation. In most programming languages, validators are associated with fields or properties of data entities, through attributes, for example, in C#. This approach is, in my opinion, very wrong and has proved several times in my practice to be a real pain as one will almost always find a particular case where these validating attributes are not correct. In the case of business identifiers, product owners would sometimes insist on the fact that no entity should ever be created without such a value, and then, within a year or so, realize that there is this particular case where the identifier is not known yet and we should still have the entity in the system. This can, for example, be the case with a medical patient database where the product owner will assure you that an entity without a social security number would make no sense as it is absolutely mandatory before even considering providing medication to them… After insisting on putting a strict <code>NOT NULL</code> validator on this for data quality reasons, the same person may come back a few months later, when the database is in production and a major impact change would have a huge cost, telling you that they forgot the particular case of a newborn that should be given drugs but they do not have a social security number yet.</p>
<p>In this particular example, I have personally taken the habit of never describing any entity attribute as mandatory, as only the context of its use makes it mandatory or not. And it is so easy to add a business rule or a form behavior blocking the <code>null</code> value that it really is not a problem to not put it on the entity itself. On the other hand, sorting out the mess when this mandatory characteristic has been implemented down the lowest levels of your information system is such a pain and a cause of errors that it is, in my opinion, never justified to call a field “mandatory” (except the one exception for a technical identifier, as there is otherwise no way to univocally retrieve an entity once created).</p>
<p class="callout-heading">Important note</p>
<p class="callout">I really like it when I read articles such as <a href="https://jonhilton.net/why-use-blazor-edit-forms/">https://jonhilton.net/why-use-blazor-edit-forms/</a>, where the author questions there being “too much magic” in the technology exposed. Generally, there is, indeed, and such critical eyes are the best reads one can have on a given technology, rather than the numerous blog articles that simply explain how to use a function without digging into when it is useful and when it is actually more of a danger than a real advantage. This article really has a great point of view on validation included in forms and data definitions.</p>
<p>The same goes for cardinalities as for the identifiers cited previously, by the way: if you do not have the absolute, definitive, and fully responsible engagement of your product owner that an attribute should be with a zero or one cardinality, always make it as an array with <em class="italic">N</em> cardinality. What is the worst that could happen? The arrays always being filled with only one item? Well, that does not really matter, does it? A developer complaining that, on all these occasions, they must type <code>deliveryAddresses[0]</code> instead of <code>deliveryAddress</code>? Show them how to create a property in the language they used and it will be sorted out. As far as the GUI is concerned, we will simply display a single piece of information for as long as we do not have a use case corresponding to handling several values in the array. Only when this new business case appears, where we need to handle several pieces of data, will we adjust the GUI with a list instead of a single text zone, for example. But the great thing about this approach is that this will be done smoothly, as the previously unique data will simply become the first of the list, and much more importantly, all the clients of the API will remain compatible and will not be broken by this new use. They will even be able to continue using only the first piece of data in the list as long as they do not want to use the other ones and stick to the old behavior. Since all clients and the server can advance at their own pace on the business change, we know we have a low coupling.</p>
<p>This extends to many other technical approaches that are supposed to help the business but, in the end, can hinder it. To name just a last example, most of the technical approaches to data robustness actually go against the business concepts. The outbox pattern (<a href="https://microservices.io/patterns/data/transactional-outbox.html">https://microservices.io/patterns/data/transactional-outbox.html</a>), for example, should only be used when eventual consistency is not an option. But when you know that even banks have always used eventual consistency (and will definitely go on doing so in the future), that limits the usefulness of such techniques quite a lot. Of course, understanding the business in depth is less fun than using the latest technology or pattern that will drop your rate of transaction errors to a bare minimum. But it is the only way to win in the long term.</p>
<p>So, once again, because this is such an important message, <strong class="bold">think of the business functions first and then find the technology that adapts to it</strong>. In order to do so, the easiest way is to imagine what would happ<a id="_idTextAnchor390"/>en in the real world between business actors if there were no computers involved.</p>
<h1 id="_idParaDest-223"><a id="_idTextAnchor391"/>Summary</h1>
<p>In this chapter, the principles of MDM have been applied and implementation techniques exposed, not only from the architectural point of view but also with technical choices that may prove useful when constructing a data referential. The main behaviors of such server applications have been covered and their evolution in time has been described with a few examples. This should make you quite knowledgeable about how to implement your own data referential.</p>
<p>We will come back to the subject of MDM in <a href="B21293_15.xhtml#_idTextAnchor548"><em class="italic">Chapter 15</em></a>, where we will go down to the lowest level of implementation, with actual lines of code and the design and development of two data referential implementations in C#, in order to deal with authors and with books, respectively. This will be the final piece where we will join the principles of service management and APIs learned about in <a href="B21293_08.xhtml#_idTextAnchor271"><em class="italic">Chapter 8</em></a>, the domain-driven design of the entities shown in <a href="B21293_09.xhtml#_idTextAnchor318"><em class="italic">Chapter 9</em></a>, and the architecture approaches described in the present chapter.</p>
<p>But before we reach this point, we will study the two other parts of an ideal information system, just like we did here for the MDM part. The next chapter is about business process modeling and how we can use <strong class="bold">BPMN</strong> (short for, <strong class="bold">Business Process Modeling Notation</strong>) and BPMN engines in order to implement business processes inside our information systems. Some other subjects such as middleware, no-code/low-code approaches, and orchestration versus choreography will be exposed in the next chapter as well.</p>
</div>
</body></html>