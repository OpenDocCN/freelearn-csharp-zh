- en: Masterful Memory Management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Memory efficiency is an important element of performance optimization. It's
    possible for games of limited scope, such as hobby projects and prototypes, to
    get away with ignoring memory management. These games will tend to waste a lot
    of resources and potentially leak memory, but this won't be a problem if we limit
    its exposure to friends and coworkers. However, anything we want to release professionally
    needs to take this subject seriously. Unnecessary memory allocations lead to poor
    user experience due to excessive garbage collection (costing precious CPU time)
    and memory leaks, which will lead to crashes. None of these situations are acceptable
    in modern game releases.
  prefs: []
  type: TYPE_NORMAL
- en: Using memory efficiently with Unity requires a solid understanding of the underlying
    Unity engine, the Mono platform, and the C# language. Also, if we're making use
    of the new IL2CPP scripting backend, then it would be wise to become familiar
    with its inner workings. This can be a bit of an intimidating place for some developers
    since many pick Unity3D for their game development solution primarily to avoid
    the kind of low-level work that comes from engine development and memory management.
    We'd prefer to focus on higher-level concerns related to gameplay implementation,
    level design, and art asset management, but, unfortunately, modern computer systems
    are complex tools, and ignoring low-level concerns for too long could potentially
    lead to disaster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Understanding what is happening with memory allocations and C# language features,
    how they interact with the Mono platform, and how Mono interacts with the underlying
    Unity engine are absolutely paramount to making high-quality, efficient script
    code. So, in this chapter, you will learn about all of the nuts and bolts of the
    underlying Unity engine: the Mono platform, the C# language, **Intermediate Language
    to C++** (**IL2CPP**), and the .NET Framework.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, it is not necessary to become absolute masters of the C# language
    to use it effectively. This chapter will boil these complex subjects down to a
    more digestible form and is split into the following subjects:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Overview of the Mono platform:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Native and managed memory domains
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Garbage collection
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory fragmentation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a project using IL2CPP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to profile memory issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Implement various memory-related performance enhancements:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimizing garbage collection
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Using value types and reference types properly
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Using strings responsibly
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A multitude of potential enhancements related to the Unity engine
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Object and Prefab pooling
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The Mono platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mono is a magical sauce mixed into the Unity recipe, which gives it a lot of
    its cross-platform capability. Mono is an open source project that built its own
    platform of libraries based on the API, specifications, and tools from Microsoft's
    .NET Framework. Essentially, it is an open source recreation of the .NET library,
    was accomplished with little-to-no access to the original source code, and is
    fully compatible with the original library from Microsoft.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the Mono project is to provide cross-platform development through
    a framework that allows code written in a common programming language to run against
    many different hardware platforms, including Linux, macOS, Windows, ARM, PowerPC,
    and more. Mono even supports many different programming languages. Any language
    that can be compiled into .NET's **Common Intermediate Language** (**CIL**) is
    sufficient to integrate with the Mono platform. This includes C# itself, but also
    several other languages, such as F#, Java, Visual Basic .NET, pythonnet, and IronPython.
  prefs: []
  type: TYPE_NORMAL
- en: A common misconception about the Unity engine is that it is built on top of
    the Mono platform. This is untrue, as its Mono-based layer does not handle many
    important game tasks such as audio, rendering, physics, and keeping track of time.
    Unity Technologies built a native C++ backend for the sake of speed and allowed
    its users control of this game engine through Mono as a scripting interface. As
    such, Mono is merely an ingredient of the underlying Unity engine. This is equivalent
    to many other game engines, which run C++ under the hood, handling important tasks
    such as rendering, animation, and resource management, while providing a higher-level
    scripting language for gameplay logic to be implemented. As such, the Mono platform
    was chosen by Unity Technologies to provide this feature.
  prefs: []
  type: TYPE_NORMAL
- en: Native code is a common vernacular for code that is written specifically for
    the given platform. For instance, writing code to create a window object or interface
    with networking subsystems in Windows would be completely different to code performing
    the tasks for a macOS, Unix, PlayStation 4, Xbox One, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Scripting languages typically abstract away complex memory management through
    automatic garbage collection and provide various safety features, which simplify
    the act of programming at the expense of runtime overhead. Some scripting languages
    can also be interpreted at runtime, meaning that they don't need to be compiled
    before execution. The raw instructions are converted dynamically into machine
    code and executed the moment they are read during runtime; of course, this often
    makes the code relatively slow. The last feature, and probably the most important
    one, is that they allow simpler syntax of programming commands. This usually improves
    development workflow immensely, as team members without much experience using
    languages such as C++ can still contribute to the code base. This enables them
    to implement things such as gameplay logic in a simpler format at the expense
    of a certain amount of control and runtime execution speed.
  prefs: []
  type: TYPE_NORMAL
- en: Note that such languages are often called **managed languages**, which feature
    **managed code**. Technically, this was a term coined by Microsoft to refer to
    any source code that must run inside their **Common Language Runtime** (**CLR**)
    environment, as opposed to code that is compiled and run natively through the
    target OS.
  prefs: []
  type: TYPE_NORMAL
- en: However, because of the prevalence and common features that exist between the
    CLR and other languages that feature their own similarly designed runtime environments
    (such as Java), the term **managed** has since been hijacked. It tends to be used
    to refer to any language or code that depends on its own runtime environment,
    and that may or may not include automatic garbage collection. For the rest of
    this chapter, we will adopt this definition and use the term **managed** to refer
    to code that both depends on a separate runtime environment to execute and is
    being monitored by automatic garbage collection.
  prefs: []
  type: TYPE_NORMAL
- en: The runtime performance cost of managed languages is always greater than the
    equivalent native code, but it is becoming less significant every year. This is
    partly due to gradual optimizations in tools and runtime environments, and partly
    due to the computing power of the average device gradually becoming greater although
    the main point of controversy with using managed languages still remains their
    automatic memory management. Managing memory manually can be a complex task that
    can take many years of difficult debugging to be proficient at, but many developers
    feel that managed languages solve this problem in ways that are too unpredictable,
    risking too much product quality. Such developers might claim that managed code
    will never reach the same level of performance as native code, and hence it is
    foolhardy to build high-performance applications with them.
  prefs: []
  type: TYPE_NORMAL
- en: This is true to an extent, as managed languages invariably inflict runtime overheads,
    and we lose partial control over runtime memory allocations. This would be a deal-breaker
    for high-performance server architecture; however, for game development, it becomes
    a balancing act since not all resource usage will necessarily result in a bottleneck,
    and the best games aren't necessarily the ones that use every single byte to their
    fullest potential. For example, imagine a user interface that refreshes in 30
    ms via native code versus 60 Âµs in managed code due to an extra 100% overhead
    (an extreme example). The managed code version is still fast enough that the user
    will never be able to notice the difference, so is there really any harm in using
    managed code for such a task?
  prefs: []
  type: TYPE_NORMAL
- en: In reality, at least for game development, working with managed languages often
    just means that developers have a unique set of concerns to worry about compared
    to native code developers. As such, the choice to use a managed language for game
    development is partly a matter of preference and partly a compromise of control
    versus development speed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s revisit a topic we touched upon in earlier chapters but didn''t quite
    flesh out: the concept of memory domains in the Unity engine.'
  prefs: []
  type: TYPE_NORMAL
- en: Memory domains
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Memory space within the Unity engine can be essentially split into three different
    memory domains. Each domain stores different types of data and takes care of a
    very different set of tasks. Let''s take a look at each of them:'
  prefs: []
  type: TYPE_NORMAL
- en: The first memory domainâthe managed domainâshould be very familiar. This domain
    is where the Mono platform does its work, where any `MonoBehaviour` scripts and
    custom C# classes we write will be instantiated at runtime, and so we will interact
    with this domain very explicitly through any C# code we write. It is called the
    managed domain because this memory space is automatically managed by a **Garbage
    Collector** (**GC**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second domainâthe native domainâis more subtle since we only interact with
    it indirectly. Unity has an underlying native code foundation, which is written
    in C++ and compiled into our application differently, depending on which platform
    is being targeted. This domain takes care of allocating internal memory space
    for things such as asset data (for example, textures, audio files, and meshes)
    and memory space for various subsystems such as the Rendering Pipeline, physics
    system, and user input system. Finally, it includes partial native representations
    of important gameplay objects such as GameObjects and components so that they
    can interact with these internal systems. This is where a lot of built-in Unity
    classes keep their data, such as the `transform` and `Rigidbody` components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third and final memory domains are those of external libraries, such as
    DirectX and OpenGL libraries, as well as any custom libraries and plugins we include
    in our project. Referencing these libraries from our C# code will cause a similar
    memory context switch and subsequent cost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The managed domain also includes wrappers for the very same object representations
    that are stored within the native domain. As a result, when we interact with components
    such as `transform`, most instructions will ask Unity to dive into its native
    code, generate the result there, and then copy it back to the managed domain for
    us. This is where the native-managed bridge between the managed domain and native
    domains derives from, which was briefly mentioned in previous chapters. When both
    domains have their own representations for the same entity, crossing the bridge
    between them requires a memory context switch that can potentially inflict some
    fairly significant performance hits on our game. Obviously, crossing back and
    forth across this bridge should be minimized as much as possible due to the overhead
    involved. We covered several techniques for this in Chapter 2, *Scripting Strategies**.*
  prefs: []
  type: TYPE_NORMAL
- en: Memory in most modern OS splits runtime memory space into two categories.
  prefs: []
  type: TYPE_NORMAL
- en: The stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The stack is a special reserved space in memory, dedicated to small, short-lived
    data values, which are automatically deallocated the moment they go out of scope,
    hence why it is called the stack. It literally operates as a stack data structure,
    pushing and popping data from the top. Allocation to the stack complies with the
    following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: The stack contains any local variables we declare and handles the loading and
    unloading of functions as they're called. These function calls to expand and contract
    through what is known as the call stack. When the call stack is done with the
    current function, it jumps back to the previous point on the call stack and continues
    from where it left off.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The start of the previous memory allocation is always known, and there's no
    reason to perform any clean-up operations since any new allocations can simply
    overwrite the old data. Hence, the stack is relatively quick and efficient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The total stack size is usually very small, usually on the order of MB. It's
    possible to cause a stack overflowÂ by allocating more space than the stack can
    support. This can occur during exceptionally large call stacks (for example, an
    infinite loop) or having a large number of local variables, but in most cases,
    causing a stack overflow is rarely a concern despite its relatively small size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The heap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The heap represents all remaining memory space, and it is used for the overwhelming
    majority of memory allocation.
  prefs: []
  type: TYPE_NORMAL
- en: Since we want most of the memory allocated to persist longer than the current
    function call, we couldn't allocate it on the stack since it would just get overwritten
    when the current function ends. So, instead, whenever a data type is too big to
    fit in the stack or must persist outside the function it was declared in, it is
    allocated on the heap.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's nothing physically different between the stack and the heap; they're
    both just memory spaces containing bytes of data that exist in RAM, which have
    been requested and set aside for us by the OS. The only difference is in when,
    where, and how they are used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In native code, such as code written in languages such as C++, these memory
    allocations are handled manually in that we are responsible for ensuring that
    all pieces of memory we allocate are properly and explicitly deallocated when
    they are no longer needed. If this is not done properly, then we could easily
    and accidentally introduce memory leaks since we are likely to keep allocating
    more and more memory space from RAM that is never cleaned up until there is no
    more space to allocate and the application crashes.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, in managed languages, this process is automated through the GC. During
    the initialization of our Unity app, the Mono platform will request a given chunk
    of memory from the OS and use it to generate a heap memory space that our C# code
    can use (often known as the **managed heap**). This heap space starts off fairly
    small, less than 1 MB, but will grow as new blocks of memory are needed by our
    script code. This space can also shrink by releasing it back to the OS if Unity
    determines that it's no longer needed.
  prefs: []
  type: TYPE_NORMAL
- en: Garbage collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The GC has an important job, which is to ensure that we don't use more managed
    heap memory than we need, and that memory that is no longer needed will be automatically
    deallocated. For instance, if we create `GameObject` and then later destroy it,
    the GC will flag the memory space used by `GameObject` for eventual deallocation
    later. This is not an immediate process, as the GC only deallocates memory when
    necessary.
  prefs: []
  type: TYPE_NORMAL
- en: When a new memory request is made, and there is enough empty space in the managed
    heap to satisfy the request, the GC simply allocates the new space and hands it
    over to the caller. However, if the managed heap does not have room for it, then
    the GC will need to scan all of the existing memory allocations for anything that
    is no longer being used and cleans them up first. It will only expand the current
    heap space as the last resort.
  prefs: []
  type: TYPE_NORMAL
- en: 'The GC in the version of Mono that Unity uses is a type of tracing GC, which
    uses a **Mark-and-Sweep** strategy. This algorithm works in two phases: each allocated
    object is tracked with an additional bit. This flags whether the object has been
    marked or not. These flags start set to `false` to indicate that it has not yet
    been marked.'
  prefs: []
  type: TYPE_NORMAL
- en: When the collection process begins, it marks all objects that are still reachable
    to the program by setting their flags to `true`. Either the reachable object is
    a direct reference, such as static or local variables on the stack, or it is an
    indirect reference through the fields (member variables) of other directly or
    indirectly accessible objects. In essence, it is gathering a set of objects that
    are still referenceable to our application. Everything that is not still referenceable
    would be effectively invisible to our application and can be deallocated by the
    GC.
  prefs: []
  type: TYPE_NORMAL
- en: The second phase involves iterating through this catalog of references (which
    the GC will have kept track of throughout the lifetime of the application) and
    determining whether or not it should be deallocated based on its **marked** status.
    If the object is marked, then it is still being referenced by something else,
    and so the GC leaves it alone. However, if it is not marked, then it is a candidate
    for deallocation. During this phase, all marked objects are skipped over, but
    not before setting their flag back to `false` for the first phase of the next
    garbage collection scan.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, the GC maintains a list of all objects in memory, while our application
    maintains a separate list containing only a portion of them. Whenever our application
    is done with an object, it simply forgets it exists, removing it from its list.
    Hence, the list of objects that can be safely deallocated would be the difference
    between the GC's list and our application's list.
  prefs: []
  type: TYPE_NORMAL
- en: Once the second phase ends, all unmarked objects are deallocated to free space,
    and then the initial request to create the object is revisited. If the GC has
    freed up enough space for the object, then it is allocated within that newly-freed
    space and returned to the caller. However, if it is not, then we hit the last-resort
    situation and must expand the managed heap by requesting it from the OS, at which
    point the object space can finally be allocated and returned to the caller.
  prefs: []
  type: TYPE_NORMAL
- en: In an ideal world, where we only keep allocating and deallocating objects, but
    only a finite number of them exist at once, the heap would maintain a roughly
    constant size because there's always enough space to fit the new objects we need.
    However, all objects in an application are rarely deallocated in the same order
    they were allocated, and even more rarely do they all have the same size in memory.
    This leads to memory fragmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Memory fragmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fragmentation occurs when objects of different sizes are allocated and deallocated
    in alternating orders and if lots of small objects are deallocated, following
    by lots of large objects being allocated.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is best explained through an example. The following shows four steps we
    take in allocating and deallocating memory in a typical heap memory space:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/573bbf10-163c-4ab8-a83e-aa2705b2387c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The memory allocation takes place as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We start with an empty heap space
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then allocate four objects on the heap, **A**, **B**, **C**, and **D**, each
    64-bytes in size
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At a later time, we deallocate two of the objects, **A** and **C**, freeing
    up 128-bytes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then try to allocate a new object that is 128-bytes in size
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deallocating objects **A** and **C** technically frees 128 bytes worth of space,
    but since the objects were not contiguous (adjoining neighbors) in memory, we
    cannot allocate an object larger than both individual spaces there. New memory
    allocations must always be contiguous in memory; therefore, the new object must
    be allocated in the next available contiguous 128-byte space available in the
    managed heap. We now have two empty 64-byte holes in our memory space, which will
    never be reused unless we allocate objects sized 64 bytes or smaller.
  prefs: []
  type: TYPE_NORMAL
- en: Over long periods of time, our heap memory can become riddled with more, smaller
    empty spaces such as these as objects of different sizes are deallocated, and
    then the system later tries to allocate new objects within the smallest available
    space that it can fit within, leaving some small remainder that becomes harder
    to fill. In the absence of background techniques that automatically clean up this
    fragmentation, this effect would occur in literally any memory spaceâRAM, heap
    space, and even hard drivesâwhich are just larger, slower, and more permanent
    memory storage areas (this is why it's a good idea to defragment our hard drives
    from time to time).
  prefs: []
  type: TYPE_NORMAL
- en: 'Memory fragmentation causes two problems:'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, it effectively reduces the total usable memory space for new objects
    over long periods of time, depending on the frequency of allocations and deallocations.
    This is likely to result in the GC having to expand the heap to make room for
    new allocations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secondly, it makes new allocations take longer to resolve due to the extra time
    it takes to find a new memory space large enough to fit the object.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This becomes important when new memory allocations are made in a heap since
    the location of available space becomes just as important as how much free space
    is available. There is no way to split an object across partial memory locations,
    so the GC must either continue searching until it finds a large enough space or
    the entire heap size must be increased to fit the new object, costing even more
    time after it just spent a bunch of time doing an exhaustive search.
  prefs: []
  type: TYPE_NORMAL
- en: Garbage collection at runtime
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, in a worst-case scenario, when a new memory allocation is being requested
    by our game, the CPU would have to spend cycles completing the following tasks
    before the allocation is finally completed:'
  prefs: []
  type: TYPE_NORMAL
- en: Verify that there is enough contiguous space for the new object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If there is not enough space, iterate through all known direct and indirect
    references, marking everything they connect to as reachable
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterate through all of these references again, flagging unmarked objects for
    deallocation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterate through all flagged objects to check whether deallocating some of them
    would create enough contiguous space for the new object
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If not, request a new memory block from the OS to expand the heap
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Allocate the new object at the front of the newly allocated block and return
    it to the caller
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This can be a lot of work for the CPU to handle, particularly if this new memory
    allocation is an important object such as a particle effect, a new character entering
    the scene, or a cutscene transition. Users are extremely likely to note moments
    where the GC is freezing gameplay to handle this extreme case. To make matters
    worse, the garbage collection workload scales poorly as the allocated heap space
    grows since sweeping through a few MBs of space will be significantly faster than
    scanning several GBs of space.
  prefs: []
  type: TYPE_NORMAL
- en: All of this makes it absolutely critical to control our heap space intelligently.
    The lazier our memory usage tactics are, the worse the GC will behave in an almost
    exponential fashion, as we are more and more likely to hit this worst-case scenario.
    So, it's a little ironic that, despite the efforts of managed languages to make
    the memory management problem easier, managed language developers still find themselves
    being just as, if not more, concerned with memory consumption than developers
    of native applications. The main difference is in the types of problems they're
    trying to solve.
  prefs: []
  type: TYPE_NORMAL
- en: Threaded garbage collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The GC runs on two separate threads: the main thread and what is called the
    **finalizer thread**. When the GC is invoked, it will run on the main thread and
    flag heap memory blocks for future deallocation. This does not happen immediately.
    The finalizer thread, controlled by Mono, can have a delay of several seconds
    before the memory is finally freed and available for reallocation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94a86feb-5908-4874-ba02-0fb1c4c8da8b.png)'
  prefs: []
  type: TYPE_IMG
- en: We can observe this behavior in the Total Allocated block (the green line, with
    apologies to that 5% of the population with deuteranopia/deuteranomaly) of the
    Memory Area within the Profiler window. It can take several seconds for the total
    allocated value to drop after a garbage collection has occurred. Owing to this
    delay, we should not rely on memory being available the moment it has been deallocated,
    and as such, we should never waste time trying to eke out every last byte of memory
    that we believe should be available. We must ensure that there is always some
    kind of buffer zone available for future allocations.
  prefs: []
  type: TYPE_NORMAL
- en: Blocks that have been freed by the GC may sometimes be given back to the OS
    after some time, which would reduce the reserved space consumed by the heap and
    allow the memory to be allocated for something else, such as another application.
    However, this is very unpredictable and depends on the platform being targeted,
    so we shouldn't rely on it. The only safe assumption to make is that as soon as
    the memory has been allocated to Mono, it's then reserved and is no longer available
    to either the native domain or any other application running on the same system.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section, we will look at another essential element of the development
    process: code compilation. During code compilation, the C# code will be transformed
    into real instructions executed by the CPU. Surprisingly, there are multiple ways
    of performing this conversion; let''s see how to choose among them.'
  prefs: []
  type: TYPE_NORMAL
- en: Code compilation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we make changes to our C# code, it is automatically compiled when we switch
    back from our favorite IDE (which is typically either MonoDevelop or the much
    more feature-rich Visual Studio) to the Unity Editor. However, the C# code is
    not converted directly into machine code, as we would expect static compilers
    to do if we are using languages such as C++.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, the code is converted into an intermediate stage called **Common Intermediate
    Language** (**CIL**), which is an abstraction above the native code. This is how
    .NET can support multiple languagesâeach uses a different compiler, but they're
    all converted into CIL, so the output is effectively the same regardless of the
    language that we pick. CIL is similar to Java bytecode, upon which it is based,
    and the CIL code is entirely useless on its own, as CPUs have no idea how to run
    the instructions defined in this language.
  prefs: []
  type: TYPE_NORMAL
- en: At runtime, this intermediate code is run through the Mono **Virtual Machine**
    (**VM**), which is an infrastructure element that allows the same code to run
    against multiple platforms without the need to change the code itself. This is
    an implementation of the .NET CLR. If we're running on iOS, we run on the iOS-based
    VM infrastructure, and if we're running on Linux, then we simply use a different
    one that is better suited for Linux. This is how Unity allows us to write code
    once, and it works magically on multiple platforms.
  prefs: []
  type: TYPE_NORMAL
- en: Within the CLR, the intermediate CIL code will actually be compiled into the
    native code on demand. This immediate native compilation can be accomplished either
    by an **Ahead-Of-Time** (**AOT**) or **Just-In-Time** (**JIT**) compiler. Which
    one is used will depend on the platform that is being targeted. These compilers
    allow code segments to be compiled into native code, allowing the platform's architecture
    to complete the written instructions without having to write them ourselves. The
    main difference between the two compiler types is when the code is compiled.
  prefs: []
  type: TYPE_NORMAL
- en: AOT compilation is the typical behavior for code compilation and happens early
    (AOT) either during the build process or in some cases during app initialization.
    In either case, the code has been precompiled, and no further runtime costs are
    inflicted due to dynamic compilation since there are always machine code instructions
    available whenever the CPU needs them.
  prefs: []
  type: TYPE_NORMAL
- en: JIT compilation happens dynamically at runtime in a separate thread and begins
    just before execution (JIT for execution). Often, this dynamic compilation causes
    the first invocation of a piece of code to run a little (or a lot) more slowly
    because the code must finish compiling before it can be executed. However, from
    that point forward, whenever the same code block is executed, there is no need
    for recompilation, and the instructions run through the previously compiled native
    code.
  prefs: []
  type: TYPE_NORMAL
- en: A common adage in software development is that 90% of the work is being done
    by only 10 percent of the code. This generally means that JIT compilation turns
    out to be a net positive on performance than if we simply tried to interpret the
    CIL code directly. However, because the JIT compiler must compile code quickly,
    it is not able to make use of many optimization techniques that static AOT compilers
    can exploit.
  prefs: []
  type: TYPE_NORMAL
- en: Not all platforms support JIT compilation, but some scripting functionalities
    are not available when using AOT. Unity provides a complete list of these restrictions
    at [https://docs.unity3d.com/Manual/ScriptingRestrictions.html](https://docs.unity3d.com/Manual/ScriptingRestrictions.html).
  prefs: []
  type: TYPE_NORMAL
- en: A few years ago, Unity Technologies was faced with a choice to either continue
    to support the Mono platform, which Unity was finding more and more difficult
    to keep up with, or implement their own scripting backend. They chose the latter
    option, and multiple platforms now support IL2CPP.
  prefs: []
  type: TYPE_NORMAL
- en: The Unity Technologies' initial post about IL2CPP, the reasoning behind the
    decision, and its long-term benefits can be found at [https://blogs.unity3d.com/2014/05/20/the-future-of-scripting-in-unity/](https://blogs.unity3d.com/2014/05/20/the-future-of-scripting-in-unity/).
  prefs: []
  type: TYPE_NORMAL
- en: IL2CPP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: IL2CPP is a scripting backend designed to convert Mono's CIL output directly
    into the native C++ code. This leads to improved performance since the application
    will now be running native code. This ultimately gives Unity Technologies more
    control of runtime behavior since IL2CPP provides its own AOT compiler and VM,
    allowing custom improvements to subsystems such as the GC and compilation process.
    IL2CPP does not intend to replace the Mono platform completely, but it is an additional
    tool we can enable, which improves part of the functionality that Mono provides.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that IL2CPP is automatically enabled for iOS and WebGL projects. For other
    platforms that support it, IL2CPP can be enabled under Edit | Project Settings
    | Player | Other Settings | Configure | Scripting Backend:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/69ce6f1d-e1db-431b-9048-4cc06c0226c4.png)'
  prefs: []
  type: TYPE_IMG
- en: A list of platforms currently supporting IL2CPP can be found at [https://docs.unity3d.com/Manual/IL2CPP.html](https://docs.unity3d.com/Manual/IL2CPP.html).
  prefs: []
  type: TYPE_NORMAL
- en: Profiling memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two issues we are concerned about when it comes to memory management:
    how much we''re consuming and how often we''re allocating new blocks. Let''s cover
    each of these topics separately.'
  prefs: []
  type: TYPE_NORMAL
- en: Profiling memory consumption
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We do not have direct control over what is going on in the native domain since
    we don't have the Unity engine source code and hence can't add any code that will
    interact with it directly. We can, however, control it indirectly using various
    script-level functions that serve as interaction points between managed and native
    code. There are technically a variety of memory allocators available, which are
    used internally for things such as GameObjects, graphics objects, and the Profiler,
    but these are hidden behind the native-managed bridge.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we can observe how much memory has been allocated and reserved in
    this memory domain via the Memory Area of the Profiler window. Native memory allocations
    show up under the values labeled Unity, and we can even get more information using
    Detailed mode and sampling the current frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/30b8e292-906d-40d6-a003-a90237837821.png)'
  prefs: []
  type: TYPE_IMG
- en: Under the Scene Memory section of breakdown view, we can observe that `MonoBehaviour`
    objects always consume a constant amount of memory, regardless of their member
    data. This is the memory consumed by the native representation of the object.
  prefs: []
  type: TYPE_NORMAL
- en: Note that memory consumption in Edit mode is always wildly different from that
    of a standalone version due to various debugging and editor hook data being applied.
    This adds a further incentive to avoid using Edit mode for benchmarking and instrumentation
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: We can also use the `Profiler.GetRuntimeMemorySize()` method to get the native
    memory allocation size of a particular object.
  prefs: []
  type: TYPE_NORMAL
- en: Managed object representations are intrinsically linked to their native representations.
    The best way to minimize our native memory allocations is to simply optimize our
    managed memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can verify how much memory has been allocated and reserved for the managed
    heap using the Memory Area of the Profiler window, under the values labeled Mono,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2540c86-793d-47ab-9656-a3ea1a558ff2.png)'
  prefs: []
  type: TYPE_IMG
- en: We can also determine the current used and reserved heap space at runtime using
    the `Profiler.GetMonoUsedSize()` and `Profiler.GetMonoHeapSize()` methods, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling memory efficiency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The best metric we can use to measure the health of our memory management is
    simply watching the behavior of the GC. The more work it's doing, the more waste
    we're generating and the worse our application's performance is likely to become.
  prefs: []
  type: TYPE_NORMAL
- en: We can use both the CPU Usage Area (the GarbageCollector checkbox) and Memory
    Area (the GC Allocated checkbox) of the Profiler window to observe the amount
    of work the GC is doing and the time it is taking to do it. This can be relatively
    straightforward for some situations, where we only allocated a temporary small
    block of memory or we just destroyed a `GameObject`Â instance.
  prefs: []
  type: TYPE_NORMAL
- en: However, root-cause analysis for memory efficiency problems can be challenging
    and time-consuming. When we observe a spike in the GC's behavior, it could be
    a symptom of allocating too much memory in a previous frame and merely allocating
    a little more in the current frame, requiring the GC to scan a lot of fragmented
    memory, determine whether there is enough space, and decide whether to allocate
    a new block. The memory it cleaned up could have been allocated a long time ago,
    and we may only be able to observe these effects when our application runs over
    long periods of time and could even happen when our scene is sitting relatively
    idle, giving no obvious cause for the GC to trigger suddenly. Even worse, the
    Profiler can only tell us what happened in the last few seconds or so, and it
    won't be immediately obvious what data was being cleaned up.
  prefs: []
  type: TYPE_NORMAL
- en: We must be vigilant and test our application rigorously, observing its memory
    behavior while simulating a typical play session if we want to be certain we are
    not generating memory leaks or creating a situation where the GC has too much
    work to complete in a single frame.
  prefs: []
  type: TYPE_NORMAL
- en: Memory management performance enhancements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In most game engines, we would have the luxury of being able to port inefficient
    managed code into faster native code if we were hitting performance issues. This
    is not an option unless we invest serious cash in obtaining the Unity source code,
    which is offered as a license separate from the Free/Personal/Pro licensing system,
    and on a per case, per-title basis. We could also purchase a license of Unity
    Pro with the hope of using native plugins, but doing so rarely leads to a performance
    benefit since we must still cross the native-managed bridge to invoke function
    calls inside of it. Native plugins are normally used to interface with systems
    and libraries that are not built specifically for C#. This forces the overwhelming
    majority of us into a position of needing to make our C# script-level code as
    performant as possible ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, we should now have enough understanding of Unity engine internals
    and memory spaces to detect and analyze memory performance issues and understand
    and implement enhancements for them. So, let's cover some performance enhancements
    we can apply.
  prefs: []
  type: TYPE_NORMAL
- en: Garbage collection tactics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One strategy to minimize garbage collection problems is concealment by manually
    invoking the GC at opportune moments when we're certain the player would not notice.
    Garbage collection can be manually invoked by calling `System.GC.Collect()`.
  prefs: []
  type: TYPE_NORMAL
- en: Good opportunities to invoke a collection may occur while loading between levels,
    when the gameplay is paused, shortly after a menu interface has been opened, during
    cutscene transitions, or any break in gameplay when the player would not witness,
    or care about, a sudden performance drop. We could even use the `Profiler.GetMonoUsedSize()`
    and `Profiler.GetMonoHeapSize()` methods at runtime to determine whether a garbage
    collection needs to be invoked soon.
  prefs: []
  type: TYPE_NORMAL
- en: We can also cause the deallocation of a handful of specific objects. If the
    object in question is one of the Unity object wrappers, such as a `GameObject`
    or `MonoBehaviour` component, then the finalizer will first invoke the `Dispose()`
    method within the native domain. At this point, the memory consumed by both the
    native and managed domains will then be freed. In some rare instances, if the
    Mono wrapper implements the `IDisposable` interface class (that is, it has a `Dispose()`
    method available from script code), then we can actually control this behavior
    and force the memory to be freed instantly.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a number of different object types in the Unity engine (most of which
    are introduced in Unity 5 or later), which implement the `IDisposable` interface
    class, as follows: `NetworkConnection`, `WWW`, `UnityWebRequest`, `UploadHandler`,
    `DownloadHandler`, `VertexHelper`, `CullingGroup`, `PhotoCapture`, `VideoCapture`,
    `PhraseRecognizer`, `GestureRecognizer`, `DictationRecognizer`, `SurfaceObserver`,
    and more.'
  prefs: []
  type: TYPE_NORMAL
- en: These are all utility classes for pulling in potentially large datasets where
    we might want to ensure immediate destruction of the data it has acquired since
    they normally involve allocating several buffers and memory blocks in the native
    domain to accomplish their tasks. If we kept all of this memory for a long time,
    it would be a colossal waste of precious space. So, by calling their `Dispose()`
    method from script code, we can ensure that the memory buffers are freed promptly
    and precisely when they need to be.
  prefs: []
  type: TYPE_NORMAL
- en: All other asset objects offer some kind of unloading method to clean up any
    unused asset data, such as `Resources.UnloadUnusedAssets()`. Actual asset data
    is stored within the native domain, so the GC technically isn't involved here,
    but the idea is basically the same. It will iterate through all assets of a particular
    type, check whether they're no longer being referenced, and, if so, deallocate
    them. However, again, this is an asynchronous process, and we cannot guarantee
    exactly when the deallocation will occur. This method is automatically called
    internally after a scene is loaded, but this still doesn't guarantee instant deallocation.
    The preferred approach is to use `Resources.UnloadAsset()` instead, which will
    unload one specific asset at a time. This method is generally faster since time
    will not be spent iterating through an entire collection of asset data to figure
    out what is unused.
  prefs: []
  type: TYPE_NORMAL
- en: However, the best strategy for garbage collection will always be avoidance;
    if we allocate as little heap memory and control its usage as much as possible,
    then we won't have to worry about the GC inflicting frequent, expensive performance
    costs. We will cover many tactics for this throughout the remainder of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Manual JIT compilation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If JIT compilation is causing a runtime performance loss, be aware that it is
    actually possible to force JIT compilation of a method at any time via reflection.
    Reflection is a useful feature of the C# language that allows our code base to
    explore itself introspectively for type information, methods, values, and metadata.
    Using reflection is often a very costly process. It should be avoided at runtime
    or, at the very least, only used during initialization or other loading times.
    Not doing so can easily cause significant CPU spikes and gameplay freezing.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can manually force JIT compilation of a method using reflection to obtain
    a function pointer to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code only works on `public` methods. Obtaining `private` or `protected`
    methods can be accomplished through the use of `BindingFlags`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This kind of code should only be run for very targeted methods where we are
    certain that JIT compilation is causing CPU spikes. This can be verified by restarting
    the application and profiling a method's first invocation versus all subsequent
    invocations. The difference will tell us the JIT compilation overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the official method for forcing JIT compilation in the .NET library
    is `RuntimeHelpers.PrepareMethod()`, but this is not properly implemented in the
    current default version of Mono that comes with Unity (Mono version 2.6.5). Since
    Unity 2018.1, the .NET 4.x runtime is no longer considered experimental; however,
    it is not supported on all platforms, and it is still not the suggested one. The
    aforementioned workaround is not pretty, but it is still the best and most consistent
    way to proceed.
  prefs: []
  type: TYPE_NORMAL
- en: Value types and reference types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Not all memory allocations we make within Mono will go through the heap. The
    .NET Framework (and, by extension, the C# language, which merely implements the
    .NET specification) has the concept of value types and reference types, and only
    the latter needs to be marked by the GC while it is performing its Mark-and-Sweep
    algorithm. Reference types are expected to (or need to) last a long time in memory
    due to their complexity, their size, or how they're used. Large datasets and any
    kind of object instantiated from a `class`Â instance is a reference type. This
    also includes arrays (regardless of whether it is an array of Value types or reference
    types), delegates, all classes, such as `MonoBehaviour`, `GameObject`, and any
    custom classes we define.
  prefs: []
  type: TYPE_NORMAL
- en: Reference types are always allocated on the heap, whereas value types can be
    allocated either on the stack or the heap. Primitive data types such as `bool`,
    `int`, and `float` are examples of value types. These values are typically allocated
    on the stack, but as soon as a value type is contained within a reference type,
    such as `class` or an array, then it is implied that it is either too large for
    the stack or will need to survive longer than the current scope and must be allocated
    on the heap, bundled with the reference type it is contained within.
  prefs: []
  type: TYPE_NORMAL
- en: 'All of this can be best explained through examples. The following code will
    create an integer as a value type that exists on the stack only temporarily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As soon as the `TestFunction()` method ends, the integer is deallocated from
    the stack. This is essentially a free operation since, as mentioned previously,
    it doesn't bother doing any cleanup; it just moves the stack pointer back to the
    previous memory location in the call stack (back to whichever function called
    `TestFunction()` on the `TestComponent` object). Any future stack allocations
    simply overwrite the old data. More importantly, no heap allocation took place
    to create the data, so the GC does not need to track its existence.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if we created an integer as a member variable of the `MonoBehaviour`
    class definition, then it is now contained within a reference type (`class`) and
    must be allocated on the heap along with its container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `_data` integer is now an additional piece of data that consumes space in
    the heap alongside the `TestComponent` object it is contained within. If `TestComponent`
    is destroyed, then the integer is deallocated along with it, but not before then.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, if we put the integer into a normal C# class, then the rules for
    reference types still apply and the object is allocated on the heap:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'So, there is a big difference between creating a temporary value type within
    a `class` method versus storing long-term value type as a member field of `class`.
    In the former case, we''re storing it in the stack, but in the latter case, we''re
    storing it within a reference type, which means it can be referenced elsewhere.
    For example, imagine that `DoSomething()` has stored the reference to `dataObj`
    within a member variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we would not be able to deallocate the object pointed to `dataObj`
    as soon as the `TestFunction()` method ends because the total number of things
    referencing the object would go from `2` to `1`. This is not `0`, and hence the
    GC would still mark it during Mark-and-Sweep. We would need to set the value of
    `_testDataObj` to `null` or make it reference something else before the object
    is no longer reachable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that a value type must have a value and can never be `null`. If a stack-allocated
    value type is assigned to a reference type, then the data is simply copied. This
    is true even for arrays of value types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: When the initial array is created (during object initialization), `1000` integers
    will be allocated on the heap set to a value of `0`. When the `StoreANumber()`
    method is called, the value of `num` is merely copied into the zeroth element
    of the array rather than storing a reference to it.
  prefs: []
  type: TYPE_NORMAL
- en: The subtle change in the referencing capability is what ultimately decides whether
    something is a reference type or a value type, and we should try to use value
    types whenever we have the opportunity so that they generate stack allocations
    instead of heap allocations. Any situation where we're just sending around a piece
    of data that doesn't need to live longer than the current scope is a good opportunity
    to use a value type instead of a reference type. Ostensibly, it does not matter
    if we pass the data into another method of the same class or a method of another
    classâit still remains a value type that will exist on the stack until the method
    that created it goes out of the scope.
  prefs: []
  type: TYPE_NORMAL
- en: Pass by value and by reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Technically, something is duplicated every time a data value is passed as an
    argument from one method to another, and this is true whether it is a value type
    or a reference type. When we're passing the object's data, this is known as **passing
    by value**. When we're simply copying a reference to something else, it is called
    **passing by reference**.
  prefs: []
  type: TYPE_NORMAL
- en: An important difference between value types and reference types is that a reference
    type is merely a pointer to another location in memory that consumes only 4 or
    8-bytes in memory (32 bit or 64 bit, depending on the architecture), regardless
    of what it is actually pointing to. When a reference type is passed as an argument,
    it is only the value of this pointer that gets copied into the function. Even
    if the reference type points to a humongous array of data, this operation will
    be very quick since the data being copied is very small.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, a value type contains the full and complete bits of data stored within
    a concrete object. Hence, all of the data of a value type will be copied whenever
    they are passed between methods or stored in other value types. In some cases,
    it can mean that passing a large value type as arguments around too much can be
    more costly than just using a reference type and letting the GC take care of it.
    For most value types, this is not a problem since they are comparable in size
    to a pointer, but this becomes important when we begin to talk about the `struct`
    type in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Data can also be passed around by reference using the `ref` keyword, but this
    is very different from the concept of value and reference types, and it is very
    important to keep them distinct in our mind when we try to understand what is
    going on under the hood. We can pass a value type by value or by reference, and
    we can pass a reference type by value or by reference. This means that there are
    four distinct data passing situations that can occur, depending on which type
    is being passed and whether the `ref` keyword is being used or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'When data is passed by reference (even if it is a value type), then making
    any changes to the data will change the original. For example, the following code
    would print the value as `10`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Removing the `ref` keyword from both places would make it print the value `5`
    instead (and removing it from only one of them would lead to a compiler error
    since the `ref` keyword needs to be present in both locations or neither). This
    understanding will come in handy when we start to think about some of the more
    interesting data types we have access to, namely, structs, arrays, and strings.
  prefs: []
  type: TYPE_NORMAL
- en: Structs are value types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `struct` type is an interesting special case in C#. A `struct` object can
    contain `private`, `protected`, and `public` fields; have methods; and be instantiated
    at runtime, just like a `class` type. However, there is a fundamental difference
    between the two: a `struct` type is a value type, and a `class` type is a reference
    type. Consequently, this leads to some important differences between the two,
    namely, that a `struct` type cannot support inheritance, their properties cannot
    be given custom default values (member data always defaults to values such as
    `0` or `null` since it is a value type), and their default constructors cannot
    be overridden. This greatly restricts their usage compared to classes, so simply
    replacing all classes with structs (under the assumption that it will just allocate
    everything on the stack) is not as easy as it sounds.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if we''re using a class in a situation whose only purpose is to send
    a blob of data to somewhere else in our application, and it does not need to last
    beyond the current scope, then we might be able to use a `struct` type instead,
    since a `class` type would result in a heap allocation for no particularly good
    reason:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we're using a `class` type to pass a bunch of data from one
    subsystem (the combat system) to another (the UI system). The only purpose of
    this data is to be calculated and read by various subsystems, so this is a good
    candidate to convert into a `struct` type.
  prefs: []
  type: TYPE_NORMAL
- en: 'Merely changing the `DamageResult` definition from a `class` type to a `struct`
    type could save us quite a few unnecessary garbage collections since it would
    be allocated on the stack as a value type instead of the heap as a reference type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This is not a catch-all solution. Since structs are value types, the entire
    blob of data will be duplicated and provided to the next method in the call stack,
    regardless of how large or small it is. So, if a `struct` object is passed by
    a value between five different methods in a long chain, then five different stack
    copies will occur at the same time. Recall that stack deallocations are effectively
    free, but stack allocations (which involve copying of data) is not. This data
    copying is pretty much negligible for small values, such as a handful of integers
    or floating-point values, but passing around ridiculously large datasets through
    structs over and over again is obviously not a trivial task and should be avoided.
  prefs: []
  type: TYPE_NORMAL
- en: We can work around this problem by passing the `struct` object by reference
    using the `ref` keyword to minimize the amount of data being copied each time
    (just a single pointer). However, this can be dangerous since passing by reference
    allows any subsequent methods to make changes to the `struct` object, in which
    case it would be prudent to make its data values `readonly`. This means that the
    values can only be initialized in the constructor, and never again, even by its
    own member functions, which prevents accidental changes as it's passed through
    the chain.
  prefs: []
  type: TYPE_NORMAL
- en: 'All of the preceding is also true when structs are contained within reference
    types, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: To the untrained eye, the preceding code appears to be attempting to store a
    stack-allocated struct (`ds`) within a reference type (`StructHolder`). Does this
    mean that a `StructHolder` object on the heap can now reference an object on the
    stack? If so, what will happen when the `StoreStruct()` method goes out of scope
    and the `struct` object is (effectively) erased? It turns out that these are the
    wrong questions.
  prefs: []
  type: TYPE_NORMAL
- en: What's actually happening is that while a `DataStruct` object (`_memberStruct`)
    has been allocated on the heap within the `StructHolder` object, it is still a
    value type and does not magically transform into a reference type when it is a
    member variable of a reference type. So, all of the usual rules for value types
    apply. The `_memberStruct` variable cannot have a value of `null`, and all of
    its fields will be initialized to `0` or `null` values. When `StoreStruct()` is
    called, the data from `ds` will be copied into `_memberStruct` in its entirety.
    There are no references to stack objects taking place, and there is no concern
    about lost data.
  prefs: []
  type: TYPE_NORMAL
- en: Arrays are reference types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The purpose of arrays is to contain large datasets, which makes them difficult
    to be treated as a value type since there's probably not enough room on the stack
    to support them. Therefore, they are treated as a reference type so that the entire
    dataset can be passed around via a single reference (if it were a value type,
    we would need to duplicate the entire array every time it is passed around). This
    is true irrespective of whether the array contains value types or reference types.
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that the following code will result in a heap allocation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'However, the following, functionally equivalent, code would not result in any
    heap allocations since the `struct` objects being used are value types, and hence,
    it would be created on the stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The subtle difference in the second example is that only one `TestStruct` exists
    on the stack at a time, whereas the first example needs to allocate `1000` of
    them via an array. Obviously, these methods are kind of ridiculous as they're
    written, but they illustrate an important point to consider. The compiler isn't
    smart enough to automatically find these situations for us and make the appropriate
    changes. Opportunities to optimize our memory usage through value type replacements
    will be entirely down to our ability to detect them and understand why conversions
    from reference types to value types will result in stack allocations, rather than
    heap allocations.
  prefs: []
  type: TYPE_NORMAL
- en: Note that when we allocate an array of reference types, we're creating an array
    of references, which can provide each reference other locations on the heap. However,
    when we allocate an array of value types, we're creating a packed list of value
    types on the heap. Each of these value types will be initialized with a value
    of `0` (or equivalent) since they cannot be `null`, while each reference within
    an array of reference types will always initialize to `null` since no references
    have been assigned yet.
  prefs: []
  type: TYPE_NORMAL
- en: Strings are immutable reference types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We briefly touched upon the subject of strings in Chapter 2, *Scripting Strategies*,
    but now it's time to go into more detail about why proper string usage is extremely
    important.
  prefs: []
  type: TYPE_NORMAL
- en: Strings are essentially arrays of characters, and so they are considered reference
    types and follow all of the same rules as other reference types; they will be
    allocated on the heap, and a pointer is all that is copied from one method to
    the next. Since a string is effectively an array, this implies that the characters
    it contains must be contiguous in memory. However, we often find ourselves expanding,
    contracting, or combining strings to create other strings. This can lead us to
    make some faulty assumptions about how strings work. We might assume that because
    strings are such common, ubiquitous objects, performing operations on them is
    fast and cheap. Unfortunately, this is incorrect. Strings are not made to be fast.
    They are only made to be convenient.
  prefs: []
  type: TYPE_NORMAL
- en: The string object class is immutable, which means they cannot be changed after
    they've been allocated. Therefore, when we change a string, we are actually allocating
    a whole new string on the heap to replace it, where the contents of the original
    will be copied and modified as needed into a whole new character array, and the
    original string object reference now points to a completely new string object.
    In which case, the old string object might no longer be referenced anywhere, will
    not be marked during *Mark-and-Sweep*, and will eventually be purged by the GC.
    As a result, lazy string programming can result in a lot of unnecessary heap allocations
    and garbage collection.
  prefs: []
  type: TYPE_NORMAL
- en: 'A good example to illustrate how strings are different than normal reference
    types is the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: If we were under the mistaken assumption that strings worked just like other
    reference types, then we might be forgiven for assuming that the log output of
    the following to be `World!`. It appears as though `testString`, a reference type,
    is being passed into `DoSomething()`, which would change what `testString` is
    referencing to, in which case, the `Log` statement will print out the new value
    of the string.
  prefs: []
  type: TYPE_NORMAL
- en: However, this is not the case, and it will simply print out `Hello`. What is
    actually happening is that the `localString` variable, within the scope of `DoSomething()`,
    starts off referencing the same place in memory as `testString` due to the reference
    being passed by value. This gives us two references pointing to the same location
    in memory as we would expect if we were dealing with any other reference type.
    So far, so good.
  prefs: []
  type: TYPE_NORMAL
- en: However, as soon as we change the value of `localString`, we run into a little
    bit of a conflict. Strings are immutable, and we cannot change them, so, therefore,
    we must allocate a new string containing the `World!` value and assign its reference
    to the value of `localString`; now, the number of references to the `Hello` string
    returns back to one. The value of `testString`, therefore, has not been changed,
    and that is still the value that will be printed by `Debug.Log()`. All we've succeeded
    in doing by calling `DoSomething()` is creating a new string on the heap that
    gets garbage-collected and doesn't change anything. This is the textbook definition
    of wasteful.
  prefs: []
  type: TYPE_NORMAL
- en: If we change the method definition of `DoSomething()` to pass the string by
    reference via the `ref` keyword, the output would indeed change to `World!`. Of
    course, this is also what we would expect to happen with a value type, which leads
    a lot of developers to incorrectly assume that strings are value types. However,
    this is an example of the fourth and final data-passing case, where a reference
    type is being passed by reference, which allows us to change what the original
    reference is referencing.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s recap:'
  prefs: []
  type: TYPE_NORMAL
- en: If we pass a value type by value, we can only change the value of a copy of
    its data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we pass a value type by reference, we can change the value of the original
    data passed in
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we pass a reference type by value, we can make changes to the object referenced
    by the original reference variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we pass a reference type by reference, we can change to which object the
    original reference is pointing to
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we find functions that seem to generate a lot of GC allocations the moment
    they are called, then we might be causing undue heap allocations due to a misunderstanding
    of the preceding rules.
  prefs: []
  type: TYPE_NORMAL
- en: String concatenation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concatenation is the act of appending strings to one another to form a larger
    string. As you've learned, any such cases are likely to result in excess heap
    allocations. The biggest offender in a string-based memory waste is concatenating
    strings using the `+` operator and `+=` operators, because of the allocation chaining
    effect they cause.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following code tries to combine a group of string objects
    together to print some information about a combat result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'An example output of this function might be a string that reads as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This function features a handful of string literals (hardcoded strings that
    are allocated during application initialization) such as `" dealt "`, `" damage
    to "`, and `" blocked)"`, which are simple constructs for the compiler to pre-allocate
    for us. However, because we are using other local variables within this combined
    string, it cannot be compiled away at build time, and, therefore, the complete
    string is regenerated dynamically at runtime each time the function is called.
  prefs: []
  type: TYPE_NORMAL
- en: A new heap allocation will be generated each time a `+` or `+=` operator is
    executed. Only a single pair of strings will be merged at a time, and it allocates
    a new string object each time. Then, the result of one merger will be fed into
    the next and merged with the next string and so on until the final string object
    has been built.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the previous example will result in nine different strings being allocated
    all in one statement. All of the following strings would be allocated to satisfy
    this instruction, and all would eventually need to be garbage collected (note
    that the operators are resolved from right to left):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: That's 262 characters being used, instead of 49\. In addition, because a character
    is a 2-byte data type (for Unicode strings), that's 524 bytes of data being allocated
    when we only need 98 bytes. The chances are that if this code exists in the code
    base once, it exists all over the place; so, for an application that's doing a
    lot of lazy string concatenation like this, that is a ton of memory being wasted
    on generating unnecessary strings.
  prefs: []
  type: TYPE_NORMAL
- en: Note that big, constant string literals can be safely combined using the `+`
    and `+=` operators. The compiler knows that you will eventually need the full
    string and pre-generates the string automatically. This helps us to make a huge
    block of text more readable within the code base, but only if they will result
    in a constant string.
  prefs: []
  type: TYPE_NORMAL
- en: Better approaches for generating strings are to use either the `StringBuilder`
    class or one of several string class methods for string formatting.
  prefs: []
  type: TYPE_NORMAL
- en: StringBuilder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Conventional wisdom says that if we roughly know the final size of the resultant
    string, then we can allocate an appropriate buffer AOT and save ourselves undue
    allocations. This is the purpose of the `StringBuilder` class. It is effectively
    a mutable (changeable) string-based object that works like a dynamic array. It
    allocates a block of space, which we can copy future string objects into, and
    allocates additional space whenever the current size is exceeded. Of course, expanding
    the buffer should be avoided as much as possible by predicting the maximum size
    we will need and allocating a sufficiently sized buffer AOT.
  prefs: []
  type: TYPE_NORMAL
- en: When we use `StringBuilder`, we can retrieve the resultant string object by
    calling the `ToString()` method. This still results in one additional memory allocation
    for the completed string, but, at the very least, we only allocated one large
    string as opposed to dozens of smaller strings, had we used the `+` or `+=` operators.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the previous example, we might allocate a `StringBuilder` buffer of `100`
    characters to make room for long character names and damage values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: String formatting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we don't know the final size of the resultant string, then using a `StringBuilder`
    class is unlikely to generate a buffer that fits the result size exactly. We will
    either end up with a buffer that's too large (wasted space) or, worse, a buffer
    that's too small, which must keep expanding as we generate the complete string.
    In this scenario, it might be best to use one of the various string class formatting
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three string class methods available for generating strings: `string.Format()`,
    `string.Join()`, and `string.Concat()`. Each operates slightly differently, but
    the overall output is the same. A new string object is allocated, containing the
    contents of the string objects we pass into them, and it is all done in a single
    action, which reduces excess string allocations.'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, regardless of the approach we use, if we're converting other
    objects into additional string objects (such as the calls to generate the strings
    for `"Orc"`, `"Dwarf"`, or `"Slashing"` in the preceding example), then this will
    allocate an additional string object on the heap. There is nothing we can do about
    this allocation, except perhaps cache the result so that we don't need to recalculate
    it each time it's needed.
  prefs: []
  type: TYPE_NORMAL
- en: It can be surprisingly hard to say which one of these string generation approaches
    would be more beneficial in a given situation, as there are a lot of silly little
    nuances involved that tend to explode into religious debate (just do a Google
    search for `C# string concatenation performance`*,* and you'll see what I mean),
    so the simplest approach is to implement one or the other using the conventional
    wisdom described previously. Whenever we run into bad performance with one of
    the string-manipulation methods, we should also try the other to check whether
    it results in performance improvement. The best way to be certain is to profile
    them both for comparison and then pick the best options.
  prefs: []
  type: TYPE_NORMAL
- en: Boxing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Everything in C# is an object (caveats apply), meaning that they derive from
    the `System.Object` class. Even primitive data types such as `int`, `float`, and
    `bool` are implicitly derived from `System.Object`, which is itself a reference
    type. This is a special case, which allows them access to helper methods such
    as `ToString()` so that they can customize their string representation, but without
    actually turning them into reference types. Whenever one of these value types
    is implicitly treated in such a way that it must act as an object, the CLR automatically
    creates a temporary object to store, or *box*, the value inside so that it can
    be treated as a typical reference type object. As we should expect, this results
    in a heap allocation to create the containing vessel.
  prefs: []
  type: TYPE_NORMAL
- en: Note that boxing is not the same thing as using value types as member variables
    of reference types. Boxing only takes place when value types are treated as reference
    types via conversion or casting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out these examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code will cause the `i` integer variable to be boxed inside the
    `obj` object:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code will use the `obj` object representation to replace the
    value stored within the integer, and unbox it back into an integer, storing it
    in `i`. The final value of `i` would be `256`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The preceding types can be changed dynamically.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is perfectly legal C# code, where we override the type of `obj`,
    converting it into `float`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is also legalâconversion into `bool`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that attempting to unbox `obj` into a type that isn''t the most recently
    assigned type would result in `InvalidCastException`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: All of this can be a little tricky to wrap our head around until we remember
    that, at the end of the day, everything is just bits in memory and that we are
    free to interpret them any way we like. After all, data types such as `int`, `float`,
    and so on are just an abstraction over binary lists of `0` and `1`. What's important
    is knowing that we can treat our primitive types as objects by boxing them, converting
    their types, and then unboxing them into a different type at a later time, but
    each time we do this results in a heap memory allocation.
  prefs: []
  type: TYPE_NORMAL
- en: Note that it's possible to convert a boxed object's type using one of the many
    `System.Convert.Toâ¦()` methods.
  prefs: []
  type: TYPE_NORMAL
- en: Boxing can be either implicit, as shown in the preceding examples, or explicit,
    by typecasting to `System.Object`. Unboxing must always be explicit by typecasting
    back to its original type. Whenever we pass a value type into a method that uses
    `System.Object` as arguments, boxing will be applied implicitly.
  prefs: []
  type: TYPE_NORMAL
- en: Methods such as `String.Format()`, which take `System.Object` as arguments,
    are one such example. We typically use them by passing in value types, such as
    `int`, `float`, and `bool`, to generate a string with. Boxing is automatically
    taking place in these situations, causing additional heap allocations that we
    should be aware of. `Collections.Generic.ArrayList` is another such example since
    `ArrayList` always contains converts its inputs into `System.Object` references,
    regardless of what types are stored within.
  prefs: []
  type: TYPE_NORMAL
- en: Any time we use a function definition that takes `System.Object` as arguments,
    and we're passing in value types, we should be aware that we're implicitly causing
    heap allocations due to boxing.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of data layout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The importance of how our data is organized in memory can be surprisingly easy
    to forget about but can result in a fairly big performance boost if it is handled
    properly. Cache misses should be avoided whenever possible, which means that in
    most cases, arrays of data that are contiguous in memory should be iterated over
    sequentially as opposed to any other iteration style.
  prefs: []
  type: TYPE_NORMAL
- en: This means that data layout is also important for garbage collection since it
    is done in an iterative fashion, and if we can find ways to have the GC skip over
    problematic areas, then we can potentially save a lot of iteration time.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, we want to keep large groups of reference types separated from large
    groups of value types. If there is even one reference type within a value type,
    such as `struct`, then the GC considers the entire object, and all of its data
    members, indirectly referenceable objects. When it comes time to Mark-and-Sweep,
    it must verify all fields of the object before moving on. However, if we separate
    the various types into different arrays, then we can make the GC skip the majority
    of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, if we have an array of `struct` objects that looks like the following
    code, then the GC will need to iterate over every member of every `struct`, which
    could be fairly time-consuming:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'However, if we reorganize all pieces of this data into multiple arrays of each
    time, then the GC will ignore all of the primitive data types and only check the
    string objects. The following code will result in much a faster garbage collection
    sweep:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The reason this works is that we're giving the GC fewer indirect references
    to check. When the data is split into separate arrays (reference types), it finds
    three arrays of value types, marks the arrays, and then immediately moves on because
    there's no reason to mark the contents of an array of value types. It must still
    iterate through all of the string objects within `myStrings` since each is a reference
    type and it needs to verify that there are no indirect references within it. Technically,
    the string objects cannot contain indirect references, but the GC works at a level
    where it only knows whether the object is a reference type or value type and,
    therefore, can't tell the difference between a string and class. However, we have
    still spared the GC from needing to iterate over an extra 3,000 pieces of data
    (the 3,000 values in `myInts`, `myFloats`, and `myBools`).
  prefs: []
  type: TYPE_NORMAL
- en: Arrays from the Unity API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Several instructions within the Unity API result in heap memory allocations,
    which we should be aware of. This essentially includes everything that returns
    an array of data. For example, the following methods allocate memory on the heap:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Each and every time we call a Unity API method that returns an array will cause
    a whole new version of that data to be allocated. Such methods should be avoided
    whenever possible or at the very least called once and cached so that we don't
    cause memory allocations more often than necessary.
  prefs: []
  type: TYPE_NORMAL
- en: There are other Unity API calls where we provide an array of elements to a method,
    and it writes the necessary data into the array for us. One such example is providing
    a `Particle[]` array to `ParticleSystem` to get its `Particle` data. The benefit
    of these types of API calls is that we can avoid reallocating large arrays, whereas
    the downside is that the array needs to be large enough to fit all of the objects.
    If the number of objects we need to acquire keeps increasing, then we may find
    ourselves reallocating larger arrays. In the case of `ParticleSystem`, we need
    to be certain we create an array large enough to contain the maximum number of
    `Particle` objects it generates at any given time.
  prefs: []
  type: TYPE_NORMAL
- en: Unity Technologies have hinted in the past that they may eventually change some
    of the API calls that return arrays into the form that requires an array to be
    provided. The API of the latter form can be confusing for new programmers at first
    glance; however, unlike the first form, it allows responsible programmers to use
    memory much more efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Using InstanceIDs for dictionary keys
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in Chapter 2, *Scripting Strategies*, dictionaries are used to
    map associations between two different objects, which are very quick at telling
    us whether a mapping exists, and if so, what that mapping is. It's common practice
    to map `MonoBehaviour` or `ScriptableObject` reference as the key of a dictionary,
    but this causes some problems. When the dictionary element is accessed, it will
    need to call into several derived methods of `UnityEngine.Object`, which both
    of these object types derive from. This makes element comparison and mapping acquisition
    relatively slow.
  prefs: []
  type: TYPE_NORMAL
- en: This can be improved by making use of `Object.GetInstanceID()`, which returns
    an integer representing a unique identification value for that object that never
    changes and is never reused between two objects during the entire lifecycle of
    the application. If we cache this value in the object somehow and use it as the
    key in our dictionary, then the element comparison will be around two to three
    times faster than if we used the object reference directly.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are caveats to this approach. If the instance ID value is not
    cached (we keep calling `Object.GetInstanceID()` each time we need to index into
    our dictionary) and we are compiling with Mono (and not IL2CPP), then element
    acquisition could end up being slow. This is because it will call some thread-unsafe
    code to acquire the instance ID, in which case, the Mono compiler cannot optimize
    the loop, and, therefore causes some additional overhead by comparison to caching
    the instance ID value. If we are compiling with IL2CPP, which doesn't have this
    problem, then the benefits are still not as great (only around 50% faster) than
    if we had simply cached the value beforehand. Therefore, we should aim to cache
    the integer value in some way so that we avoid having to call `Object.GetInstanceID()`
    too often.
  prefs: []
  type: TYPE_NORMAL
- en: foreach loops
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `foreach` loop keyword is a bit of a controversial issue in Unity development
    circles. It turns out that a lot of `foreach` loops implemented in Unity C# code
    will incur unnecessary heap memory allocations during these calls, as they allocate
    an `Enumerator` object as a class on the heap, instead of a `struct` on the stack.
    It all depends on the given collection's implementation of the `GetEnumerator()`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Note that it is safe to use `foreach` loops on typical arrays. The Mono compiler
    secretly converts `foreach` over arrays into simple for loops.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since Unity 2018.1, Unity uses an upgraded Mono runtime (4.0.30319) and some
    compiler fixes many of the previous issues with `foreach`. As a consequence, `foreach`
    is no more a big issue in the general case. Yet, `foreach` still has a bad reputation
    among developers. The fact that sometimes they can actually be problematic makes
    everything more complicated. As usual, there is only one way to be sure: use the
    Profiler and check whether `foreach` is actually creating problems in your specific
    situation.'
  prefs: []
  type: TYPE_NORMAL
- en: In any case, even in the worst scenarioâthat is, your `foreach` loop is actually
    doing heap allocationsâthe cost is fairly negligible, as the heap allocation cost
    does not scale with the number of iterations. Only one `Enumerator` object is
    allocated and reused over and over again, which only costs a handful of bytes
    of memory overall. So, unless our `foreach` loops are being invoked for every
    update (which is typically dangerous in, and of, itself), the costs will be mostly
    negligible on small projects. The time taken to convert everything into a `for`
    loop may not be worth it.
  prefs: []
  type: TYPE_NORMAL
- en: Coroutines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned before, starting a coroutine costs a small amount of memory, to
    begin with, but note that no further costs are incurred when the method calls
    `yield`. If memory consumption and garbage collection are significant concerns,
    we should try to avoid having too many short-lived coroutines and avoid calling
    `StartCoroutine()` too much during runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Closures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Closures are useful, but dangerous tools. Anonymous methods and lambda expressions
    are not always closures, but they can be. It all depends on whether the method
    uses data outside of its own scope and parameter list or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following anonymous function would not be a closure, since
    it is self-contained and functionally equivalent to any other locally defined
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'However, if the anonymous function pulled in data from outside itself, it becomes
    a closure, as it closes the environment around the required data. The following
    would result in a closure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In order to complete this transaction, the compiler must define a new custom
    class that can reference the environment where the `i` data value would be accessible.
    At runtime, it creates the corresponding object on the heap and provides it to
    the anonymous function. Note that this includes value types (as per the preceding
    example), which were originally on the stack, possibly defeating the purpose of
    them being allocated on the stack in the first place. So, we should expect each
    invocation of the second method to result in heap allocations and inevitable garbage
    collection.
  prefs: []
  type: TYPE_NORMAL
- en: The .NET library functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The .NET library offers a huge amount of common functionalities that help to
    solve numerous problems that programmers may come across during day-to-day implementation.
    Most of these classes and functions are optimized for general use cases, which
    may not be optimal for a specific situation. It may be possible to replace a particular
    .NET library class with a custom implementation that is more suited to our specific
    use case.
  prefs: []
  type: TYPE_NORMAL
- en: There are also two big features in the .NET library that often become big performance
    hogs whenever they're used. This tends to be because they are only included as
    a quick-and-dirty solution to a given problem without much effort put into optimization.
    These features are **LINQ** and **r****egular expressions**.
  prefs: []
  type: TYPE_NORMAL
- en: LINQ provides a way to treat arrays of data as miniature databases and perform
    queries against them using a SQL-like syntax. The simplicity of its coding style
    and complexity of the underlying system (through its usage of closures) implies
    that it has a fairly large overhead cost. LINQ is a handy tool, but is not really
    intended for high-performance, real-time applications, such as games, and does
    not even function on platforms that do not support JIT compilation, such as iOS.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, regular expressions through the `Regex` class allow us to perform
    complex string parsing to find substrings that match a particular format, replace
    pieces of a string, or construct strings from various inputs. Regular expressions
    are very useful tools but tends to be overused in places where they areÂ largely
    unnecessary or in so-called clever ways to implement a feature such as text localization,
    when straightforward string replacement would be far more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Specific optimizations for both of these features go far beyond the scope of
    this book, as they could fill an entire book by themselves. We should either try
    to minimize their usage as much as possible, replace their usage with something
    less costly, bring in a LINQ or regex expert to solve the problem for us or do
    some Googling on the subject to optimize how we're using them.
  prefs: []
  type: TYPE_NORMAL
- en: One of the best ways to find the correct answer online is to simply post the
    wrong answer. People will either help us out of kindness or will take such a great
    offense from our implementation that they will consider it their civic duty to
    correct us. Just be sure to do some kind of research on the subject first. Even
    the busiest of people are generally happy to help if they can see that we've put
    in our fair share of effort beforehand.
  prefs: []
  type: TYPE_NORMAL
- en: Temporary work buffers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we get into the habit of using large, temporary work buffers for one task
    or another, then it just makes sense that we should look for opportunities to
    reuse them, instead of reallocating them over and over again, as this lowers the
    overhead involved in allocation and garbage collection (often called **memory
    pressure**). It might be worthwhile to extract such functionality from case-specific
    classes into a generic *G**od* class that contains a big work area for multiple
    classes to reuse.
  prefs: []
  type: TYPE_NORMAL
- en: Object pooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Speaking of temporary work buffers, object pooling is an excellent way of both
    minimizing and establishing control over our memory usage by avoiding deallocation
    and reallocation. The idea is to formulate our own system for object creation,
    which hides away whether the object we're getting has been freshly allocated or
    has been recycled from an earlier allocation. The typical terms to describe this
    process are to spawn and despawn the object rather than creating and deleting
    them in memory. When an object is despawned, we're simply hiding it, making it
    lay dormant until we need it again, at which point it is respawned from one of
    the previously despawned objects and used in place of an object we might have
    otherwise newly allocated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s cover a quick implementation of an object pooling system:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define a common interface for the object we want to use in the object
    pool. An important feature of this system is to allow the pooled object to decide
    how to recycle itself when the time comes. The following interface class called
    `IPoolableObject` will satisfy this requirement nicely:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This interface class defines two methods: `New()` and `Respawn()`. These should
    be called when the object is first created and when it has been respawned, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to implement a class that manages the poolable objects. The following
    `ObjectPool` class definition is a fairly simple implementation of the object
    pooling concept:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This class allows `ObjectPool` to be used with any object type so long as it
    fits the following two criteria: it must implement the `IPoolableObject` interface
    class, and the derived class must allow for a parameter-less constructor (specified
    by the `new()` keyword in the class declaration).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we need to implement the `IPoolableObject` interface for any object
    we want to pool. An example poolable object would look like so: it must implement
    two `public` methods, `New()` and `Respawn()`, which are invoked by the `ObjectPool`
    class at the appropriate times:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, just consider this usage example: we want to have a continuous wave of
    monsters. Obviously, we do not want to create new enemies continuously, instead,
    we want to recycle the enemies killed by the player. To do that, first we create
    a pool of 100 `EnemyObject` objects (we assume we never need to show more than
    100 enemies on screen at the same time):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The first 100 calls to `Spawn()` on `ObjectPool` will cause the enemy to be
    respawned, providing the caller with a unique instance of the object each time.
    If there are no more enemies to provide (we have called `Spawn()` more than 100
    times), then we will allocate a new `EnemyObject`Â instance and push it onto the
    stack. Finally, if `Reset()` is called on `ObjectPool`, it will begin again from
    the start, recycling enemies and providing them to the caller.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we are using the `Peek()` method on the `Stack` object so that we
    don't remove the old instance from the stack. We want `ObjectPool` to maintain
    references to all of the enemies we create.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, note that this pooling solution will not work for classes we haven''t
    defined and cannot derive from `IPoolableObject`, such as `Vector3` and `Quaternion`.
    This is normally dictated by the `sealed` keyword in the class definition. In
    these cases, we would need to define a containing class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We could extend this system in a number of ways, such as defining a `Despawn()`
    method to handle destruction of the object, making use of the `IDisposable` interface
    class and `using` blocks when we wish to automatically spawn and despawn objects
    within a small scope, and/or allowing objects instantiated outside the pool to
    be added to it.
  prefs: []
  type: TYPE_NORMAL
- en: Prefab pooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous pooling solution is useful for typical C# objects, but it won't
    work for specialized Unity objects, such as `GameObject` and `MonoBehaviour`.
    These objects tend to consume a large chunk of our runtime memory, can cost us
    a great deal of CPU usage when they're created and destroyed, and tend to risk
    a large amount of garbage collection at runtime. For instance, during the lifecycle
    of a small RPG game, we might spawn a thousand Orc creatures, but at any given
    moment, we may only need a maximum of 10 of them. It would be nice if we could
    perform similar pooling as before but, for Unity Prefabs, to save on a lot of
    unnecessary overhead creating and destroying 990 Orcs we don't need.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to push the overwhelming majority of object instantiation to scene
    initialization rather than letting them get created at runtime. This can provide
    some big runtime CPU savings and avoids a lot of spikes caused by object creation/destruction
    and garbage collection at the expense of scene loading times and runtime memory
    consumption. As a result, there are quite a few pooling solutions available on
    the Asset StoreÂ to handle this task, with varying degrees of simplicity, quality,
    and feature sets.
  prefs: []
  type: TYPE_NORMAL
- en: It is often recommended that pooling should be implemented in any game that
    intends to be deployed on mobile devices, due to the greater overhead costs involved
    in the allocation and deallocation of memory compared to desktop applications.
  prefs: []
  type: TYPE_NORMAL
- en: However, creating a pooling solution is an interesting topic, and building one
    from scratch is a great way of getting to grips with a lot of important internal
    Unity engine behavior. Also, knowing how such a system is built makes it easier
    to extend if we wish it to meet the needs of our particular game, rather than
    to rely on a prebuilt solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The general idea of Prefab pooling is to create a system that contains lists
    of active and inactive GameObjects that were all instantiated from the same Prefab
    reference. The following diagram shows how the system might look after several
    spawns, despawns, and respawns of various objects derived from four different
    Prefabs (**Orc**, **Troll**, **Ogre**, and **Dragon**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a9a8f86-8bc4-4750-8b4a-50ed0ec65a19.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the **Heap Memory** area in the previous screenshot represents the
    objects as they exist in memory, while the **Pooling System** area represents
    the **Pooling System's** references to those objects.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, several instances of each Prefab were instantiated (**11 Orcs**,
    **8 Trolls**, **5 Ogres**, and **1 Dragon**). Currently, only 11 of these objects
    are active, while the other 14 have been previously despawned and are inactive.
    Note that the despawned objects still exist in memory, although they are not visible
    and cannot interact with the game world until they have been respawned. Naturally,
    this costs us a constant amount of heap memory at runtime to maintain the inactive
    objects, but when a new object is instantiated, we can reuse one of the existing
    inactive objects rather than allocating more memory to satisfy the request. This
    saves significant runtime CPU costs during object creation and destruction and
    avoids garbage collection.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the chain of events that needs to occur when **New
    Orc is spawned**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66c18f0a-1750-47bb-ba14-a631fe3f9bc8.png)'
  prefs: []
  type: TYPE_IMG
- en: The first object in the **Inactive** Orc pool (**Orc7**) is reactivated and
    moved into the **Active** pool. We now have six active Orcs and five inactive
    Orcs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the order of events when an **Ogre** object is
    despawned:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f60b2b9-e1fb-40a0-8743-cf891a46b8a5.png)'
  prefs: []
  type: TYPE_IMG
- en: This time, the object is deactivated and moved from the **Active** pool into
    the **Inactive** pool, leaving us with one active **Ogre** and four inactive Ogres.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the following diagram shows what happens when a new object is spawned,
    but there are no inactive objects to satisfy the request:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ac207546-6fe8-475f-8dcd-fecbec9b3985.png)'
  prefs: []
  type: TYPE_IMG
- en: In this scenario, more memory must be allocated to instantiate the new **Dragon**
    object since there are no **Dragon** objects in its **Inactive** pool to reuse.
    Therefore, to avoid runtime memory allocations for our GameObjects, it is critical
    that we know beforehand how many we will need and that there is sufficient memory
    space available to contain them all at once. This will vary depending on the type
    of object in question and requires occasional testing and sanity checking to ensure
    that we have a sensible number of each Prefab instantiated at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: With all of this in mind, let's create a pooling system for Prefabs.
  prefs: []
  type: TYPE_NORMAL
- en: Poolable components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s first define an interface class for a component that can be used in
    the pooling system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The approach for `IPoolableComponent` will be very different from the approach
    taken for `IPoolableObject`. The objects being created this time are GameObjects,
    which are a lot trickier to work with than standard objects because of how much
    of their runtime behavior is already handled through the Unity engine and how
    little low-level access we have to it.
  prefs: []
  type: TYPE_NORMAL
- en: GameObjects do not give us access to an equivalent `New()` method that we can
    invoke any time the object is created, and we cannot derive from the `GameObject`
    class to implement one. GameObjects are created either by placing them in a scene
    or by instantiating them at runtime through `GameObject.Instantiate()`, and the
    only inputs we can apply are an initial position and rotation. Of course, their
    components have an `Awake()` callback that we can define, which is invoked the
    first time the component is brought to life, but this is merely a compositional
    objectâit's not the actual parent object we're spawning and despawning.
  prefs: []
  type: TYPE_NORMAL
- en: So, because we have control over only a `GameObject` class's components, it
    is assumed that the `IPoolableComponent` interface class is implemented by at
    least one of the components that is attached to the `GameObject` class we wish
    to pool.
  prefs: []
  type: TYPE_NORMAL
- en: The `Spawned()` method should be invoked on every implementing component each
    time the pooled `GameObject` class is respawned, while the `Despawned()` method
    gets invoked whenever it is despawned. This gives us entry points to control the
    data variables and behavior during the creation and destruction of the parent
    `GameObject` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The act of despawning `GameObject` is trivial: turn its `active` flag to `false`
    through `SetActive()`. This disables `Collider` and `Rigidbody` for physics calculations,
    removes it from the list of renderable objects, and essentially takes care of
    disabling all interactions with all built-in Unity engine subsystems in a single
    stroke. The only exception is any coroutines that are currently invoking on the
    object since, as you learned in [Chapter 2](https://cdp.packtpub.com/unity_2017_game_optimization__second_edition/wp-admin/post.php?post=123&action=edit#post_44),
    *Scripting Strategies*, coroutines are invoked independently of any `Update()`
    and `GameObject` activity. We will, therefore, need to call `StopCoroutine()`
    or `StopAllCoroutines()` during the despawning of such objects.'
  prefs: []
  type: TYPE_NORMAL
- en: Also, components typically hook into our own custom gameplay subsystems as well,
    so the `Despawn()` method allows our components to take care of any custom cleanup
    before shutting down. For example, we would probably want to use `Despawn()` to
    deregister the component from the messaging system we defined in [Chapter 2](https://cdp.packtpub.com/unity_2017_game_optimization__second_edition/wp-admin/post.php?post=123&action=edit#post_44),
    *Scripting Strategies*.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, successfully respawning the `GameObject` is a lot more complicated.
    When we respawn an object, there will be many settings that were left behind when
    the object was previously active, and these must be reset to avoid conflicting
    behaviors. A common problem with this is the Rigidbody's `linearVelocity` and
    `angularVelocity` properties. If these values are not explicitly reset before
    the object is reactivated, then the newly respawned object will continue moving
    with the same velocity the old version had when it was despawned.
  prefs: []
  type: TYPE_NORMAL
- en: 'This problem becomes further complicated by the fact that built-in components
    are `sealed`, which means that they cannot be derived from. So, to avoid these
    issues, we can create a custom component that resets the attached `Rigidbody`Â instance
    whenever the object is despawned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Note that the best place to perform the cleanup task is during despawning, because
    we cannot be certain in what order the `GameObject` class's `IPoolableComponent`
    interface classes will have their `Spawned()` methods invoked. It is unlikely
    that another `IPoolableComponent` will change the object's velocity during despawning,
    but it is possible that a different `IPoolableComponent` attached to the same
    object might want to set the Rigidbody's initial velocity to some important value
    during its own `Spawned()` method. Ergo, performing the velocity reset during
    the `ResetPooledRigidbodyComponent` class's `Spawned()` method could potentially
    conflict with other components and cause some very confusing bugs.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, creating poolable components that are not self-contained and tend to
    tinker with other components like this is one of the biggest dangers of implementing
    a pooling system. We should minimize such design and routinely verify them when
    we're trying to debug strange issues in our game.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the sake of illustration, here is the definition of a simple poolable component
    making use of the `MessagingSystem` class we defined in Chapter 2, *Scripting
    Strategies*. This component automatically handles some basic tasks every time
    the object is spawned and despawned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The Prefab pooling system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hopefully, we now have an understanding of what we need from our pooling system,
    so all that''s left is to implement it. The requirements are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It must accept requests to spawn a `GameObject`Â instance from a Prefab, an
    initial position, and an initial rotation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a despawned version already exists, it should respawn the first available
    one
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If it does not exist, then it should instantiate a new `GameObject`Â instance
    from the Prefab
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In either case, the `Spawned()` method should be invoked on all `IPoolableComponent`
    interface classes attached to `GameObject`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It must accept requests to despawn a specific `GameObject`Â instance:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the object is managed by the pooling system, it should deactivate it and
    call the `Despawned()` method on all `IPoolableComponent` interface classes attached
    to `GameObject`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the object is not managed by the pooling system, it should send an error
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The requirements are fairly straightforward, but the implementation requires
    some investigation if we wish to make the solution performance-friendly. Firstly,
    a typical singleton would be a good choice for the main entry point since we want
    this system to be globally accessible from anywhere:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The main task for object spawning involves accepting a Prefab reference and
    figuring whether we have any despawned GameObjects that were originally instantiated
    from the same reference. To do this, we will essentially want our pooling system
    to keep track of two different lists for any given Prefab reference: a list of
    active (spawned) GameObjects and a list of inactive (despawned) objects that were
    instantiated from it. This information would be best abstracted into a separate
    class, which we will name `PrefabPool`.'
  prefs: []
  type: TYPE_NORMAL
- en: To maximize the performance of this system (and hence make the largest gains
    possible, relative to just allocating and deallocating objects from memory all
    of the time), we will want to use some fast data structures in order to acquire
    the corresponding `PrefabPool` objects whenever a spawn or despawn request comes
    in.
  prefs: []
  type: TYPE_NORMAL
- en: Since spawning involves being given a Prefab, we will want a data structure
    that can quickly map Prefabs to the `PrefabPool` that manages them. Also, since
    despawning involves being given `GameObject`, we will want another data structure
    that can quickly map spawned GameObjects to the `PrefabPool`Â instance that originally
    spawned them. A pair of dictionaries would be a good choice for both of these
    needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define these dictionaries in our `PrefabPoolingSystem` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll define what happens when we `Spawn` an object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The `Spawn()` method will be given a `prefab` reference, an initial `position`,
    and an initial `rotation`. We need to figure out which `PrefabPool` the `prefab`
    reference belongs to (if any), ask it to spawn a new `GameObject`Â instance using
    the data provided, and then return the spawned object to the requestor. We will
    first check our Prefab-to-pool map to check whether a pool already exists for
    this Prefab. If not, we immediately create one. In either case, we then ask `PrefabPool`
    to spawn us a new object. `PrefabPool` will either end up respawning an object
    that was despawned earlier or instantiate a new one (if there aren't any inactive
    instances left).
  prefs: []
  type: TYPE_NORMAL
- en: This class doesn't particularly care how `PrefabPool` creates the object. It
    just wants the instance generated by the `PrefabPool` class so that it can be
    entered into the GameObject-to-pool map and returned to the requestor.
  prefs: []
  type: TYPE_NORMAL
- en: 'For convenience, we can also define an overload that places the object at the
    world''s center. This is useful for GameObjects that aren''t visible and just
    need to exist in the scene:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Note that no actual spawning and despawning are taking place, yet. This task
    will eventually be implemented within the `PrefabPool` class.
  prefs: []
  type: TYPE_NORMAL
- en: Despawning involves being given `GameObject` and then figuring out which `PrefabPool`
    is managing it. This could be achieved by iterating through our `PrefabPool` objects
    and checking whether they contain the given `GameObject`Â instance. However, if
    we end up generating a lot of Prefab pools, then this iterative process can take
    a while. We will always end up with as many `PrefabPool` objects as we have Prefabs
    (at least, so long as we manage all of them through the pooling system). Most
    projects tend to have dozens, hundreds, if not thousands, of different Prefabs.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the GameObject-to-pool map is maintained to ensure that we always have
    rapid access to `PrefabPool` that originally spawned the object. It can also be
    used to quickly check whether the given `GameObject`Â instance is even managed
    by the pooling system to begin with. Here is the method definition for the despawning
    method, which takes care of these tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Note that the `Despawn()` method of both `PrefabPoolingSystem` and `PrefabPool`
    returns a Boolean that can be used to check whether the object was successfully
    despawned.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, thanks to the two maps we're maintaining, we can quickly access
    the `PrefabPool`Â instance that manages the given reference, and this solution
    will scale for any number of Prefabs that the system manages.
  prefs: []
  type: TYPE_NORMAL
- en: Prefab pools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have a system that can handle multiple Prefab pools automatically,
    the only thing left is to define the behavior of the pools. As mentioned previously,
    we will want the `PrefabPool` class to maintain two data structures: one for active
    (spawned) objects that have been instantiated from the given Prefab and another
    for inactive (despawned) objects.'
  prefs: []
  type: TYPE_NORMAL
- en: Technically, the `PrefabPoolingSystem` class already maintains a map of which
    Prefab is governed by which `PrefabPool`, so we can actually save a little memory
    by making the `PrefabPool` class dependent upon the `PrefabPoolingSystem` class
    to give it the reference to the Prefab it is managing. Consequently, the two data
    structures would be the only member variables `PrefabPool` needs to keep track
    of.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, for each spawned `GameObject`, it must also maintain a list of all
    of its `IPoolableComponent` references to invoke the `Spawned()` and `Despawned()`
    methods on them. Acquiring these references can be a costly operation to perform
    at runtime, so it would be best to cache the data in a simple struct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: This `struct` will contain a reference to `GameObject` and the precached list
    of all of its `IPoolableComponent` components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can define the member data of our `PrefabPool` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The data structure for the active list should be a dictionary to do a quick
    lookup for the corresponding `PoolablePrefabData` component from any given `GameObject`
    reference. This will be useful during object despawning.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, the inactive data structure is defined as `Queue`, but it will work
    equally well as `List`, `Stack`, or really any data structure that needs to regularly
    expand or contract, where we only need to pop items from one end of the group,
    since it does not matter which object it is. It only matters that we retrieve
    one of them. `Queue` is useful in this case because we can both retrieve and remove
    the object from the data structure in a single call to `Dequeue()`.
  prefs: []
  type: TYPE_NORMAL
- en: Object spawning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s define what it means to spawn `GameObject` in the context of our pooling
    system: at some point, `PrefabPool` will get a request to spawn `GameObject` from
    a given Prefab, at a particular position and rotation. The first thing we should
    check is whether or not we have any inactive instances of the Prefab. If so, then
    we can dequeue the next available one from `Queue` and respawn it. If not, then
    we need to instantiate a new `GameObject` from the Prefab using `GameObject.Instantiate()`.
    At this moment, we should also create a `PoolablePrefabData` object to store the
    `GameObject` reference and acquire the list of all `MonoBehaviours` that implement
    `IPoolableComponent` that are attached to it.'
  prefs: []
  type: TYPE_NORMAL
- en: Either way, we can now activate `GameObject`, set its position and rotation,
    and call the `Spawned()` method on all of its `IPoolableComponent` references.
    Once the object has been respawned, we can add it to the list of active objects
    and return it to the requestor.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the definition of the `Spawn()` method that defines this behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Instance prespawning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since we are using `GameObject.Instantiate()` whenever the `PrefabPool` has
    run out of despawned instances, this system does not completely rid us of runtime
    object instantiation, hence heap memory allocation. It's important to prespawn
    the expected number of instances that we will need during the lifetime of the
    current scene so that we minimize or remove the need to instantiate more during
    runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we shouldn't prespawn too many objects. It would be wasteful to prespawn
    100 explosion particle effects if the most we will ever expect to see in the scene
    at any given time is three or four. Conversely, spawning too few instances will
    cause excessive runtime memory allocations, and the goal of this system is to
    push the majority of allocation to the start of a scene's lifetime. We need to
    be careful about how many instances we maintain in memory so that we don't waste
    more memory space than necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define a method in our `PrefabPoolingSystem` class that we can use to
    quickly prespawn a given number of objects from a Prefab. This essentially involves
    spawning `N` objects and then immediately despawning them all:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We would use this method during scene initialization to prespawn a collection
    of objects to use in the level. Take, for example, the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Object despawning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, there is the act of despawning the objects. As mentioned previously,
    this primarily involves deactivating the object, but we also need to take care
    of various bookkeeping tasks and invoking `Despawned()` on all of its `IPoolableComponent`
    references.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the method definition for `PrefabPool.Despawn()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: First, we verify that the object is being managed by the pool and then we grab
    the corresponding `PoolablePrefabData`Â instance to access the list of `IPoolableComponent`
    references. Once `Despawned()` is invoked on all of them, we deactivate the object,
    remove it from the active list, and push it into the inactive queue so that it
    can be respawned later.
  prefs: []
  type: TYPE_NORMAL
- en: Prefab pool testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following class definition allows us to perform a simple hands-on test
    with the `PrefabPoolingSystem` class; it will support three Prefabs and prespawn
    five instances of each during application initialization. We can press the *1*,
    *2*, *3*, or *4* keys to spawn an instance of each type and then press *Q*, *W*,
    *E*, and *R* to despawn a random instance of each type, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Once we spawn more than five instances of any of the Prefabs, it will need to
    instantiate a new one in memory, costing us some memory allocation. However, if
    we observe the Memory Area in the Profiler window, while we only spawn and despawn
    instances that already exist, then we will notice that absolutely no new allocations
    take place.
  prefs: []
  type: TYPE_NORMAL
- en: Prefab pooling and scene loading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is one important caveat to this system that has not yet been mentioned:
    the `PrefabPoolingSystem` class will outlast the scene''s lifetime since it is
    a static class. This means that when a new scene is loaded, the pooling system''s
    dictionaries will attempt to maintain references to any pooled instances from
    the previous scene, but Unity forcibly destroys these objects regardless of the
    fact that we are still keeping references to them (unless they were set to `DontDestroyOnLoad()`),
    and so the dictionaries will be full of `null` references. This would cause some
    serious problems for the next scene.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We should, therefore, create a method in `PrefabPoolingSystem` that resets
    the pooling system in preparation for this likely event. The following method
    should be called before a new scene is loaded so that it is ready for any early
    calls to `Prespawn()` in the next scene:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Note that if we also invoke a garbage collection during scene transitions, there's
    no need to explicitly destroy the `PrefabPool` objects these dictionaries were
    referencing. Since these were the only references to the `PrefabPool` objects,
    they will be deallocated during the next garbage collection. If we aren't invoking
    garbage collection between scenes, then the `PrefabPool` and `PooledPrefabData`
    objects will remain in memory until that time.
  prefs: []
  type: TYPE_NORMAL
- en: Prefab pooling summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This pooling system provides a decent solution to the problem of runtime memory
    allocations for GameObjects and Prefabs, but, as a quick reminder, we need to
    be aware of the following caveats:'
  prefs: []
  type: TYPE_NORMAL
- en: We need to be careful about properly resetting important data in respawned objects
    (such as `Rigidbody` velocity)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We must ensure that we don't prespawn too few, or too many, instances of a Prefab
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We should be careful of the order of execution of `Spawned()` and `Despawned()`
    methods on `IPoolableComponent` and not assume that they will be called in a particular
    order
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We must call `Reset()` on `PrefabPoolingSystem` when loading a new scene to
    clear any `null` references to objects, which may no longer exist
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are several other features that we could implement. These will be left
    as academic exercises if we wish to extend this system in the future:'
  prefs: []
  type: TYPE_NORMAL
- en: Any `IPoolableComponent` added to the `GameObject` after initialization will
    not have their `Spawned()` or `Despawned()` methods invoked since we only collect
    this list when `GameObject` is first instantiated. We could fix this by changing
    `PrefabPool` to keep acquiring `IPoolableComponent` references every time `Spawned()`
    and `Despawned()` are invoked at the cost of additional overhead during spawning/despawning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any `IPoolableComponent` attached to children of the Prefab's root will also
    not be counted. This could be fixed by changing `PrefabPool` to use `GetComponentsInChildren<T>`
    at the cost of additional overhead if we're using Prefabs with deep hierarchies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prefab instances that already exist in the scene will not be managed by the
    pooling system. We could create a component that needs to be attached to such
    objects and that notifies the `PrefabPoolingSystem` class of its existence in
    its `Awake()` callback, which passes the reference along to the corresponding
    `PrefabPool`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We could implement a way for `IPoolableComponent` to set a priority during acquisition
    and directly control the order of execution for their `Spawned()` and `Despawned()`
    methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We could add counters that keep track of how long objects have been sitting
    in the Inactive list relative to total scene lifetime and print out the data during
    shutdown. This could tell us whether or not we're prespawning too many instances
    of a given Prefab.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This system will not interact kindly with Prefab instances that set themselves
    to `DontDestroyOnLoad()`. It might be wise to add a Boolean to every `Spawn()`
    call to say whether the object should persist or not and keep them in a separate
    data structure that is not cleared out during `Reset()`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We could change `Spawn()` to accept an argument that allows the requestor to
    pass custom data to the `Spawned()` function of `IPoolableObject` for initialization
    purposes. This could use a system similar to how custom message objects were derived
    from the `Message` class for our messaging system in [Chapter 2](https://cdp.packtpub.com/unity_2017_game_optimization__second_edition/wp-admin/post.php?post=123&action=edit#post_44),
    *Scripting Strategies*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IL2CPP optimizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unity Technologies have released a few blog posts on interesting ways to improve
    the performance of IL2CPP in some circumstances, but they can be difficult to
    manage. If you''re using IL2CPP and need to eke out the last little bit of performance
    from our application that we can, then check out the blog series at the following
    links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://blogs.unity3d.com/2016/07/26/il2cpp-optimizations-devirtualization/](https://blogs.unity3d.com/2016/07/26/il2cpp-optimizations-devirtualization/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://blogs.unity3d.com/2016/08/04/il2cpp-optimizations-faster-virtual-method-calls/](https://blogs.unity3d.com/2016/08/04/il2cpp-optimizations-faster-virtual-method-calls/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://blogs.unity3d.com/2016/08/11/il2cpp-optimizations-avoid-boxing/](https://blogs.unity3d.com/2016/08/11/il2cpp-optimizations-avoid-boxing/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WebGL optimizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unity Technologies have also released several blog posts covering WebGL applications,
    which includes some crucial information about memory management that all WebGL
    developers should know. These can be found at the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://blogs.unity3d.com/2016/09/20/understanding-memory-in-unity-webgl/](https://blogs.unity3d.com/2016/09/20/understanding-memory-in-unity-webgl/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://blogs.unity3d.com/2016/12/05/unity-webgl-memory-the-unity-heap/](https://blogs.unity3d.com/2016/12/05/unity-webgl-memory-the-unity-heap/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've covered a humongous amount of theory and language concepts in this chapter,
    which have hopefully shed some light on how the internals of the Unity engine
    and C# language work. These tools try their best to spare us from the burden of
    complex memory management, but there is still a whole host of concerns we need
    to keep in mind as we develop our game. Between the compilation processes, multiple
    memory domains, the complexities of value types versus reference types, passing
    by value versus passing by reference, boxing, object pooling, and various quirks
    within the Unity API, you have a lot of things to worry about. However, with enough
    practice, you will learn to overcome them without needing to keep referring to
    giant tomes such as this!
  prefs: []
  type: TYPE_NORMAL
- en: With this chapter, we have covered all of the possible optimization areas in
    classic Unity. However, with the 2019.1 release, Unity officially introduced the
    **Data-Oriented Technology Stack** (**DOTS**), a set of new fundamental APIs to
    access a completely new optimization level, especially in modern massively multi-threading
    systems. Follow me to the next chapter, where we will explore this new frontier.
  prefs: []
  type: TYPE_NORMAL
