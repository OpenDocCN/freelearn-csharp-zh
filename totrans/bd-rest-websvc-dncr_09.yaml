- en: Scaling RESTful Services (Performance of Web Services)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the world of the web, everyone is either writing or looking for a web application.
    As demand increases, every web application needs to be able to serve more requests—sometimes
    thousands of requests a day. Applications should therefore be written to handle
    this huge requests.
  prefs: []
  type: TYPE_NORMAL
- en: Say, as an example, that you are part of a development and support team that
    is responsible for developing the company's flagship product, FlixOne Store. This
    product is popular and gains traction, leading to your e-commerce website (FlixOne)
    being inundated with consumer traffic. The payment service in your system is slow,
    which has almost brought the whole thing down, causing you to lose customers.
    Although this is an imaginary scenario, it can happen in real life and can lead
    to a loss of business. To avoid such a scenario, you should think about the scalability
    of the FlixOne application.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability is one of the most important non-functional requirements for a critical
    system. Serving a couple of users with hundreds of transactions is not the same
    as serving millions of users with several million transactions.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss scalability in general. We'll also discuss
    how to scale RESTful services, what to consider when we design them, and how to
    avoid cascading failures using different patterns including techniques, libraries,
    and tools that can also be helpful for our regular applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will have learned about:'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load balancing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to scalability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering is a way to provide the same service on more than one server. With
    the addition of more servers, you can avoid uncontrolled situations, such as failovers,
    system crashes, and so on. In the context of databases, clustering refers to the
    ability of several server instances to connect with a single server. Fault tolerance
    and load balancing are two of the main advantages of clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A load balancer is a useful tool when clustering. You can define a **load balance**
    as a device that helps to distribute network or application traffic within and
    across the cluster servers, and to improve the responsiveness of the application.
  prefs: []
  type: TYPE_NORMAL
- en: In implementation, a load balancer is placed between the client and the servers.
    It helps to balance multiple application requests across multiple servers. In
    the other words, a load balancer reduces individual server time and prevents application
    server failure.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A load balancer works to make sure that an application''s server is available.
    If one application''s server is unavailable, the load balancer redirects all new
    requests to the available servers, as illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/867aa115-47b5-4579-8b1b-e94c3e9f7bc8.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, you can see a load balancer in its typical environment,
    where a system accepts multiple requests from different sources over the internet,
    which are then managed from multiple servers by the load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: In .NET, this arrangement is also known as a web farm ([https://www.codeproject.com/Articles/114910/What-is-the-difference-between-Web-Farm-and-Web-Ga](https://www.codeproject.com/Articles/114910/What-is-the-difference-between-Web-Farm-and-Web-Ga)).
  prefs: []
  type: TYPE_NORMAL
- en: 'A load balancer uses various algorithms, also known as load balancer methods:
    the least connection method, round-robin method, least response time method, least
    bandwidth method, least packets method, custom load method, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: A load balancer plays an important role in the scalability of an application
    as it makes sure that an application's server is available for server requests.
    Note that you will need to arrange your hardware infrastructure without a code
    change to cater for a load balancer (however, there will be some scenarios that
    call for a code change). There are a lot of load balancers on the market, such
    as Incapsula ([https://www.incapsula.com/](https://www.incapsula.com/)), F5 ([https://www.f5.com/](https://www.f5.com/)),
    Citrix Netscaler ([https://www.citrix.com/](https://www.citrix.com/)), Dyn ([https://dyn.com/](https://dyn.com/)),
    Amazon Elastic Load Balancing, and Amazon ELB ([https://aws.amazon.com/](https://aws.amazon.com/)).
  prefs: []
  type: TYPE_NORMAL
- en: In the coming sections, we will look at the different ways you can scale systems.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to scalability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every application has its own ability to serve requests. An application’s ability
    refers to its performance and how it meets its objectives when load is increased.
  prefs: []
  type: TYPE_NORMAL
- en: Many web applications refer to this as a number of requests in a stipulated
    time.
  prefs: []
  type: TYPE_NORMAL
- en: It’s very important to make the right design decision when designing your web
    application; design decisions impact the scalability of your service. Be sure
    to strike the right balance so that your approach considers your services as well
    as their infrastructure, along with any need for scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Performance and scalability are two different characteristics of a system. Performance
    deals with the throughput of the system, whereas scalability deals with serving
    the desired throughput for a larger number of users, or a larger number of transactions.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling in (vertical scaling)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Scaling in** or **scaling up** (also called **vertical scaling**) is a way
    of achieving scalability through the addition of more resources, such as memory
    or faster processors, to the same machine. This is not always applicable to all
    applications, as costing is also a factor when considering vertical scaling.'
  prefs: []
  type: TYPE_NORMAL
- en: You can also upgrade your resources or hardware instead of adding new resources
    to your machine. For example, if you have 8 GB of RAM, you can upgrade it to 16
    GB, and the same thing would be applicable for processors and other resources.
    Unfortunately, with upgrades in hardware, there is a limit to how much you can
    scale the machine. This may lead to simply shifting the bottleneck, rather than
    solving the real problem of improving scalability.
  prefs: []
  type: TYPE_NORMAL
- en: You can also migrate your application to an entirely different machine, such
    as simply migrating your application to a more powerful MacOS, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling vertically does not involve any code changes so it is an easy task,
    but it does involve extra cost as it is quite an expensive technique. Stack Overflow
    is one of those rare examples of a .NET-based system that is scaled vertically.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling out (horizontal scaling)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scaling up, scaling out, or horizontal scaling adds more servers or nodes to
    service requests, rather than resources. If you do not want to scale up your application,
    there is always a way to scale it out.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling out is a successful strategy when the application code does not depend
    on the server it is running on. However, if a request needs to be executed on
    a specific server, that is, if the application code has server affinity, it will
    be difficult to scale that out. In the case of stateless code, it is easier execute
    on any server. Hence, scalability is improved when stateless code is run on horizontally-scaled
    machines or clusters.Due to the nature of horizontal scaling, it is a commonly
    used approach across the industry. There are  many examples of large scalable
    systems managed in this way, such as Google, Amazon, and Microsoft.
  prefs: []
  type: TYPE_NORMAL
- en: Linear scalability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear scalability refers to scaling an application vertically with the application
    of Amdahl's law ([https://en.wikipedia.org/wiki/Amdahl%27s_law](https://en.wikipedia.org/wiki/Amdahl%27s_law)).
    Here, you can also think about parallel computing.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel computing is a type of computing architecture that indicates simultaneous
    processing with the execution of several processors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The benefits of linear scalability in your application include:'
  prefs: []
  type: TYPE_NORMAL
- en: No code changes are required
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extra resources can be easily added
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is physical availability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed caching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the help of distributed caching techniques, we can improve the scalability
    of our RESTful web services (web API). A distributed cache can be stored on multiple
    nodes of a cluster. A distributed cache enhances a web service's throughput, as
    the cache no longer requires an I/O trip to any external resource.
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach has the following advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Clients get the same results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The distributed cache is backed up by a persistence store and runs as a different
    remote process; even if the app server restarts or has any problems, it in no
    way affects the cache
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The source's data store has fewer requests made to it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caching persisted data (data-tier caching)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to application performance, you should also be considering database
    performance. By caching persisted data, you will get better performance after
    adding a caching layer to your database. This is also important when read requests
    are heavily used in an application. We will now take a look at EF Core’s levels
    of caching as an example.
  prefs: []
  type: TYPE_NORMAL
- en: First-level caching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is an inbuilt session cache enabled by EF Core. From the first request
    from a service, an object is retrieved from the database and is stored in an EF
    Core session. In other words, EF Object Context and DbContext maintain state information
    about the entities they are managing. As soon as the context is no longer available,
    its state information is also gone.
  prefs: []
  type: TYPE_NORMAL
- en: Second-level caching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Second-level caching is important for applications that have been developed
    in a mostly distributed manner or have-long running requests that need persisted
    data, such as web applications. Second-level caching exists outside the scope
    of a transaction or application, and these caches are available for any context
    or instance. You can use the caching mechanism available to your application instead
    of writing your own code, such as Memcached.
  prefs: []
  type: TYPE_NORMAL
- en: Application caching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Application caching or application-tier caching helps to cache any object in
    an application. This further improves the scalability of an application. In the
    following section, we will discuss the various caching mechanisms available.
  prefs: []
  type: TYPE_NORMAL
- en: CacheCow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CacheCow comes into the picture when you want to implement HTTP caching on both
    the client and server. This is a lightweight library and ASP.NET web API support
    is currently available. CacheCow is open source and comes with an MIT license
    that is available on GitHub ([https://github.com/aliostad/CacheCow](https://github.com/aliostad/CacheCow)).
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started with CacheCow, you need to get ready for both the server and
    client by taking the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Install the `Install-Package CacheCow.Server` NuGet package within your ASP.NET
    Web API project; this will be your server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install the `Install-Package CacheCow.Client` NuGet package within your client
    project; the client application will be WPF, Windows Form, Console, or any other
    web application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a cache store. You need to create a cache
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: store
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: on the server side that requires a database for storing cache metadata ([https://github.com/aliostad/CacheCow/wiki/Getting-started#cache-store](https://github.com/aliostad/CacheCow/wiki/Getting-started#cache-store)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Memcached
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Memcached is an open source project that is customizable; you can use the source
    code and add to and update it as per your requirements. Memcached is defined by
    its official page ([https://memcached.org/](https://memcached.org/)) as:'
  prefs: []
  type: TYPE_NORMAL
- en: '"An in-memory key-value store for small chunks of arbitrary data (strings,
    objects) from results of database calls, API calls, or page rendering."'
  prefs: []
  type: TYPE_NORMAL
- en: Refer to [https://www.deanhume.com/memcached-for-c-a-walkthrough/](https://www.deanhume.com/memcached-for-c-a-walkthrough/)
    for a complete walkthrough.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Redis Cache
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Azure Redis Cache is built on top of an open source store called Redis ([https://github.com/antirez/redis](https://github.com/antirez/redis)),
    which is an in-memory database and persists on disk. As per Microsoft''s description
    ([https://azure.microsoft.com/en-in/services/cache/](https://azure.microsoft.com/en-in/services/cache/)):'
  prefs: []
  type: TYPE_NORMAL
- en: '"Azure Redis Cache is based on the popular open source Redis cache. It gives
    you access to a secure, dedicated Redis cache, managed by Microsoft and accessible
    from any application within Azure."'
  prefs: []
  type: TYPE_NORMAL
- en: 'Getting started with Azure Redis Cache is very simple if you take the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a web API project. Refer to our code example in previous chapters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement Redis. For a referral point, use [https://github.com/StackExchange/StackExchange.Redis](https://github.com/StackExchange/StackExchange.Redis).
    Also, install the `Install-Package StackExchange.Redis` NuGet package.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update your config file for CacheConnection ([https://docs.microsoft.com/en-us/azure/redis-cache/cache-dotnet-how-to-use-azure-redis-cache#NuGet](https://docs.microsoft.com/en-us/azure/redis-cache/cache-dotnet-how-to-use-azure-redis-cache#NuGet)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, publish on Azure ([https://docs.microsoft.com/en-us/azure/redis-cache/cache-web-app-howto#publish-and-run-in-azure](https://docs.microsoft.com/en-us/azure/redis-cache/cache-web-app-howto#publish-and-run-in-azure)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Communication (asynchronous)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The term communication is self-explanatory; it is the act of interaction between
    services. Examples of this include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A service communicating with another service within the same application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A service communicating with another service outside of the application (external
    services)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A service communicating with a component (internal or external)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This communication happens over the HTTP protocol as messages or data traverse
    over the wire.
  prefs: []
  type: TYPE_NORMAL
- en: Your application's performance impacts how services communicate with each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Asynchronous communication is one of the methods that help to scale applications.
    In ASP.NET Core, we can achieve this by using asynchronous HTTP calls (asynchronous
    programming): [https://docs.microsoft.com/en-us/dotnet/standard/asynchronous-programming-patterns/](https://docs.microsoft.com/en-us/dotnet/standard/asynchronous-programming-patterns/)'
  prefs: []
  type: TYPE_NORMAL
- en: You should be careful with operations while handling asynchronous communications,
    for example, when adding a new product with an image. A system is designed so
    that it creates a thumbnail of the images in different sizes. This is a time-consuming
    task that could lead to a performance hit if handled incorrectly. From a design
    perspective, an asynchronous operation would not work in this scenario. Here,
    you should implement something like a task with a callback that tells the system
    when a job is complete. Sometimes, you may also require middleware to handle requests.
  prefs: []
  type: TYPE_NORMAL
- en: The best way to implement asynchronous communication is with an asynchronous
    RESTful API.
  prefs: []
  type: TYPE_NORMAL
- en: When creating a scalable system, you must always think about asynchronous communication.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed scalability, including the libraries available
    to help with it, tools, and so on. We then discussed how to scale RESTful services,
    what to consider when we design them, and how to avoid cascading failure using
    different patterns.
  prefs: []
  type: TYPE_NORMAL
- en: In the coming chapters, we will discuss and build a web client to call and consume
    RESTful services.
  prefs: []
  type: TYPE_NORMAL
