- en: Caching Strategies for Distributed Systems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式系统的缓存策略
- en: In the previous chapter, we learned all about common patterns for applying security
    to a network-hosted application. In this chapter, we will look at various ways
    to improve the performance of our network software by establishing intermediate
    caches. We'll see how using a cache to persist frequently accessed highly available
    data can grant us those performance improvements. We'll look at what a cache is
    and some of the various ways it can be used. Then, we'll undertake a thorough
    examination of one of the most common and complex architectural patterns for network
    caches. Finally, we'll demonstrate how to use caching at various levels of the
    application architecture to achieve our goals with a reasonable balance of developer
    effort and latency reduction.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了将安全性应用于托管在网络上的应用程序的常见模式。在本章中，我们将探讨通过建立中间缓存来提高我们网络软件性能的各种方法。我们将看到使用缓存来持久化频繁访问的高可用数据如何使我们获得这些性能提升。我们将探讨缓存是什么以及它可以以各种方式被使用的各种方法。然后，我们将对网络缓存的常见且复杂的架构模式进行彻底的审查。最后，我们将演示如何在应用架构的各个级别使用缓存，以在合理的开发人员努力和延迟减少之间达到我们的目标。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: The potential performance improvements to be gained by caching the results of
    common requests
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过缓存常见请求的结果所能获得的潜在性能提升
- en: Basic patterns for caching session data to enable reliable interactions with
    parallel deployments of an application
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓存会话数据的基本模式，以实现与应用程序并行部署的可靠交互
- en: Understanding out to leverage caches within our .NET Core applications
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解如何在我们的 .NET Core 应用程序中利用缓存
- en: The strengths and weaknesses of various cache providers, including distributed
    network-hosted caches, memory caches, and database caches
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种缓存提供者的优缺点，包括分布式网络托管缓存、内存缓存和数据库缓存
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter will have sample code to demonstrate each of the caching strategies
    we discuss. To work with that code, you'll need your trusty IDE (Visual Studio)
    or code editor (Visual Studio Code). You can download the sample code to work
    directly with it from this book's GitHub repository: [https://github.com/PacktPublishing/Hands-On-Network-Programming-with-C-and-.NET-Core/tree/master/Chapter
    15](https://github.com/PacktPublishing/Hands-On-Network-Programming-with-C-and-.NET-Core/tree/master/Chapter%2015).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将包含示例代码，以演示我们讨论的每种缓存策略。要使用该代码，您需要您信任的 IDE（Visual Studio）或代码编辑器（Visual Studio
    Code）。您可以从本书的 GitHub 仓库直接下载示例代码：[https://github.com/PacktPublishing/Hands-On-Network-Programming-with-C-and-.NET-Core/tree/master/Chapter%2015](https://github.com/PacktPublishing/Hands-On-Network-Programming-with-C-and-.NET-Core/tree/master/Chapter%2015)。
- en: Check out the following video to see the code in action: [http://bit.ly/2HY67CM](http://bit.ly/2HY67CM)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频以查看代码的实际运行情况：[http://bit.ly/2HY67CM](http://bit.ly/2HY67CM)
- en: We'll also be using the Windows Subsystem for Linux to host the Linux-based
    Redis cache server on our local machine. Of course, if you're already running
    on a *nix system such as OS X or a Linux distribution, you don't have to worry
    about this. Alternatively, when you're running the application locally, if you
    don't have admin privileges or don't have any interest in learning the Redis cache
    server, you can modify the sample code slightly to use a different cache provider,
    which you'll learn about as we move through this chapter. However, I would recommend
    familiarizing yourself with the Redis cache since it is widely used and is an
    excellent choice for most circumstances. If you choose to do so, you can find
    instructions for installing the Linux Subsystem here: [https://docs.microsoft.com/en-us/windows/wsl/install-win10.](https://docs.microsoft.com/en-us/windows/wsl/install-win10)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用 Windows Subsystem for Linux 在本地机器上托管基于 Linux 的 Redis 缓存服务器。当然，如果您已经在运行类似
    OS X 或 Linux 发行版的 *nix 系统上运行，您不必担心这一点。或者，当您在本地运行应用程序时，如果您没有管理员权限或对学习 Redis 缓存服务器不感兴趣，您可以稍微修改示例代码以使用不同的缓存提供者，您将在本章的学习过程中了解到。然而，我建议您熟悉
    Redis 缓存，因为它被广泛使用，并且是大多数情况下的优秀选择。如果您选择这样做，您可以在以下链接中找到安装 Linux 子系统的说明：[https://docs.microsoft.com/en-us/windows/wsl/install-win10](https://docs.microsoft.com/en-us/windows/wsl/install-win10)
- en: Once that's done, you can find the instructions for installing and running Redis
    here: [https://redislabs.com/blog/redis-on-windows-10/.](https://redislabs.com/blog/redis-on-windows-10/)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，您可以在以下链接中找到安装和运行 Redis 的说明：[https://redislabs.com/blog/redis-on-windows-10/](https://redislabs.com/blog/redis-on-windows-10/)
- en: Why cache at all?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么需要缓存呢？
- en: While it introduces additional complexity for the developers who are responsible
    for implementing it, a well-designed caching strategy can improve application
    performance significantly. If your software relies heavily on network resources,
    maximizing your use of caches can save your users time in the form of faster performance,
    and your company money in the form of lower network overhead. However, knowing
    when to cache data, and when it would be inappropriate to do so, is not always
    intuitive for developers. So, when should you be leveraging a caching strategy,
    and why?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然它给负责实施它的开发者带来了额外的复杂性，但一个精心设计的缓存策略可以显著提高应用程序的性能。如果你的软件严重依赖网络资源，最大限度地利用缓存可以在更快的性能上节省用户的时间，并在降低网络开销上为公司节省金钱。然而，知道何时缓存数据，何时这样做不合适，对于开发者来说并不总是直观的。那么，你何时应该利用缓存策略，为什么？
- en: An ideal caching scenario
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理想的缓存场景
- en: Suppose you have an application that builds reports based on quarterly sales
    numbers. Imagine it has to pull hundreds of thousands of records from a handful
    of different databases, each with variable response times. Once it's acquired
    all of that data, it has to run extensive aggregation calculations on the records
    returned to produce the statistics that are displayed in the report output. Moreover,
    these reports are generated by dozens or even hundreds of different business analysts
    in a given day. Each report aggregates, for the most part, the same information,
    but some reports are structured to highlight different aspects of the data for
    different business concerns of the analysts. A naive approach to this problem
    would simply be to access the requested data on demand and return the results,
    reliably, with terrible response times. But does that necessarily have to be the
    case?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个基于季度销售数据生成报告的应用程序。想象一下，它必须从几个不同的数据库中提取数十万条记录，每个数据库的响应时间各不相同。一旦获取了所有这些数据，它必须在返回的记录上运行广泛的聚合计算，以生成报告中显示的统计数据。此外，这些报告是由数十甚至数百名不同的业务分析师在一天内生成的。大多数情况下，每个报告都会汇总相同的信息，但有些报告的结构是为了突出不同分析师不同业务关注点的数据。对这个问题的天真方法就是按需访问请求的数据，并可靠地返回结果，但响应时间极差。但这一定是必须的吗？
- en: What I've just described is actually an ideal scenario for designing and implementing
    a caching strategy. What I've described is a system reliant on data owned by an
    external resource or process, which means that round-trip latency can be eliminated.
    I also noted that it is quarterly sales data, which means that it is presumably
    only ever updated, at most, once every three months. Finally, I mentioned that
    there are users generating reports with this remotely accessed data dozens to
    hundreds of times a day. For a distributed system, there is an almost no more
    obvious circumstance for pre-caching the remote data for faster and more reliable
    access in your on-demand application operations.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我刚才描述的实际上是一个设计和实施缓存策略的理想场景。我描述的是一个依赖于外部资源或过程拥有的数据的系统，这意味着往返延迟可以被消除。我还指出这是季度销售数据，这意味着它可能最多每三个月更新一次。最后，我提到有用户每天数十次甚至数百次使用这些远程访问的数据生成报告。对于一个分布式系统来说，几乎没有比在按需应用程序操作中预先缓存远程数据以实现更快和更可靠的访问更明显的情境了。
- en: 'The contexts that motivate cache usage won''t always be so cut and dried, but
    in general these three criteria will be a strong guide for when you should consider
    it. Just always ask yourself if any of them are met:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动缓存使用的情境不会总是那么明确，但总的来说，这三个标准将是一个强有力的指导，告诉你何时应该考虑使用缓存。但始终要问自己是否满足以下任何一个条件：
- en: Accessing resources external to your application's hosted context
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问应用程序托管环境外的资源
- en: Accessing resources that are infrequently updated
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问那些更新频率不高的资源
- en: Accessing resources that are frequently used by your application
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问应用程序频繁使用的资源
- en: If any of those circumstances are met, you should start to think about what
    benefits you might reap by caching. If all of them are met, you would need to
    make a strong case for why you wouldn't implement a caching strategy. Of course,
    to understand why these criteria make caching necessary, you must first understand
    exactly what caching is and, just as importantly, what it isn't.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足上述任何一种情况，你就应该开始思考通过缓存可能带来的好处。如果所有这些条件都满足，你可能需要提出一个强有力的理由来解释为什么你不应该实施缓存策略。当然，要理解为什么这些标准使缓存变得必要，你必须首先确切地了解缓存是什么，同样重要的是，它不是什么。
- en: The principles of a data cache
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据缓存的原则
- en: Put simply, a cache is nothing more than an intermediary data store that can
    serve its data faster than the source from which the cached data originated. There
    are a number of reasons a cache could provide these speed improvements, and each
    of them requires their own consideration, so let's look at a few.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，缓存不过是一个中间数据存储，它可以比缓存数据来源更快地提供其数据。缓存可以提供这些速度提升的原因有很多，每个原因都需要单独考虑，所以让我们看看几个例子。
- en: Caching long-running queries
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缓存长时间运行的查询
- en: If you have any formal database design experience, you likely know that relational
    databases tend toward highly normalized designs to eliminate duplicate data storage,
    and increase stability and data integrity. This normalization breaks out data
    records into various tables with highly atomic field definitions, and cross-reference
    tables for aggregating hierarchical data structures. And if you're not familiar
    with those principles of database design, then it's sufficient to say that it
    typically improves the usage of space at the cost of access time for a record
    lookup.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有任何正式的数据库设计经验，你很可能知道关系数据库倾向于高度规范化的设计，以消除重复数据存储，并提高稳定性和数据完整性。这种规范化将数据记录分解成具有高度原子字段定义的各种表，以及用于聚合分层数据结构的交叉引用表。如果你不熟悉这些数据库设计原则，那么可以说，它通常以牺牲记录查找的访问时间为代价，提高了空间利用率。
- en: If you have a well-normalized relational database storing information that you
    would like to access as de-normalized flat records representing their application
    models, the queries that are used to flatten those records are often time-consuming
    and redundant. In this case, you might have a cache that stores flattened records,
    whose design is optimized for use by your application. This means that whenever
    a record needs to be updated, the process of de-normalizing the data can happen
    exactly once, sending the flattened structure to your cache. Thus, your application's
    interaction with the underlying data store can be reduced from a potentially beefy
    aggregation query against multiple tables to a simple lookup against a single
    application-specific table.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个高度规范化的关系数据库，存储的信息你想以去规范化扁平记录的形式访问，表示其应用程序模型，那么用于扁平化这些记录的查询通常既耗时又冗余。在这种情况下，你可能有一个存储扁平化记录的缓存，其设计优化了你的应用程序的使用。这意味着每当需要更新记录时，去规范化数据的过程可以恰好发生一次，将扁平结构发送到你的缓存。因此，你的应用程序与底层数据存储的交互可以从对多个表的潜在大量聚合查询减少到对单个应用程序特定表的简单查找。
- en: In this scenario, simply adding an intermediary cache for the long-running data-access
    operations can guarantee a performance improvement, even if the actual data storage
    system for the cache is the same as the origin. The primary performance bottleneck
    being mitigated by the cache is the query operation itself, so even if there is
    no reduction in network latency, you could still expect to reap some meaningful
    benefits.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，只需为长时间运行的数据访问操作添加一个中间缓存，就可以保证性能提升，即使缓存的实际数据存储系统与原始系统相同。缓存缓解的主要性能瓶颈是查询操作本身，因此即使没有减少网络延迟，你仍然可以期待获得一些有意义的收益。
- en: It should be mentioned that this strategy can be applied to any long-running
    operations in your application flow. I used the example of slow database queries
    simply because those are the most commonly encountered bottlenecks with larger
    enterprise systems. However, in your work, you may find it beneficial to cache
    the results of computationally intensive operations executed within your application's
    host process. In this case, you'd likely be using an in-memory cache or a cache
    hosted on your own system, so there would be no possibility of improving your
    latency. But imagine deploying your application to a cloud hosting provider that's
    charging you by your application's uptime. In that scenario, cutting out a multi-second
    calculation from your application's most frequently used workflow could save thousands
    in compute costs over time. When you're caching the results of a method call or
    calculation local to your system, this is called **memoization.**
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 应该指出的是，这种策略可以应用于应用程序流程中的任何长时间运行的操作。我使用慢速数据库查询作为例子，因为那些是大型企业系统中最常见到的瓶颈。然而，在你的工作中，你可能发现缓存应用程序主机进程中执行的计算密集型操作的结果是有益的。在这种情况下，你可能会使用内存缓存或托管在你自己系统上的缓存，因此没有可能提高你的延迟。但想象一下，如果你的应用程序部署到按应用程序运行时间收费的云托管提供商，那么在应用程序最常用的流程中删除多秒的计算可以节省数千美元的计算成本。当你缓存系统本地的方法调用或计算的结果时，这被称为**记忆化**。
- en: Caching high-latency network requests
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缓存高延迟网络请求
- en: Another common motivating factor for caching is high-latency network requests.
    In this scenario, your software would be dependent on a network resource, but
    some aspect of the network infrastructure makes accessing that resource unacceptably
    slow. It could be that your application is hosted behind a firewall, and the request
    validation protocols for an incoming or outgoing request introduce high latency.
    Or, it might just be a matter of locality, with your application server hosted
    in a separate physical region from your nearest data center.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存的另一个常见动机因素是高延迟的网络请求。在这种情况下，你的软件将依赖于网络资源，但网络基础设施的某些方面使得访问该资源变得非常慢。这可能是因为你的应用程序托管在防火墙后面，而传入或传出的请求验证协议引入了高延迟。或者，这可能是由于地理位置的问题，你的应用程序服务器托管在与你最近的数据中心不同的物理区域。
- en: Whatever the reason, a common solution to this problem is to minimize the impact
    of the network latency by caching the results in a more local data store. Let's
    suppose, for instance, the issue is a gateway or firewall introducing unacceptable
    latency to your data access requests. In that case, you could stand up a cache
    behind the firewall to eliminate the latency it introduces. With this sort of
    caching strategy, your objective is to store your cached data on some host that
    introduces less latency than the source. Even if the time to look up a record
    in your cache is no faster than the time to look up the same record at the source,
    the minimized latency is the objective.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 无论原因如何，解决这个问题的常见方法是通过在更靠近数据存储的地方缓存结果来最小化网络延迟的影响。例如，假设问题是网关或防火墙向你的数据访问请求引入了不可接受的延迟。在这种情况下，你可以在防火墙后面建立一个缓存来消除它引入的延迟。在这种类型的缓存策略中，你的目标是存储你的缓存数据在某些主机上，这些主机引入的延迟比源主机要少。即使在你缓存中查找记录的时间不比在源主机上查找同一记录的时间快，最小化延迟仍然是目标。
- en: Caching to preserve state
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缓存以保留状态
- en: The last strategy for caching data is to facilitate state management. In cloud-deployed
    application architectures, you might have a user interacting with multiple instances
    of your application, running in parallel containers on different servers. However,
    if their interaction with your application depends on persisting any sort of session
    state, you'll need to share that state across all instances of your application
    that might service an individual request over the course of that session. When
    this is the case, you'll likely use a shared cache that all of the instances of
    your application can access and read from to determine if a request from a user
    relies on the state that was determined by another instance.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存数据的最后一种策略是促进状态管理。在云部署的应用程序架构中，你可能有用户与多个应用程序实例交互，这些实例在不同的服务器上并行运行。然而，如果他们的应用程序交互依赖于持久化任何类型的会话状态，你将需要在会话期间可能为单个请求服务的应用程序的所有实例之间共享该状态。在这种情况下，你可能会使用一个共享缓存，所有应用程序实例都可以访问并从中读取，以确定用户的请求是否依赖于由另一个实例确定的某个状态。
- en: When to write to a cache
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 何时写入缓存
- en: As I've described it so far, a cache might just sound like a duplicated data
    store that is optimized for your application. In some sense, that's true, but
    it's not technically correct, since a cache should never mirror its source system
    perfectly. After all, if you can store a complete copy of the underlying data
    store in a higher performance cache, what value is there in the underlying data
    store in the first place?
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如我之前所描述的，缓存可能听起来就像是一个优化了的应用程序的数据存储副本。在某种程度上，这是真的，但技术上并不正确，因为缓存永远不应该完美地镜像其源系统。毕竟，如果你可以在一个高性能缓存中存储底层数据存储的完整副本，那么底层数据存储最初的价值在哪里呢？
- en: Instead, a cache will typically only contain a very small subset of the underlying
    data store. In most cases, the small size of a cache is necessary for its performance
    benefits, since even a simple query will scale linearly over the size of the set
    being queried. But if our data cache is not a perfect mirror of the source system,
    then we must determine which data makes it into our cache and when.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，缓存通常只包含底层数据存储的一小部分。在大多数情况下，缓存的小尺寸对于其性能优势是必要的，因为即使是一个简单的查询也会随着查询集合的大小线性扩展。但是，如果我们的数据缓存不是源系统的完美镜像，那么我们必须确定哪些数据被包含在我们的缓存中，以及何时被包含。
- en: Pre-caching data
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预缓存数据
- en: One simple but effective strategy for writing data to a cache is called **pre-caching**.
    In a system that pre-caches it's data, the developers will determine what is likely
    to be the lowest-performing or most-frequently requested data access operations,
    whose results are leastlikely to change over the lifetime of the application.
    Once that determination is made, those operations are performed once, usually
    in the initialization of the application, and loaded into the cache before any
    requests are received or processed by the application.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据写入缓存的简单而有效策略被称为**预缓存**。在一个预缓存其数据的系统中，开发者将确定哪些可能是性能最低或最频繁请求的数据访问操作，这些操作的结果在应用程序的生命周期内最不可能发生变化。一旦做出这种决定，这些操作就会执行一次，通常是在应用程序的初始化过程中，并在接收到或由应用程序处理任何请求之前将其加载到缓存中。
- en: The example I mentioned earlier, involving frequently requested reports of quarterly
    sales data, is an ideal scenario for pre-cached data. In this case, we could request
    the sales data and run all of the statistical operations necessary for the output
    of the reports at the startup of our application. Then, we could cache the finished
    view models for each kind of report the application serviced. Upon receiving a
    request for a report, our application could reliably query the cache for the view
    model, and then populate the report template accordingly, saving time and compute
    costs over the course of the lifetime of the application.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前提到的例子，涉及频繁请求的季度销售数据报告，是预缓存数据的理想场景。在这种情况下，我们可以在应用程序启动时请求销售数据并运行所有必要的统计操作以生成报告。然后，我们可以缓存应用程序服务的每种报告类型的完成视图模型。在接收到报告请求时，我们的应用程序可以可靠地查询缓存以获取视图模型，然后根据报告模板相应地填充，从而在应用程序的生命周期中节省时间和计算成本。
- en: The downside to this approach, though, is that it requires synchronization of
    any updates to the underlying data with refreshes of the application's cache.
    In cases where the application and the underlying data store are owned and managed
    by the same team of engineers, this synchronization is trivial. However, if the
    data store is owned by a different team from the engineers responsible for the
    application, the coordination introduces a risk. One failure to synchronize updates
    could result in stale data being served to customers. To mitigate this risk, you
    should establish a clear and resilient strategy for refreshing your cache, and
    automate the task as much as is possible.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法的一个缺点是它需要同步底层数据更新与应用程序缓存的刷新。在应用程序和底层数据存储由同一团队工程师拥有和管理的情况下，这种同步是微不足道的。然而，如果数据存储由负责应用程序的不同团队拥有，协调会引入风险。一次更新同步失败可能导致向客户提供过时数据。为了减轻这种风险，你应该制定一个明确且弹性强的缓存刷新策略，并在可能的情况下自动化这项任务。
- en: On-demand cache writing
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 按需缓存写入
- en: Most implementations of a caching system will be designed to write data to the
    cache on demand. In an on-demand system, the application will have a need for
    some piece of data that it can be certain is stored in the underlying database.
    However, prior to making the slower data access request all the way back to the
    underlying database, the application will first check for the requested data in
    the cache. If the data is found, it's called a **cache hit**. With a cache hit,
    the cache entry is used, and no additional call is made back to the dataset, thus
    improving the performance of the application.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数缓存系统的实现都将设计为按需将数据写入缓存。在一个按需系统中，应用程序需要某些数据，它可以确信这些数据存储在底层数据库中。然而，在将较慢的数据访问请求发送到底层数据库之前，应用程序将首先检查缓存中是否有请求的数据。如果找到数据，就称为**缓存命中**。在缓存命中时，使用缓存条目，并且不会对数据集进行额外的调用，从而提高应用程序的性能。
- en: In the alternative situation, where the requested data hasn't been written to
    the cache, the application has what's called a **cache miss**. With a miss, the
    application must make the slower call to the underlying data store. At this point,
    though, the cost of accessing the requested data from the lower-performing system
    has been paid. So now, the application can use whatever heuristics have been set
    for it to determine if the retrieved data should then be written to the cache,
    thus saving time on subsequent requests for the same piece of data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一种情况下，当请求的数据尚未写入缓存时，应用程序会出现所谓的**缓存未命中**。在未命中时，应用程序必须对底层数据存储进行较慢的调用。然而，此时访问请求数据的成本已经付出。因此，现在应用程序可以使用为其设置的任何启发式方法来确定检索到的数据是否应该写入缓存，从而在后续请求相同数据时节省时间。
- en: Cache replacement strategies
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缓存替换策略
- en: If your cache is of a fixed limited size, you may find yourself needing to define
    a cache replacement policy. A cache replacement policy is how you determine when
    to replace older records with newer, potentially more relevant, ones. This will
    generally happen when your application experiences a cache miss. Once the data
    is retrieved, the application will determine whether or not to write it to the
    cache. If it does end up writing a record to the cache, it will need to determine
    which record to remove. The trouble, though, is that it is very difficult to determine
    a consistent heuristic for identifying which records won't be needed again soon.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的缓存大小固定有限，你可能会发现自己需要定义一个缓存替换策略。缓存替换策略是你确定何时用新的、可能更相关的记录替换旧记录的方法。这通常发生在应用程序遇到缓存未命中时。一旦数据被检索，应用程序将确定是否将其写入缓存。如果最终将记录写入缓存，它将需要确定要删除哪个记录。然而，问题在于很难确定一个一致的启发式方法来识别哪些记录很快就不会再被需要。
- en: You might have come up with a seemingly obvious answer in your own head just
    thinking about it; I certainly did when I first learned about this problem. But
    most of the obvious solutions don't hold up to scrutiny. For example, a fairly
    popular replacement policy involves eliminating the record that was least recently
    used. This just means replacing the entry that hasn't generated a cache hit for
    the longest series of cache queries. However, it may be the case that the longer
    it's been since a record has been used, the more likely it is that it will be
    the next record used, with records looked up in a cyclical order. In that case,
    eliminating the least recently used record would increase the chances of another
    cache miss in subsequent requests.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能在心中想到了一个看似明显的答案，仅仅通过思考就能得出；当我第一次了解到这个问题时，我也是这样。但大多数显而易见的解决方案在仔细审查时并不成立。例如，一种相当流行的替换策略涉及删除最近最少使用的记录。这仅仅意味着替换了在一系列缓存查询中未生成缓存命中的条目。然而，可能的情况是，一个记录被使用的时间越长，它被下一次使用的可能性就越大，记录的查找是按循环顺序进行的。在这种情况下，删除最近最少使用的记录会增加后续请求中另一个缓存未命中的可能性。
- en: Alternatively, you could try the least frequently used replacement policy. This
    would drop the record with the fewest cache hits out of all records on the system,
    regardless of how recently those hits occurred. Of course, the drawback for this
    approach is that, without accounting for recency, you ignore the possibility that
    a recently used record might have been queried recently because it will become
    relevant for a series of subsequent operations the user intends to perform with
    it. By eliminating it due to its low hit rate, and ignoring the recency of the
    hit, you increase the chances of a cache miss in the future.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以尝试使用最少使用替换策略。这将丢弃系统上所有记录中缓存命中次数最少的记录，无论这些命中发生的时间有多近。当然，这种方法的缺点是，没有考虑到最近的使用情况，你忽略了最近使用过的记录可能因为用户打算使用它进行一系列后续操作而变得相关的可能性。由于它的命中率低而被删除，并且忽略了命中的最近性，这增加了未来缓存未命中的可能性。
- en: Each cache replacement policy has its own drawbacks and should be considered
    within the context of your application. However, there is a key metric by which
    you might determine the relative success of your replacement policy. Once your
    initial heuristic is designed and deployed, you can track your cache's hit ratio.
    A cache's hit ratio is, quite simply, the number of cache hits divided by the
    number of cache misses. The closer that number is to 1.0, the better your cache
    replacement policy is.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 每种缓存替换策略都有其自身的缺点，应该在应用程序的上下文中考虑。然而，有一个关键指标可以帮助你确定替换策略的相对成功程度。一旦你的初始启发式方法设计并部署，你可以跟踪你的缓存命中率。一个缓存的命中率非常简单，就是缓存命中次数除以缓存未命中次数。这个数字越接近1.0，你的缓存替换策略就越好。
- en: Cache invalidation
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缓存失效
- en: As you consider your cache replacement strategies, you may find that a piece
    of information stored in your cache will only be relevant to your application
    for a short period of time. When that's the case, instead of waiting for a new
    cache miss to overwrite the irrelevant data, you may want to expire the entry
    after a certain timeout. This is what's known as **cache invalidation**. Put simply,
    cache invalidation is the process of determining that a record in your cache should
    no longer be used to service subsequent requests.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当你考虑你的缓存替换策略时，你可能会发现缓存中存储的一些信息可能只对应用程序在短时间内相关。在这种情况下，与其等待新的缓存未命中来覆盖无关数据，你可能会想要在某个超时后使条目过期。这就是所谓的**缓存失效**。简单来说，缓存失效是确定你的缓存中的记录不应再用于服务后续请求的过程。
- en: In cases, as I've described, where you have a known time-to-live for any given
    record written to your cache, invalidating those records is as simple as setting
    and enforcing an expiration on the record as it's being written. However, there
    are other cases where it might not be so obvious that a cached record should be
    invalidated. Consider a web browser caching a response from the server. Without
    a predetermined expiration date, the browser can't know for sure that the cached
    response still represents the current state of the server without first checking
    the server, thus eliminating the performance benefit of the cache.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在我描述的情况下，如果你为写入缓存中的任何给定记录有一个已知的有效期，使这些记录失效就像在写入记录时设置并执行一个过期策略一样简单。然而，在其他情况下，可能并不明显地需要使缓存记录失效。考虑一个网络浏览器缓存来自服务器的响应。如果没有预定的过期日期，浏览器无法确定缓存响应是否仍然代表服务器的当前状态，除非首先检查服务器，从而消除了缓存带来的性能优势。
- en: Since you should never be serving stale or invalid data to your users, you should
    always design some mechanism for invalidating your cached records. I've just discussed
    the two most common, and you'd be hard-pressed to find a reason not to implement
    at least one of them. So, if you have control over the cache itself, as in the
    case of a cache contained within your application architecture, you should always
    be diligent to invalidate cache records whenever the underlying data store is
    updated. And for the cases when you're not in control of your responses being
    cached, make sure you're always setting a reasonable cache expiration on your
    responses.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你不应该向用户提供过时或无效的数据，你应该始终设计一些机制来使你的缓存记录无效。我刚刚讨论了两种最常见的方法，你很难找到不实施其中至少一种的理由。所以，如果你控制着缓存本身，比如在你的应用程序架构中包含的缓存，你应该始终在底层数据存储更新时勤勉地使缓存记录无效。而对于你无法控制响应被缓存的情况，确保你始终为你的响应设置合理的缓存过期时间。
- en: At this point, we've learned about what a cache is, why you might implement
    one in your software architecture, and what sort of strategies are at your disposal
    for optimizing its performance. So, now it's time to look at one of the most common
    caching strategies in modern cloud-deployed network architectures.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了什么是缓存，为什么你可能在你的软件架构中实现一个缓存，以及你可以利用哪些策略来优化其性能。因此，现在是我们来看看现代基于云部署的网络架构中最常见的缓存策略之一的时候了。
- en: Distributed caching systems
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式缓存系统
- en: In the previous section, I discussed using a cache for the purpose of preserving
    application state between parallel deployments of the same application. This is
    one of the most common use cases for caching in modern cloud-based architectures.
    However, useful as it may be, this sort of distributed session cache can introduce
    a whole host of challenges to the application design.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我讨论了使用缓存来在相同应用程序的并行部署之间保留应用程序状态的目的。这是现代基于云的架构中缓存最常见用例之一。然而，尽管这种分布式会话缓存可能非常有用，但它可能会给应用程序设计带来一系列挑战。
- en: A cache-friendly architecture
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缓存友好的架构
- en: Caches have historically been used to improve performance by reducing latency
    or operation times. With a session cache for distributed architecture, however,
    the cache itself is not intended to provide any specific performance improvement
    on a given operation. Instead, it's designed to facilitate a necessary interaction
    between multiple instances of an application. Its design is to facilitate state
    management that would otherwise involve complicated orchestration of multiple
    hosts. To understand how this is done, let's consider an example.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存在历史上一直被用来通过减少延迟或操作时间来提高性能。然而，对于分布式架构中的会话缓存，缓存本身并不旨在对特定操作提供任何特定的性能改进。相反，它旨在促进多个应用程序实例之间必要的交互。其设计是为了促进状态管理，否则将涉及多个主机复杂的编排。为了理解这是如何实现的，让我们考虑一个例子。
- en: Suppose you have a cloud-hosted API that is responsible for verifying a user's
    identity and age. To do so, it requests various pieces of information that, taken
    together, could serve as verification of the user. In an effort to design your
    user experience to be as non-intrusive as possible, you start by asking only a
    few questions about their birth date and current address, which are most likely
    to successfully verify their age and identity. Once they've submitted their answers,
    your application would attempt to verify them. If it succeeds, your user can proceed,
    but if the initial set of questions is unsuccessful, your application follows
    up with a handful more questions that, when combined with the answers from the
    first set, are highly likely to verify the user's identity. The user submits their
    answers, and you continue again with failure, resulting in one final question
    requesting the last four digits of the user's social security number. Upon submission,
    the user is either successfully verified or rendered permanently unable to access
    your system.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个托管在云上的API，该API负责验证用户的身份和年龄。为了做到这一点，它请求各种信息，这些信息加在一起可以用来验证用户。为了设计尽可能不干扰用户体验，你首先只问一些关于他们的出生日期和当前地址的问题，这些问题最有可能成功地验证他们的年龄和身份。一旦他们提交了答案，你的应用程序将尝试验证他们。如果成功，用户可以继续，但如果最初的问题集不成功，你的应用程序将跟进更多的问题，这些问题与第一组答案结合在一起，高度可能验证用户的身份。用户提交了答案，你再次尝试失败，最终提出一个要求用户提供社会保险号码最后四位数字的问题。提交后，用户要么成功验证，要么永久无法访问你的系统。
- en: That process is relatively straightforward from a business-logic perspective,
    but how would you implement it on a network process that is meant to remain entirely
    stateless between network requests? And as a cloud-hosted application, how would
    you maintain the current state of the user's position in that workflow across
    multiple possible instances of your app server processing a given request?
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从业务逻辑的角度来看，这个过程相对简单，但你如何在保持网络请求之间完全无状态的网络上实现它？作为一个云托管的应用程序，你如何维护用户在流程中的当前位置，这跨越了处理给定请求的多个可能的应用服务器实例？
- en: The case for a distributed cache
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式缓存的案例
- en: There are a handful of techniques available in this particular case, each with
    their own advantages and drawbacks. You could use sticky sessions to force subsequent
    requests from a particular host in a given session to be serviced by the same
    app server that processed the initial request. This would allow for some minor
    local state management over the course of a session. The downside to this is that
    it eliminates the performance benefits of horizontal scaling in a cloud-hosted
    system. If a user is always forced to interact with a single server over the course
    of a session, regardless of the traffic to that server or the availability of
    other servers, they may as well be interacting with a single-instance monolithic
    application architecture. Moreover, you would no longer be staying true to the
    architectural ideal of a "stateless" service, as you would be implementing some
    mechanism for preserving a user's position in the workflow over the course of
    interaction on your active service.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定情况下，有一些技术可用，每种技术都有其自身的优点和缺点。您可以使用粘性会话来强制后续请求来自特定主机在给定会话中由处理初始请求的同一应用服务器提供服务。这将在会话期间允许进行一些轻微的本地状态管理。这种方法的缺点是，它消除了云托管系统中水平扩展的性能优势。如果用户在整个会话过程中始终被迫与单个服务器交互，无论该服务器的流量如何或其他服务器的可用性如何，他们实际上就是在与单体应用架构的单实例交互。此外，您将不再坚持“无状态”服务的架构理想，因为您将实现某种机制来在您的活动服务交互过程中保留用户在工作流程中的位置。
- en: Alternatively, you could use the same principle we saw in the *Self-encoded
    tokens* section of [Chapter 14](bf84cf6c-16d3-4225-b590-b3657aaa3832.xhtml), *Authentication
    and Authorization on Networks*. In this case, you'd be self-encoding the user's
    current state in the response from your server, and your user would be responsible
    for returning that self-encoded state back to the server in subsequent requests.
    This allows each request body to serve as a breadcrumb trail leading back to the
    first interaction with the client, from which the server could rebuild the state
    that was created in previous interactions with each subsequent interaction.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用我们在第14章“网络上的身份验证和授权”中看到的相同原则，即*自编码令牌*部分。在这种情况下，您将在服务器的响应中自行编码用户的当前状态，而您的用户将负责在后续请求中将该自编码状态返回到服务器。这允许每个请求体充当一条通往客户端首次交互的面包屑路径，服务器可以根据之前的交互重建在后续交互中创建的状态。
- en: This approach will increase the complexity of your request/response models,
    though. It will also introduce the added risk of unenforceable limits on verification
    attempts. Suppose, for security purposes, your business rules dictate that a user
    should only be allowed to attempt each round of questions once. If you self-encode
    the state of the user session in your request/response models, you're relying
    on your users to return an accurate representation of each previous attempt with
    each of their requests. It would be easy for a dedicated malicious actor to make
    as many attempts as they please by simply scrubbing the workflow state from their
    subsequent requests.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法会增加您请求/响应模型的复杂性。它还会引入无法强制执行的验证尝试限制的风险。假设出于安全考虑，您的业务规则规定用户只能尝试每轮问题一次。如果您在请求/响应模型中自行编码用户会话的状态，您就依赖于您的用户在每次请求中返回每个先前尝试的准确表示。一个有意的恶意行为者可以通过简单地从后续请求中清除工作流程状态来随意进行尽可能多的尝试。
- en: In this scenario, I would argue that the most reliable solution for maintaining
    state across requests is a distributed cache shared by each app server in your
    cloud environment. This prevents you from maintaining state across your app servers,
    thus preserving the stateless principle of cloud-deployed service architecture,
    while still allowing your services to maintain full control over a user's progression
    through your verification workflow.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我会认为，在请求之间维护状态的可靠解决方案是云环境中每个应用服务器共享的分布式缓存。这阻止了您在应用服务器之间维护状态，从而保留了云部署服务架构的无状态原则，同时仍然允许您的服务完全控制用户通过验证工作流程的进度。
- en: 'To implement this, you would host the cache provider on its own server, independently
    of any instances of your application servers in your cloud. Any server that successfully
    processes a given step in the workflow would refuse to fully resolve the interaction
    and provide a response to the client unless and until the result of that step
    was successfully written back to your data cache. In this way, the current app
    server instance could do the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，您将需要在独立于您的云中应用程序服务器实例的服务器上托管缓存提供程序。任何成功处理工作流程中给定步骤的服务器都会拒绝完全解析交互并向客户端提供响应，除非并且直到该步骤的结果成功写回到您的数据缓存中。这样，当前的应用程序服务器实例可以执行以下操作：
- en: Verify that no other instances have successfully processed the same request
    already by confirming that no record of the workflow step exists for the user
    in the cache
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过确认用户在缓存中不存在该工作流程步骤的记录，验证没有其他实例已成功处理相同的请求
- en: Notify other instances of the application that the step had been processed so
    that they would cease to duplicate the transaction
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通知其他应用程序实例该步骤已被处理，以便它们停止重复事务
- en: The benefits of a distributed cache
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式缓存的好处
- en: The system I've described has the benefit of ensuring the consistency of the
    user state throughout their interaction across alldeployed instances of your application,
    with only the minor orchestration of reading from and writing to a cache. This
    data consistency, is key even when your distributed cache is not used for state
    management. It can prevent multiple instances of an application from trying to
    perform two incompatible modifications to the same piece of data, or allow synchronization
    of transactions across multiple app servers prior to committing them to the underlying
    database. And most importantly, from the developer's perspective, it can eliminate
    difficult to reproduce and difficult to track down bugs caused by race conditions
    between services.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我所描述的系统具有确保用户状态在其与所有部署的应用程序实例交互过程中一致性的好处，只需对缓存进行少量读取和写入操作即可实现。这种数据一致性，即使在您的分布式缓存不用于状态管理时也是关键的。它可以防止多个应用程序实例尝试对同一份数据进行两个不兼容的修改，或者在将事务提交到底层数据库之前，允许跨多个应用服务器同步事务。最重要的是，从开发者的角度来看，它可以消除由服务之间的竞争条件引起的难以重现和难以追踪的bug。
- en: Hosting the cache server independently of all other applications also provides
    it with a measure of resilience against app restarts or crashes. By isolating
    your data store, you can isolate costs associated with higher availability and
    resiliency guarantees from your cloud provider. It can also help to minimize the
    memory footprint of your application containers living on your app servers. If
    you pay for RAM usage, this can save you thousands as your cache scales out. So,
    how exactly do we reap these benefits in our code?
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 独立于所有其他应用程序托管缓存服务器也为其提供了一定程度的对应用程序重启或崩溃的弹性。通过隔离您的数据存储，您可以隔离与您的云提供商提供的更高可用性和弹性保证相关的成本。这还可以帮助最小化您在应用程序服务器上运行的容器内存占用。如果您按RAM使用付费，这可以在缓存扩展时为您节省数千美元。那么，我们如何在代码中具体获得这些好处呢？
- en: Working with caches in code
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在代码中处理缓存
- en: To see how we can benefit from the various caching mechanisms supported by .NET
    Core, we'll be setting up a somewhat complicated demo application structure. The
    first thing we'll do is create a remote data store that has long-running operations
    to return results from queries. Once that's done, we'll set up an application
    dependent on that data, and provide it a caching strategy to mitigate the impact
    of our artificially slowed down remote data storage.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解我们如何从.NET Core支持的多种缓存机制中受益，我们将设置一个相对复杂的演示应用程序结构。我们首先要做的事情是创建一个远程数据存储，它具有长时间运行的操作以返回查询结果。一旦完成，我们将设置一个依赖于该数据的应用程序，并为其提供一个缓存策略，以减轻我们人为减缓的远程数据存储的影响。
- en: Writing our backing data system
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写我们的后端数据系统
- en: 'We''ll be creating our backing data system as a simple Web API project. The
    goal is to expose a couple of endpoints on a single controller that expose data
    of different types to demonstrate how we can write the values to our cache regardless
    of the type discrepancy between the records. First, let''s create our project
    with the .NET Core CLI. Let''s look at the following command:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建我们的后端数据系统作为一个简单的Web API项目。目标是创建一个控制器，在该控制器上暴露几个端点，以展示如何将值写入我们的缓存，而不管记录之间的类型差异。首先，让我们使用.NET
    Core CLI创建我们的项目。让我们看看以下命令：
- en: '[PRE0]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, since we''ll be hosting this project at the same time as our cache-ready
    application, we''ll want to configure it to use its own port, instead of the default
    settings. Within your `Program.cs` file, modify your `CreateWebHostBuilder(string[]
    args)` method to use whatever custom URLs you want this application to listen
    on:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，由于我们将在缓存准备好的应用程序的同时托管此项目，我们希望配置它使用自己的端口，而不是默认设置。在你的`Program.cs`文件中，修改你的`CreateWebHostBuilder(string[]
    args)`方法以使用你希望此应用程序监听的任何自定义URL：
- en: '[PRE1]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, we''ll modify the `ValuesController.cs` class to serve up our data. First,
    we''ll change the name of the class to `DataController` so that our routing is
    a bit more intuitive. We''ll be getting rid of all of the preconfigured endpoints
    and replacing them with three new endpoints, each returning a unique data type.
    First, though, let''s create a new data type for us to return. It will be a simple
    model with an ID and two arbitrary properties; one will be of the `string` type,
    and the other will be a `List<string>`:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将修改`ValuesController.cs`类以提供我们的数据。首先，我们将类的名称更改为`DataController`，以便我们的路由更加直观。我们将移除所有预配置的端点，并用三个新的端点替换它们，每个端点返回唯一的数据类型。首先，让我们为我们返回的数据创建一个新的数据类型。它将是一个简单的模型，具有ID和两个任意属性；一个将是`string`类型，另一个将是`List<string>`：
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'With this model set up, we can define the endpoints we''ll be exposing. For
    this demonstration, we''ll return a simple `List<string>` string, a single `OutputRecord`
    instance, and a `List<OutputRecord>` method. So, by the time we''ve defined a
    lookup endpoint for each data type, we''ll have methods returning simple strings,
    lists of strings, complex records, and lists of complex records. Let''s look at
    the following code:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型设置好之后，我们可以定义我们将要公开的端点。在这个演示中，我们将返回一个简单的`List<string>`字符串，一个单独的`OutputRecord`实例，以及一个`List<OutputRecord>`方法。因此，当我们为每种数据类型定义了查找端点之后，我们将有返回简单字符串、字符串列表、复杂记录和复杂记录列表的方法。让我们看看以下代码：
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'These define our simple string responses, and will be relatively straightforward
    to test with our cache. For our `OutputRecord` endpoints, though, we''ll want
    to apply unique data to each property so that we can confirm that the full object
    is properly cached. So, the endpoint returning a single `OutputRecord` instance
    will look like this:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这些定义了我们的简单字符串响应，并且用我们的缓存进行测试将会相对简单。然而，对于我们的`OutputRecord`端点，我们希望为每个属性应用独特的数据，以便我们可以确认整个对象被正确缓存。因此，返回单个`OutputRecord`实例的端点将看起来像这样：
- en: '[PRE4]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This gives us an object with distinct property values, tied together by the
    same ID, which will make it easy for us to validate the behavior of our cache.
    Finally, we''ll define an endpoint to return a list of the `OutputRecord` instances:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们一个具有不同属性值的对象，它们通过相同的ID连接在一起，这将使我们能够轻松验证缓存的行为。最后，我们将定义一个端点来返回`OutputRecord`实例的列表：
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Each of these endpoints returns some trivial object or string with the provided
    ID that's used in the response objects, but this will just be a way of distinguishing
    one response from the next. The important aspect of our responses will be the
    perceivable delay we'll be applying. For that, we'll a five second delay to each
    method prior to returning their result. This will give us an obvious way to identify
    when the backing data store has been hit versus when our user-facing application
    has a successful cache hit.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这些端点中的每一个都返回一些带有提供ID的简单对象或字符串，这些对象用于响应对象中，但这只是区分一个响应与下一个响应的一种方式。我们响应的重要方面将是我们将应用的感知延迟。为此，我们将在返回结果之前为每个方法添加五秒钟的延迟。这将给我们一个明显的方式来识别当后端数据存储被击中时与当我们的用户界面应用程序成功缓存击中时的情况。
- en: 'To simulate this delay, we''ll sleep the current thread for five seconds, and
    then return some arbitrary string that incorporates the given ID:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟这种延迟，我们将使当前线程休眠五秒钟，然后返回一个包含给定ID的任意字符串：
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Each additional method will do the same thing, applying the delay and then
    initializing its expected return type with its arbitrary values. Now, if you run
    the application and ping your `/data/value/1234` endpoint, you should see the
    result come back after five seconds:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 每个额外的方法都将做同样的事情，应用延迟然后使用任意值初始化其预期的返回类型。现在，如果你运行应用程序并ping你的`/data/value/1234`端点，你应该会在五秒后看到结果返回：
- en: '![](img/e26b5043-1f91-4888-a09d-441220d4f5ab.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e26b5043-1f91-4888-a09d-441220d4f5ab.png)'
- en: Note the response time of 5269ms. This delay will be our indication of a cache
    miss, going forward. And with our data store ready, we can build our application
    and define its caching strategy.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注意响应时间为5269ms。这个延迟将是我们未来缓存未命中的指示。并且有了我们的数据存储就绪，我们可以构建我们的应用程序并定义其缓存策略。
- en: Leveraging a cache
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用缓存
- en: To start working with our cache, we'll first install and run a local instance
    of a Redis server. Redis is an open source, in-memory data store. It's frequently
    used in enterprise deployments as a simple key-value data store or cache. It's
    also supported out of the box by Azure cloud hosting environments, making it very
    popular for .NET based microservices and cloud-based applications.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用我们的缓存，我们首先需要安装并运行一个Redis服务器的本地实例。Redis是一个开源的内存数据存储。它通常在企业部署中用作简单的键值数据存储或缓存。它还由Azure云托管环境直接支持，这使得它对于基于.NET的微服务和基于云的应用程序非常受欢迎。
- en: 'To install it, follow the instructions in the *Technical requirements* section
    of this chapter. Once you''ve done so, you''ll have your local instance running.
    If you''ve already installed the server, make sure it''s up and running by opening
    your Windows Subsystem for Linux interface, and enter the following commands to
    verify its listening port:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装它，请遵循本章*技术要求*部分中的说明。一旦完成，你将有一个本地实例正在运行。如果你已经安装了服务器，请确保它已启动并运行，通过打开你的Windows子系统Linux界面，并输入以下命令以验证其监听端口：
- en: '![](img/f5d37ee1-04b1-41e7-b710-bcf809ee71b5.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f5d37ee1-04b1-41e7-b710-bcf809ee71b5.png)'
- en: 'Once you''ve got your Redis instance running, you''ll be ready to implement
    your sample microservice. Since we''ll be loading cache misses from our backend
    API, we''ll want to configure `HttpClient` for that particular application in
    our `Startup.cs` file. For this, I''ve created a static `Constants` class just
    to avoid magic strings used in my code, and used a `DATA_CLIENT` property to register
    a named instance of `HttpClient` inside my `ConfigureServices(IServiceCollection
    services)` method:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你的Redis实例运行起来，你就可以实现你的示例微服务了。由于我们将从我们的后端API加载缓存未命中，我们希望在`Startup.cs`文件中为该特定应用程序配置`HttpClient`。为此，我创建了一个静态的`Constants`类，以避免在我的代码中使用魔法字符串，并在`ConfigureServices(IServiceCollection
    services)`方法中使用`DATA_CLIENT`属性注册一个命名的`HttpClient`实例：
- en: '[PRE7]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we''ll create a service client to abstract the details of the HTTP requests
    we''ll be making behind a clean data-access interface, using the same patterns
    we established in Chapter 9,* HTTP in .NET*. Our interface definition will provide
    the following simple methods:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个服务客户端来抽象我们将要进行的HTTP请求的细节，使用我们在第9章中建立的相同模式，即*.NET中的HTTP*。我们的接口定义将提供以下简单的方法：
- en: '[PRE8]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Within the implementing class for this interface, we''ll have a private instance
    of  `IHttpClientFactory`, and we''ll be using our named `HttpClient` instance
    to access our backend data store. This common task is isolated to a private method
    for the actual HTTP interaction:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现此接口的类中，我们将有一个`IHttpClientFactory`的私有实例，我们将使用我们的命名`HttpClient`实例来访问我们的后端数据存储。这个常见任务被隔离到一个私有方法中，用于实际的HTTP交互：
- en: '[PRE9]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, each of the public interface methods implements an endpoint-specific
    variation of the general pattern established here:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，每个公共接口方法都实现了这里建立的一般模式的端点特定变体：
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Extending this logic for all four of our access methods, we''ll complete our
    backend data client. At this point, we should modify our controller to expose
    each of the backend API endpoints, and use them to test our data-access service.
    We''ll expose the same service contract we had in our backend API, with four endpoints
    for each type of record we could look up. Instead of renaming our file, we''ll
    just redefine our controller''s route, and define a public constructor to allow
    the dependency injection framework to provide our `DataService` instance (just
    don''t forget to register the concrete implementation in your `Startup.cs`). Lets,
    look at the following code:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 将此逻辑扩展到我们所有四种访问方法，我们将完成我们的后端数据客户端。在这个时候，我们应该修改我们的控制器以公开每个后端API端点，并使用它们来测试我们的数据访问服务。我们将公开与我们的后端API相同的同一服务合约，为每种可能查找的记录类型提供四个端点。我们不会重命名我们的文件，而是重新定义我们控制器的路由，并定义一个公共构造函数以允许依赖注入框架提供我们的`DataService`实例（只是别忘了在`Startup.cs`中注册具体的实现）。让我们看看以下代码：
- en: '[PRE11]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Once we have our data service, we can use our API endpoints to call into each
    requested object from our backend system:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了我们的数据服务，我们就可以使用我们的API端点从我们的后端系统调用每个请求的对象：
- en: '[PRE12]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: At this point, by running both your backend API and your cache service API,
    you should be able to request the same values from your cache service, with the
    same five second delay. So, now that our application is fully wired up to request
    data from our backend service, let's improve its performance with caching.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，通过运行你的后端 API 和缓存服务 API，你应该能够从你的缓存服务请求相同的值，并且有相同的五秒延迟。所以，现在我们的应用程序已经完全连接到请求后端服务的数据，让我们通过缓存来提高其性能。
- en: The distributed cache client in .NET
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: .NET 的分布式缓存客户端
- en: 'One of the major benefits of using Redis for our distributed caching solution
    is that it''s supported by .NET Core out of the box. There''s even an extension
    method on the `IServicesCollection` class specifically for registering a Redis
    cache for use within your application. Simply install the `Microsoft.Extensions.Caching.Redis`
    NuGet package for your current project, and then add the following code:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Redis 作为我们的分布式缓存解决方案的主要好处之一是它由 .NET Core 默认支持。甚至还有一个针对 `IServicesCollection`
    类的扩展方法，专门用于在应用程序中使用 Redis 缓存进行注册。只需为你的当前项目安装 `Microsoft.Extensions.Caching.Redis`
    NuGet 包，然后添加以下代码：
- en: '[PRE13]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This will automatically register an instance of the `RedisCache` class as the
    concrete implementation for any instances of `IDistributedCache` you inject into
    any of your services. The localhost configuration setting will use the default
    configurations for a local deployment of the Redis client, so there's no need
    to specify an IP address and port unless you explicitly change it on your local
    deployment. Meanwhile, the `InstanceName` field will give the entries stored in
    the cache that were set by this application an application-specific prefix. So,
    in this example, if I set a record with the  `1234` key with my setting of local,
    that key will be stored in the cache as `local1234`. The `RedisCache` instance
    that is registered by the `AddDistributedRedisCache()` method will automatically
    look for keys with the `InstanceName` prefix that we've specified in our options.
    We'll see this later when we inspect our cache instance.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这将自动将 `RedisCache` 类的实例注册为任何注入到你的服务中的 `IDistributedCache` 实例的具体实现。本地主机配置设置将使用
    Redis 客户端本地部署的默认配置，因此除非你明确更改本地部署，否则不需要指定 IP 地址和端口。同时，`InstanceName` 字段将为由该应用程序设置的缓存条目提供应用程序特定的前缀。所以，在这个例子中，如果我用本地设置设置了一个
    `1234` 键的记录，那么这个键将存储在缓存中为 `local1234`。通过 `AddDistributedRedisCache()` 方法注册的 `RedisCache`
    实例将自动查找我们选项中指定的 `InstanceName` 前缀的键。我们将在稍后检查我们的缓存实例时看到这一点。
- en: 'With our Redis cache running, and our `IDistributedCache` instance configured
    and registered with our dependency injection container, we can write a `CacheService`
    class. This will follow a similar pattern to our `DataService` class, where it
    exposes only a small number of logical operations as public methods, hiding the
    details of the cache interactions. Our interface for this `CacheService` class
    is as follow:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 Redis 缓存运行，并且我们的 `IDistributedCache` 实例配置并注册到我们的依赖注入容器后，我们可以编写一个 `CacheService`
    类。这个类将遵循与我们的 `DataService` 类相似的模板，其中只公开少量逻辑操作作为公共方法，隐藏缓存交互的细节。我们为这个 `CacheService`
    类的接口如下：
- en: '[PRE14]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here, we're making the distinction between writing a single string and writing
    a more complex record to distinguish between the need to serialize and deserialize
    our entries in each method implementation.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们区分了写入单个字符串和写入更复杂的记录，以区分在每个方法实现中序列化和反序列化我们条目的需求。
- en: Getting and setting cache records
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取和设置缓存记录
- en: The `IDistributedCache` class provides a simple mechanism for interacting with
    our cached data. It operates on a dumb get/set pattern whereby attempts to get
    a record will either return the cached byte array or string based on the given
    ID, or return null if no record exists. There's no error handling or state checking.
    The speed of the cache is dependent on this simple interaction mechanism and fail
    state.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`IDistributedCache` 类提供了一个简单的机制来与我们的缓存数据交互。它基于简单的 get/set 模式运行，尝试获取记录将根据给定的
    ID 返回缓存的字节数组或字符串，如果不存在记录则返回 null。没有错误处理或状态检查。缓存的速度取决于这种简单的交互机制和失败状态。'
- en: Likewise, setting a record is equally easy. Simply define your ID for the record,
    and then provide some serialized representation of the record for storage. This
    serialized format can be either a string with the `SetString(string id, string
    value)` method, or a byte array using the `Set(string id, byte[] value)` method.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，设置记录同样简单。只需定义记录的ID，然后提供记录的序列化表示形式以供存储。这种序列化格式可以是使用`SetString(string id, string
    value)`方法的字符串，或者使用`Set(string id, byte[] value)`方法的字节数组。
- en: 'Additionally, when you write a value to the cache, you can set additional options
    for your cache record to specify expiration time spans. The kinds of expiration
    settings you can apply are as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当你向缓存写入值时，你可以为你的缓存记录设置额外的选项来指定过期时间范围。你可以应用的过期设置类型如下：
- en: '**AbsoluteExpiration**: This sets the expiration to a specific moment in time
    at which point the record will be invalidated, no matter how recently it has been
    used.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**绝对过期**：这将在特定的时间点设置过期，此时记录将被失效，无论它最近是否被使用。'
- en: '**AbosluteExpirationRelativeToNow**: This sets a fixed moment at which the
    record will be invalidated no matter how recently it has been used. The only difference
    with this and AbsoluteExpiration is that the expiration time is expressed in terms
    of some length of time from the moment the record is set in the cache.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**绝对过期相对于现在**：这将在记录被设置在缓存中的那一刻起，设置一个固定的时间点，无论它最近是否被使用，记录都将被失效。与绝对过期不同的是，过期时间是以记录设置在缓存中的那一刻起的一段时间长度来表示的。'
- en: '**SlidingExpiration**: This sets an expiration time relative to the last time
    the record was accessed. So, if the sliding expiration is set for 60 minutes,
    and the record isn''t accessed again for 62 minutes, it will have expired. However,
    if the record is accessed again in 58 minutes, the expiration is reset for 60
    minutes from that second access.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**滑动过期**：这将在记录最后访问的时间点设置一个过期时间。因此，如果滑动过期设置为60分钟，并且记录在62分钟后没有被再次访问，它将过期。然而，如果记录在58分钟后再次被访问，过期时间将从第二次访问的那一秒重置为60分钟。'
- en: 'So, let''s look at how we''ll implement this cache. First, we''ve got to inject
    the `IDistributedCache` instance that was registered in our `Startup.cs` class:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们看看我们将如何实现这个缓存。首先，我们必须注入在`Startup.cs`类中注册的`IDistributedCache`实例：
- en: '[PRE15]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then, we''ll implement the methods of our interface. The first method is fairly
    straightforward and only notifies our consumers if there has been a cache hit:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将实现我们的接口方法。第一个方法相当直接，仅在我们缓存命中时通知我们的消费者：
- en: '[PRE16]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we''ll implement our record retrieval methods. The only difference with
    each of these is that retrieval of complex data types (records and lists of strings)
    will require an extra step of deserialization. Other than that, though, our `Fetch...()`
    methods should look fairly straightforward:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将实现我们的记录检索方法。这些方法之间的唯一区别是，检索复杂的数据类型（记录和字符串列表）将需要额外的反序列化步骤。除此之外，我们的`Fetch...()`方法应该看起来相当直接：
- en: '[PRE17]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, we''ll need to implement the write methods. For the sake of demonstration,
    we''ll write all of our records with a 60-minute sliding expiration time using
    the `DistributedCacheEntryOptions` class. After that, we can simply pass in our
    key to the cache, along with a serialized value (we''ll be using JSON here, to
    take advantage of the `Newtonsoft.Json` libraries) and our expiration options:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要实现写入方法。为了演示，我们将使用`DistributedCacheEntryOptions`类将所有记录的滑动过期时间设置为60分钟。之后，我们可以简单地传递我们的键和序列化值（我们将使用JSON，以利用`Newtonsoft.Json`库）以及过期选项到缓存中：
- en: '[PRE18]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'And with that, our cache should be ready to use. Now, it''s time to pull it
    all together in our controller endpoints. For this, the interaction pattern will
    be the same across each method, with the only difference being the type of read/write
    operation we perform on our cache. So Let''s look at how we''ll implement our
    cache strategy:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们的缓存应该已经准备好使用了。现在，是时候在我们的控制器端点中整合所有内容了。为此，每个方法的交互模式都将相同，唯一的区别是我们对缓存执行的读写操作类型。所以，让我们看看我们将如何实现我们的缓存策略：
- en: '[PRE19]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The first thing you'll notice is that I apply a suffix to our given ID that
    matches my route. This is to allow duplicate IDs in my cache for each distinct
    data type. Next, we check our `HasCacheRecord` (key) method to determine whether
    we have a cache hit. If we do, we simply fetch the cache record and return the
    result. When we have a miss, though, we have to fetch the data from our underlying
    data store. Once we have it, we write it to our cache for faster retrieval in
    any subsequent requests, and then return the value.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先会注意到，我为给定的ID添加了一个后缀，这与我的路由匹配。这是为了允许我的缓存中每个不同数据类型的重复ID。接下来，我们检查我们的`HasCacheRecord`（键）方法，以确定我们是否有一个缓存命中。如果有，我们只需获取缓存记录并返回结果。然而，当我们没有命中时，我们必须从我们的底层数据存储中获取数据。一旦我们有了它，我们就将其写入我们的缓存，以便在后续请求中更快地检索，然后返回该值。
- en: After applying this pattern with the appropriate modifications to each of our
    endpoints, we're ready to test. To confirm the behavior of our cache, first run
    the same query against any endpoint with a new ID, twice in a row. If everything's
    working properly, you should have a five second delay on your first request, and
    almost zero delays on your subsequent request.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在对每个端点进行适当的修改后应用此模式，我们就准备好进行测试了。为了确认缓存的运行行为，首先对具有新ID的任何端点运行相同的查询，连续两次。如果一切正常，你的第一次请求应该有5秒的延迟，而后续请求的延迟几乎为零。
- en: 'Once you have at least a record or two stored in your cache, you can observe
    the values with your redis-cli in your Windows Subsystem for Linux console. The
    `RedisCache` class will store the entries as hash types in the underlying cache,
    so you''ll need to look for the key values using those commands. The operations
    I performed to look up the records I wrote while testing the app are as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你在缓存中存储了至少一条或两条记录，你就可以在Windows Subsystem for Linux控制台中使用redis-cli观察这些值。`RedisCache`类将条目作为哈希类型存储在底层缓存中，因此你需要使用这些命令来查找键值。我在测试应用程序时查找记录的操作如下：
- en: '![](img/9584f3f5-0bfa-480a-b9dd-01b711217714.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9584f3f5-0bfa-480a-b9dd-01b711217714.png)'
- en: The first command, `keys *`, simply searches all active keys that match the
    given pattern (* is the wildcard, so `keys *` matches all keys). Then, I used
    the `hgetall [key]` command to get each property in my entry's hash. In that output,
    you can clearly see the JSON written to the cache from my application, demonstrating
    the successful and expected interactions between my app and my cache.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个命令`keys *`简单地搜索所有匹配给定模式（*是通配符，所以`keys *`匹配所有键）的活跃键。然后，我使用了`hgetall [key]`命令来获取我条目哈希中的每个属性。在这个输出中，你可以清楚地看到从我应用程序写入缓存中的JSON，这证明了我的应用程序和缓存之间成功且预期的交互。
- en: I'd also like to point out the key structure. As I mentioned before, the keys
    I set (in this case, 2345 records) are prefixed with  `InstanceName` of  `RedisCacheOptions`,
    with which I configured  `RedisCache` in the `Startup.cs` file. And with that
    output, you've seen the full interaction pattern established by Microsoft for
    working with a Redis cache instance.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我还想指出键的结构。正如我之前提到的，我设置的键（在这种情况下，2345条记录）以`RedisCacheOptions`的`InstanceName`为前缀，我在`Startup.cs`文件中配置了`RedisCache`。通过这个输出，你已经看到了微软为与Redis缓存实例交互而建立的完整交互模式。
- en: Cache providers
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缓存提供者
- en: While we demonstrated the use of a data cache with an instance of the `IDistributedCache`
    class in our sample code, that is hardly the only cache provider we have access
    to with .NET Core. Before we close out the subject of caches, I just want to briefly
    discuss the other two most common providers in the framework.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在示例代码中演示了使用`IDistributedCache`类实例的数据缓存，但这并不是.NET Core中我们所能访问的唯一缓存提供者。在我们结束缓存主题之前，我想简要讨论框架中另外两个最常见的提供者。
- en: The SqlServerCache provider
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SqlServerCache提供者
- en: Redis is certainly popular among engineers as being a high-performance cache
    implementation. However, it's hardly the only distributed provider out there.
    In fact, Microsoft's own SQL Server can serve as a cache when the situation calls
    for it, and they've defined a similar implementation for the `IDistributedCache`
    class to expose it.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Redis无疑是工程师们中流行的高性能缓存实现。然而，它并不是唯一的分布式提供者。实际上，当需要时，微软自己的SQL Server也可以作为缓存使用，并且他们为`IDistributedCache`类定义了一个类似的实现来公开它。
- en: 'One of the biggest differences with the `SqlServerCache` provider and the `RedisCache`
    instance is in the configuration it requires. Where Redis is a simple key-value
    store, `SqlServer` remains a full-featured relational database. Thus, to provide
    the lightweight interactions necessary for a high performing cache, you''ll have
    to specify the precise schema, table, and database connection you intend to leverage
    when you set it as your `IDistributedCache` provider. And since SQL Server doesn''t
    support the hash tables that Redis does, the table to which your application connects
    for caching should implement the expected structure of an `IDistributedCache`
    record. Thankfully, the .NET Core CLI provides a utility command for establishing
    just such a table: the `sql-cache create` command. And notably, since your application
    should only ever be interacting with injected instances of `IDistributedCache`,
    you won''t even notice the difference, except perhaps in performance. However,
    for the sake of performance, I would recommend using Redis wherever possible.
    It is quickly becoming the industry standard and its speed is truly unmatched
    by SQL Server.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `SqlServerCache` 提供程序和 `RedisCache` 实例最大的不同之处在于它们所需的配置。由于 Redis 是一个简单的键值存储，`SqlServer`
    仍然是一个功能齐全的关系型数据库。因此，为了提供高性能缓存所需的轻量级交互，您必须在将其设置为 `IDistributedCache` 提供程序时指定您打算利用的确切模式、表和数据库连接。并且由于
    SQL Server 不支持 Redis 所支持的哈希表，您的应用程序连接用于缓存的表应实现 `IDistributedCache` 记录的预期结构。幸运的是，.NET
    Core CLI 提供了一个用于建立此类表的实用命令：`sql-cache create` 命令。值得注意的是，由于您的应用程序应该始终只与注入的 `IDistributedCache`
    实例交互，您甚至可能不会注意到差异，除非是在性能方面。然而，出于性能的考虑，我建议尽可能使用 Redis。它正迅速成为行业标准，其速度确实无法与 SQL Server
    相匹敌。
- en: The MemoryCache provider
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MemoryCache 提供程序
- en: Finally, if your application either doesn't have the need, or doesn't have the
    means, to support a standalone cache instance, you can always leverage an in-memory
    caching strategy. The `MemoryCache` class of the `System.Runtime.Caching` namespace
    will provide exactly that. Configuring it is as simple as invoking the `services.AddMemoryCache()`
    method in `Startup.cs`, and it provides a similar interface to the `IDistributedCache`
    class we've already looked at.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果您的应用程序既没有需求，也没有支持独立缓存实例的手段，您始终可以依赖内存缓存策略。`System.Runtime.Caching` 命名空间中的
    `MemoryCache` 类将提供所需的一切。配置它就像在 `Startup.cs` 中调用 `services.AddMemoryCache()` 方法一样简单，并且它提供了一个与我们已经查看过的
    `IDistributedCache` 类相似的接口。
- en: It does bring some major caveats with it, however. Since you're hosting the
    cache within your application's own process, memory becomes a much more valuable
    resource. Disciplined use of a cache replacement policy, and an aggressive expiration
    time, becomes much more important with an in-memory caching solution. Additionally,
    since any state that must persist over the lifetime of a session will only be
    persisted within a single instance of your application, you'll need to implement
    sticky sessions. This will ensure that users will always interact with the app
    server that has their data cached in its memory.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，它也带来了一些重要的注意事项。由于您在应用程序自己的进程中托管缓存，内存变得更为宝贵。对缓存替换策略的纪律性使用和积极的过期时间在内存缓存解决方案中变得尤为重要。此外，由于任何必须在会话生命周期内持久化的状态都只会在应用程序的单个实例中持久化，您需要实现粘性会话。这将确保用户始终与具有其数据缓存在内存中的应用服务器交互。
- en: Ultimately, your business needs and environmental constraints will play a large
    role in determining what sort of caching policies and strategies you should be
    taking advantage of in your application. However, with the information in this
    chapter, you should be well-suited to making the best possible decisions for your
    circumstances. Meanwhile, we'll be continuing our consideration of performance
    optimization in the next chapter as we consider performance monitoring and data
    tracing in a network-hosted application.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，您的业务需求和环境限制将在确定您应在应用程序中利用哪些缓存策略和策略方面发挥重要作用。然而，凭借本章中的信息，您应该能够为您的具体情况做出最佳决策。同时，我们将在下一章继续考虑性能优化，届时我们将考虑网络托管应用程序中的性能监控和数据跟踪。
- en: Summary
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we took an extensive tour of the motivations for, and use cases
    of, data caches in a distributed network application. We started by exploring
    some common business and design problems that would likely reap the benefits of
    a caching strategy. In doing so, we identified some of the basic considerations
    you can make when determining if the complexity of introducing a cache management
    system is the right decision for your application. Then, we looked at exactly
    which benefits could be gained from caching, and precisely how caching can provide
    them.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们广泛探讨了分布式网络应用程序中数据缓存的动机和用例。我们首先探索了一些可能从缓存策略中受益的常见商业和设计问题。在这样做的时候，我们确定了在确定引入缓存管理系统复杂性的复杂性是否是您应用程序的正确决策时可以做出的基本考虑。然后，我们探讨了缓存可以带来的确切好处，以及缓存如何精确地提供这些好处。
- en: Once we learned why we might use a cache, we looked at some of the common problems
    that must be solved for when implementing a cache. First, we tackled the tactics
    of pre-caching data and caching results on demand. Then, we looked at how to determine
    which data or resources should be cache. We learned about establishing a cache
    replacement policy that is well-suited to your application's most common data
    interactions, and how to invalidate records in your cache to make sure you're
    never returning stale results.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们了解了为什么我们可能会使用缓存，我们就研究了在实现缓存时必须解决的常见问题。首先，我们解决了预缓存数据和按需缓存结果的战略。然后，我们探讨了如何确定哪些数据或资源应该被缓存。我们学习了如何建立适合您应用程序最常见数据交互的缓存替换策略，以及如何使缓存中的记录失效以确保您永远不会返回过时的结果。
- en: Finally, we saw how we could use a cache in our applications. We learned how
    to run a distributed cache, and we saw how to write to and read from that cache
    within the code. We saw that a cache record could be an arbitrary data structure
    with an arbitrary key, and how to detect hits within our cache instance. Finally,
    we looked at alternative caching mechanisms available with C# and .NET Core.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们看到了如何在我们的应用程序中使用缓存。我们学习了如何运行分布式缓存，并看到了如何在代码中写入和读取该缓存。我们看到了缓存记录可以是任意数据结构，具有任意键，以及如何在我们的缓存实例中检测命中。最后，我们探讨了
    C# 和 .NET Core 中可用的替代缓存机制。
- en: In the next chapter, we'll continue with our focus on optimizing our application
    performance for a network, and look at the tools available to us for monitoring
    our application's performance and identifying any bottlenecks in our network.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续关注优化我们的应用程序在网络中的性能，并探讨可用于监控应用程序性能和识别网络中任何瓶颈的工具。
- en: Questions
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What are three criteria that should motivate a caching strategy?
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该有哪些标准来激励缓存策略？
- en: What are three common pain points caches can help resolve?
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 缓存可以帮助解决哪些常见的痛点？
- en: What is a cache hit? What is a cache miss?
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 缓存命中是什么？缓存未命中是什么？
- en: What is a cache replacement policy? What are some common cache replacement policies?
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是缓存替换策略？有哪些常见的缓存替换策略？
- en: What is a hit ratio, and how does it relate to a replacement strategy?
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 命中率是什么，它与替换策略有何关系？
- en: What is cache invalidation?
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 缓存失效是什么？
- en: What are some of the benefits of using a distributed cache?
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用分布式缓存有哪些好处？
- en: Further reading
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: For even more hands-on guidance for building caches in a modern .NET context,
    I recommend the book *The Modern C# Challenge*, by *Rod Stephens*. It takes a
    deep dive into the same sorts of patterns and practices we discussed in this chapter
    with an incredibly approachable presentation. It can be found through Packt publishing,
    here: [https://www.packtpub.com/application-development/modern-c-challenge-0.](https://www.packtpub.com/application-development/modern-c-challenge-0)
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在现代 .NET 环境中构建缓存的更多实践指导，我推荐由 *Rod Stephens* 编著的书籍 *The Modern C# Challenge*。它深入探讨了与本章中讨论的相同类型的模式和惯例，并以极其易于理解的方式呈现。您可以通过
    Packt 出版公司找到它，网址为：[https://www.packtpub.com/application-development/modern-c-challenge-0.](https://www.packtpub.com/application-development/modern-c-challenge-0)
- en: Alternatively, if you want to consider other challenges inherent to distributed,
    horizontally scaled application architectures, you should check out *Microservice
    Patterns and Best Practices* by *Vinicius Feitosa Pacheco*. It's also available
    from Packt, and you can get it here: [https://www.packtpub.com/application-development/microservice-patterns-and-best-practices.](https://www.packtpub.com/application-development/microservice-patterns-and-best-practices)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果你想考虑分布式、水平扩展的应用架构固有的其他挑战，你应该查看*Vinicius Feitosa Pacheco*的*《微服务模式和最佳实践》*。它也由Packt出版，你可以在这里获取： [https://www.packtpub.com/application-development/microservice-patterns-and-best-practices.](https://www.packtpub.com/application-development/microservice-patterns-and-best-practices)
