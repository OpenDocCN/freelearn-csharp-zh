- en: AR for Tourism with ARKit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore ARKit, Apple's own AR SDK, which offers many
    features such as spatial tracking, image tracking, collaborative AR, and more.
    We will also learn how to exploit world tracking in order to create a different
    AR experience for the tourism sector.
  prefs: []
  type: TYPE_NORMAL
- en: The main goals of this chapter are to introduce you to ARKit and how it works
    by searching and tracking flat surfaces in the real world. Then, you will learn
    how to use these features to place elements in AR and anchor them in order to
    create a dimensional portal that will introduce the user to a 3D world. The second
    goal of this chapter is to present a different way of viewing tourism. You will
    learn how to take advantage of AR to create unique experiences that can be implemented
    in museums, landscapes, points of interest, and so on. By doing this, you will
    be able to modify and apply this project to your own interests.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Using AR for tourism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring ARKit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing an ARKit app
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an AR portal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The technical requirements for this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A Mac computer with macOS Sierra 10.12.4 or above (we used a Mac mini, Intel
    Core i5, 4 GB memory, with macOS Mojave in this book)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latest version of Xcode (10.3 in this book)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An ARKit-compatible iPhone/iPad with iOS 11+ (we tested the project on an iPad
    Pro 10.5 with iOS 13.1.1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code files and resources for this chapter can be found here: [https://github.com/PacktPublishing/Enterprise-Augmented-Reality-Projects/tree/master/Chapter08](https://github.com/PacktPublishing/Enterprise-Augmented-Reality-Projects/tree/master/Chapter08)
  prefs: []
  type: TYPE_NORMAL
- en: You can view the compatible devices at [https://developer.apple.com/library/archive/documentation/DeviceInformation/Reference/iOSDeviceCompatibility/DeviceCompatibilityMatrix/DeviceCompatibilityMatrix.html](https://developer.apple.com/library/archive/documentation/DeviceInformation/Reference/iOSDeviceCompatibility/DeviceCompatibilityMatrix/DeviceCompatibilityMatrix.html)
  prefs: []
  type: TYPE_NORMAL
- en: When we explore ARKit, we will explain the device requirements you'll need,
    depending on the ARKit features that you want to use, since not all devices can
    use them.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, take into account that this chapter is dedicated to iOS devices. Therefore,
    you will need an Apple account (free or developer account) to compile the project
    on your iOS device. You can find more information here: [https://developer.apple.com/support/compare-memberships/](https://developer.apple.com/support/compare-memberships/)
  prefs: []
  type: TYPE_NORMAL
- en: Using AR for tourism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AR is mainly a visual technology. Although it can combine other effects such
    as sounds to make the experience more realistic or vibrations on the phone when
    we are playing a game, its main attraction is the visual content it displays over
    the real world. That makes this technology perfect for enhancing traveling experiences,
    from showing skyline information to making animals in a museum come to life or
    even translating signs and guides in real time.
  prefs: []
  type: TYPE_NORMAL
- en: Back in the late 2000s/early 2010s, when smartphones started to become popular,
    some of the first AR apps that appeared were tourism-oriented. In 2010, for example,
    the Museum of London released an iPhone app that showed historical photos of the
    city over the real places. This example has also been carried to other cities
    and scopes, such as in Navarra, Spain, where users can replay scenes of famous
    films shot in different locations of the region in AR by pointing their mobile
    devices to the panels that had been placed in said locations. In this case, the
    mobile devices use image recognition (the panel) to launch the AR experience.
    However, back in the early 2010s, the most stable and widespread AR technology
    was location-based and used a device's GPS, accelerometer, and compass. AR engines
    and apps such as Layar and Wikitude were mostly used as they allowed developers
    to generate routes, gymkhanas, and even games based on **points of interest**
    (**POIs**) across cities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nowadays, some of the most common uses of AR in tourism are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: To serve as a live guide in the streets of a new city, where a user can go around
    the city while the AR app is showing them where the most interesting points to
    see are through the use of arrows.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To show attractions and POIs over a map. Here, when a user points at a map with
    the camera, the POIs pop up from it and they can interact with them to find out
    more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To provide extra information about paintings, sculptures, or monuments. Here,
    when a user points at a painting, it can show a video about the artist and the
    place and time where it was painted, or even make it come to life as a 3D simulation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apart from all these experiences, when AR is combined with other immersive technologies
    such as virtual worlds or 360º videos, the experience goes one step ahead, allowing
    users to visit several places at the same time (for example, in a museum network,
    while visiting one of them, to be able to virtually visit the others).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn about how to mix these experiences using Apple's
    ARKit SDK to create an art experience. We will create an AR portal, and when users
    go through it, they will land on a painting represented in 3D. In our case, we
    will use a painting from Van Gogh (Bedroom in Arles) that's been downloaded from
    Sketchfab to give the users the illusion of being inside a 3D Van Gogh painting: [https://sketchfab.com/3d-models/van-gogh-room-311d052a9f034ba8bce55a1a8296b6f9](https://sketchfab.com/3d-models/van-gogh-room-311d052a9f034ba8bce55a1a8296b6f9).
  prefs: []
  type: TYPE_NORMAL
- en: To implement this, we will create an app oriented to the tourism field that
    can be displayed in museums, galleries, and more. Regarding the tool we are going
    to use, in the next section, we will explain how ARKit works and its main features.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring ARKit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apple launched the first version of ARKit in 2017 along with Xcode 9 and iOS
    11 to bring AR to iOS devices. The framework, which is included in Xcode, offered
    developers the possibility to produce AR experiences in their apps or games with
    software that''s combined with an iOS device''s motion features and camera tracking.
    It allows users to place virtual content in the real world. Months after its official
    release, it added new features such as 2D image detection and face detection.
    The main features that are available for iOS 11 and above are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Tracking and visualizing planes (iOS 11.3+), such as a table or the ground,
    in the physical environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking known 2D images (iOS 11.3+) in the real world and placing AR content
    over them (image recognition)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking faces (iOS 11.0+) in the camera feed and laying virtual content over
    them (for example, a virtual avatar face) that react to facial expressions in
    real-time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apart from these features, the AR experience can also be enhanced by using sound
    effects attached to virtual objects or integrating other frameworks such as vision
    to add computer vision algorithms to the app, or Core ML, for machine learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In 2018, with the iOS 12 release, ARKit 2 was launched with new features:'
  prefs: []
  type: TYPE_NORMAL
- en: 3D object tracking, where real-world objects are the ones that trigger the AR
    elements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiuser AR experiences, allowing users near each other to share the same AR
    environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding realistic reflections to the AR objects to make the experience more realistic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saving the world-mapping data so that when a user places a virtual element in
    the real world, the next time the app restarts, the virtual elements will appear
    in the same place
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the time of writing this book, iOS 13 with ARKit 3 has just been launched
    and promises a huge improvement to the current state since it's added a new way
    of interacting with virtual elements, such as hiding virtual objects when a person
    is detected in front of them. It also allows users to interact with 3D objects
    by gestures and captures not only facial expressions but the motions of a person.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the changes that are made in each iOS launch, not all the features
    that we mentioned here are available on all devices. The developers' page at [https://developer.apple.com/documentation/arkit](https://developer.apple.com/documentation/arkit)
    enumerates the current ARKit features and required minimum Xcode and iOS versions
    to develop and test with.
  prefs: []
  type: TYPE_NORMAL
- en: For this project, we will be using plane detection, which is a basic feature
    that can be run on iOS 11 and above. We will look at this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Developing an ARKit app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To start developing an app with ARKit, make sure you have the required hardware
    we discussed in the *Technical requirements* section, including an iOS device,
    since it's necessary to run the app.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will also need an Apple account to build your project on a real device.
    If you don''t have a paid account, you can also sign in with your regular free
    Apple ID. The current limits when using a free account are as follows: up to three
    installed apps on your device at the same time and the ability to create up to
    10 different apps every seven days.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will create an AR project using Xcode's template and go
    through its main components. Then, we will modify the basic app to visualize the
    detected surfaces and display important messages to users.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a new AR project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s create a new AR application using ARKit. We will start by creating the
    project in Xcode, the developer toolset for macOS that we can download freely
    from the Mac App Store, by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new project, select Augmented Reality App, and click Next. This will
    give us the basic frame so that we can start working with AR:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8b402154-364c-49ec-baf9-ae93b83c45b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Selecting the Augmented Reality App template
  prefs: []
  type: TYPE_NORMAL
- en: 'Fill in the Product Name, Team (here, you have to enter your Apple ID, and
    although you can leave it as None (as shown in the following screenshot), you
    will have to fill it in later to deploy the project onto the device), and Organization
    Name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7dd9290f-7072-4965-a964-19f21df413e4.png)'
  prefs: []
  type: TYPE_IMG
- en: Filling in the main values of our project
  prefs: []
  type: TYPE_NORMAL
- en: Press Next, select a location for the project, and press Create.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you didn''t enter your developer ID in *step 2*, the project''s general
    window for the Signing tab will show an error that will prevent the app from building
    on a device, as shown in the following screenshot. Fix it now to be able to run
    the app:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/efca0b52-6969-408c-9094-de5aa0229329.png)'
  prefs: []
  type: TYPE_IMG
- en: The signing tab of the project shows an error when a team hasn't been selected
  prefs: []
  type: TYPE_NORMAL
- en: 'We already have an AR app ready to be executed. To test it, connect your device
    and select it from the top-left corner of the window, as shown in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d73d79c6-36d2-407c-84c3-6856757d4e43.png)'
  prefs: []
  type: TYPE_IMG
- en: Selecting the device to run the app
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the app by clicking on the play button in the top-left corner of the window
    (see the preceding screenshot). The first time you launch it, it will ask for
    the camera''s permission, and as soon as the camera feed appears, it will anchor
    a spaceship to the middle of your environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7eb32643-e5e9-4b69-ad42-90d1439d9971.png)'
  prefs: []
  type: TYPE_IMG
- en: The ship appears anchored to the real environment
  prefs: []
  type: TYPE_NORMAL
- en: 'Since ARKit will be tracking your environment, try to move around the ship and
    get close to it or look at it from different angles, as shown in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d6eb9708-0bbc-4649-8f20-9944e543b52e.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's take a look at the main parts that form this project, that is, `Main.storyboard`,
    where the UI elements are, and `ViewController.swift`, where the logic of the
    app resides.
  prefs: []
  type: TYPE_NORMAL
- en: Main.storyboard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Open the `Main.storyboard` file by clicking on it. This file contains the UI
    elements. At the moment, it only contains an ARSCNView, which occupies the whole
    screen.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you right-click on the scene view, you will see the Referencing Outlets
    that are linked to the View Controller with the sceneView variable, as shown in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c927ebb-8c95-48eb-b1be-73a14a456178.png)'
  prefs: []
  type: TYPE_IMG
- en: The UI element ScenView and its Referencing Outlets
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a closer look at the view controller element.
  prefs: []
  type: TYPE_NORMAL
- en: ViewController.swift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, click on `ViewController.swift`, which is where the main logic of our application
    lies.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing you will see is that the class requires the `ARKit` library
    for the AR, the `UIKit` library for the interface, and the `SceneKit` library.
    This last one is a high-level 3D framework that Apple offers so that you can create,
    import, and manipulate 3D objects and scenes. We will be using it later in this
    chapter to import our external 3D models into the scene.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our only variable at the moment is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This is the `ARSCNView` element from `Main.storyboard`. It will allow us to
    control the AR scene.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our `ViewController` class inherits from `UIViewController`. From here, we
    have three `override` methods. The first one is `viewDidLoad`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This method is called when the view is loaded in memory and is used to initialize
    elements. In this case, we attach the `sceneView` element's delegate to the class
    itself, we activate the statistics that will appear at the bottom of the app,
    we create a scene with the ship model, and we assign that scene to the scene from
    the `scenView` element. This will make the battleship appear as soon as the app
    is launched.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second method is `viewWillAppear`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This method is called when the device is ready to show the interface to the
    user. The AR session is launched from here. The `ARSession`is the main element
    that controls the AR experience; it reads the data from the sensors of your device,
    controls the device's camera, and analyzes the image coming from it to create
    a correspondence between the real world and a virtual space where you place the
    AR objects.
  prefs: []
  type: TYPE_NORMAL
- en: Before running the session, we need to make use of the `ARConfiguration` class,
    which will determine the ARKit features that have been enabled for the session.
    In this case, it will track the device's position and orientation in relation
    to surfaces, people, images, or objects using the device's back camera and then
    run the session with that configuration. We could use a different configuration
    if we wanted to track only people's faces or only images, for example. (See [https://developer.apple.com/documentation/arkit/arconfiguration](https://developer.apple.com/documentation/arkit/arconfiguration)
    for more information.)
  prefs: []
  type: TYPE_NORMAL
- en: 'The third override method is `viewWillDisappear`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This method is called when the view is about to be removed. When that happens,
    the view's session is paused.
  prefs: []
  type: TYPE_NORMAL
- en: These are the methods we have implemented at the moment. Now, we are going to
    start changing and adding code to see how ARKit tracks planes and find out about
    the different state changes.
  prefs: []
  type: TYPE_NORMAL
- en: Modifying the basic app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Starting from the current code in `ViewController.swift`, we are going to modify
    it so that it only detects horizontal surfaces (not verticals, faces, or known
    images) and displays those horizontal surfaces as they are being detected to show
    extra information about the `ARSession`, such as whenever `ARTracking` is ready
    or if a new horizontal surface has been detected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we do this, though, we will delete the ship model as we no longer need
    it. Follow these steps to delete the ship model:'
  prefs: []
  type: TYPE_NORMAL
- en: Delete the `art.scnassets` folder from the project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the `viewDidLoad()` method of `ViewController.swift`, delete the reference
    to the ship from the scene, leaving the `scene` line like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We are also going to enable an AR debug option that will let us see the feature
    points of our environment. As we have seen already in this book, feature points
    are unique points of an image that make it possible for that image to be identified
    and tracked. The more features we have, the more stable the tracking will be.
    We will activate this option for now so that we can see how well our environment
    is detected, and deactivate it for our final app. For that, in the `viewDidLoad()`
    method, after the `sceneView.showStatistics = true` line, add the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: And with this, we can proceed to plane detection.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting and showing horizontal planes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned previously, `ARSession` is the main element of the AR app. Another
    essential element of ARKit is `ARAnchor`, which is the representation of an interesting
    point in the real world, along with its position and orientation. We can use anchors
    to place and track virtual elements in the real world that are relative to the
    camera. When we add those anchors to the session, ARKit optimizes the world-tracking
    accuracy around that anchor, meaning that we can walk around the virtual objects
    as if they were placed in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from adding anchors manually, some ARKit features can automatically add
    their own special anchors to a session. For example, when we activate the plane
    detection feature in the `ARConfiguration` class, the `ARPlaneAnchor` elements
    are created automatically whenever a new plane is detected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s make these plane anchors visible so that we can see how ARKit works.
    Let''s get to it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `viewWillAppear` method, *after*the `configuration` definition, add
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now, ARKit will only look for horizontal surfaces to track.
  prefs: []
  type: TYPE_NORMAL
- en: 'Uncomment the `renderer` method that''s already present in the class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This method is called when a new `ARAnchor` is added to the scene. It helps
    us create a `SceneKit` node called `SCNNode` for that anchor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, to paint the planes, *between* the creation of the node and the `return`
    statement, add the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Here, if our anchor is an `ARPlaneAnchor`, we create a new semi-transparent
    orange plane that's the same size as the `planeAnchor`. Then, we create a `SCNNode`
    with that plane and add it to the empty node that was already created. Finally,
    we return this parent node so that it's related to our current anchor and displayed
    in the scene.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of this method, we could also implement another method from `ARSCNViewDelegate`: `func
    renderer(_ renderer: SCNSceneRenderer, didAdd node: SCNNode, for anchor: ARAnchor){}`.
    Here, the empty node has already been created, so we would only have to add the
    same preceding code extract.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you test the app now, you will see how it paints orange planes whenever
    it detects a flat surface. However, you will see that once a plane is painted,
    even if ARKit detects more points around it, the plane''s size is not updated,
    as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8d5c686b-1ad5-4ce4-99a0-3616ee5af2af.png)'
  prefs: []
  type: TYPE_IMG
- en: ARKit detecting a flat surface and placing a plane
  prefs: []
  type: TYPE_NORMAL
- en: Let's add some more code to make the displayed plane change its size and position
    when more points are detected.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the beginning of the class, after `sceneView`, create an array to save all
    the planes in the scene and their respective anchors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, in the `renderer` method, *after* creating the plane, and *before* the
    `return` statement, add the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This will save our plane node and anchor for later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add a new method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This method is called when the anchor is updated. First, we check that the anchor
    is an `ARPlaneAnchor`. Then, we take the plane that corresponds to that anchor
    from the array and change its `width` and `height`. Finally, we update the `position`
    of the `child` node (remember that we added our plane node to an empty node; we
    want to update our plane's position, not the empty node's position).
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the app to see how the more you move the device around, the bigger the
    planes will become:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/2d086147-7369-4893-ba00-c8e395bca86a.png)'
  prefs: []
  type: TYPE_IMG
- en: The detected plane becomes bigger as ARKit detects more of its points
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how plane anchors are created and how to visualize
    them.
  prefs: []
  type: TYPE_NORMAL
- en: When testing the app, you may have noticed that the device needs a little time
    before it starts showing the plane anchors. That's the time when ARKit is initializing.
    In the next section, we are going to create a track for the ARKit state changes
    so that the user knows when the app is ready.
  prefs: []
  type: TYPE_NORMAL
- en: Adding ARSessionDelegate to track state changes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our class, we already have three methods that can be used to notify session
    changes to the user when the session fails or is interrupted. We are also going
    to add two new methods, that is, for when an anchor is added to the session and
    when the camera changes its tracking state. But before we do any of this, we need
    to create a label in our UI to display all the messages.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a label UI element
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s add a new UI element, that is, a label, to show notifications to users.
    For that, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open `Main.storyboard` and open the library button located in the top-right
    corner of the screen (the first button of its set, with a square inside a circle):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/5b903266-34ee-43ef-9e3c-21c4a7e1814d.png)'
  prefs: []
  type: TYPE_IMG
- en: Library button
  prefs: []
  type: TYPE_NORMAL
- en: 'Find and drag a Label onto the view:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/46f84b3d-49b1-4d12-b294-ae4df4aa3722.png)'
  prefs: []
  type: TYPE_IMG
- en: Selecting the Label from the library
  prefs: []
  type: TYPE_NORMAL
- en: With the label selected, use *Ctrl* + mouse drag to create constraints for the
    label regarding the view, or in the toolbar *below* the phone view, click on the
    Add New Constraints button. Constraints help us fix the elements on the screen
    so that they appear properly on any device screen and in any orientation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, modify the values of the left, right, and bottom constraints to `20`,
    `20`, and `0`, respectively (the icon of the constraint will turn a bright red).
    Check the Height constraint box and set it to `60`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/4be967d7-1d1f-4059-9e1d-3fa993424d9e.png)'
  prefs: []
  type: TYPE_IMG
- en: Adding constraints to the label
  prefs: []
  type: TYPE_NORMAL
- en: 'The new constraints will appear on the right-hand side window, under the Show
    the Size Inspector tab, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/57657991-3f36-40c0-b2bd-2b133ce38ed3.png)'
  prefs: []
  type: TYPE_IMG
- en: The four new constraints added at the bottom of the Show the Size tab
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Show the Attributes Inspector tab, *delete* the default Text, set the
    label Color to *green*, check the Dynamic Type checkbox, and set the Alignment
    of the text to centered, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/518357cc-5622-4835-a711-2ea12e3cc48b.png)'
  prefs: []
  type: TYPE_IMG
- en: Changing the properties of the label
  prefs: []
  type: TYPE_NORMAL
- en: 'To connect the label to our `ViewController.swift` script and use it in our
    methods, click on the Show the Assistant Editor button in the top-right corner
    (two circles intersecting) to open the script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a62ddb0d-00bf-4fae-b2c1-d308fb55a449.png)'
  prefs: []
  type: TYPE_IMG
- en: Selecting the Assistant Editor
  prefs: []
  type: TYPE_NORMAL
- en: 'Press *Ctrl* + drag the label (you will see a blue line as you drag the mouse)
    from the hierarchical view to the code, below the `sceneView` variable. Release
    the mouse. Then, on the pop-up window shown in the following screenshot, enter
    the Name of the variable, which will be `infoLabel` in this case, and click Connect:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/52710e29-4e60-4d9b-bacf-6ea9a3091ce3.png)'
  prefs: []
  type: TYPE_IMG
- en: The label from the UI will be attached to the infoLabel variable
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the `ViewController.swift` file to check that the new variable has been
    added correctly, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can start sending messages to the user through the interface.
  prefs: []
  type: TYPE_NORMAL
- en: Sending notifications to the user
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have our label to display messages to the user, let''s use the
    `session` methods we already have in our `ViewController` class and create new
    ones to display useful information, as shown in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `ViewController.swift` file, within the `didFailWithError` session,
    add a new message:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This message will appear when there is an error in the `ARSession`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `sessionWasInterrupted` method, add the following message:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This will be executed when the session is interrupted; for example, when the
    app is minimized.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `sessioInterruptionEnded` method, add the following message and code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: When the session interruption finishes, we have to reset the tracking. For that,
    we will create the configuration parameter again and run the session by removing
    the previously existing anchors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create a new method that will detect whenever the tracking state
    has changed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Your Xcode window will look like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dbf9bedd-e177-4a9d-8f5d-35bf8f635b38.png)'
  prefs: []
  type: TYPE_IMG
- en: The new method in ViewController
  prefs: []
  type: TYPE_NORMAL
- en: This method checks the tracking state and displays messages when the session
    is initializing or there are problems. We also display a message when the tracking
    state is normal, but we haven't found a plane anchor yet, so the user keeps looking
    around.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have to notify the user when an anchor has been added so that they
    don''t have to look around anymore. For that, we are going to use `ARSessionDelegateProtocol`.
    The first thing we will do is add the delegate to the class, as shown in the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The class declaration will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61312d51-3339-4462-abef-7e4b3e1851e0.png)'
  prefs: []
  type: TYPE_IMG
- en: The ViewController class with the added delegate
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `viewWillAppear` method, just after the `sceneView.session.run(configuration)` line,
    add the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'With this line, we assign the delegate to the class. The `viewWillAppear` method
    will now look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/53c58d57-e002-4862-967c-c365228d0c79.png)'
  prefs: []
  type: TYPE_IMG
- en: The viewWillAppear method
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, create the new method to show the message when an anchor has been added:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the app to see how the label changes according to the state of the camera,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/3e8b4532-5542-4e3a-a36f-db6ef3c36432.png)'
  prefs: []
  type: TYPE_IMG
- en: The label notifying the user that the AR session is initializing
  prefs: []
  type: TYPE_NORMAL
- en: 'After the AR has been initialized, the label changes to detecting the horizontal
    surfaces message, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1e5f1fa-49d9-4a72-96d8-4daa0dabb0d5.png)'
  prefs: []
  type: TYPE_IMG
- en: The label asking the user to move the device to detect horizontal surfaces
  prefs: []
  type: TYPE_NORMAL
- en: The debugging messages of the `cameraDidChangeTrackingState` session method
    come from the *Tracking and Visualizing Planes* example, which is available at
    [https://developer.apple.com/documentation/arkit/tracking_and_visualizing_planes](https://developer.apple.com/documentation/arkit/tracking_and_visualizing_planes).
    This example project uses more methods to show planes and messages that we won't
    need for this project. You can download and test it if you want to learn about
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our app's base, let's create an AR portal where we will display
    a *door* to a virtual 3D painting. Once we physically go through that door, we
    will find ourselves immersed in that virtual painting.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an AR portal
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Currently, we have an app that detects horizontal planes and notifies us about
    the state of the tracking system. Now, we want to create an AR experience where
    users will tap on the screen to create a portal and, when going through it, see
    themselves inside a 3D representation of Van Gogh''s *Bedroom in Arles* painting.
    The following diagram depicts the scene in SceneKit''s coordinate system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/98445e9c-d3b8-40a1-8c31-31da4433b42c.png)'
  prefs: []
  type: TYPE_IMG
- en: The scene in the XYZ coordinates
  prefs: []
  type: TYPE_NORMAL
- en: From the user's perspective, we will have a portal with a hole in it, as shown
    in the previous diagram. The hole will let us see the model in the background,
    that is, the 3D painting. The portal will not be gray; it will be transparent
    so that we can see the camera feed instead of the whole 3D painting scene that
    hides behind it. In this section, we will learn how to make this possible.
  prefs: []
  type: TYPE_NORMAL
- en: For that, we need to add the 3D model, create the screen-tapping functionality,
    and create the actual portal, which only shows part of the painting from the outside.
  prefs: []
  type: TYPE_NORMAL
- en: From this point on, you can specify whether you want the showFeaturePoints option
    and the two `renderer` methods showing the planes. We will leave them until the
    end of the project because it's best if we understand how ARKit works first.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following subsections, we will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Import a 3D model into our project and scene
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add the user interaction so that when a user touches the screen, the 3D model
    will appear over the touched anchor plane
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add the walls to the portal to make the model invisible from the outside, except
    for the door
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improve the app by adding textures to the walls and a compass image to help
    users find out where the portal will appear
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's start by adding the 3D model we want to show.
  prefs: []
  type: TYPE_NORMAL
- en: Importing the 3D model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SceneKit's preferred 3D model type is Collada (`.dae` files). However, it can
    also read OBJ (`.obj`) files, and in this case, the downloaded model is in the
    latter format. We have slightly modified it using a 3D modeling program and put
    the pivot point on the floor, in the nearest edge away from us. This way, when
    we place it in the real world, we will see the model in front of us, instead of
    surrounding us. If you try this with another model or you don't know how to modify
    a pivot point, you can adjust the transform's position directly in the code (we
    will explain how later).
  prefs: []
  type: TYPE_NORMAL
- en: Adding the model to the project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To import the model and show it in our scene, download it from the resources
    of this project and follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Right-click on the project and select New Group, as shown in the following
    screenshot. Call it `Models`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ab1a3672-02ee-48b6-91b4-697bf26913f8.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating a New Group to contain our model
  prefs: []
  type: TYPE_NORMAL
- en: 'Right-click on the `Models` folder and select Add Files to "ARPortal"…, as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/028756b0-d07d-4140-aebb-9c2529a1a9d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Adding files to our folder
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the `vangogh_room` folder, which contains the model, material,
    and textures. Select it, make sure that it''s added to our app in Add to targets,
    and click Add, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7a74e69c-9b32-4193-a3c9-fcd1213c86b0.png)'
  prefs: []
  type: TYPE_IMG
- en: Selecting the folder and adding it to the target app
  prefs: []
  type: TYPE_NORMAL
- en: 'If you unfold the `vangogh_room` folder, you will see all the files inside
    it. Click on the `vangogh_room.obj` file to visualize the 3D model on the screen.
    When we create the portal, we will enter the model from the open wall, as shown
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f12fe0e6-64eb-40a7-8a49-9595e325f9f8.png)'
  prefs: []
  type: TYPE_IMG
- en: The .obj file displayed in 3D
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can use our model in our code.
  prefs: []
  type: TYPE_NORMAL
- en: Adding the model to our scene
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have imported the model into our project, let''s add it to our
    scene using code, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new file by right-clicking on the `ARPortal` folder and selecting
    New File…, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/591f9573-6060-49de-8f3a-de014427a839.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating a new file
  prefs: []
  type: TYPE_NORMAL
- en: 'Select Swift File and click Next:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/0205cf66-b19d-42df-b7a2-2931706b9f16.png)'
  prefs: []
  type: TYPE_IMG
- en: Selecting Swift File
  prefs: []
  type: TYPE_NORMAL
- en: Call it `Portal.swift` and click Create. This will be the class where we will
    create our full portal, including the 3D model we imported previously.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Delete the code and add the `ARKit` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `class`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Our portal will be of the `SCNNode` type.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, inside the class, we''ll create a new method to load the 3D model. Add
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Here, we load the scene from the `.obj` file and take the node out of it. Then,
    we attach the node as a child of our portal.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you use another model and it doesn''t appear where you want it to be (displaced
    in one or more axes), you can adjust its position inside this method by adding
    the following before the `self.addChildNode(modelNode)` line: `modelNode.position
    = SCNVector3(x: YourValue, y: YourValue, z: YourValue)`. You can check how coordinates
    work in SceneKit by looking at the diagram at the beginning of this section.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, `override` the `init` method, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In the first method, we initialize our portal by adding the 3D model. The second
    method is required for a subclass of `SCNNode`.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have added our 3D model, we want to show it in our scene.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could just open `ViewController.swift` and add the following at the end
    of the `viewDidLoad()` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The `viewDidLoad` method will now look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/739265e0-4a5a-4d2e-9948-3e7b4c56148f.png)'
  prefs: []
  type: TYPE_IMG
- en: The viewDidLoad method with the Portal added
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the 3D model will appear like the ship we saw at the beginning
    of this chapter, from the start of the session and in the middle of the screen
    (we would have to translate it downward for a better view). However, we want to
    add a twist and attach it to one of the plane anchors when the user taps on the
    screen, so delete those two lines. We'll learn how to do this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Including user interaction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s add some user interaction to the app. Instead of just making the virtual
    content appear in the scene from the beginning, we will make it appear when the
    user taps the screen. For that, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In `Main.storyboard`, click on the library button (the square inside a circle
    button) and look for Tap Gesture Recognizer. Drag it onto the view:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/76088aa8-5eb7-442e-8f6d-e03af4963264.png)'
  prefs: []
  type: TYPE_IMG
- en: Selecting Tap Gesture Recognizer from the library
  prefs: []
  type: TYPE_NORMAL
- en: 'It will appear on the hierarchy, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d02d7268-ce13-4502-9ac6-1c0cafca5611.png)'
  prefs: []
  type: TYPE_IMG
- en: Tap Gesture Recognizer in the View Controller Scene hierarchy
  prefs: []
  type: TYPE_NORMAL
- en: 'Show the Assistant Editor (two circles intersecting button) and open `ViewController.swift` on
    the right-hand side. *Ctrl* + drag the Tap Gesture Recognizer over to the code,
    as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e639ee1d-23e1-4a6f-8c94-724b5638c928.png)'
  prefs: []
  type: TYPE_IMG
- en: Dragging the Tap Gesture Recognizer to the code to create the connection
  prefs: []
  type: TYPE_NORMAL
- en: 'Fill in the box that appears with the following parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Connection: `Action`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object: `View Controller`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Name: `didTapOnScreen`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Type: `UITapGestureRecognizer`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Open the `ViewController.swift` file to ensure the method has been created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, include the following variable at the beginning of the class, after the
    `planes` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We will use this variable to make sure we only have one portal in the view.
  prefs: []
  type: TYPE_NORMAL
- en: 'Modify the text of the session''s `didAdd` method to instruct the user to place
    the portal when an anchor is detected, but no portal has been added yet. For that,
    modify `infoLabel.text`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, go to the `didTapOnScreen` method and add the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we are taking out the location of the tap and checking whether the tap
    is hitting an existing plane. (You can check the options for the `ARHitTestResult`
    here: [https://developer.apple.com/documentation/arkit/arhittestresult](https://developer.apple.com/documentation/arkit/arhittestresult).)
    We take the first plane of the hit results. If the portal already exists, we delete
    it to ensure we only have one portal in view. If the user taps in different places
    on the screen, the portal will appear to move from one place to the other. Then,
    we will apply the position from the plane to our portal object. Finally, we will
    add the portal to our scene and we''ll clear the information label as we no longer
    need to tell our users to tap on the screen. The resulting method will look like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6eea8e1c-ac1f-455d-9546-7e7a21faac6c.png)'
  prefs: []
  type: TYPE_IMG
- en: The didTapOnScreen method
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `sessionWasInterrupted` method, after `infoLabel.text`, we are going
    to delete the portal so that it doesn''t appear when the session is recovered.
    For that, add the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The `sessionWasInterrupted` method should look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4a93b66-7582-4a9f-84d7-88fe4aef6029.png)'
  prefs: []
  type: TYPE_IMG
- en: The sessionWasInterrupted method
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the app and tap the screen to place the 3D model. It will appear over the
    orange planes, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/aeb20833-bbf8-4034-a4b6-11c71866e392.png)'
  prefs: []
  type: TYPE_IMG
- en: The 3D painting appearing in AR when tapping on the screen
  prefs: []
  type: TYPE_NORMAL
- en: 'If you enter the model and look back, you will see the real world from the
    open wall in the 3D model, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e4bd540a-2240-4d92-99cf-8b7a1d67a4cc.png)'
  prefs: []
  type: TYPE_IMG
- en: The opening to the real world from the 3D room
  prefs: []
  type: TYPE_NORMAL
- en: Move around the room and explore the 3D space from different angles. You will
    see how you are immersed in the virtual environment while the real world is still
    on the other side.
  prefs: []
  type: TYPE_NORMAL
- en: Now, it's time to actually create a portal that hides most of the model until
    the *door* is crossed. For that, we are going to create transparent walls and
    play with the rendering order property.
  prefs: []
  type: TYPE_NORMAL
- en: Adding the walls of the portal
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The main trick of an AR portal is based on two things: the transparency and
    the rendering order. We are going to create a wall with an opening (we will call
    it the *doo*r) in the middle, through which we will see the 3D painting. Open
    the `Portal.swift` file and follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new method called `createWall` with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we created a parent node. Then, we created a box with the given size
    and set its material to white. We created a node with the box geometry that we
    attached to the parent node and returned it. This will be our base to create the
    portal walls:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f1c4372-d514-4496-bac5-2a5727620054.png)'
  prefs: []
  type: TYPE_IMG
- en: The new createWall method
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create another method called `createPortal`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Inside it, let''s define the variables for the sizes of the walls. Add the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We will have four walls, that is, on the left, right, top, and bottom of our
    portal door. The first three have to be big enough to hide the model from being
    displayed behind the portal. Their length will be the only thing that's displayed
    (thereby simulating a door to another dimension). The bottom wall will close that
    opening.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, create the main node and the four walls using the previous method `createWall`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the walls to the portal node and then the portal itself to the class''
    node, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Our 3D portal is ready. Your code should appear as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2907acc-775e-4e63-b09d-bc9bbd7e0b66.png)'
  prefs: []
  type: TYPE_IMG
- en: The createPortal method
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s call this method from `init`. After adding the 3D model, add the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run the app, when we tap on the floor, we will see a white wall, and
    through its gap, the 3D model. When we cross it, we will find ourselves in the
    3D painting, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/82486f5f-0d73-4962-923f-89d8e2f0eb07.png)'
  prefs: []
  type: TYPE_IMG
- en: The white wall covering most of the 3D painting
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also see the walls so that we can go back to the real world:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/37db3ab1-3198-4281-b976-ca8530bf32de.png)'
  prefs: []
  type: TYPE_IMG
- en: The view from inside the 3D painting
  prefs: []
  type: TYPE_NORMAL
- en: 'However, from the outside, we don''t want to see a white wall; the portal should
    just be an opening in the air (the gap on the wall). To get that effect, let''s
    add another box to the `createWall` method before the `return node` call. Add
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: This new wall, located before the other one, has a transparency of near 0 (if
    set to `0`, it is painted black) so that we can see the background. But this alone
    won't work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add a rendering order of `100` to `wallNode` and a rendering order of `10`
    to `maskedWallNode`, but leave the final code for the `createWall` method like
    this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The `createWall` method should now look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d3d94cbd-181c-4ecd-a45e-0af78c299c9b.png)'
  prefs: []
  type: TYPE_IMG
- en: The createWall method with the final code
  prefs: []
  type: TYPE_NORMAL
- en: 'To finish this, add a rendering order of `200` to `modelNode`, which we created
    in the `add3DModel` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The lower the rendering order, the faster the object will be rendered in the
    scene. This means that we will see `maskedWallNode` first, then `wallNode`, and
    finally `modelNode`. This way, whenever the masked wall is in view, the 3D elements
    behind its surface won't be rendered, leaving us with a transparent surface that
    will show the camera feed directly. The only part of the 3D painting that will
    be rendered will be the one that's not covered by the walls, that is, our portal
    door. Once we go through the portal, we will see the 3D painting in full, and
    when we look back, we will see our white wall.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the app to see how the portal appears by touching the screen. In the following
    screenshot, we can see how the masked wall shows us the camera feed hiding the
    rest of the elements. Here, we can only see the opening from the wall (since we
    have placed it behind the masked wall in the *z *axis) and the painting through
    the opening:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can comment on the `showFeaturePoints` line and the `renderer` methods so
    that the feature points and anchor planes don't interfere with our 3D scene.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aa05326a-34fc-4777-84f3-7e176824db18.png)'
  prefs: []
  type: TYPE_IMG
- en: The portal appearing in AR
  prefs: []
  type: TYPE_NORMAL
- en: Once inside the painting, if we look back, we will still see the white walls
    and the opening to go back to the real world, like before.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the scene is ready, let's improve it by adding some texture to the
    walls.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the portal
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are going to improve our portal by adding some texture to the walls and
    a compass image that will show where the portal will appear. For that, we have
    to add the texture image and the compass image to our project. To do this, follow
    these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Right-click on the ARPortal project and select New File... In this case, select
    SceneKit Catalog from the Resource tab, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b7a7b3e9-df78-487e-b35b-b45eb5c39717.png)'
  prefs: []
  type: TYPE_IMG
- en: Selecting a new SceneKit Catalog
  prefs: []
  type: TYPE_NORMAL
- en: 'Name it `Media.scnassets` and create it, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d105107c-e110-4365-9352-ba39e3b7084c.png)'
  prefs: []
  type: TYPE_IMG
- en: The SceneKit catalog has been successfully created
  prefs: []
  type: TYPE_NORMAL
- en: Right-click on the newly created `Media.scnassets` and select Add Files to "Media.scnassets"....
    Now, select the `wood.jpg` image from the resources of this project and accept
    it. Our image is ready to be used.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *step 3* with the `compass.png` image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open `Portal.swift` and, in the `createWall` method, find the following line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Change it so that it reads like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have added the wooden texture to our portal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will paint the compass in `ViewController.swift`. For that, create
    the following variable after the portal variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, uncomment the first `renderer` method, which is where the anchor planes
    are added (if you had it commented), and substitute the code inside it with the
    following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Here, when a plane anchor is detected, if there is no portal or compass in view
    (the first time a plane is detected), we will paint a semitransparent compass
    showing the user the place and orientation of the future portal. Then, we'll save
    the node as the anchor so that we can use it in *step 9*.
  prefs: []
  type: TYPE_NORMAL
- en: Comment the `didUpdate` renderer method if you haven't. We won't need it anymore.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the `didTapOnScreen` method, remove the code between `portal = Portal()`
    and `self.sceneView.scene.rootNode.addChildNode(portal!)` and include the following
    code instead:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Here, when the user taps on the screen for the first time (the compass is in
    view), we place the portal using the compass position and rotation, and we delete
    the compass as we no longer need it. From that moment on, if the user keeps tapping
    on the screen, the portal will move according to the taps and anchor plane position,
    like before.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the app to see how the compass appears as soon as the device detects an
    anchor plane:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/3595a725-8ac4-45a2-a992-a973fc70a4b1.png)'
  prefs: []
  type: TYPE_IMG
- en: The compass appearing on the screen, signaling the place and orientation of
    the future portal
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, tap the screen to see how the portal has a wooden frame. This can be seen
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f7a79db0-7ce4-411a-a4fe-5699e063428c.png)'
  prefs: []
  type: TYPE_IMG
- en: The portal now has a wooden frame
  prefs: []
  type: TYPE_NORMAL
- en: 'From the inside, the texture of the walls has changed too, as shown in the
    following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c2498a8b-33ee-46b7-82fe-1432036002f4.png)'
  prefs: []
  type: TYPE_IMG
- en: The view from the inside of the 3D painting
  prefs: []
  type: TYPE_NORMAL
- en: And that's it. Now, you can place the portal wherever you want and play with
    different options such as changing the 3D painting or creating more than one portal
    at the same time. It's up to you!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned what ARKit is and how it works. We have seen
    how the camera recognizes the feature points and how it creates anchor planes
    where you can place virtual elements. We have also learned to use SceneKit to
    insert 3D models into the scene and manipulate them to create an AR portal that
    leads the user to a 3D environment.
  prefs: []
  type: TYPE_NORMAL
- en: By now, you should have the basic skills to continue improving the current project
    and adapt it to your personal needs in the tourism sector or even other sectors.
    You could try to add buttons to the interface to change the 3D content of the
    portal, add particle effects when the portal first appears, or even try to use
    360º videos instead of 3D models. AR is a transversal tool that can be used in
    several ways and applied to many different needs, so you could also create this
    portal for fields such as marketing or retail.
  prefs: []
  type: TYPE_NORMAL
- en: This is the last chapter of this book and it is dedicated to the use of AR in
    enterprises. By now, you should have a broader idea of how AR can be useful for
    many purposes, including tourism. Now, all you need to do is start playing around
    with the different frameworks and tools you have learned about and adapt them
    to your own needs and projects. Have fun!
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you want to explore ARKit, either to push the current project further or
    to try out new things, we recommend Apple''s example projects, which can be found
    at [https://developer.apple.com/documentation/arkit](https://developer.apple.com/documentation/arkit).
    Some of these projects are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The mentioned [https://developer.apple.com/documentation/arkit/tracking_and_visualizing_planes](https://developer.apple.com/documentation/arkit/tracking_and_visualizing_planes) project,
    which you can use to track and visualize the plane anchors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [https://developer.apple.com/documentation/arkit/detecting_images_in_an_ar_experience](https://developer.apple.com/documentation/arkit/detecting_images_in_an_ar_experience) project,
    which you can use to detect 2D images (the portal could appear over a real painting
    instead of in the middle of the street, for example)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [https://developer.apple.com/documentation/arkit/tracking_and_visualizing_faces](https://developer.apple.com/documentation/arkit/tracking_and_visualizing_faces) project,
    which you can use to track faces and animate virtual avatars through real expressions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the launch of iOS 13, there are even more features and projects you can
    try out, such as occluding virtual objects when people move in front of them or
    interacting with the virtual content by using gestures.
  prefs: []
  type: TYPE_NORMAL
