<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-39"><a id="_idTextAnchor038"/>2</h1>
<h1 id="_idParaDest-40"><a id="_idTextAnchor039"/>Native Monitoring in .NET</h1>
<p>In this chapter, we’ll explore the out-of-the-box diagnostic capabilities of modern .NET applications, starting with logs and ad hoc diagnostics, and then move on to examine what OpenTelemetry provides on top of that. We’ll create a sample application and instrument it, showcasing cross-process log correlation, and learn how we can capture verbose logs with <code>dotnet-monitor</code>. Then, we’ll investigate .NET runtime counters and export them to Prometheus. Finally, we’ll configure OpenTelemetry to collect traces and metrics from .NET, ASP.NET Core, and Entity Framework, and check out how basic auto-instrumentations address observability needs.</p>
<p>The following topics are what we’ll cover:</p>
<ul>
<li>Native log correlation in ASP.NET Core applications</li>
<li>Minimalistic monitoring with .NET runtime counters</li>
<li>Install OpenTelemetry and enable common instrumentations</li>
<li>Tracing and performance analysis with HTTP and database instrumentations</li>
</ul>
<p>By the end of this chapter, you’ll be ready to use distributed tracing instrumentation in .NET libraries and frameworks, enable log correlation and metrics, and leverage multiple signals together to debug and monitor your applications.</p>
<h1 id="_idParaDest-41"><a id="_idTextAnchor040"/>Technical requirements</h1>
<p>We’re going to start building a sample application and will use the following tools for it:</p>
<ul>
<li>.NET SDK 7.0 or newer</li>
<li>Visual Studio or Visual Studio Code with the C# development setup are recommended, but any text editor would work</li>
<li>Docker and <code>docker-compose</code></li>
</ul>
<p>The application code can be found in the book’s repository on GitHub at <a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter2">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter2</a>.</p>
<h1 id="_idParaDest-42"><a id="_idTextAnchor041"/>Building a sample application</h1>
<p>As shown in <em class="italic">Figure 2</em><em class="italic">.1</em>, our application <a id="_idIndexMarker095"/>consists of two REST services and a MySQL database:</p>
<div><div><img alt="Figure 2.1 – Meme service diagram" src="img/B19423_02_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – Meme service diagram</p>
<ul>
<li><strong class="bold">Frontend</strong>: ASP.NET Core Razor Pages<a id="_idIndexMarker096"/> application that serves user requests to upload and download images</li>
<li><strong class="bold">Storage</strong>: ASP.NET Core <a id="_idIndexMarker097"/>WebAPI application that uses Entity Framework Core to store images in a MySQL database or in memory for local development</li>
</ul>
<p>We’ll see how to run the full application using Docker later in this chapter. For now, run it locally and explore the basic logging and monitoring features that come with modern .NET.</p>
<p>We’re going to use the <code>Microsoft.Extensions.Logging.ILogger</code> API throughout this book. <code>ILogger</code> provides convenient APIs to write structured logs, along with verbosity control and the ability to send logs anywhere.</p>
<p>ASP.NET Core and Entity Framework use <code>ILogger</code>; all we need to do is configure the logging level for specific categories or events to log incoming requests or database calls, and supply additional context with logging scopes. We’re going to cover this in detail in <a href="B19423_08.xhtml#_idTextAnchor131"><em class="italic">Chapter 8</em></a>, <em class="italic">Writing Structured and Correlated Logs</em>. For now, let’s see log correlation in action.</p>
<h2 id="_idParaDest-43"><a id="_idTextAnchor042"/>Log correlation</h2>
<p>ASP.NET Core enables<a id="_idIndexMarker098"/> log correlation across multiple services by default. It creates an activity that loggers can access using <code>Activity.Current</code> and configures <code>Microsoft.Extensions.Logging</code> to populate the trace context on <a id="_idIndexMarker099"/>logging scopes. ASP.NET Core and <code>HttpClient</code> also support W3C Trace Context by default, so the context is automatically propagated over HTTP.</p>
<p>Some logging providers, for example, OpenTelemetry, don’t need any configuration to correlate logs, but our meme application uses a console provider, which does not print any logging scopes by default.</p>
<p>So let’s configure our console provider to print scopes and we’ll see the trace context on every log record. Let’s also set the default level to <code>Information</code> for all categories just so we can see the output:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">appsettings.json</p>
<pre class="source-code">
"Logging": {
  "LogLevel" : {"Default": "Information"},
  "Console" : {<strong class="bold">"IncludeScopes" : true</strong>}
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter2/storage/appsettings.json">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter2/storage/appsettings.json</a></p>
<p class="callout-heading">Note</p>
<p class="callout">Usually, you would use <code>Information</code> for application code only and set <code>Warning</code> or <code>Error</code> for frameworks and third-party libraries.</p>
<p>Let’s check it out – start the storage first, then, in a different terminal, start the frontend:</p>
<pre class="console">
storage$ dotnet run
frontend$ dotnet run</pre>
<p>Make sure to keep both terminals open so we can check logs later. Now, let’s get the preloaded meme from the frontend in your browser – hit http://localhost:5051/Meme?name=dotnet and then check the logs.</p>
<p>On the frontend, you <a id="_idIndexMarker100"/>may see something like this (other logs and scopes are omitted <a id="_idIndexMarker101"/>for brevity):</p>
<pre class="source-code">
info: System.Net.Http.HttpClient.storage.LogicalHandler[101]
      =&gt; <strong class="bold">SpanId:4a8f253515db7fec, TraceId:e61779</strong>
<strong class="bold">      </strong><strong class="bold">516785b4549905032283d93e09, ParentId:00000000000000</strong>
<strong class="bold">      00</strong> =&gt; HTTP GET http://localhost:5050/memes/dotnet
      End processing HTTP request after 182.2564ms - 200
info: Microsoft.AspNetCore.Hosting.Diagnostics[2]
      =&gt; <strong class="bold">SpanId:4a8f253515db7fec, TraceId:e6177951678</strong>
<strong class="bold">      5b4549905032283d93e09, ParentId:0000000000000000</strong> =&gt;
      Request finished HTTP/1.1 GET http://localhost:
        5051/Meme?name=dotnet - 200 256.6804ms</pre>
<p>The first record describes the outgoing call to the storage service. You can see the status, duration, HTTP method, and URL, as well as the trace context. The second record describes an incoming HTTP call and has similar information.</p>
<p class="callout-heading">Note</p>
<p class="callout">This trace context is the same on both log records and belongs to the incoming HTTP request.</p>
<p>Let’s see what happened on the storage:</p>
<pre class="source-code">
info: Microsoft.AspNetCore.Hosting.Diagnostics[2]
      =&gt; <strong class="bold">SpanId:5a496fb9adf85727, TraceId:e61779516785b</strong>
<strong class="bold">      4549905032283d93e09, ParentId:67b3966e163641c4</strong>
      Request finished HTTP/1.1 GET http://localhost:
        5050/memes/dotnet - 200 1516 102.8234ms</pre>
<p class="callout-heading">Note</p>
<p class="callout">The <code>TraceId</code> value is the same on the frontend and storage, so we have cross-process log correlation out of the box.</p>
<p>If we had OpenTelemetry <a id="_idIndexMarker102"/>configured, we’d see a trace similar to that shown in <em class="italic">Figure 2</em><em class="italic">.2</em>:</p>
<div><div><img alt="Figure 2.2 – Communication between the frontend and storage" src="img/B19423_02_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – Communication between the frontend and storage</p>
<p>We already know that ASP.NET Core creates an activity for each request – it reads trace context headers by default, but we can configure a different propagator. So, when we make an outgoing request from the frontend to storage, <code>HttpClient</code> creates another activity – a child of the ASP.NET Core one. <code>HttpClient</code> injects the trace context from its activity to the outgoing request headers so that they flow to the storage service, where ASP.NET Core parses them and creates a new activity, which becomes a child of the outgoing request.</p>
<p>Even though we didn’t export activities, they are created and are used to enrich the logs with trace context, enabling correlation across different services.</p>
<p>Without exporting activities, we achieve correlation but not causation. As you can see in the logs, <code>ParentId</code> on storage is not the same as <code>SpanId</code> on the outgoing HTTP request.</p>
<p class="callout-heading">Hint on causation</p>
<p class="callout">What happens here is that the outgoing request activity is created inside <code>HttpClient</code>, which does not write logs with <code>ILogger</code>. The log record on the outgoing request we just saw is written by the handler in the <code>Microsoft.Extensions.Http</code> package. This handler is configured by ASP.NET Core. When the handler logs that the request is starting, the <code>HttpClient</code> activity has not yet been created, and when the handler logs that the request is ended, the <code>HttpClient</code> activity is already stopped.</p>
<p>So, with ASP.NET Core <a id="_idIndexMarker103"/>and ILogger, we can easily enable log correlation. However, logs don’t <a id="_idIndexMarker104"/>substitute distributed traces – they just provide additional details. Logs also don’t need to duplicate traces.</p>
<p><em class="italic">Avoiding duplication is important: once, the author saved a company $80k a month by dropping logs that were duplicated by </em><em class="italic">rich events.</em></p>
<p>Going forward in this book, we’ll use logs for debugging and capturing additional information that covers gaps in traces.</p>
<h3>Using logs in production</h3>
<p>To record logs in production, where <a id="_idIndexMarker105"/>we have multiple instances of services and restricted access to them, we usually need a log management system – a set of tools that collect and send logs to a central location, potentially enriching, filtering, or parsing them along the way. OpenTelemetry can help us collect logs, but we also need a backend to store, index, and query the logs using any context, including <code>TraceId</code>. With this, we can easily navigate from traces to logs when needed.</p>
<h2 id="_idParaDest-44"><a id="_idTextAnchor043"/>On-demand logging with dotnet-monitor</h2>
<p>It could be useful to dynamically increase log<a id="_idIndexMarker106"/> verbosity at runtime to get more detailed information while reproducing the issue or, when the log<a id="_idIndexMarker107"/> exporting pipeline is broken, get logs from the service directly.</p>
<p>It’s possible with <code>dotnet-monitor</code> – a diagnostics tool that’s able to connect to a specific .NET process and capture logs, counters, profiles, and core dumps. We’ll talk about it in <a href="B19423_03.xhtml#_idTextAnchor052"><em class="italic">Chapter 3</em></a>, The <em class="italic">.NET </em><em class="italic">Observability Ecosystem</em>.</p>
<p>Let’s install and start <code>dotnet-monitor</code> to see what it can do:</p>
<pre class="console">
$ dotnet tool install -g dotnet-monitor
$ dotnet monitor collect</pre>
<p class="callout-heading">Note</p>
<p class="callout">If you’re on macOS or Linux, you need to authenticate requests to the <code>dotnet-monitor</code> REST API. Please refer to the documentation at <code>https://github.com/dotnet/dotnet-monitor/blob/main/documentation/authentication.md</code> or, for demo purposes only, disable authentication with the <code>dotnet monitor collect –</code><code>no-auth</code> command.</p>
<p>If you still have frontend and storage<a id="_idIndexMarker108"/> services running, you should see them among the other .NET processes on your machine when you open <code>https://localhost:52323/processes</code> in your browser:</p>
<pre class="source-code">
{"pid": 27020, …, "name": "storage"},
{"pid": 29144, … "name": "frontend"}</pre>
<p>Now let’s capture some <a id="_idIndexMarker109"/>debug logs from storage via <code>dotnet-monitor</code> by requesting the following:</p>
<p><code>https://localhost:52323/logs?pid=27020&amp;level=debug&amp;duration=60</code></p>
<p>It connects to the process first, enables the requested log level, and then starts streaming logs to the browser for 60 seconds. It doesn’t change the logging level in the main logging pipeline, but will return the requested logs directly to you, as shown in <em class="italic">Figure 2</em><em class="italic">.3</em>:</p>
<div><div><img alt="Figure 2.3 – A﻿d hoc logging with dynamic level using dotnet-monitor" src="img/B19423_02_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – Ad hoc logging with dynamic level using dotnet-monitor</p>
<p>You can apply a more advanced configuration <a id="_idIndexMarker110"/>using the POST logs API – check out https://github.com/dotnet/dotnet-monitor to learn more about it and other <code>dotnet-monitor</code> features.</p>
<p>Using <code>dotnet-monitor</code> in production <a id="_idIndexMarker111"/>on a multi-instance service with restricted SSH access can be challenging. Let’s see how we can do it by running <code>dotnet-monitor</code> as a sidecar in Docker. It’s also possible to run it as a sidecar in Kubernetes.</p>
<h1 id="_idParaDest-45"><a id="_idTextAnchor044"/>Monitoring with runtime counters</h1>
<p>So, we have correlated logs from the<a id="_idIndexMarker112"/> platform and services with which we can debug issues. But what about system health and performance? .NET and ASP.NET Core expose event counters that can give some insights into the overall system state.</p>
<p>We can collect counters with OpenTelemetry without running and managing <code>dotnet-monitor</code>. But if your metrics pipeline is broken (or if you don’t have one yet), you can attach <code>dotnet-monitor</code> to your process for ad hoc analysis.</p>
<p><code>dotnet-monitor</code> listens to <code>EventCounters</code> reported by the .NET runtime and returns them on <a id="_idIndexMarker113"/>an HTTP endpoint in <strong class="bold">Prometheus </strong><strong class="bold">exposition format</strong>.</p>
<p class="callout-heading">Note</p>
<p class="callout"><strong class="bold">Prometheus</strong> is a metrics platform that scrapes and <a id="_idIndexMarker114"/>stores metrics. It supports multidimensional data and allows us to slice, dice, filter, and calculate derived metrics<a id="_idIndexMarker115"/> using <strong class="bold">PromQL</strong>.</p>
<p>We’re going to run our service as a set of Docker containers with <code>dotnet-monitor</code> running as a sidecar for the frontend and storage, and configure Prometheus to scrape metrics from <code>dotnet-monitor</code> sidecars, as shown in <em class="italic">Figure 2</em><em class="italic">.4</em>:</p>
<div><div><img alt="Figure 2.4 – Meme services with runtime counters in Prometheus" src="img/B19423_02_04.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4 – Meme services with runtime counters in Prometheus</p>
<p>This makes our setup closer to real life, where we don’t have the luxury of running <code>dotnet-monitor</code> on the service instance.</p>
<p>So, let’s go ahead and run our <a id="_idIndexMarker116"/>application. Open the terminal, navigate to the <code>chapter2</code> folder, and run the following:</p>
<pre class="console">
$ docker-compose -f ./docker-compose-dotnet-monitor.yml
  up --build</pre>
<p>You might see some errors while MySQL is starting up. Let’s ignore them for now. After a few seconds, you should be able to reach the frontend via the same URL as before.</p>
<p>Let’s explore the CPU and memory counters published by the .NET Runtime:</p>
<ul>
<li><code>cpu-usage</code> event counter (reported as <code>systemruntime_cpu_usage_ratio</code> metric to Prometheus): Represents the CPU usage as a percentage.</li>
<li><code>gc-heap-size</code> (or <code>systemruntime_gc_heap_size_bytes</code>): Represents the approximate allocated managed memory size in megabytes.</li>
<li><code>time-in-gc</code> (or <code>systemruntime_time_in_gc_ratio</code>): Represents time spent on garbage collection since the last garbage collection.</li>
<li><code>gen-0-gc-count</code>, <code>gen-1-gc-count</code>, and <code>gen-2-gc-count</code> (or <code>systemruntime_gen_&lt;gen&gt;_gc_count</code>): Represents the count of garbage collections in the corresponding generation per interval. The default update interval is 5 seconds, but you can adjust it. Generation sizes are also exposed as counters.</li>
<li><code>alloc-rate</code> (or <code>systemruntime_alloc_rate_bytes</code>): Represents the allocation rate in bytes per interval.</li>
</ul>
<p>You can also find counters<a id="_idIndexMarker117"/> coming from Kestrel, Sockets, TLS, and DNS that can be useful to investigate specific issues such as DNS outages, long request queues, or socket exhaustion on HTTP servers. Check out the .NET documentation for the full list (https://learn.microsoft.com/dotnet/core/diagnostics/available-counters).</p>
<p>ASP.NET Core and <code>HttpClient</code> request counters don’t have dimensions, but would be useful if you didn’t have OpenTelemetry tracing or metrics and wanted to get a very rough idea about throughput and failure rate across all APIs.</p>
<p>Prometheus scrapes metrics from the <code>dotnet-monitor</code> metrics endpoint. We can access it ourselves to see the raw data, as shown in <em class="italic">Figure 2</em><em class="italic">.5</em>:</p>
<div><div><img alt="Figure 2.5 – Frontend metrics in Prometheus exposure format" src="img/B19423_02_05.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.5 – Frontend metrics in Prometheus exposure format</p>
<p>It’s also possible to query and <a id="_idIndexMarker118"/>plot basic visualizations with Prometheus, as you can see in <em class="italic">Figure 2</em><em class="italic">.6</em>. Just hit <code>http://localhost:9090/graph</code>. For any advanced visualizations or dashboards, we would need tooling that integrates with <a id="_idIndexMarker119"/>Prometheus, such as <strong class="bold">Grafana</strong>.</p>
<div><div><img alt="Figure 2.6 – GC memory heap size for frontend and storage services in Prometheus" src="img/B19423_02_06.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.6 – GC memory heap size for frontend and storage services in Prometheus</p>
<p>As you can see, even basic ASP.NET Core applications come with minimal monitoring capabilities – counters for overall system health and correlated logs for debugging. With <code>dotnet-monitor</code> we can even retrieve<a id="_idIndexMarker120"/> telemetry at runtime without changing the code or restarting the application (well, of course, only if we have access to the application instance).</p>
<p>With some additional infrastructure changes to run <code>dotnet-monitor</code> as a sidecar and logging management solution, we would be able to build a very basic production monitoring solution.</p>
<p>We still lack distributed tracing and metrics that have rich context. Let’s now see how to enable them with OpenTelemetry instrumentation and improve this experience further.</p>
<h1 id="_idParaDest-46"><a id="_idTextAnchor045"/>Enabling auto-collection with OpenTelemetry</h1>
<p>In this section, we’re going<a id="_idIndexMarker121"/> to add OpenTelemetry to our demo application and enable auto-collection for ASP.NET Core, <code>HttpClient</code>, Entity Framework, and runtime metrics. We’ll see what it adds to the bare-bones monitoring<a id="_idIndexMarker122"/> experience.</p>
<p>We’ll export traces to Jaeger and metrics to Prometheus, as shown in <em class="italic">Figure 2</em><em class="italic">.7</em>:</p>
<div><div><img alt="Figure 2.7 – Meme services sending telemetry to Jaeger and Prometheus" src="img/B19423_02_07.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.7 – Meme services sending telemetry to Jaeger and Prometheus</p>
<p>You can also send data directly to your observability backend if it has an OTLP endpoint, or you can configure a backend-specific exporter in the application. So, let’s get started and instrument our application with OpenTelemetry.</p>
<h2 id="_idParaDest-47"><a id="_idTextAnchor046"/>Installing and configuring OpenTelemetry</h2>
<p>OpenTelemetry comes as a set of <a id="_idIndexMarker123"/>NuGet packages. Here are a few that we’re using in our demo app:</p>
<ul>
<li><code>OpenTelemetry</code>: The SDK that contains all that we need to produce traces and metrics and configure a <a id="_idIndexMarker124"/>generic processing and export pipeline. It does not collect any telemetry on its own.</li>
<li><code>OpenTelemetry.Exporter.Jaeger</code>: This package contains a trace exporter that publishes spans to Jaeger.</li>
<li><code>OpenTelemetry.Exporter.Prometheus.AspNetCore</code>: This package contains the Prometheus exporter. It creates a new <code>/metrics</code> endpoint for Prometheus to scrape metrics from.</li>
<li><code>OpenTelemetry.Extensions.Hosting</code>: This package simplifies OpenTelemetry configuration in ASP.NET Core applications.</li>
<li><code>OpenTelemetry.Instrumentation.AspNetCore</code>: This package enables ASP.NET Core tracing and metrics auto-instrumentation.</li>
<li><code>OpenTelemetry.Instrumentation.Http</code>: This package enables tracing and metrics<a id="_idIndexMarker125"/> auto-instrumentation for <code>System.Net.HttpClient</code>.</li>
<li><code>OpenTelemetry.Instrumentation.EntityFrameworkCore</code>: Tracing<a id="_idIndexMarker126"/> instrumentation for Entity Framework Core. We only need it for the storage service.</li>
<li><code>OpenTelemetry.Instrumentation.Process</code> and <code>OpenTelemetry.Instrumentation.Runtime</code>: These two packages enable process-level metrics for CPU and memory utilization and include the runtime counters we saw previously with <code>dotnet-monitor</code>.</li>
</ul>
<p>You can also enable other counter sources one by one with the <code>OpenTelemetry.Instrumentation.EventCounters</code> package.</p>
<h3>Distributed tracing</h3>
<p>To configure tracing, first call <code>AddOpenTelemetry</code> extension method on <code>IServiceCollection </code>and then call the W<code>ithTracing</code> method, as shown in the following <a id="_idIndexMarker127"/>example:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">Program.cs</p>
<pre class="source-code">
builder.Services.AddOpenTelemetry().WithTracing(
  tracerProviderBuilder =&gt; tracerProviderBuilder
    .AddJaegerExporter()
    .AddHttpClientInstrumentation()
    .AddAspNetCoreInstrumentation()
    .AddEntityFrameworkCoreInstrumentation());</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter2/storage/Program.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter2/storage/Program.cs</a></p>
<p>Here, we’re adding the Jaeger exporter and enabling <code>HttpClient</code>, ASP.NET Core, and Entity Framework instrumentations (on storage).</p>
<p>We’re also configuring the <a id="_idIndexMarker128"/>service name via the <code>OTEL_SERVICE_NAME</code> environment variable in <code>launchSetting.json</code> and in <code>docker-compose-otel.yml</code> for Docker runs. The OpenTelemetry SDK reads it and sets the <code>service.name</code> resource attribute accordingly.</p>
<p>The Jaeger host is configured with the <code>OTEL_EXPORTER_JAEGER_AGENT_HOST</code> environment variable in <code>docker-compose-otel.yml</code>.</p>
<p>We’ll talk more about configuration in <a href="B19423_05.xhtml#_idTextAnchor083"><em class="italic">Chapter 5</em></a>, <em class="italic">Configuration and Control Plane</em>, and learn how to configure sampling, enrich telemetry, and add custom sources.</p>
<h3>Metrics</h3>
<p>Metrics configuration is similar – we first call the <code>AddOpenTelemetry</code> extension method <a id="_idIndexMarker129"/>on <code>IServiceCollection</code> and then in the <code>WithMetrics</code> callback set up the Prometheus exporter and auto-instrumentations for <code>HttpClient</code>, ASP.NET Core, Process, and Runtime. Entity Framework’s instrumentation does not report metrics.</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">Program.cs</p>
<pre class="source-code">
builder.Services.AddOpenTelemetry()
        ...
        .WithMetrics(meterProviderBuilder =&gt; meterProviderBuilder
            .AddPrometheusExporter()
            .AddHttpClientInstrumentation()
            .AddAspNetCoreInstrumentation()
            .AddProcessInstrumentation()
            .AddRuntimeInstrumentation());</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter2/storage/Program.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter2/storage/Program.cs</a></p>
<p>We also need to <a id="_idIndexMarker130"/>expose the Prometheus endpoint after building the application instance:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">Program.cs</p>
<pre class="console">
var app = builder.Build();
app.UseOpenTelemetryPrometheusScrapingEndpoint();</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter2/storage/Program.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter2/storage/Program.cs</a></p>
<p>We’re ready to run the application!</p>
<pre class="console">
$ docker-compose -f docker-compose-otel.yml up –-build</pre>
<p>You should see logs from all services including some errors while MySQL is starting up. Check the frontend to make sure it works: <code>https://localhost:5051/Meme?name=dotnet</code>.</p>
<h1 id="_idParaDest-48"><a id="_idTextAnchor047"/>Exploring auto-generated telemetry</h1>
<p>The meme service is <a id="_idIndexMarker131"/>now up and running. Feel free to upload your favorite memes and if you see any issues, use telemetry to debug them!</p>
<h2 id="_idParaDest-49"><a id="_idTextAnchor048"/>Debugging</h2>
<p>If you try to upload something right <a id="_idIndexMarker132"/>after the service starts, you might get an error like the one shown in <em class="italic">Figure 2</em><em class="italic">.8</em>. Let’s figure out why!</p>
<div><div><img alt="Figure 2.8 – Error from application with traceparent" src="img/B19423_02_08.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.8 – Error from application with traceparent</p>
<p>We can approach this investigation from two angles. The first is to use the <code>traceparent</code> shown on the page; the second is to filter the traces from the frontend based on the error status. In any case, let’s go to Jaeger – our<a id="_idIndexMarker133"/> tracing backend running on <code>http://localhost:16686/</code>. We can search by <code>Trace ID</code> or filter by service and error, as shown in <em class="italic">Figure 2</em><em class="italic">.9</em>:</p>
<div><div><img alt="Figure 2.9 – Find the trace in Jaeger by Trace ID (1) or with a combination of the service (2) and error (3)" src="img/B19423_02_09.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.9 – Find the trace in Jaeger by Trace ID (1) or with a combination of the service (2) and error (3)</p>
<p>If we open the trace, we’ll see<a id="_idIndexMarker134"/> that the storage service refused the connection – check out <em class="italic">Figure 2</em><em class="italic">.10</em>. What happened here?</p>
<div><div><img alt="Figure 2.10 – Drill down into the trace: the frontend service could not reach the storage" src="img/B19423_02_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.10 – Drill down into the trace: the frontend service could not reach the storage</p>
<p>Since there are no traces from the <a id="_idIndexMarker135"/>storage, let’s check the storage logs with <code>docker logs chapter2-storage-1</code>. We’ll query the logs in a more convenient way in <a href="B19423_08.xhtml#_idTextAnchor131"><em class="italic">Chapter 8</em></a>, <em class="italic">Writing Structured and Correlated Logs</em>. For now, let’s just grep storage logs around the time the issue occurred and find the relevant record, as shown in <em class="italic">Figure 2</em><em class="italic">.11</em>:</p>
<div><div><img alt="Figure 2.11 – Connection error in storage stdout" src="img/B19423_02_11.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.11 – Connection error in storage stdout</p>
<p>Apparently, the storage was not able to connect to the MySQL server and it could not start until the connection was established. If we dig further into the MySQL logs, we’ll discover that it took a while for it to start, but then everything worked just fine.</p>
<p>Some action items from this investigation are to enable retries on the frontend and investigate the slow start for MySQL. If it happens in production where there are multiple instances of storage, we should also dig into the load balancer and service discovery behavior.</p>
<p>What tracing brings here is <em class="italic">convenience</em> – we could have done the same investigation with logs alone, but it would have taken longer and would be more difficult. Assuming we dealt with a more complicated case with dozens of requests over multiple services, parsing logs would simply not be a reasonable option.</p>
<p>As we can see in this<a id="_idIndexMarker136"/> example, tracing can help us narrow the problem down, but sometimes we still need logs to understand what’s going on, especially for issues during startup.</p>
<h2 id="_idParaDest-50"><a id="_idTextAnchor049"/>Performance</h2>
<p>Let’s check out <a id="_idIndexMarker137"/>some metrics collected by the HTTP and runtime instrumentations.</p>
<p>OpenTelemetry defines <code>http.server.duration</code> and <code>http.client.duration</code> <strong class="bold">histogram</strong> metrics with<a id="_idIndexMarker138"/> low-cardinality attributes for method, API route (server only), and status code. These metrics allow us to calculate latency percentiles, throughputs, and error rates.</p>
<p>With OpenTelemetry metrics, ASP.NET Core instrumentation can populate API routes so we can finally analyze latency, throughput, and error rate per route. And histograms give us even more flexibility – we can now check the distribution of latency rather than the median, average, or a predefined set of percentiles.</p>
<h3>Latency</h3>
<p>HTTP client latency can be <a id="_idIndexMarker139"/>defined as the time between initiating a request and the response being received. For servers, it’s the time between receiving a request and the end of the server’s response.</p>
<p class="callout-heading">Tip</p>
<p class="callout">When analyzing latency, filter out errors and check the distribution of latency rather than just averages or medians. It’s common to check the 95th percentile (aka P95).</p>
<p><em class="italic">Figure 2</em><em class="italic">.12</em> shows P95 latency for the <code>PUT /meme</code> API on the client and server side:</p>
<div><div><img alt="Figure 2.12 – Server versus client PUT /meme latency P95 in milliseconds" src="img/B19423_02_12.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.12 – Server versus client PUT /meme latency P95 in milliseconds</p>
<h4>Time to first byte versus time to last byte</h4>
<p>In .NET, <code>HttpClient</code> buffers a response <a id="_idIndexMarker140"/>before returning it, but it can be configured to return the response right after the headers are received with <code>HttpCompletionOptions</code>. <code>HttpClient</code> instrumentation can’t measure time-to-last-byte in this case.</p>
<p>The distinction between <em class="italic">time to first body byte</em> versus <em class="italic">time to last byte</em> can be important on frontends with clients using unreliable connections or when transferring a lot of data. In such cases, it’s useful to instrument stream operations and then measure the time to first byte<em class="italic"> and</em> time to last byte. You can use the difference between these metrics to get an idea about connection quality and optimize the end user experience.</p>
<h3>Error rate</h3>
<p>Error rate is just a rate of unsuccessful<a id="_idIndexMarker141"/> requests per a given period of time. The key <a id="_idIndexMarker142"/>question here is what constitutes an error:</p>
<ul>
<li><code>1xx</code>, <code>2xx</code>, and <code>3xx</code> status codes indicate success.</li>
<li><code>5xx</code> codes cover errors such as the lack of a response, a disconnected client, network, or DNS issues.</li>
<li>Status codes in the <code>4xx</code> range are hard to categorize. For example, <code>404</code> could represent an issue – maybe the client expected to retrieve the data but it’s not there – but could also be a positive scenario, where the client is checking whether a resource exists before creating or updating it. There are similar concerns with other statuses.</li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">OpenTelemetry only marks client spans with <code>4xx</code> as errors. We’ll see in <a href="B19423_05.xhtml#_idTextAnchor083"><em class="italic">Chapter 5</em></a>, <em class="italic">Configuration and Control Plane</em>, how to tailor it to your needs.</p>
<p>It’s also common to treat latency <a id="_idIndexMarker143"/>above a given threshold as an error to measure availability, but we don’t strictly need it for observability purposes.</p>
<p><em class="italic">Figure 2</em><em class="italic">.13</em> shows an example <a id="_idIndexMarker144"/>of a server error rate chart for a single API grouped by error code:</p>
<div><div><img alt="Figure 2.13 – Error rate per second for ﻿the GET/meme API grouped by error code" src="img/B19423_02_13.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.13 – Error rate per second for the GET/meme API grouped by error code</p>
<p>It is also important to calculate the error rate per API route and method on servers. Because of different request rates, it’s easy to miss spikes or changes in less frequently called APIs.</p>
<p class="callout-heading">Tip</p>
<p class="callout">Returning precise status codes for ‘‘known’’ errors and letting the service return <code>500</code> only for unhandled exceptions makes it easier to use your service, but also simplifies monitoring and alerting. By looking at the error code, we can discern the possible reasons and not waste time on known cases. Any <code>500</code> response becomes important to investigate and fix or handle properly.</p>
<p>To check resource <a id="_idIndexMarker145"/>consumption, we can use runtime and process<a id="_idIndexMarker146"/> metrics. For example, <em class="italic">Figure 2</em><em class="italic">.14</em> shows CPU usage:</p>
<div><div><img alt="Figure 2.14 – CPU usage query" src="img/B19423_02_14.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.14 – CPU usage query</p>
<p>The query returns the average CPU utilization percentage across all instances for each service represented by job dimension – we configured jobs in <code>configs/prometheus-otel.yml</code>.</p>
<p>The state dimension divides processor time into user and privileged (system) time. To calculate the total average CPU usage per instance per service, we could write another Prometheus query:</p>
<pre class="source-code">
sum by (job, instance) (rate(process_cpu_time_s[1m]) * 100)</pre>
<p>The query calculates the total CPU usage per instance and then calculates the average value per service.</p>
<p>As you can see, the <a id="_idIndexMarker147"/>Prometheus query language is a powerful tool allowing<a id="_idIndexMarker148"/> us to calculate derived metrics and slice, dice, and filter them.</p>
<p>We’ll see more examples of runtime metrics and performance analysis in <a href="B19423_04.xhtml#_idTextAnchor068"><em class="italic">Chapter 4</em></a>, <em class="italic">Low-Level Performance Analysis with </em><em class="italic">Diagnostic Tools</em>.</p>
<h1 id="_idParaDest-51"><a id="_idTextAnchor050"/>Summary</h1>
<p>In this chapter, we explored .NET diagnostics and monitoring capabilities supported by the platform and frameworks. ASP.NET Core context propagation is enabled by default and logging providers can use it to correlate logs. We need a log management system to be able to store logs from multiple instances of a service and efficiently query them.</p>
<p><code>dotnet-monitor</code> allows the streaming of logs on demand from specific instances of your service, and the scraping of event counters with Prometheus to get a basic idea about service health. It can also be used for low-level performance analysis and can be run in production.</p>
<p>Then, we enabled OpenTelemetry auto-instrumentation for the HTTP stack and Entity Framework. HTTP and DB traces enable basic debugging capabilities, providing generic information on what happened for each remote call. You can search for traces based on attributes and query them using your tracing backend. With tracing, we can easily find a problematic service or component, and when that’s not enough, we can retrieve logs to get more details about the issue. With logs correlated to traces, we can easily navigate between them.</p>
<p>HTTP metrics enable common performance analysis. Depending on your backend, you can query, filter, and derive metrics and build dashboards and alerts based on them.</p>
<p>Now that we’ve got hands-on experience with basic distributed tracing and metrics, let’s explore the .NET ecosystem more and see how you can leverage instrumentation for common libraries and infrastructure.</p>
<h1 id="_idParaDest-52"><a id="_idTextAnchor051"/>Questions</h1>
<ol>
<li>How would you show trace context on a Razor page?</li>
<li>Imagine that the observability backend stopped receiving telemetry from some instances of the service. What can we do to understand what’s going on with these instances?</li>
<li>With the help of the Prometheus documentation (https://prometheus.io/docs/prometheus/latest/querying/basics/), write a query with PromQL to calculate the throughput (requests per second) per service and API.</li>
<li>With our meme service, how would you find out when a meme was uploaded and how many times it had been downloaded if you know only the meme’s name?</li>
</ol>
</div>
</body></html>