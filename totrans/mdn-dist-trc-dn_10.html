<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-162"><a id="_idTextAnchor161"/>10</h1>
<h1 id="_idParaDest-163"><a id="_idTextAnchor162"/>Tracing Network Calls</h1>
<p>In this chapter, we’ll apply what we learned about tracing in <a href="B19423_06.xhtml#_idTextAnchor098"><em class="italic">Chapter 6</em></a>, <em class="italic">Tracing Your Code</em>, to instrument client and server communication via gRPC.</p>
<p>We’ll start by instrumenting unary gRPC calls on the client and server according to OpenTelemetry semantic conventions. Then, we’ll switch to streaming and explore different ways to get observability for individual messages. We’ll see how to describe them with events or individual spans and learn how to propagate context within individual messages. Finally, we’ll see how to use our instrumentation to investigate issues.</p>
<p>In this chapter, you’ll learn how to do the following:</p>
<ul>
<li>Instrument network calls on the client and server following OpenTelemetry semantic conventions and propagate context over the wire</li>
<li>Instrument gRPC streaming calls according to your application needs</li>
<li>Apply telemetry to get insights into network call latency and failure rates and investigate issues</li>
</ul>
<p>Using gRPC as an example, this chapter will show you how to trace network calls and propagate context through them. With this chapter, you should also be able to instrument advanced streaming scenarios and pick the appropriate observability signals and granularity for your traces.</p>
<h1 id="_idParaDest-164"><a id="_idTextAnchor163"/>Technical requirements</h1>
<p>The code for this chapter is available in the book’s repository on GitHub at <a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter10">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/tree/main/chapter10</a>.</p>
<p>To run the samples and perform analysis, we’ll need the following tools:</p>
<ul>
<li>.NET SDK 7.0 or later</li>
<li>Docker and <code>docker-compose</code></li>
</ul>
<h1 id="_idParaDest-165"><a id="_idTextAnchor164"/>Instrumenting client calls</h1>
<p>Network calls are <a id="_idIndexMarker547"/>probably the most important thing to instrument in any distributed application since network and downstream services are unreliable and complex resources. In order to understand how our application works and breaks, we need to know how the services we depend on perform.</p>
<p>Network-level metrics can help us measure essential things such as latency, error rate, throughput, and the number of active requests and connections. Tracing enables context propagation and helps us see how requests flow through the system. So, if you instrument your application at all, you should start with incoming and outgoing requests.</p>
<p>When instrumenting the client side of calls, we need to pick the right level of the network stack. Do we want to trace TCP packets? Can we? The answer depends, but distributed tracing is usually applied on the application layer of the network stack where protocols such as HTTP or AMQP live.</p>
<p>In the case of HTTP on .NET, we apply instrumentation on the <code>HttpClient</code> level – to be more precise, on the <code>HttpMessageHandler</code> level, which performs individual HTTP requests, so we trace individual retries and redirects.</p>
<p>If we instrument <code>HttpClient</code> methods, in many cases, we collect the duration of the request, which includes all attempts to get a response with back-off intervals between them. The error rate would show the rate without transient failures. This information is very useful, but it describes network-level calls very indirectly and heavily depends on the upstream service configuration and performance.</p>
<p>Usually, gRPC runs on top of HTTP/2 and to some extent can be covered by HTTP instrumentation. This is the case for unary calls, when a client sends a request and awaits a response. The key difference with HTTP instrumentation is that we’d want to collect a gRPC-specific set of attributes, which includes the service and method names as well as the gRPC status code.</p>
<p>However, gRPC also supports streaming when the client establishes a connection with the server, and then they can send each other multiple asynchronous messages within the scope of one HTTP/2 call. We’ll talk about streaming calls later in the <em class="italic">Instrumenting streaming calls</em> section<a id="_idIndexMarker548"/> of this chapter. For now, let’s focus on unary calls.</p>
<h2 id="_idParaDest-166"><a id="_idTextAnchor165"/>Instrumenting unary calls</h2>
<p>We’re going <a id="_idIndexMarker549"/>to use gRPC implementation in the <code>Grpc.Net.Client</code> NuGet<a id="_idIndexMarker550"/> package, which has an OpenTelemetry instrumentation library available.</p>
<p class="callout-heading">Note</p>
<p class="callout">OpenTelemetry provides two flavors of gRPC instrumentation: one for the <code>Grpc.Net.Client</code> package called <code>OpenTelemetry.Instrumentation.GrpcNetClient</code> and another one for the lower-level <code>Grpc.Core.Api</code> package called <code>OpenTelemetry.Instrumentation.GrpcCore</code>. Depending on how you use gRPC, make sure to use one or another.</p>
<p>These instrumentations should cover most gRPC tracing needs and you can customize them further using the techniques described in <a href="B19423_05.xhtml#_idTextAnchor083"><em class="italic">Chapter 5</em></a>, <em class="italic">Configuration and Control Plane</em>. For example, the <code>OpenTelemetry.Instrumentation.GrpcNetClient</code> instrumentation allows the suppression of the underlying HTTP instrumentation or the enrichment of corresponding activities.</p>
<p>Here, we’re going to write our own instrumentation as a learning exercise, which you can apply to other protocols or use to satisfy additional requirements that you might have.</p>
<p>We can wrap every gRPC call with instrumentation code, but this would be hard to maintain and would pollute the application code. A better approach would be to implement instrumentation in a gRPC <code>Interceptor</code>.</p>
<p>So, we know where instrumentation should be done, but what should we instrument? Let’s start with gRPC OpenTelemetry semantic conventions – the tracing conventions are available at <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/trace/semantic_conventions/rpc.md">https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/trace/semantic_conventions/rpc.md</a>.</p>
<p>The conventions are currently experimental and some changes (such as attribute renames) should be expected.</p>
<p>For unary client calls, the tracing specification recommends using the <code>{package.service}/{method}</code> pattern for span names and the following set of essential attributes:</p>
<ul>
<li>The <code>rpc.system</code> attribute has to match <code>grpc</code> – it helps backends understand that it’s a gRPC call.</li>
<li>The <code>rpc.service</code> and <code>rpc.method</code> attributes should describe the gRPC service and method. Even though this information is available in the span name, individual service and method attributes help query and filter spans in a more reliable and efficient way.</li>
<li>The <code>net.peer.name</code> and <code>net.peer.port</code> attributes describe remote endpoints.</li>
<li><code>rpc.grpc.status_code</code> describes the numeric representation of the gRPC status code.</li>
</ul>
<p>So, in the interceptor, we need to do a few things: start a new <code>Activity</code> with the recommended<a id="_idIndexMarker551"/> name and a set of attributes, inject the<a id="_idIndexMarker552"/> context into the outgoing request, await the response, set the status, and end the activity. This is demonstrated in the following code snippet:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">client/GrpcTracingInterceptor.cs</p>
<pre class="source-code">
public override AsyncUnaryCall&lt;Res&gt;
  AsyncUnaryCall&lt;Req, Res&gt;(Req request,
    ClientInterceptorContext&lt;Req, Res&gt; ctx,
    AsyncUnaryCallContinuation&lt;Req, Res&gt; continuation)
{
<strong class="bold">  var activity = Source.StartActivity(ctx.Method.FullName,</strong>
<strong class="bold">    </strong><strong class="bold">ActivityKind.Client);</strong>
<strong class="bold">  ctx = InjectTraceContext(activity, ctx);</strong>
  if (activity?.IsAllDataRequested != true)
    return continuation(request, ctx);
<strong class="bold">  SetRpcAttributes(activity, ctx.Method);</strong>
  var call = continuation(request, context);
  return new AsyncUnaryCall&lt;Res&gt;(
<strong class="bold">    HandleResponse(call.ResponseAsync, activity, call),</strong>
    call.ResponseHeadersAsync,
    call.GetStatus,
    call.GetTrailers,
    call.Dispose);
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter10/client/GrpcTracingInterceptor.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter10/client/GrpcTracingInterceptor.cs</a></p>
<p>Here, we override the interceptor’s <code>AsyncUnaryCall</code> method: we start a new <code>Activity</code> with the<a id="_idIndexMarker553"/> client kind and inject a trace context<a id="_idIndexMarker554"/> regardless of the sampling decision. If the activity is sampled out, we just return a continuation call and avoid any additional performance overhead.</p>
<p>If the activity is sampled in, we set the gRPC attributes and return the continuation call with the modified response task:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">client/GrpcTracingInterceptor.cs</p>
<pre class="source-code">
private async Task&lt;Res&gt; HandleResponse&lt;Req, Res&gt;(Task&lt;Res&gt;
  original, Activity act, AsyncUnaryCall&lt;Req&gt; call)
{
  try
  {
    var response = await original;
    SetStatus(act, call.GetStatus());
    return response;
  }
  ...
  finally
  {
    act.Dispose();
  }
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter10/client/GrpcTracingInterceptor.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter10/client/GrpcTracingInterceptor.cs</a></p>
<p>We dispose of <code>Activity</code> explicitly here since the <code>AsyncUnaryCall</code> method is synchronous and will end before the request is complete, but we need the activity to last until <a id="_idIndexMarker555"/>we<a id="_idIndexMarker556"/> get the response from the server.</p>
<p>Let’s take a closer look at each of the operations, starting with context injection:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">client/GrpcTracingInterceptor.cs</p>
<pre class="source-code">
private ClientInterceptorContext&lt;Req, Res&gt;
  InjectTraceContext&lt;Req, Res&gt;(Activity? act,
    ClientInterceptorContext&lt;Req, Res&gt; ctx)
  where Req: class where Res: class
{
  ...
<strong class="bold">  _propagator.Inject(new PropagationContext(</strong>
<strong class="bold">      act.Context, Baggage.Current),</strong>
<strong class="bold">    ctx.Options.Headers,</strong>
<strong class="bold">    static (headers, k, v) =&gt; headers.Add(k, v));</strong>
  return ctx;
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter10/client/GrpcTracingInterceptor.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter10/client/GrpcTracingInterceptor.cs</a></p>
<p>Here, we inject the context using the <code>OpenTelemetry.Context.Propagation.TextMapPropagator</code> class and the <code>Inject</code> method. We’ll see how the propagator is configured a bit later.</p>
<p>We created an instance of the <code>PropagationContext</code> structure – it contains everything that needs to be propagated, namely <code>ActivityContext</code> and the current <code>Baggage</code>.</p>
<p>The context is injected into the <code>ctx.Options.Headers</code> property, which represents gRPC metadata. The metadata is later on transformed into HTTP request headers by <code>GrpcNetClient</code>.</p>
<p>The last parameter of the <code>Inject</code> method is a function that tells the propagator how to inject key-value pairs with trace context into the provided metadata. The propagator, depending on its implementation, may follow different formats and inject different headers. Here, we don’t need to worry about it.</p>
<p>Okay, we <a id="_idIndexMarker557"/>injected<a id="_idIndexMarker558"/> the context to enable correlation with the backend, and now it’s time to populate the attributes:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">client/GrpcTracingInterceptor.cs</p>
<pre class="source-code">
private void SetRpcAttributes&lt;Req, Res&gt;(Activity act,
  Method&lt;Req, Res&gt; method)
{
  act.SetTag("rpc.system", "grpc");
  act.SetTag("rpc.service", method.ServiceName);
  act.SetTag("rpc.method", method.Name);
  act.SetTag("net.peer.name", _host);
  if (_port != 80 &amp;&amp; _port != 443)
    act.SetTag("net.peer.port", _port);
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter10/client/GrpcTracingInterceptor.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter10/client/GrpcTracingInterceptor.cs</a></p>
<p>Here, we populate the service and method names from the information provided in the call context. But the host and port come from instance variables we passed to the interceptor constructor – this information is not available in the client interceptor.</p>
<p>Finally, we should populate the gRPC status code and <code>Activity</code> status once the response<a id="_idIndexMarker559"/> is <a id="_idIndexMarker560"/>received:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">client/GrpcTracingInterceptor.cs</p>
<pre class="source-code">
private static void SetStatus(Activity act, Status status)
{
  act.SetTag("rpc.grpc.status_code",
    (int)status.StatusCode);
  var activityStatus = status.StatusCode != StatusCode.OK ?
    ActivityStatusCode.Error : ActivityStatusCode.Unset;
  act.SetStatus(activityStatus, status.Detail);
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter10/client/GrpcTracingInterceptor.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter10/client/GrpcTracingInterceptor.cs</a></p>
<p>We left <code>Activity.Status</code> unset if the request was successful following the gRPC semantic conventions. It makes sense for generic instrumentation libraries since they don’t know what represents a success. In a custom instrumentation, we may know better and can be more specific.</p>
<p>This is it; we just<a id="_idIndexMarker561"/> finished<a id="_idIndexMarker562"/> unary call instrumentation on the client. Let’s now configure a gRPC client to use.</p>
<h2 id="_idParaDest-167"><a id="_idTextAnchor166"/>Configuring instrumentation</h2>
<p>Let’s set <a id="_idIndexMarker563"/>up a<a id="_idIndexMarker564"/> tracing interceptor on <code>GrpcClient</code> instances. In the demo application, we use <code>GrpcClient</code> integration with ASP.NET Core and set it up in the following way:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">client/Program.cs</p>
<pre class="source-code">
builder.Services
  .AddGrpcClient&lt;Nofitier.NofitierClient&gt;(o =&gt; {
    o =&gt; o.Address = serverEndpoint; ... })
<strong class="bold">  .AddInterceptor(() =&gt; new GrpcTracingInterceptor(</strong>
<strong class="bold">    serverEndpoint, contextPropagator))</strong>
  ...</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter10/client/Program.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter10/client/Program.cs</a></p>
<p>Here, we added <code>GrpcClient</code>, configured the endpoint, and added a tracing interceptor. We passed the options – the service endpoint and context propagator – explicitly.</p>
<p>The propagator is the implementation of the <code>TextMapPropagator</code> class – we use a composite one that supports W3C Trace Context and Baggage formats:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">client/Program.cs</p>
<pre class="source-code">
CompositeTextMapPropagator contextPropagator = new (
  new TextMapPropagator[] {
    new TraceContextPropagator(),
    new BaggagePropagator() });
Sdk.SetDefaultTextMapPropagator(contextPropagator);</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter10/client/Program.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter10/client/Program.cs</a></p>
<p>The last step is to configure OpenTelemetry and enable <code>ActivitySource</code> we use in the interceptor:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">client/Program.cs</p>
<pre class="source-code">
builder.Services.AddOpenTelemetry()
  .WithTracing(b =&gt; b.AddSource("Client.Grpc")...);</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter10/client/Program.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter10/client/Program.cs</a></p>
<p>That’s it for<a id="_idIndexMarker565"/> the unary client calls. Let’s now <a id="_idIndexMarker566"/>instrument the server.</p>
<h1 id="_idParaDest-168"><a id="_idTextAnchor167"/>Instrumenting server calls</h1>
<p>Service-side <a id="_idIndexMarker567"/>instrumentation is similar. We can use the gRPC interceptor again and this time override the <code>UnaryServerHandler</code> method. Once the request is received, we should extract the context and start a new activity. It should have the <code>server</code> kind, a name that follows the same pattern as for the client span – <code>{package.service}/{method}</code> – and attributes very similar to those we saw on the client. Here’s the interceptor code:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">server/GrpcTracingInterceptor.cs</p>
<pre class="source-code">
var traceContext = _propagator.Extract(default,
  ctx.RequestHeaders,
  static (headers, k) =&gt; new[] { headers.GetValue(k) });
Baggage.Current = traceContext.Baggage;
using var activity = Source.StartActivity(ctx.Method,
  ActivityKind.Server, traceContext.ActivityContext);
if (activity?.IsAllDataRequested != true)
  return await continuation(request, ctx);
SetRpcAttributes(activity, ctx.Host, ctx.Method);
try
{
  var response = await continuation(request, ctx);
  SetStatus(activity, ctx.Status);
  return response;
}
catch (Exception ex) {...}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter10/server/GrpcTracingInterceptor.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter10/server/GrpcTracingInterceptor.cs</a></p>
<p>We extract the<a id="_idIndexMarker568"/> trace context and baggage using the propagator and then pass the extracted parent trace context to the new activity and add the attributes. The server interceptor callback is asynchronous, so we can await a response from the server and populate the status.</p>
<p>That’s it; now we just need to configure interceptors and enable <code>ActivitySource</code>:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">server/Program.cs</p>
<pre class="source-code">
builder.Services
  .AddSingleton&lt;TextMapPropagator&gt;(contextPropagator)
  .AddGrpc(o =&gt; {
    <strong class="bold">o.Interceptors.Add&lt;GrpcTracingInterceptor&gt;();</strong> ...});
builder.Services.AddOpenTelemetry()
    .WithTracing(b =&gt; <strong class="bold">b.AddSource("Server.Grpc")</strong>...);</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter10/server/Program.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter10/server/Program.cs</a></p>
<p>We added gRPC services, configured the tracing interceptor, and enabled the new activity source. It’s time to check out generated traces.</p>
<p>Run the application with <code>$docker-compose up --build</code> and then hit the frontend at <code>http://localhost:5051/single/hello</code>. It will send a message to the server <a id="_idIndexMarker569"/>and return a response or show a transient error. An example of a trace with an error is shown in <em class="italic">Figure 10</em><em class="italic">.1</em>:</p>
<div><div><img alt="Figure 10.1 – gRPC trace showing error on server" src="img/B19423_10_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – gRPC trace showing error on server</p>
<p>Here, we see two spans from the client application and one from the server. They describe an incoming request collected by the ASP.NET Core instrumentation and client and server sides of the gRPC call. <em class="italic">Figure 10</em><em class="italic">.2</em> shows client span attributes where we can see the destination and status:</p>
<div><div><img alt="Figure 10.2 – gRPC client attributes" src="img/B19423_10_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – gRPC client attributes</p>
<p>This instrumentation allows us to trace unary calls for any gRPC service, which is similar to the HTTP instrumentation we saw in <a href="B19423_02.xhtml#_idTextAnchor038"><em class="italic">Chapter 2</em></a>, <em class="italic">Native Monitoring in .NET</em>. Let’s now explore<a id="_idIndexMarker570"/> instrumentation for streaming calls.</p>
<h1 id="_idParaDest-169"><a id="_idTextAnchor168"/>Instrumenting streaming calls</h1>
<p>So far in the <a id="_idIndexMarker571"/>book, we have covered the instrumentation of synchronous calls where the application makes a request and awaits its completion. However, it’s common to use gRPC or other protocols, such as SignalR or WebSocket, to communicate in an asynchronous way when the client and server establish a connection and then send each other messages.</p>
<p>Common use cases for this kind of communication include chat applications, collaboration tools,  and other cases when data should flow in real time and frequently in both directions.</p>
<p>The call starts when the client initiates a connection and may last until the client decides to disconnect, the connection becomes idle, or some network issue happens. In practice, it means that such calls may last for days.</p>
<p>While a connection is alive, the client and server can write each other messages to corresponding network streams. It’s much faster and more efficient when the client and server communicate frequently within a relatively short period of time. This approach minimizes the overhead created by DNS lookup, protocol negotiation, load balancing, authorization, and routing compared to request-response communication when at least some of these operations would happen for each request.</p>
<p>On the downside, the application could become more complex as in many cases, we’d still need to correlate client messages with service replies to them and come up with our own semantics for metadata and status codes.</p>
<p>For <a id="_idIndexMarker572"/>observability, it means that out-of-the-box instrumentation is rarely enough and at least some custom instrumentation is necessary. Let’s see why.</p>
<h2 id="_idParaDest-170"><a id="_idTextAnchor169"/>Basic instrumentation</h2>
<p>Some applications<a id="_idIndexMarker573"/> pass completely independent messages within one streaming call and would want different traces to describe individual messages. Others use streaming to send scoped batches of messages and would rather expect one trace to describe everything that happens within one streaming call. When it comes to streaming, there is no single solution.</p>
<p>gRPC auto-instrumentations follow OpenTelemetry semantic conventions and provide a default experience where a streaming call is represented with client and server spans, even if the call lifetime is unbound. Individual messages are described with span events with attributes covering the direction, message identifier, and size.</p>
<p>You can find a full instrumentation implementation that follows these conventions in the <code>client/GrpcTracingInterceptor.cs</code> and <code>server/GrpcTracingInterceptor.cs</code> files in the book’s repository. Let’s look at the traces it produces.</p>
<p>Go ahead and start the application with <code>$ docker-compose up --build</code> and then hit the client application at <code>http://localhost:5051/streaming/hello?count=2</code>. It will send two messages to the server and read all the responses.</p>
<p>Check out Jaeger at <code>http://localhost:16686/</code>. You should see a trace similar to the one shown in <em class="italic">Figure 10</em><em class="italic">.3</em>:</p>
<div><div><img alt="Figure 10.3 – Streaming call with events" src="img/B19423_10_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – Streaming call with events</p>
<p>Similarly to a unary call, the trace consists of three spans. The only difference is that client and server gRPC spans have events – two events per message, indicating when the message was sent and received. The <code>message.id</code> attribute here represents the sequence number of a message in a request or response stream and might be used to correlate request and response messages.</p>
<p>The trace<a id="_idIndexMarker574"/> shown in <em class="italic">Figure 10</em><em class="italic">.3</em> represents the best we can achieve with auto-instrumentation that is not aware of our specific streaming usage. Let’s see how we can improve it.</p>
<h2 id="_idParaDest-171"><a id="_idTextAnchor170"/>Tracing individual messages</h2>
<p>Let’s pretend<a id="_idIndexMarker575"/> that the client initiates a very long stream – in this case, the previous trace would not be very helpful. Assuming messages are not too frequent and verbose, we might want to instrument each specific message and see how server response messages correlate with client messages.</p>
<p>To instrument individual messages, we’d have to propagate context inside the message, which is not possible in an interceptor where we operate with generic message types.</p>
<p>Our message protobuf definition contains text and an attribute map that we can use to pass trace context:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">client\Protos\notifier.proto</p>
<pre class="source-code">
message Message {
  string text = 1;
<strong class="bold">  map&lt;string, string&gt; attributes = 2;</strong>
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter10/client/notifier.proto">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter10/client/notifier.proto</a></p>
<p>We’re going to create one client span per message to describe and identify it, and a server span that will represent processing the message.</p>
<p>If we have hundreds of messages during one streaming call, having all of them in one trace will be hard to read. Also, typical sampling techniques would not apply – depending on the sampling decision made for the whole streaming call, we’ll sample in or drop all per-message spans.</p>
<p>Ideally, we want to have a trace per message flow and have a link to the long-running HTTP requests that carried the message over. This way, we still know what happened with the transport and what else was sent over the same HTTP request, but we’ll make independent sampling decisions and will have smaller and more readable traces.</p>
<p class="callout-heading">Note</p>
<p class="callout">Tracing individual messages is reasonable when messages are relatively big and processing them takes a reasonable amount of time. Alternative approaches may include custom correlation or context propagation for sampled-in messages only.</p>
<p>Let’s go ahead<a id="_idIndexMarker576"/> and instrument individual messages: we’ll need to start a new activity per message with the <code>producer</code> kind indicating an async call. We need to start a new trace and use <code>Activity.Current</code> as a link rather than a parent:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">client/controllers/StreamingController.cs</p>
<pre class="source-code">
IEnumerable&lt;ActivityLink&gt;? links = null;
if (Activity.Current != null)
{
  links = new[] {
    new ActivityLink(Activity.Current.Context) };
<strong class="bold">  Activity.Current = null;</strong>
}
<strong class="bold">using var act = Source.StartActivity("SendMessage",</strong>
<strong class="bold">  ActivityKind.Producer,</strong>
<strong class="bold">  default(ActivityContext),</strong>
<strong class="bold">  links: links)</strong>;</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter10/client/Controllers/StreamingController.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter10/client/Controllers/StreamingController.cs</a></p>
<p>We created a link from the current activity and then set <code>Activity.Current</code> to <code>null</code>, which<a id="_idIndexMarker577"/> forces the <code>StartActivity</code> method to create an orphaned activity.</p>
<p class="callout-heading">Note</p>
<p class="callout">Setting <code>Activity.Current</code> should be done with caution. In this example, we’re starting a new task specifically to ensure that it won’t change the <code>Activity.Current</code> value beyond the scope of this task.</p>
<p>We have an activity; now it’s time to inject the context and send a message to the server:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">client/controllers/StreamingController.cs</p>
<pre class="source-code">
<strong class="bold">_propagator.Inject(</strong>
<strong class="bold">  new PropagationContext(act.Context, Baggage.Current),</strong>
<strong class="bold">  message,</strong>
<strong class="bold">  static (m, k, v) =&gt; m.Attributes.Add(k, v));</strong>
try
{
  await requestStream.WriteAsync(message);
}
catch (Exception ex)
{
  act?.SetStatus(ActivityStatusCode.Error, ex.Message);
}</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter10/client/Controllers/StreamingController.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter10/client/Controllers/StreamingController.cs</a></p>
<p>Context injection looks similar to what we did in the client interceptor earlier in this chapter, except here we inject it into message attributes rather than gRPC call metadata.</p>
<p>On the server side, we need to extract the context from the message, then use it as a parent. We should also set <code>Activity.Current</code> as a link so we don’t lose correlation between the message processing and streaming calls. The new activity has a <code>consumer</code> kind, which indicates <a id="_idIndexMarker578"/>the processing side of the async call, as shown in this code snippet:</p>
<p class="SC---Heading" lang="en-US" xml:lang="en-US">server/NotifierService.cs</p>
<pre class="source-code">
<strong class="bold">var context = _propagator.Extract(default,</strong>
<strong class="bold">  message,</strong>
<strong class="bold">  static (m, k) =&gt; m.Attributes.TryGetValue(k, out var v)</strong>
<strong class="bold">      ? new [] { v } : Enumerable.Empty&lt;string&gt;());</strong>
var link = Activity.Current == null ?
   default : new ActivityLink(Activity.Current.Context);
using var act = Source.StartActivity(
  "ProcessMessage",
  ActivityKind.Consumer,
<strong class="bold">  context.ActivityContext,</strong>
<strong class="bold">  links: new[] { link });</strong>
...</pre>
<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter10/server/NotifierService.cs">https://github.com/PacktPublishing/Modern-Distributed-Tracing-in-.NET/blob/main/chapter10/server/NotifierService.cs</a></p>
<p>We can now enable corresponding client and server activity sources – we used different names for per-message tracing and interceptors, so we can now control instrumentations individually. Go ahead and enable <code>Client.Grpc.Message</code> and <code>Server.Grpc.Message</code> sources on the client and server correspondingly and then start an application.</p>
<p>If we hit the streaming endpoint at <code>http://localhost:5051/streaming/hello?count=5</code> and then went to Jaeger, we’d see six traces – one for each message sent and one for the gRPC call.</p>
<p>Per-message <a id="_idIndexMarker579"/>traces consist of two spans, like the one shown in <em class="italic">Figure 10</em><em class="italic">.4</em>:</p>
<div><div><img alt="Figure 10.4 – Tracing messages in individual traces" src="img/B19423_10_04.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4 – Tracing messages in individual traces</p>
<p>Here, we see that sending this message took about 1 ms and processing it took about 100 ms. Both spans have links (references in Jaeger terminology) to spans describing the client and server sides of the underlying gRPC call.</p>
<p>If we didn’t force new trace creation for individual messages, we’d see only one trace containing all the spans, as shown in <em class="italic">Figure 10</em><em class="italic">.5</em>:</p>
<div><div><img alt="Figure 10.5 – Tracing ﻿a streaming call with all messages in one trace" src="img/B19423_10_05.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5 – Tracing a streaming call with all messages in one trace</p>
<p>Depending on your scenario, you might prefer to separate traces, have one big trace, or come up with something else.</p>
<p>Note that we can now remove our custom tracing interceptor and enable shared gRPC and HTTP client instrumentation libraries. If you do this, the per-message instrumentation will remain exactly the same and will keep working along with auto-instrumentation.</p>
<p>With this, you<a id="_idIndexMarker580"/> should be able to instrument unary or streaming gRPC calls and have an idea of how to extend it to other cases, including SignalR or socket communication.</p>
<p>Let’s now see how to use gRPC instrumentation to investigate issues.</p>
<h1 id="_idParaDest-172"><a id="_idTextAnchor171"/>Observability in action</h1>
<p>There are several<a id="_idIndexMarker581"/> issues in our server application. First issue reproduces sporadically when you hit the frontend at <code>http://localhost:5051/single/hello</code> several times. You might notice that some requests take longer than others. If we look at the duration metrics or Jaeger’s duration view, we’ll see something similar to <em class="italic">Figure 10</em><em class="italic">.6</em>:</p>
<div><div><img alt="Figure 10.6 – Duration view in Jaeger" src="img/B19423_10_06.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.6 – Duration view in Jaeger</p>
<p>We see that most of the calls are fast (around 100 ms), but there is one that takes longer than a <a id="_idIndexMarker582"/>second. If we click on it, Jaeger will open the corresponding trace, like the one shown in <em class="italic">Figure 10</em><em class="italic">.7</em>:</p>
<div><div><img alt="Figure 10.7 – Long trace with errors" src="img/B19423_10_07.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.7 – Long trace with errors</p>
<p>Apparently, there were three attempts to send the message – the first two were not successful, but the third one succeeded. So retries are the source of long latency. We can investigate the error by expanding the exception event – we’ll see a full stack trace there.</p>
<p>Notably, we see retries only on the service side here. There is just one gRPC span on the client side. What happens here is that we enable a retry policy on the gRPC client channel, which internally adds a retry handler to the <code>HttpClient</code> level. So, our tracing interceptor is not called on tries and traces the logical part of the gRPC call.</p>
<p>The official <code>OpenTelemetry.Instrumentation.GrpcNetClient</code> instrumentation works properly and traces individual tries on the client as well.</p>
<p>Let’s look at another problem. Send the following request: <code>http://localhost:5051/streaming/hello?count=10</code>. It will return a few messages and then stop. If we look into Jaeger traces, we’ll see a lot of errors for individual messages. Some<a id="_idIndexMarker583"/> of them will have just a client span, like the one shown in <em class="italic">Figure 10</em><em class="italic">.8</em>:</p>
<div><div><img alt="Figure 10.8 – Client error without server span" src="img/B19423_10_08.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.8 – Client error without server span</p>
<p>There is not much information in the span, but luckily, we have a link to the gRPC call. Let’s follow it to see whether it explains something. The corresponding trace is shown in <em class="italic">Figure 10</em><em class="italic">.9</em>:</p>
<div><div><img alt="Figure 10.9 – Error in the middle of the gRPC stream" src="img/B19423_10_09.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.9 – Error in the middle of the gRPC stream</p>
<p>Here, we see a familiar trace, but the processing has failed while parsing the message text. The server span has six events, indicating that two messages were received and the response was successfully sent to the server. The third one was received but then instead of the response, we see an exception with a stack trace to help us investigate further.</p>
<p>If we expand the <a id="_idIndexMarker584"/>client gRPC span, we’ll see more exceptions for each message that was attempted to be sent after the server error has happened.</p>
<p>But there were no retries – why? In our case, gRPC retries, as we’ve seen in the previous example, are applied on the HTTP level. In the case of streaming, it means that after the first response is received from the server, the HTTP response, including status codes and headers, is received and the rest of the communication happens within the request and response streams. You can read more about this in the Microsoft gRPC documentation at <a href="https://learn.microsoft.com/en-us/aspnet/core/grpc/retries">https://learn.microsoft.com/en-us/aspnet/core/grpc/retries</a>.</p>
<p>So, once an unhandled exception is thrown on the server for a particular message, it ends the gRPC call and corresponding request and response streams on the client and server. It affects all remaining messages on the client and explains the partial response we noticed.</p>
<p>Distributed tracing helps us see what happens and learn more about the technologies we use. In addition to tracing, OpenTelemetry defines a set of metrics to monitor on the client and server sides, which includes the duration, the failure rate that can be derived from it, the <a id="_idIndexMarker585"/>number of requests and responses, and the payload sizes.</p>
<h1 id="_idParaDest-173"><a id="_idTextAnchor172"/>Summary</h1>
<p>In this chapter, we got hands-on experience in instrumenting network calls using gRPC as an example. Before starting instrumentation, we learned about the available instrumentation libraries and what OpenTelemetry semantic conventions recommend recording for gRPC.</p>
<p>First, we instrumented unary calls with client and server spans and propagated context through gRPC metadata. Then, we experimented with gRPC streaming, which needs a different approach to tracing. The generic instrumentation of streaming calls suggests creating an event per individual request and response message in the stream and provides a basic level of observability. Depending on our scenarios and observability needs, we can add another layer of instrumentation to trace individual messages. These custom spans work on top of the generic gRPC instrumentation.</p>
<p>Finally, we used tracing to get insights into high latency and transient error scenarios, which also helped us understand gRPC internals.</p>
<p>You’re now ready to instrument your network stack with tracing or enrich existing instrumentation libraries by adding custom layers of instrumentation specific to your application. In the next chapter, we’ll look into messaging scenarios and dive even deeper into observability for asynchronous processing.</p>
<h1 id="_idParaDest-174"><a id="_idTextAnchor173"/>Questions</h1>
<ol>
<li>When using gRPC, would you write your own instrumentation or reuse an existing one?</li>
<li>Let’s imagine we want to instrument gRPC communication between the client and server when the client initiates a connection at startup time and keeps it open forever (until the server or client stops) and then reuses this connection for all the communication. Which tracing approach would you choose? Why?</li>
</ol>
</div>
</body></html>